{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "save_folder = './MLP/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306, 4)\n",
      "                0           1           2           3\n",
      "count  306.000000  306.000000  306.000000  306.000000\n",
      "mean    52.457516   62.852941    4.026144    1.264706\n",
      "std     10.803452    3.249405    7.189654    0.441899\n",
      "min     30.000000   58.000000    0.000000    1.000000\n",
      "25%     44.000000   60.000000    0.000000    1.000000\n",
      "50%     52.000000   63.000000    1.000000    1.000000\n",
      "75%     60.750000   65.750000    4.000000    2.000000\n",
      "max     83.000000   69.000000   52.000000    2.000000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('haberman.data', header = None)\n",
    "#df.columns = ['Idade', 'Ano_da_Operacao', 'No_de_Nos_Axilares_Pos', 'Estado_Sobrevivencia']\n",
    "\n",
    "print(df.shape) # (306, 4)\n",
    "df.describe().to_excel(save_folder + 'describe.xlsx')\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "y = y - 1\n",
    "\n",
    "X.hist(sharey=True)\n",
    "plt.title('Variaveis de Entrada')\n",
    "plt.tight_layout()\n",
    "plt.xlim(0, 100)\n",
    "plt.savefig(save_folder + 'X_hist.png')\n",
    "plt.show()\n",
    "\n",
    "y.hist()\n",
    "plt.title('Variavel Alvo')\n",
    "plt.savefig(save_folder + 'y_hist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    225\n",
      "2     81\n",
      "Name: survival_status, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['survival_status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDAAAAQwCAYAAAATlK4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd5hU1fnA8e+5U7f3BZal96ogICBICfYau0ZjSTTRWGJ+KaYYE6OJppqYxGhiL9EosUexYUGRLkjvfWELW6fP3PP7Y7bvzJbZ3ZldeD/Pw/PAPXvffdmduTPz3nPeo7TWCCGEEEIIIYQQQvRkRqITEEIIIYQQQgghhGiLFDCEEEIIIYQQQgjR40kBQwghhBBCCCGEED2eFDCEEEIIIYQQQgjR40kBQwghhBBCCCGEED2eNdEJtFdubq4ePHhwotMQQnSRVatWlWqt8zp6nlwLhDj6yPVACAFyLRBCNIh2Peg1BYzBgwezcuXKRKchhOgiSqk9sZwn1wIhjj5yPRBCgFwLhBANol0Pek0BQwghItHBneD7CLQbbOPAPhOl7IlOSwjRA2ntAd/HENwGKgMc81DWwkSnJYQQQhxTtPaDbwkEN4FKBccclHVwu86VAoYQotfS7ufR7v80HPC+D9aXIf1ulJGRuMSEED2ODh1GV/0cQiUNB72vQMr1KOfpiUtMCCGEOIZo8wi68ucQOthw0PMqpFyJSrqgzfOliacQolfSgW1Nixd1gvvA/VT8ExJC9Gja9XDT4gWA1mjXP9Ghw4lJSgghhDjGaNfjTYsX9cefQQfbXkUmBQwhRO/k+zDqkPYtQetA/HIRQvRo2iwH/xdRBjX4PolvQkIIIcQxSGsv+D+P/gWtvL+vIwUMIUSvpHVNK4OB8B8hhIBwj5xWx1u5ngghhBCia2gv6FAr422/HksBQwjRKynbmOiD1kEoIzl+yQghejajLxhZ0cetrVxPhBBCCNE1VAZY+kUfb8frsRQwhBC9k2MOWPpEHFJJl8Q5GSFET6aUBZV0YeRB61CwT41vQkIIIcQxSCmFSo7yPt1SAI5ZbcaQXUiEEL2SUkmQfg/a/Rj4l4E2wToAlXQJyjEj0ekJIXoYlXQmKAva8zKEikHZUI5ZkHwtSsn9HCGEECIelGMOQLgZf6gIlAXs01Ep16GUvc3zpYAhhOi1lCUHlfYDtPaA9qGMzESnJITowZTzNHCcAroKVHK73igJIYQQomspxxywnwy6EpQTpZztPlcKGEKIXk+pJFBJiU5DCNELKGWAkmKnEEIIkUhKqZhej6WAkSCD73izQ1+/+76zuimT7teR/2tv/n8KIYQQQgghhOg+suhTCCGEEEIIIYQQPZ4UMIQQQgghhBBCCNHjSQFDCCGEEEIIIYQQPZ4UMIQQQgghhBBCCNHjxaWJp1IqGXgRSAEqgUuAW4HzgD3ANVrrQDxyEUIIIYQQQgghRO8TrxkYpwPLtNZzgeXAZcA8rfUsYB1wfpzyEEIIIYQQQgghRC8UrwLGDsBR+/dMYDDwYe2/3wOmRzpJKXWDUmqlUmplSUlJd+cohBBCCCGEEEKIHipeBYxtwIlKqQ3AFGA7UFU7VglkRTpJa/2I1nqK1npKXl5efDIVQgghhBBCCCFEjxOvAsbVwCKt9TjgTcK9N9Jrx9KBijjlIYQQQgghhBBCiF4oXgUMBRyp/Xsp4SUkc2r/vQD4PE55CCGEEEIIIYQQoheKyy4kwHPAC0qpq4AAcClwvVJqCbAXeCBOeQghhBBCCCGEEKIXiksBQ2tdAZzW7PD9tX+EEEKIHkHrEPiXQ2A9qCRwnIyyDkx0WiJGOrgX/J+A6QbbWLBPRylLotMSQgghjmlaB8C/DAKbwEgBx1yUpaBd58ZrBoYQQgjRo2mzBl31SwjuaDjo+S8kX4pKvjRxiYmYaM9/0a5nGg543wLrEEj/JcpITVxiQgghxDFMmxXoqrsguK/hoPslSLkWlXROm+fHqweGEEII0bO5n2tavKil3S+gA1sSkJCIlQ7uaFq8qBPcBe6n4p+QEEIIIQDQriebFi/qjz8enjnZBilgCCGEOOZpbaJ9H0b/At8HcctFdIFWfpfa93F46qoQQggh4kprL/g/jf4Frb0XqyVLSASD73izQ1+/+76zuikTIYRIlCBoLwAaTdAXRFkUVmv4ZVLrGlQi0xMdos2qVgb94T/KVn/oyKFyvC4ffQbnYbFIjwwhjgblhyvw1HjJH5iL1SYfeYToEbQXdLCV8eo2Q8izWQghxDFPKTvaOpia0g1UllYRCoQAcCTbye6bhT1lVIIzFB2hrKPQvk8iD1oHoIwUAIp2Hua5X/+XHWt3A5CRm8aZ31zA7AunxylTIURXK95bwnP3/petq3cCkJadymnXzGP+5bMSnJkQApUBlj4QOhx53Nr2+y1ZQiKEEEIAm9cdz5Gi8vriBYDP7Wfn+mpc3hMTmJnoMOc8sORFHFJJFwPgqnTxwI2P1BcvACpLq/n3fS+z7H+r45GlEKKLeVxeHrjxn/XFC4DqIzW89MfX+eS/yxKYmRACQClV/zrcgqUPOGa3GUMKGEIIIQSw8G+lvP70WMpLkwAwtWLHphye/tNEPnttU4KzEx2hVBIq/W6wTwFVu/jH0geVdivKEb4L+9mrK6k+UhPx/EVPLI5XqkKILrT8zdVUFFdGHHvnycWYphnnjIQQzSnnfFTqTQ03GpQC+zRU+j0o5WjzfFlCIoQQ4pjn9wUo2nmYIvqwaU0+aZk+An4LXne4T8KejfsTnKHoKGXpg0r/CdqsAe0GIxelGu7b7NnYsgN6nUO7ivF7/did9nikKoToIq1dq8sOluOu8pCamRLHjIQQkSjnAnDMB7MUVHKHtjeXAoYQQohjntVmISnViafGCyiqK5xNxtNz0hKTWCu+WLyez19fSU2Fm6ETB3HyJTPILchOdFoJU3rwCB+98Bm7vtxLamYyM86dynFzx9W+KWr5xigtO/rvNCnVidUub5GE6G3SssPPdXeVB1elCzOkcSTbSctKJSnNiSNZipJC9BRKGWDJ7/B58uoshBDimGcYBjPOmcIH/14ScXzGuVPinFHrXvjtq3z04mf1/9755R4+fXU5t/79egaNKUxgZomxZ9N+/nLTP2sLUGHrPtnEnItncukPz4t4zszzpvDxS0vRWrcYm372FAxDVtkK0dvMOHcKL/z2FWoqXPXHfB4fNRVuzrr+K9jstlbOFkL0BvLqLIQQQgDn3HgqIycPbXLMMBQX3X42A0f3T1BWLe3ZuK9J8aKOp8bLS394PQEZJd6Lv3utSfGizkcvfsaeTZGnlA8Y1Z8Lbz8bw2i6Qe6ISUM458ZTuyVPIUT3qiypql361fR5bbVb8Nb4EpOUEKJLyQwMIYQQAnAkObjtHzewZcV2tqzYgSPZwZTTjutxyzJWv/dl1LEda3dTVVbdI5e8dJeKkkp2frkn6via976MOitl/uWzOG7uOFYu+gKvy8fIKcMYPW04SqmIXy+E6NnWvP8lqVkpOFOduKvcmCETR7IDZ4qDbWt24nX7cCa33SRQCNFzSQFDCCFEE36vn41LtxLwBRg5ZRgZuemJTilulFKMnjaC0dNGJDqVqELBUKvjwUAwTpn0DKFg9F0FtNbs23KAZf9bzbDjB0csRuX0y+K0a+Z1Z4pCiDgJ1m6DbbUa2Bw2zJCJzWFFKYVp6javn0KInk8KGEIIIeqtfGctz9/3Mu5qDwAWi8G8y2bx1dvOlLvSPcTYmaOi9uooGNaH7L5Zcc4osXL6ZdFvaB+Kdh5uctxb46XsYDk+t49Ny7ZhGIoZ50zl8p98VfpbCHGUGndS+PpYduAIoVBdsUKRmpHM5FMmkpKenND8hBCdJ6/gQgghANi/rYgnf/58ffECIBQyee/Zj/n4pc8TmJlobMyJIxg3Y1SL44ahOP/mMxKQUeKdf/MZTXpZhAIhSvaXYXPacNROFzdNzaevLuftRz9IVJpCiG42cGwhNeU1jYoXABpXlfuYK+4KcbSSAoYQQggAPnnpc0KhyNPxP/rPp3HORkSjlOKG31/FuTeeRv6AHJLTkhg3YxS3PXQD42eNSXR6CTFh9hhue+gGxs0YRXJaEihIz00jb0BOi5lDH720FNOMvuxECNF7LXtjNdkFWWTkpmO1WTEMg6TUJPIH5bFl5fZmhQ0hRG8kS0iEEEIAULKvtJWxsjhmItpis9s4/br5nH7d/ESn0mOMmDyUEbW7yDz20+dY+c7aiF9XfaQGr8sXLnQIIY4qJftKMQyDjLx0MvKa9m9yVbpxV3lIy0pNUHZCiK4gBQwhhBAA5A/KY/OK7VHHYhUKhXj/mU/49JXlVJZWMWBkAadcPZeJJ4+NOWZ30YFtaM+LEFgPKgnlOBmSLkQZ8oa3p9u8fBuLHl/MrvV7cVd5cFW6Sc9NazEDIz0nDWdKeFmJNl3geQnt+xi0G2zjUUkXo2wjE/FfEEJ0Up9BeWhTU1lahavSjWlqHEl2MnLTyCnIJjldCpdC9AQ6sBHteQkCm0CloJzzwu+3lLPNc6WAIXqUwXe82aGv333fWd2UiRDHnjkXz+DTV5ZH7NI+99KTYo77xM+eZ9V76+r/vWPdHnb835NceedFzDx3asxxu5oObEJX/QJ0oPaAF+15FQLrIOM3KGVPaH4iurUfbuCfP3oa09ThA0pRVVaNz+Mnf2Buk6+de8lMDMNAaz+66i4I7mwY9K9CB9ZB+l0oW88rsAkhWnfi2ZN57KfPNenl5HV58bl9zDxvKhaLJYHZCSEAtP8LdPU9oGuXc2of2r0QAhsg/R6Uar3LhfTAEEIIAUC/oX247t7LSc1MqT9ms1s57Zp5zL7gxJhi7tm4r0nxorHXH1rUo9Yja/dzDcWLxoK7wPdJ/BMS7fbKX99qKF4AVpuFvAE5BHwBvDVeILyjzskXzeDUa+aGv8i3pGnxoo4OhB8LQoheZ+faPaRmpWKxNi5UKFIykinZL0shhegJtPuZhuJFY4HN4F/W5vkyA0MIIUS9SfMnMH7WaDYv207AF2DECUM7tV5449KtUccqS6vZv7WIQWMKY47fVbT2hyv/0cYDa1DOr8QxI9FepQePcHhPSYvjzhQnBcP7Mm7maKaffQLDjh9MVn5G/bgOrIkeNLARrf0y60aIXmbj0q04UxwUDO+L1+XDNE0cSXasNit7Nx3AVeWWrVSFSCBtVkW+eVAn8AU4ZrQaQwoYQgghmijdf4Q9G/cT8AVISkti9LThLfoItJfV1vrLjN1hiylu17OAsoCONiOkp+TZvTwuLyvf/oLDe0rIH5jL1DMmkZTS9nrURLLZw48xbWoqiivx1HixWC1k9cnAnmSncGQ/ppx6XKQzowdVFmSSqhC9T931wO/xU1lahRk0SU5LIjM/HcNiNJuZIYSIvzaeg6rt91tSwBBCCFHvtb8v4u3HP6j/97tPf8ToqcO58YFrsNk7/iF+0oIJvPq3ptP76/Qb2od+Q/t0Kt+uopQFbT8RfJ9FHnfE3gOkt9izcR9/vfUxXJXu+mOvP7SIm/58HUPGD0xgZq3LyE2n35B8Pnt1RZNtgKvLa8jqk8kJp0yMeJ5ynIT2fRg5qH0aSslbJCF6m8mnTGThA29QWVpdf8xd7aG8uJIFV56MM9mRwOyEEMpIQdsngT/KLEj7zDZjyO0FIYQQQHgXh8bFi/rjK7bz9qMtj7dHbkE2Z3/r1BbHHUl2Lr/jqzHF7C4q+SowslsOOGaB7YT4JxRHpmnyrx8/26R4AeCq8vCvO57BNCOsVe1B9m050KR4UaeqrAqfxx/5JNtkcJzc8riRjUr+ehdnKISIB0+1l+pyV4vjoWCIPRv3JSAjIURzKvkaMNJbHnee0q4G2nJ7QQghBACfv74q6tjS11dyzo2nxRT39OvmM2TiID59eRmVJVUMGN2fOZfMJK8wJ9ZUu4Wy9IHMP4D3HXTgS1BOlGMO2GfEvISmt9i2aidlB8sjjpUfrmTzsm2MnTEqzlm1T8n+Mop2FmNzWDGDJqbWKMCwGBgWg/8+8CY/fOLmFucppSD1NrBPrd1G1YOyjQfnaSgjo+U3EkL0eK88+BZWmwXTUJghEw0YSmFYDXZv2Ie7xkNyqmylKkQiKesAyPwjeBehAxtBpaKc81D2ae06XwoYQoher+pINV6Xj9z+2RiGTCyLVdWRmqhjke5odcSoKcMYObkP6CowcrusOaLfF6DicAVp2akkdcGbUmVkQPLFKC7uguy6X02FC3eVm5yC7E6t7W7r99vZ3393KisqR2uNUgqL1YIyTZRSKCNcdKoqq456rlIKHCdR7Zoo1xAhjgJ1z3fDYoBSoHX474AZMqkuq5YChhA9gDKy0UmXoBzFoJI7dONAChhHqcF3vNkrYwvREcX7Snnh/lfYvHw7Wmuy+2Zy5jcXMPO8qYlOrVcaPG4Am5dvizw2NvadQrRZjXY9Av7Pw00yjVRwng1JF8c8s8E0TV5/6B0+fmkpnhovNruVE045jot/cG6PbzrZFcqLK3nh/ldYv2QTpqlJz0nj1K/PYf4Vs2OKN2hsIUoptG7ZqwTCj42eauiEgTiS7LgqXYSCGgj/H5RS2Bw2Rk0bEfXc4r0lPH/fK2xZuQOtNTn9sjjz+gXMOGdKnLIXQnSl4ZOHsmnZVgK+YJPjhsUgIy+dvAG5CcpMCNGY9r6Ndr8E5hFQCm2bhEq5Pjwbtg1ym0EI0St5ajw88O1H2LRsW/2HriOHKnjmnpdY9r/VCc6ud5p90XRS0lvemVJKcdq182KKqbVGV90Nvk8bdvgwa9Du58Hz75hzXfjHN1j0xGI8NV4AAv4gn7+5iof/76mYY/YWAX+AP9/4COs+3ljfHLWqrJqX/vQGH/x7SUwx8wpzmLwgcrPLSfMn0GdQXsz5dje7005WvoVQ0KSueAHhx17AH+DMG+ZEPM9V5eZP33qYzSu2119DyorKefruF1n5ztp4pC6E6GILvn5yi+IFhGdfpGYmywwrIXoA7X0XXfNIuHgBoDX4V6Or7kJrb5vny7NYCNErLX19FRXFlRHHFkVoRCnalpWfwW0P3cCwiYPqj+X2z+baey5n/KwxsQUNrIHgjohD2vMGWns6HLKmwsWSl5dFHNu6agc71+3pcMzeZPV7X1K8tzTi2LtPfUgoFG0r2NZ9/RcXM+fimdid4d1mbA4bsy+YztV3XxpzrvHg87gpP1SJ0WwFjVJgtWne+kfkotbS11Y22amgsbcfe7+r0xRCxMGTP38+6tihXcUxXx+FEF1DaxPtWRh5MFQMvo/bjCFLSIQQvdLu9Xujjh3aXYLH5T0mlhJ0tcKRBfzfozdRXlxJwOsnb0Bu5xpYBrdGH9NeCB0A6/AOhdy/9SABf8s7bHV2r9/L0EZFmKPN7vXRO+lXllZTfriS3IIIu6m0wWa3cekPz+O8m0+noriSjLz0XvEc2rl2HT6vxmYHs24ShoK6G62bl0UuoLV2DTm44zB+rx+7s2t6tQgh4mPXur31/W90o+27laEIBsI7kQydMDhB2Qkh0JXhQkU0wW1Ay93rGpMChhCiV0rLSo065kiyY3fY4pjN0Scrv4t2YWirKZNK63DI1MyUVsdT2hjv7dKyov//LBaDpNTYiw77txXx8YtLKd5bSt6AHOZcPIPCkQUxx4uHnIICUIBuKFo0lpadXP/38sMVfPTiUvZs2Me+LQfxurw4IxRpnMkOrHZ5iyREb5OSkUzJ/rLGq8nqrw9KQU7/nrX7lRDHHJUMygY6EGW87feFsoRECNErTT/nhKhjU0+f1KkdGUQXss+CaDuO2Ma2q1lTc4UjCxgwKvKH6uS0JI6fN67DMXuTaWdNxjAiz4qZePJYUtKTI461ZdW7a7n/qr+w5OVlbF21g09fWc59V/2FFYu+6Ey63S5/4EAGj45S0FRwwe2XA7B38wHuvfwB3nnyQ7as3EF5cSXFe0upKK5qcdq0MyfLWnkheqFLfnBuuPdS44bEOtwTJ7sgi4zsjhfNhRBdRykH2GdG/wLH/DZjyKuzEKJXGjCqPxd+9+wWyxsGjS3k/FvOSFBWojllpKFSbwtX2xuz5KNSb4457jV3X0ZmXnqTY3anjevuvRxHkiPmuL1BbkE2V/z0whZFjH5D+3Dpj86PKabfF+D5+14mFDKbHDdNzQv3v4Lf648533j44dM/JC2reRMMuODmExhxQrjY+Z/fvoq7uqHniiPJTmZ+BlVl1QR8DXeCBo8fwHk3nx6XvIUQXWvQ2AERt9RWhuL4eeMTkJEQojmVch1YBzc7qFAp30RZ2971TuZHCiF6ra98bTYTZo9mxdtf4KnxMuKEoUyYPUbunPYwyjEDbKPA91G447RlKDhOQkWbmdEO/Yb24a6F32fVO+s4sP0Q2X0zmXbmpFaXFh1NZp47lVFTh7PirTXUVLgYPH4gk+aPj3nm0eZl23BVRW6o6q72sOGzLUyaP6EzKXergWPG8sS2x1n4+4fZtGw76dnJXHD7FQybNAkILx3Z+WXL5q7pOWkkpSUxdMJARkweysgpwxg/a7RcQ4TopVa9u47Ckf2ornBRXlSBNkM4U5PILcxm3+YDeN0+nMlHd5FbiJ5OGWmQ8TvwL4fgJlAp4JiDsvRt1/lSwBBC9Gq5hRmccWUlmBXgHNdlHzx2b9jHkaJy+g3tQ7+hHV/mIJpRWezYcjxVpdUUjuxH/sDON0d0JDmYed7ULkiu+/m9fjYv3442NSOnDI14h7CjsvpkMGTiIFwVLgaNG9CpZVPBRk1RA94AAX8Qq91avyNJsJWmqT2FIymZkdMmU7SrivxBuRSObdg5p/EMi+Zsdiujp43gq7eeWX9Mhw6Fd88xMsE6NuZGtsV7S9i/tYj03DSGHz8kphjtVbTrMEU7DocfFxOO3ia2QrSm7rmekp5EdWk1wQAkpTqxWCyYpiYUlF1IhOgJlLJgGrmgTVAWULm095VWChhCiF7LdL8E1b+Duq04XQ9i2qZC5j8wjNgub6UHj/CvO55h76YD9cdGTxvBN35zRcy9BY51RTsP8887nuHQrnDXaaUUx88dx9d/eclRv9wDYOnrK1n4pzfqly84kuycef0CTrlqTswxd2/Yx6M/eZayg+UAGIZi2hmTueKnF2C1dfyxP+KEoSilOLS7GJ/bV3/ckeSgz6A8Rk3t2E4x8VZdUc63J95MyUFfffO+f//mQ66//2y+etvV5A3IJbd/NqUHjkQ8f8z0EQBo7UfXPAi+TxsGLf0g7fsoa/sLED6Pj6fu+g9ffLihfi1+3yH5XH/flV1eEHVXe3j8Z/9mw2db6o8VjuzH9fdfRV6hNCwUx5Yx00fy3z+/SU25q/7Y4d0lFO8pZc5lM+R1XIgewDT9UPEtCKyGun41Nb/HTPsZRtLZbZ4flzmSSqnTlVIf1v4pUkqdr5T6gVJqiVLqWaWaL44WQojWmYFtUH1vQ/ECwhdB/3KoujOmmFprHrr9iSbFC4DNy7fx5M9f6Ey6x6yAP8CDtzxaX7yA8M95zeL1vHD/qwnMLD52rN3NM796qUnvBZ/Hz8t/+R9rPvgyppjuag9/u/XR+uIFhHtVfP7mKl7726KYYqZlpWK1WZoUL8K5+jCsBuk5Pbvx3e0zv0fJAV+TnQeCAc3D33+DA9t2opTivO+cHnEmxdjpIxk9LVzAwPVY0+IFQKgIXXU3Wnvbnc8L97/KmsXrmzQSPLSrmAdveZRgoGtnszxz94tNihcA+7cW8bfbHsM0zShnCXF0qqmoaVK8qKO1ZvmbaxKQkRCihao7wL+qoXgBYLqh6i7MYMvlns3FpYChtX5baz1Xaz0X2AusAuZprWcB64DYuo4JIY5drodBR5kK6vsA0+z4h4TNy7dTtPNwxLENn22heG9Jh2Me675YvIGK4sqIYysWfUF1eU2cM4qvD1/4rGk3/MZjz38a8Xhblv9vddR+FUteWRZTw829mw8Q8AfJ7peFzWFDKYXNYSO7bxZmyGT3hn0x5RoPZUWH2bct8uPINDV/u+UvAJxwynHc9MC1jJg0BJvdSlafDM66fgHf+sPXAdCmC+37MPI3MStbFjaiqC6vibpzS0VxJV8s3tCuOO1RVlTOuo83Rhwr3lvKxmaFDSGOdn+79bGoY+4qD5WVLYsbQoj4MU0v+D6OPKgD4PpnmzHiuoREKTUUOAxMBD6sPfwecAXwYjxzEUL0cqG90ce0B8xyMPI6FLJ4T/QChdaa4n1l5A/sWMxjXWs/01AwRNnB8qO68ebhPcXRx/aWxhgz+nlel4+qsmpy+3ds6cDh2t9TamYKqZkpEccHjxvQsUTjZPPna9Fm5CIRwKHdDTNVxs0cxbiZoyJ/oVkKupXiT+hA9LFGyg6Wt7rO/vDu6I+JjirZV4rZyv/98J5Sxs/qsm8nRI9XU+FudXzjkk3MOGtKnLIRQrRgHmrjtbbtGRjx7oFxAfAykAnUbbxeCWRF+mKl1A3ADQADBw6MR35CiN7C0h8CmyOPKScYES8rrcptY714Z9aTb16+jXee/Ih9m/eTnpvOzHOnMveymVgssTde7A1a+5laLAbZ/TLjmE3b9m89yFuPfsC21TtxJNuZdvokTrl6bsxd6/MKc9m7cT+VpdXhZSQ63FAuPTeNvP7ZMcbMBnR4VoBZBYTqH/OO5HTSsjteEMof0H2P/e42Yup4lKGiFjHyBmS0L5CRE97uV0dp+GlpX++K7H6ZWCwGoWBNuLmw9oGygkoHI6PN60xH5PbPRikVdZZPbmFsjzEheqvk9CSqSqujjo89cUQcsxFCtGDkg7KHXxsJ0rD20xL+Y/RvO0Q3phfJOcBrQAWQXnssvfbfLWitH9FaT9FaT8nLk7ueQohGkr8Z7lociePkmJp4jpk+gvyBuRHHRk8dTp9BsV2HVr27lr/e8iibl4e3qizaeZiFD7zBEz97PqZ4vcmkr0yI2j/h+PkTSM/uOb0Vdq3fy++u+ztrPviSmgoXZQfLeeuxD/jLTf8k4I++i0Vrpp0xiUO7S6guryEUDBEKhaipdHF4dwmTFkyMLeaZk3E6isEsAwKACdoNoQNMP6NvTI1RB40dEHWGxcDR/eNmj8kAACAASURBVBk6sefuapFfWEBWfpS3MwpufODmdsVRRirKMTvyoJEKjpPbFSc9O41JczMhVFTbo8cM320yS0nPOMLkBV23HW1u/5yoM0pyCrIYP2t0l30vIXqDb973tahjNqeNjNx2FjSFEN3CMJLBOhbwAybhAoYmXMwIQsr1bcfo1gwbUUr1Bfxa6zJgBVDXfn0B8Hm88hBCHB0M+zhI/UG4iltHKbAfB+m/ji2mYXDTn66h75D8JseHTRzENfdcFlNM0zR55a9vRZzmveq9dT26t0BXsDtsfOfP15JT0HRGzLgZo7jipxckKKvIXv/7oojbbe7esI/V78XWcHP/1oOk5aQ2aR6plCI1K6VJY9OOSEnezrd/VUV6s5vrk07WnP+N2HseXP/bqxg4pumdjwGjCrjhd1fFHDNeaqo1zpSmDSuVAZm5ARY/v6r9gVK+CfZm08stOai0O1GqfVvfam1y2Xe2MnZq0+d8Tl+48Z4yrMb+9ufTDlf/8hJGTGq6Q0r+wFxueuDao36GlxDNbVmxvZXR6MuthBBxpHKB5q+pCox8lI68W1hj8VxCch7wKoDWulgp9bFSagnhpp4PxDEPIcRRwki5AjPpPHC/CLoSHAvChY1OyB+Yx50vfI9tq3dSdrCcgmF9GDQ29rX/h3YVN9ktorn1Szb32N4CXWXAqP788uUfsnn5dipLqhgwqoDCkQWJTquJYCDIlpU7oo6vX7KZE8+c3OG46z/dTFpWKinpyXhcXtDgTHFgsVpY/2mUJVBtCaxi+AT41bMmm1ZCTRUMHg19BwLsR4dKUJaOzxbKys/gjqduZcfa3RTvLSVvQA7Dj2//1qGJsurdtfjdAFbSsn1YreFaZtlhOxWlDj59eTnX/urydsVSyolK/wk6uBeCO8DIANvxKNWB+z2h3TiTyrnp17B/h2b/dsjIgVGTwTAId17vwJasbUnJSOH2R77Nnk37Obj9ENl9Mxk5ZVjEHVeEONqtemddxCVlylAE/SH2bNrPoDGFCcpOCKHNKjB3g3UomC7QNbUzqrPDL5L+lWCf2mqMuBUwtNYPN/v3/cD98fr+Qoijk2GkQOo1XRpTKcXIE4bBCZ2PZbG2fgfUajs27pAahsHY6SMTnUZUylAYhiIUinyHzmKNbcJi3R1ww2KQkp7cZCz2333tS7cKMWikm1DIxJFkB5yACvdbiJHf66d4bynFe0vRWjNgVEFMS1LiydGoP0nVEXv9Tda6D/AWW8d/Hso6EKyx9t5q+H6Fw8J/mgbvnuf8oDGF8sFMHPMMS/ha3aQ3TKNanjOlZ1/PhDj6NXoNNFKA5o3D237NjncPDCGEOKb0GZRH/+F9o45P+krXrYcXsbNYLEw8eWzU8ckx9qtord/BpPkx/u7tM3BVuTmw7RDlhyupKq2mZF8ZRbuKCephqBga2EJ4ucud593P03e/yKInFvPMr17i5+f/lr2b27f7RqKMP2k0jmRH+I5ro/qT1hqtNWfdsCCu+SjrwHCT4WjsM+KXjBDHmLmXzkSbumljWw3a1KRkptBHdhITIqGUkQL2Vt5TOWa2GUMKGEII0c0u+cF52J22FsdP/frcmBuDiq533i1nRNzBY9K88TE3Q5x1wYkMmdDyTn7fIfmcevWcCGe0reJIHh8sTIZmO0/UVJq8/mTb3bsj0VrzrzueofpITZPj1Udq+OePnsY0zShn9gwZuZF3XlGoVgtT3UWl3tC0P0/d8eSLUZboBU0hROdMO31S1LHcHrbrlRDHKpV8bbg5dnOOuShb20vB472NqhBCHHNGTB7Kj5+5jQ9f+Iy9m/eTlpXKSedPY8LsMYlOTTSSPyCXnzx7Gx/9ZylbV+4Ib6N6xiSmnjEJw4it3m932rntoRtY+uoK1rz/JaGQyYTZY5h94YkkpbavKWRzy95czaL/jGDX5kzGTysiOSXAwd3prPqkkJqqYs78tqfDsbet3knxvrKIY2UHy9myYgdjeuj2g6UHj1BZUoPVbiEYCNXPwjAsCpvDxot/eI0fPPaduOakbBMg4/fgfRMd3AlGNsp5Csre8T4qQoj2W/jAm1gdFsygiRlqWEJitVso3luGu8ZDcozXXiFE11DWQZDxB/D+Dx3cBCoF5ZgH9pPadb4UMIQQIg76DMrj0h+e1y2xK0oqsdmtpGQ0X0fYs2jtCTdsMrJQXdQHQOsAmJVgpKFU59c2Z+Smc86Np1JRUoXdaWvRtyIWdoeNOZfMZOTUYZhBk/4j+nUqXmVpNQBb1+WxdV3zGTwhXFUdL2BUldW0MV7doXjxVHrgCFqbWKwWLBYLpmmilEIZ4YXvlcVH0KGycBEhjo0tlbUQnfJNlFkORkq7dzERQsSusqQKiyV8LahbSlLXFyMUClFdVi0FDCF6AGXJw0z6GoR2gcpEWds/I1kKGEII0UutX7KJV//2Nge2H0Ipxagpw7jwe2fTf3jnPiB3NW3WgPtxtG8J6AAYmZB0DjjPj/kDpdYmeF5Ee/8HZnV4ur5jDqRc06kPiivfWcsbD79D8d5SDEMxbuZoLv7+OeT2z4k55tLXV/DID56hZH8pAFl9Mrnq5xdx6tXzYorXfKvTxtKyU8nKz+h4zNEFTZvetRiPbWlKPAweV4jdaaemwtVi5wGbQzF87Bfo8uvB0heSL0U5Ylu601Ha8zLa8zqYFaBs4JgFydeiIk2bFUJ0iWHHD2LTsq0EfMEmx5WhyMxLJ6d/dpQzhRDxZFb/ETwvgVkDSmFah0P6fRi2tmd7Sg8MIYTohbau2sE//u9JDmw/BIR7GGxesZ0Hvv0I5cWVCc6ugdYaXX0v2rs4XLwAMCvQrqfB82Lsgd1Pod0vhIsXANqP9r6Lrv5tzCHXfPAlj/30OYr3hgsNpqn5cskm/njDw7irPTHF3LRsK/dd9WB98QKg/HAFf73lMZa+viKmmCecehw5BZEbdS742slt7nwTSf7API6fG3nd6cSTx9JvaJ8Ox4wXZ7IToEXxAiDgMznja7XHQ4fQ1X8OF9K6mXb/J/wYNytqDwTQ3sXh50KUIpEQovPmXHpSi+IFhK8PFpsFq1Xu3QqRaGb1n8H1RLh4AeGeXoFtUH4Nptn6jFCQAoYQQvRKbz+2GDPCBzZXpZtPXvo8ARlFEVgHgS0Rh7TnNbT2djikNmvQ3rciD/rXogPbOhwT4O3HPoh4vKK4ks9fXxlTzKfvfolQMNTiuGmaPHvPwphi2h02bnvoBkZPHV4/gyUlPYlzbzyNU74e++yCq+++lJnnTsVmD7/Bt9oszDh7Ctfec1nMMeOhpsbTovloWPj58cfvNjvamcJZO2jtQ3teizwY2BJ+TgghusVDtz8RdazsYDnBYMvihhAifkzTBM9/ogxWg+uRNmNIGVIIIXqhnet2Rx9bG30s7oKboo9pN4T2g3V4B2PuaJjNEXF8M7RjCmJjfl+AfVsORh3fsXYP86+Y3aGYAHs37o86VrSzuMPx6uQWZHPr36+nvLgSV4WL/EF52B0td7rpCLvTzpV3XsRXbzuT8kMVZPXN7JIeIN1tSRsFux0bFE32Vw3uQ5uu8FZu3SG0L/zYjia4CezHdc/3FuIYt6+VbZ+1qdm8bBvjT5IG2kIkjLmvYfZsJIEv2gwhBQwhRK/2+t/+xfvPfobHFWDUlP5ceddN5A9suW3l0SYlIxm/twLMqtoPSyq8JZVKJSWjB33oVOltjMfQD8BI69x4BFabBWeyA6/bF3E81p+pM8UBJZHHHMktt9nsqKz8jJh6XkRzaHcxnyxcRvGeEvIG5DD7wuk9evkIwMBRBRGONhQs7A6T4n1lOJMdpGalYFicEbc47TJtPabbek60QWsT/EvR/s9AB1D2SeCYh1LOTsUV4mjgSLLjroq+5E+2LhciwYwMUAboEBAETEABlvCfdrxGyhISIUSvdceCb/HwD99m+9oqDmz38MHz27lpyvfZtmpVolPrdieeOT48e8EsDRcwtAtChyF0iGlnTUp0eg0cJ4UbGEZiG42y9O1wSGUdCtZBUQaTwH5ih2MahsHU06P/3KaffUKHYwKcfPGMqGPTzuhBvydg7Ycb+PUVD7D4+SVsWLqFD//zGb/+2p9Z88GXiU6tVWNnjMJqr+v7oWky2wI4++tH8NZ4qSiupGjnYTz+E1DRHpNdQFn6gm10lMHaZp4x0tpEV/8WXf0H8C0F/0p0zT/RlT8KN8sV4hj31e+eFXXMkWwnrzA3jtkIIZozjEwwBgA+IET4NdsEAoAfkq9sO0a3ZiiEEN3kncefYf3SshbHvS6TB296MAEZxddplx5ixISWswVOPqeGidOPJCCjyJSRgUq9BZpvm2pko1Jvjj1u6m3hKn6TgzZU2ndj3oXk3O+cFnGHj7OuX8DQiVEKJm248s6LGDWl5RKZ/iP68e0/Xh1TzO4Q8Ad49t6FBANN+3WEgiGeu3chfl8rS3Z6gIycloULgAEjvEw/peGDfdGeJBb+o/t3IVCpN4PR7PsoCyr1FpTRiRkY/o/Bv7zl8eC+6GuKhTiGWIzoH22stu4rXAohOkC37A0WpsIzi9sgS0iEEL3S+898HHVs10YXruoqUtI6N1W7J7OppdzyO82G5ZqNyxU2O0yeoxk8BrRvCcpxcqJTrKccs8A6EnyLwTwC1qHgOLlT250q62DI/Cv4PoLQHjDywDkf1fxDYwekpCfzg8e/w9rFG9i6cgeOZAdTz5hE4YjYt6U1DIM/fPhLFr/wKYufW0IwGOKk86dyxje+gtHKG+1427J8OzUVrohjrioPm5Zu5bgou5Qk2v5tW6mpCGGxQShIkzpGKKB4b2Em+YVp7N2Ryda1eShjB5f/NIDN3p2zMAog60HwfQzBneFihmMeypLfqbja92krY0tQKdd1Kr4Qvd1bj32AMlTEXYnc1W5KDpSR14ltsYUQnWOGykAfAOw0zMCA+iUk3oWQdGqrMaSAIYTolXzeZp3EtYbaHRm0qQl4PdCJAsb+bUUcKSqn75B88gf0wCmn2odhwITpJhNO9AAqvHwCBTHs7NEktA5BcCNoP1hHd0mzQ2XJh+RLOx2nSUwjha0bRrJhKQwYWcCU0zp/Z91isTB5wUQmL5jYBRk2mHvJVOZ+NRUIgW0sSnW+eGGaJkv+u4yyonKmnHocA0a1nD3SXj6Pv1PjieSuDG8bbLWC1QIaTSioMENQtMfBG08mk9O/L44kO4bFIORzs+adl8kt7MfQ4zvemLW9lEoC52ldG7S153azMa0DENhI+DE3plMFw1jo4H4wi8Dog7Ie/X2JRM8QqJ0tppRq2LJYUb9jk6vcJQUMIRJJu8Pv2esXggRp6IFBu97DSgFDiCgG3/Fmh75+933R112KrnfcySPY/sWq8BuUuuKt1qAgp5+dzLzYGg+WF1fy2I+fZce6PUD4Tc+EWaO5+u5LSUqN7weAVtkmgm8RmOWgzfAxZQUjD2WLfYcD7V+JrnkoHBdAOSDpAlTyxV2QdNepKK3ip2f+mj0b91P3AMgpyObO/3yP4ccPSWxyzWjfJ2jXow3TIlUyJF+BSjoz5pjL317NH6//BzXl4VkTj/3kOcbOGMWv3rgDewwzC0acMBSL1RJxy1eLxWDklKEx59rdhk2aTHK6gasiVH8pUEoTfkMEpqkp2VeKUgrDCGK1BXnizvByi76DH+G6X99K4ZipiUm+g5TtOHRgQ+RB+/H1f9W+pWjXI2BW1p7ohOTLUEnndnuO2qxG1/wJ/A2d5LVtPCrteygjs9u/vzi2TZg9hveebjZDU4PWGkeyg8HjpZgmRCIZ1gGYRla4wN1ECLCC/aS2Y3RLZkII0c1O/8Y5JKWEIi1758TTYtuVQWvNQ7c/Xl+8qDu27pNNPHlXD1tfbh0MoUbFCwAdBLMMbR0bU0gd3I+u/l1D8QJA+9Duf6O973cu3y525zm/Yc/GfTR+AJQdPMKd595PMBiMfmKc6cBWdM0DTdd0ajfa9S+0f0VMMcuLK/jN1/5SX7yA8ON0w2ebue9rf4kpZnp2GvMvj9xccs4lM8nM67qdTrqaxWIhI6fpDBFlhCdkGYbGDIXv6gT9fjw1QayNbt0c2u3hrzf/Aa+77TW3PYLzNIi0DEXZUUkXAaCDu9A1f2goXgBoL9r1RKtLULqKrv5Dk+IFAIH16Or7u/17CzFmRvQttIP+nvPaIMQxzYy2U1AQjAltni4FDCFEr7TmnTfoMxD6DgxgWMMzL9KzQgwf7+fA9kpCoY6/Udm6cgf7tzavCId9+ckmSg+0bBqaMP6VYCkAlUL4TrMBRjoYBSj/R7HF9C0CHblZo/a+HnOqXW3nut3sXr8v4lj1kWrefnRxnDOKTnvfrJ0qGWHME9vP9Pn7Xq2fJt3c6vfW4apyxxT3/FvO4NIfnEf+gPD06rzCHC763jlcePvZMcWLl+J9B9m90UZ2Xz8paSEUYLVq8gr8mBqCgfC1QGuN1Q5eT9NfSdWRICvfeCExyXeQMtJQ6feinPPCW8EqBfYTUBn3hHfnAfD+r2lhsxHtfaNb89PBPRBYF3kwsAUd2Nat31+IR+/4d9SxUDBETY3s1iNEIpnBGqAy+he4ftlmDFlCIoTolYp2HcIMWXAkWxgwPDxN3DCsBAJWKkoC+FyVJKd3bJ3roV3FUce01hzaXUJuT1k7G9ofnhZu6dek/weADu1HtXJqNDoUuShQ//16iK2rdjasbY5g15d7oo7FXWs/txh/pvu2HIg6FgwEObD1ICMj7HzSFqUUcy6ZyZxLZqK1rl8z3tOtencZoDm0x1F7RIPPwOOyA2CxKvoOyebQznBxMhQMf75vvDHOoV2tPPZ7GGXJgdRbUKm3RPw96W54zLVbW/FD+8EW/Q65EJ3lqYl2Zzds+ZtfMP/S2LcyFkJ0UnAtEadP1zFL2gwhBQwhRK+UW5AN7CcUDGGGTDRgGAqLxUJqlhVHSsenvOcUZLU+3q/18biy9CHo2UVlaTVelxelFMnpSaTnpGGxxNb/Qxl9or+kGLHF7A6Dxha2Ol4wvG/MsYt2HmbRE4vZumonzhQH006fxPwrZmF32mMLaOQTCm2nuqwGd7UHrTVJqU7Sc9OwJsX2M+0zMHpTWYvFQv7g2He6WPPBl3z4/KcU7yslrzCXuZfO7PKGpl1t3MyJwHM0vCFq+oHeYjWw2uz1Tf0MS3iJSWO5/XvO47sjIhaZjHxgS+QTjM7tgtKm2muPu9pD9ZEagv4gVruVtKwUktOTIy9/EaILOZLseF0ttxivc/yc2JZYCiG6iHV06+Oq7ffasoRECNErTT//IsxQkFAwVNvIU2OGTAL+AJPmDcVi6Xh9duzMUeT2j7yTxcjJQ+k3tOd8yKmqmcGhXcW4KlyEAiGC/iBVpdUc3l2CX8+LLajz1CYzORpTztM7kW3XGnPiSPoNify7SEp1cs5Nse38sGfTfn537d9Y/tYaKoorObSrmNceWsSDNz8asblle3hDczm8u4SqsmqC/iChQIiacheHd5VQ42m7UVUkl/zwPCxWS8SxMTNGkpkb2+477z71Ef/80TNsWxMujG3/Yhf/+vGzvP3YBzHFi5eBo4fhTI4+nj8oD8NiJSU9XIRKzWj6MHcmG0w5+5JuzjJ+VCs7n3T381hZh3NoXxql+8vwuX2EgiF8bh+lB45QtNeBsvXMrXjF0ePSn5wXdUwZiuy+nd+tSggRO8OaA6qV3f3S7mg7RhfmI4QQcXNgR5Dc/unYHE2P9xloobhoQEwxDcPgxj9eQ15h02Uig8YWcu29l8eaard45WHNig/7YeqGT2KBgMHrTw/lszdbWVvYCmUdhkr5TnjnkfqDKvyByNmz+iD84pUfkN1sRkxyWhI/fva2mHbhAHj974vwulveuduxdjer34uyrr8NHy6s4Z0XBxMMNrzcmqbi07cL+d+TsTWU6zekDzf+6Rpsjqb/zwGj+vOz578bU0x3tYc3Hnk34thbj76Pq9IVcaynCIbAYm05f8hi1VSXhXPP6tePQaOTyWj09E7LsvLtP11PamZevFLtdso2FpV6fbhHRv1BhUo6B+U8pVu/t6fGwz9+nkdJUdOtl0sPJ/PwXX2pqejZjyPR+616K/q1WputTFsXQsRPzqtAhJ397GdhJC1o83RZQiKE6JW+/GQTR0r7YXXmklNQjKFCeNyZlB9Jp/zIHnweH44kR9uBmuk3tA93Lfw+mz7fxpGicvoOyWfE5J63heSXSzbjqRnJig8HMnjUEYIBgx0bc/B5bPhDG5l3WWx395VzPthPhMAK0H6wHYeKcUlKd+o/rB9Pbfsri1/4lG0rd9B3aB/OvP4rWK2xvawFA0E2LYveYHDdx5uYevqkDsddv2QTuzcMYMPKPgwbW4YyNLs251BT6SCrz0Yu+9H5MeV7+rXzOfniGbz58LuUH65k8lcmMOW049s+MYotK7ZHbQwa8AfZtGw7U06NfXve7vTxS58R9AEYpGb6wVQoA6orLYSCBqFgiMt/fAEjJg+h7+B89qxfyt71a0nJymL83HOwO1qZvtFLKecZYJ8N/hVAEGyTUJZW7nh1kS0rdlB6yMLjv5vKwBEVZOe5KS9NYs/WLECx6fOtMT2PhGivrSt3tDq+7pNNTJw9Jk7ZCCEiMax50HctZs3j4H0DVCZk3Ithbd8SYClgiGPK4DveTHQKoosYlvAd7VDIxpGS/k3HDIUyYp9gZhgG42aO6lR+zWltQmANBHeAkQn2k1BGStsnRmEY4ZkX+7b62bBUowyT7L5BktNt9T+bWFWUBFjzvgW/18qY6X4GdcGS4fLiChb+6U3KDpYzeuowzvr2KTEXGxqbd+lJzLs0tmJNY0oplIq6YQiWGH+mRu3jsLrc4LO3U0FDUpqBzUGnf0/JqUlc/H/ndipGHaON50vd460nstgaHkc1FS17ldiTNLNO3wu2NCCfQeNnMGj8jDhm2KC8uJI1763D7w0w+sQRDB4X22yx9lBGKjhjXE4Wo4bHtGLvtiz2bms6S6oz1+VoAv4Aq9/7krKD5fQZlMtxc8dhtcnb22NWG82HHU55bAjRUyjHVDBSwzvqGRFmZEQhz2IhRK90/LzxLH19ZcSxsTNGYXfEtoygO2izAl11NwR3NxxUT0Da91H2yTHFHDd7DM//5mVCgYbeDAeqi3CmOrniJxfEnOu7T3/Eq399C7N2qu1rDy1i0rzxXPfrK6L2XWjL249/wD++92T9dpafLFzKC797ld+8/TMGjWm9IWe8WKwWJswey9qPNkQcP37++JjiHjdvHKvf/5KqsmrqmkxWlFSSmpnCV742O9Z0u9yoacNxpjgiNr9zJjsYM2NkArJqn5POm4ZhMTBDkbcOXXBhDdod3lpR2yai0n+MUh2fndVZ7z/7CS//5c0ufW71NKOnDScp1YmnxttizJFkZ9zMrn0c7d18gL9/9/Ha51dYTr8sbn7wG/QZdPQsCxLtd9ycsax4+4uIY8pQjJoqu+AIkWhae9BV90JgY8NB95OQehPKMafN86UHhhCiVxp30iiOn9uyIVxKehLn33JGAjKKTtf8vWnxAkB70dW/Q5ux7Um/+t11TYoXdbw1Xrat2RVTzB1rd/PyX/5X/wGrzprF63n3qY9iilleXNGkeFGnqqyaX1/+QEwxu8t5N59OambLWTETZo1hYoyd6zPz0/G6vDTfMszr9pGWlRpTzO7gTHZw4XfPbrGrhVKKr956JkkpzgRl1j6mGal4oQHNkMa/usA6cD8Tp6wa7Fy3h4UPvBHxubXoiQ/jnk93sTvtXHh7y8cRwPm3nElSavvvsLXFNE3++cOnmxQvAMqKyvnXHfH/HYueIat/ZtQx6YEhRA/heqJp8QJAB9A1D6JDh9o8XQoYQoheyTAMvnn/lXztpxcyYtIQCkf2Y/7ls/jR07dSMCz2bTS7mjaPQGBVlEEf+D6OKe721bvCu0U2/pxQ++93Hl8cU8xPX14edWzJy8tiivnSH99oUbyoc2D7IXat3xtT3O7Qd3A+dzx9C1+5YjYDRhUwYtIQLv/xBdzw+6vaXGIRzYq3viB/UC6Z+RnYk+zYnXYyctPpMzg/5sag3eWk86fx3X/cwKT5Eygc2Y9J88Zz20PXM/vC6YlOrVUbP9/S6pbyD9/V9MO09n6A1rHtKhOrT1/p+udWTzXz3Knc/si3OGHBxEaPoxuYc3HXLtvZ9Pk2yorKI44d2H6IXV/u6dLvJ3qH95/6pNXx3Rv3xSkTIUQkWvvRvig3xbQJvrZ3PpMlJEKIXsswDE46fxpTThuD3+MiLbvrChdaB0C7QaWhVCdqvWZl9MYKADryG/C2hIKhhruczW52Rpq+3R4VJVUNaZkarXX9mvbKRmMdUXbgSCujmoM7DjNk/MCYYneH7L5ZXHj72WizGpQNpTo386CytArDMEjPSSMlPRmNrl+fXxHjz7Q7jZg8tEc2rW3Nhs+2tjrubb7xhfaE/6jOzYAxTRNXpRtnigNbGzvftPb8qSqN7XGgtQd0KNzroptp0w2Y7f5ew48fwvDjh3RrTm1dk3ri80t0v0gzExvbsHQLg8d2X+8ZIUQbtCvcJL7+315QFqD2ddRs7X1jmBQwhBC91pGiXfz3939i7cdFhIKavoOTOPtbZzH59Mtijqm1H9zPob3vhQsYRiY4z4SkC2IrZFj6gnKGL9ARx2N7k+9MduBxRY6Z2z+2fe4HjCpg/ZJNlB+urC2CaOxOO5l56YyaNjymmCOnDIt6h9lisTDmxNjidhftW4b2/BuCe0EZaPuJqORrUJbY1tMXjixgy4rtlOwrI+APz0Sx2qzkFGQxbkbXNoo9Vs2+aDqPfP+pRkeaFgzzCpoVEC354YZhnfDRi0t57+mPKCsqx+60Me2MyXz1tuhLbQpHFrDx88iFlsJRBR363jq4H+1+ItwUWGu0dRgq+UqUvet3idHBXWjXk+GlnxDPsgAAIABJREFUN4C2jUYlX4WyJX4Xh/4j+0UdU0pROLJjP1dxdHCkOPBF6OVTZ/bF0+KYjRCiBZUBRnb4dUVX0vCabQXLILC0fRNFlpAIIXolj6uCB775M1Z/cJBQMHzxO7Tbw6M/eYnVi16IOa6ueQDteS1cvAAwK9Du58D9VOsnRqFUEsp5euRBS7/wlqUxOOXrkZscKaW47teXxxRz8oIJlOwvw1Pjoe4Fxe/1U7yvjDEnxtZ879zvnEZadlrEsePmjiO7b1bEsUTQ/hXo6vvDxQuoncq4FF11Z+0d6I4bMnEQRTuL64sXEN6y9fCeEvoOze+KtI95fQfmYbUr6npeNPeLZ5r2x1BJX43Yo6G93nvmY1747Sv1yxf83gBLXl7G3255DB1lttXsi6bjTI7cOPSUq9puWFZHm0fQVXeCf3XDzK7gDnT1PejApo79R9r6XqHD6Kqf1xcvAAhsRlf9Ah3c2aXfKxaDxhQyemrkAuik+ePJK8yJc0aiJ/j2n66OOqYMRXp6ehyzEUI0p5RROwujgqav2UEI7UTb2t4SXgoYQoheaflr/6b0YMu7LFrDW//6X0wxdXA3+D6PPOZ9K7ysIBbJV6KSzgXVaItH20RU+i9QKrbdB25+8BvMuuDEJltx2uxWvv6LS5h5bmx3mDZ8tpXsvpnYGu3gYrFYyO6bGfN6cqvVym/e+in9hvahbq2LYTGY/JWJ/OzF78UUs7tod5TCV6i4XWsyI3n70fexWI0mO/spwGqz8NF/PosppmjK5/MS9EcuHJx87hFsltqGYEYqKuUqlPO0mL/X/7N33vFxVNfffu7Mdq263HvBFdu4YYOxjQ3GgOk1dEJJKAkkBH5pQBqBQAqE5KUTSujNGDDdYGyMsQ2uuGBc5aJmda20bea+f4wkq+yutCNZksN9Ph+Bd+7OmTO7M7Mz557zPZFwhA+ejq0xs2PDbjav+C7mWF1njN5DetQvS83yc9GvzmbinCQyJ4LvWWVpTZEGsua11ttp1bbeAbNp/Q2W0FrN/Pbdlk2uufdSJpwwpr7Nr+7QmTpvIpf//oJO9kzRWXz0H3saUAqFomMwDQMi38QZlVD51xZtqBKSdmTgrxZ2tgsKxfeG7Wvi173v2xagurIUX2qSs/vRBDOYMgLR78BG21MhNEi5Erzng7EPtHSE3qPF9Vrizld+QU1VDR8+sxhvqpeTLj++Tfa2r92J2+em1+AehEMRpClxeZwIIdi2dpdtuwNH9+Px9f9g29qd5O0oYOTUYeT0tlfmcqiQMgwJZpVldDOC05K2u3vjHjRdQ2iifsJcCCtTZt93LSttK1pm7UfLEML6cBsmQAgBS9/OorKsO/d+9EdwDEQ0DCLaoGBXEYHy+Nk429fsZNTU2NlKg8cO4PaXbiFvRwGhmjB9h/Wq10NpLQmzLNo7A6OpQnyjbSUY60B8qV6u+cullBWVU5pfRnafLNLiZHwpvh/s3rwv7pg0Jd8s28yR0zq/BEqh+N5ibALCWNM5DScfamd6outaNKECGAqF4rDElxpfXNHhEjjdNsQXa+viK0urKN5fimEYOJ0Ocvpl4/N721w3L7QU0OyVYsTD6/dy5o3t0zbWl1rb4lBW43JUASZIH4hUfGltb3/Y3sJ+pYXlLJu/krzt+WT0yOC4s4+uzfSwgwOEC9OoJlBWQjAQRNMFKWlpePzpCJuCj+7asgEhBE2rFlzetj1MHwqKcrfw+asvU5hbRLd+OUw77wJ6DLTXQrajyOnXs/4+KFZlSHUAZPBNhGM00j0LofmS3oYMr4PwEtxUWBkQWiqxklh9aQdtV5RUsmz+SvZ+u5+0nDSmnTWZvsN6t+EYJfE1SGvb9Sm5bXWdFsAAGd3SyeiW3tluKLoALo+TQIwkpTpy+qjSIoWiU9G70Tx4Qe1rAbT8W6YCGAqF4rDk6NNPZckb/4w5NvGE/jhdNh64XUezZ2spNZUHFZCNiMHerfvxpg2mv00diMOFo0+dwNcffAyyQamMDIAs5+iTz+s8x2Kwbe1OHrr5KYLVB8uIPntlGZfdeT5T5k1M2p4QGjXRcZTsfLlROUKg7AD+zEqyR7Rep6Ah08+dyqt/fyvm2KST2l90sS18s3gBj/3yuQb7n8/iVzdy9d0XMe6EczrVt0QMGTcaX6qguiJ2GckPbgpA6Etk6EurLCLtLoTe+ocYWfUwMvgRANlZMGS0YPuGcnD0AQ6WgOm6xsSTxgKw59t9PHjD4wQqaurHl7y2nAtuO7NN7USF+3hkeFXcsfZEuI9HxknzFW5754NCcag5/ca5PHvnKzHHnC4HPQcq7SGFojPR9J6YIq1WA6Mp0spYbslGu3ulUCgUHcDgo6Zzyg/HN1vec6CXs26xp62Qv6ucB//PRzTSeBq3pkrnLz+SRCIRW3YPF448upzppzVvPThgWIhTLinsBI9iI6XkuT+91ih4AWCakhfumU+gwp7g5psP55G3u3k7zEWv+diycpctm5fccS6Dxw1strznwO5c/8CVtmweCiLhGv77xxebaUlEw5Ln/vgy4ZC9z7Sj0LQImt48gJGeE2Hb+gbns1GArH6m1XZleF198KKOi38uSc+OgFncYPuCi359Tn0WwPN/fr1R8AKs4/a1v79FWVGC6eGWcE1FeE5ovtw5Arzn2rcbC/fx4J4Ww4ex4Em+nEqh6Ai2rY5fCthQTFmhUHQi0kwwVhN/rBaVgaFQKA5bTr/pt4w5fhmrFr5HTVWQIyaOZuK8C3G5k08RB3j6jpfY8GUKf7p2EFNPKie7R4S83W5WfJxGVbmDN//5HhfcdmY770XXQYaXcOFNkkmzJF8vFoRDMHKi5KjpoDu+AK7rbBcB2L1pL4W5B2KORUIR1i3eyLFnTE7KpmFE+fK9PJYv7Mfw8VX0P6KGSEhj46pU8nM9hI1PGDnt1KR9dTgcPLD0T3z4zGIWv/wFZtRk6ukTOePGuTgcXecn+NsvPqSyNPbNfaDCYPPn73fZLIylb3xCZakDh8skMytKKKih6yCEQVmRiwVPSC7/ZYPgRng5UoZbpYchQ581W9ajH9z+hMnKRQFy90wgPTuNY86YRI8BVqvdgt1F5MapwzcMk9UfrWf2xdNt7asQAvw3WsGF0DIgDM4J4JpiWxA4/rY08N8C7hMh/AUgwTUZnBPttZRWKDqAlQvXJBz/9qttDJ/Utdp3KxTfJ0xjPxBPFF9A9QuQcmlCG13n7kmhUChsMHDsNAaOjTFLaIO6mfvSIifvPZ/TbLysqHl2QmdTVVbFFwtW4UnxcOxZk9v2UCyDAAwZA0PGyJhjXYFQg8wLI2IQCUdxOHQcbmvfg4Hm3WlawoiGa7MPNDatSmXTqroaf+tBLVgdtu2vpmmc/MPZnPzD2bZtxGPPt/uoKqum3/De+DPsayAEq2N0m2g4HqiybftQc2BfEQDRsEZJYcMHa+uBPhqBYCCIy+tC0zSMaJjtq77Fn5FN32G9W7Ae+7j3+mHmmVFE9rkI0fica5oZ1MyijeOzKcI5Gpyj22xHSgOi3wIGOIY3C+oIIcA1zvrrAKKRKDs3WG2MB48dgO5o36CM4n8fM04r4zoO7C1WAQyFojMx6rIX47Uzb/k3UgUwFAqFopY5l81k5btralsZNFZGFprglGtipG53Iv/66ZMsen4J0dq0WK/fwxV/+AGn/XiOLXvCeSQyvDb2oHOMXTfbnQGj++HyOMnbXlCbpm99V26fm5zeWQyfPCRpmy63j0FHprFzQwlgNBgRIBwMP7prCVnu357P03e+xN6teYBV233cOVM59+fz0LTkZ8eHTjoOTX8D02g+JjQ4YrK9jIGO4KSr5vHQza/RXBDMYsDwIIW5BxCaQHfo7Nvp45m/PwdA/xF9uOKPF9JrUGxhTeE40tLOiIVzZLPgBUDvIT1IzUyhsjR2UGj40V3j4UmGViADj4NZq/mj+cF3SZvazLaFFe+u5o1/LqSyxAqWpWb5Oe/npzH55OalggpFPHL6ZFG4O3aGHsC0s6Z0oDcKhaIpmmsMJm5iTxAIcE5q2Ua7exUHIcTlQohFQojFQog+QojbhBCfCyGeF0I0LzpWKBSKDmbm+ceS00sjljLykLFe+rU4W9txvHDPG3zw1Cf1wQuAmqogj932LOuXbLRn1H1SrTp0E4QD4T3fpqftj8fnJiXdV5sxc/C7ClWHiEai9B7S05bdeVcdgdCaPsFLuvcJMnle5zzUxSJYHeKfNzxeH7wAq7b705c+Z+GjHyVYMz4Z3fsz89wRMcemn3UEWb3ar3tMe5OS4qd7v9i3M0LA9Xdb+i2RcJSqsho+eOlgsCJ3yz7+9ZMnCQfjZNh4ZoHeK4ZhDeG7MOYqTpeTU66OHewcfcxwhsTQROloZHQHsupvB4MXAGYVsurRuCKhh5KtX2/n2d+/Uh+8AKgsqeKZ373MtrU7O9wfxeHL1fdcEnfM4VIZPQpFl0CL8bsKgATvtS2v3r7exEYI0QeYKaU8QUp5PFbz11lSyuOA9cBZHeGHQqFQJEJGvuHxL8IMGy/Rau9zHE6YNFvy7w+rkEZB5zrYgPeeXBRzuWmavHTvAls2heZHpP0Z3DOgLq7sHI1I+z3CGfvhtjMoLSynvKiCzJ4ZOFzWDLiu66TlpOHyutiycpstuyOOyuXGuw0GjbKCQk6XZOrcCD/7hwOPvqLd/G8rq95b0+hBryFLXltOJGxPbPbcX/6Rs39yLJndre8+o5uTM2+Yyvm/+ZNtXzuCcKianv0hPbuxhofukJx6WSE6Eolk5yY3T9zVj68XN36IKSss56sPY/edF8KLSL8L4ZnV4JwYgUi9A+EcG9en4y+cxuW/u4Ceg6yOBylpXuZcNpMf/e2yNuxpOxJcCDJGug0ga2J3zTmUfPL8UmSM1H/TlHz6wucd7o/i8OW5P7wadywaMQgGu045pELxfcQ0wyBLadjFy0IATgg+36KNjiohmQvoQohFwCbgfWBx7djHwMVA/CuOQqFQdATRXLxeePB9q4SkvATSs2rHJGDsBT12qnlHU1EUTwAJ8nfYD7QIPQeR+jOkvAkw6IoJcgW7CjFNSWqmn9RMP9KUCO1gLWXe9nxGTjkiKZtShsEoYMQkDyMmWd03NB003dIEkEZu3GrNjmb/9vjfb6CihvKiCnL6tL5NaB2apjHn6luYc7UVFLArhtvRlOXvpqbKoHtfB937ghk2KcwXhIOCj1/tzspFbtK6dWPv1qLaNQxMw0TTD87h5CX4TIWWCf6fQsqNJHNOTD1tIlNPm0gkHMHhdFh6El0EGc2NP2gkGDtE5O2M3+Vo3/b8DvREcbhTnFcaf1DCtyu3M25G2/VjFAqFTcz9tbpqztq/umB6bUDDaHkSqqMCGD0Al5TyBCHEvUAGUKeGVw5kxlpJCPEj4EcA/fv37wg/FQrF95km5RP1wYs6tBjlFa1k18Y9fPzfz8jdso+0rFSOPWsyx5w+yfZDjTfNQ1WcGvv0bmm2/ZQyCDXvIMPLQIbBOQ68ZyLaELiRUrJi4WqWzV9BeXEl/Yb15oRLZzB47ABb9rJ6ZgBQuLuIiuIqa+ZWgM/vpc+wXmTWjieHE7Q0MK2fJoer8fcitOairp1F3f5XlQaoKg9gRk3cXhdp2an40n34M/0tWGiZwyV4AZCa0wuHSxCqjhKNGEgpcTh0QlIHBMHqKIEdJRgRA03X0Z1affBCSklVSYAlr3/J+iWbOGLCYEYdO4w1i75h96Y9pGWlcswZkzj2zMm1nTdiJ66Gg2EWv/wFX324lnBNmGGThjLn8pl065uN09X1goDo3SC6PfZYG65zdsnskUHR3uKYY1k97JzPHYM0DkDwTWR4jVVq55oG3tMRwtvZrn1v8WekUF0Rvw3jgFF9OtAbhULRDK0bCAfICFbwwuSgoKcOWsv3mx0VwCgH6nqRfQJMAupyXNOAslgrSSkfAx4DmDRpUmJZYYVCoWgrzgnWjb1RFGNsBMJhL5D6zeebefTWZzEMq+/1gX0l7Niwm53rc7nk9nNt2Zx21hQ+eOqTGCOCs29Kvt0nWFkIsvx3EP3u4DIjD8LLIP0ehG5PA+Tl+xaw5LXl9a8P7Cth/ZJNXPOXSxl3fPIzYd37d6N4fynlBxpkoUiorqxh1zd7GDsjecFNIQR4TkJWvxZrENz2hFEPBVNOm8B/fvsClaUHy0iikSjVlTWceNkMPD53J3rX8XhTMug71MuWr0obLDMJVOggJJGwhpRWYCMSjuDPSLfeJKFoTzGhmjC+DB9Fe4vZs2Ufr/ztLbr3z8blcdWfqzvW7eay38XWgYlGovzrJ0+yfd2u+mWFe4pZs2g9tzx+Pb0Gd42srYYI99y44qSdIeI5/dwpbP06dkBl+nlTO9ib1iGNAmT5r8AsP7gs+hKEV0L6XQjh6UTvvr9cdMdZ/PPaJ2KO6S6djJyuGxBTKL4PaFoKpmMERFZzUMdMYgUyJPi6iAYG8AVQVyx6FLAHmFn7+kQgjsS3QqFQdBxC6IjUX4PeJP3e0R/hv8WWTSklr93/Tn3woiHLFqxk79b9tuxef/8VjDpmOA3bUGmaxrxrT2DGecfYskno00bBi3rMSmT1S7ZM5u0oaBS8qMMwTF67/21Ms/nn0hJlB8ooiZMmHAlFYm6vVXgvAPexjZcJF8L/U4Sjnz2bh4CCXUV4/J5m3UbcPnfcrJz/db58P0x61kENDN0pye4ZqQ1gyNplOk63EyNqpatWV9YQDobp1jcLvTYjo7SgHNMwKCts3DJ5+TtfsXvTnpjb/uqDdY2CF3UEKmp455EP22P32h3hGodIuQxEgxpkIRDeeQhPxwfrJs4Zx8k/nI2mNbyeCU695kTGz+46HZAaUfNKo+BFPdEdEIytUaQ49Lz6l7fjjhnh2LovCoWig9F6A64mCwWIbAQt38d0SAaGlHKtEKJGCLEYOICledFLCPE5kAs80BF+KBQKRUsIx0DIeBjCX4NZCHo/cI6zXeqRv7OQwtz4Ld3WLd5IXxvdTRwOB/d9dCebln/L4peX4fS4OO3Hc+K2g2wNMvxV/MHwSls21322Ke5Y8f5S9m/LT3r/n/1dYsmkl+9bwKwfHJeUTQAhHIjUW5He3RDZAMILrikIre0lGe3J+s824fV76H1ET2oqg5iGicvrwu11sW9bPqWF5WR2T+9sNzuMdZ9tpKrcZOOqVHoNCJKRHSUSEezK9RAJaTicVomC2+fC5XERrglz1k9O5qsP1pH77b76QFA0HK0XQA0GQs20VdZ/tokBo5oHsjYsiX+Mr1+6GSlll9K/qEN4zwb38da5LQ1wTUDo9jr4tAdn3DCX6edOYf2SzQgBY2aM6tLHcaJuLTK8CuGd14HeKOpIpKcCsHT+CqafrVqpKhSdhTSrwNgBjqFW2a6sAnQQ2aA5rN8k14SENjqqhAQp5a1NFt1b+6dQKBS2KdpbzJv/fo9AWYAp8yZwzOmT22zTiMK6JR5K8tLoNdjNqGPb8ADSYLVgIEQ4GEZ36PhSvQhNtPnBJhKKEg0bCM1o1FK1zc4mNZZgrUPw3NZwljb2Ntu2UeEYAA57+hwdQd3+CQRIaWmAxOjg8H2h4fe9f5eb/Tubl9BYn1HtvwEjYlqBBQTRcJSywnKikSbink0Poy4YhGgrQsuETigZiUdmjwxmnm8zg0yhaAVO1UpVoeg6aGlYahINafm3tsMCGAqFQtHePPuHV3jtH29j1pZnfPzcEvoN68Pfl/wBn9+eiFruln08csvTlBUdTCHvObAbN/zzKnJ6N1X1bJleg3qQ3SuTjcu/JVwTrl9eqmvk9Mlm3Cx7aujRaJRfzbmLLasOlny8+/hHzLlsJjc91HL9YCyE6+j4WRgue3XoY2eOYsH/ez/mWLe+2fQ5Il4v8Phc+acfsOChDw6WTjbh0jvPS9rm4cTY40fx9iMfUry/pFEJjtvnZuKcsV161vpQMHbGKFweJ6HqcMxxKa1WqVJaL7ypXt5+9EOqK2oo2F2EETHq75fqsi7SslObBcKOinOujj1+NGs+/Sbm2LiZo7pk9oWi7QjXFGScUhHhVjP8nUXfYb3Zs2Vf3PGp8yZ1oDcKhaIpQvMjnaMgEid7sRX3mx2lgaFQKBTtysZlW3j1b2/VBy/q2LN1H/dd/m9bNo2owSO/eKZR8AIgf1cRT99uTwMCwJfqJRKMNFpmGibhYLi+o0SyPHTTU42CF2DNMn/47GI+ffFze466Z4BzRPPlWjrCd6Etk70G9WDWhdOaLdcdOuf+/DRbD3f+DD8TThwbc6x7/xymnXl00jYPJ7r3yyEYCDbTDwnXhPGlHT7dQ9oT3Rl/VrUuo8KIGETC0QbaIZJoONooO0MIK4gRCTU+X6eddTT9hsfuXjDppHEMmzC42XJ/RgqnX991shsU7Yz3fNBiNNFzDAX37I73RwHAuNkjO9sFhULRAiLlSogldOyeCs7Y93cNURkYCoXisOS1f7xjPXjEYO3ib4hGozgcyV3ivlm2hbLCGKJswI4Nu9m3LY8+Q5PLGKgoqWTftjx6DOxGZXEV4WAEzaHhz0ghJd3HVx+sY/q5yWc3LFsQv/76rYc/YNZFdjQgXJD2Owi+hwx9DkQQznHgOQOh228jev6tZzBwTH+WzV9J+YEK+g3vzQmXTI+pJ9Ba7v3gDh7++dO8+8THhGrCOJwOJswZw11v/dq2zcOFL9/5mowe6Tg9TgJl1fUaGKnZfnau302wOvS96kSSt7OAYGUw7rhhGLg8HoyIgdPlIBgIgsQS6qyLnwkreCF0HaEJIuEo3fvnkJ6dyrFnTuboU+PX4+oOnRv/dRVLX/uSVR+sJVQTZsTkocy+ZLqtrC3F4YHQu0P6fRB8GxlZAzgR7mPBcwpCfH/Ov67G+49/mnB87/b99B1ir6OWQqFoH4RjaO318y1kZDNofoT7eHDPadXElu0AhhAiRUr5/ZQ7VygUnU5JQewuFGCJ8VUcqCSrZ4zZsQSUFcQOXtRRWlCefADjQCWmKXF5XGT3af4wU9rCNuMRrGrwwCZpVDJY3iSDJBmEcIP3LKL6XIxoBI+vaW2iPSbPPYqJc8YSDkba7eH6+vuv5Nq/XkrxvjwyenbD7f5+tC0sLShHCIE/IwV/eoql5VCrCxIJR6kqreqSAYxgdQin24Gut28N+q6Ne61ToE4bRdIouKnrGt36ZZK3w2qPbJompmliRI36GyVN13A0yeK45fHrSMtKrX8ta3vWx2qP6XQ5mX3xdGZfPL1Ff6ORKEbUwO1t3+/I2ucg4EYIlWDbEQg9G1KuRHBlZ7uiqCUaSdxpZN3iTSqAoVB0AYSjL/hvQBr7gTSE3nrB9KQDGEKIY4EnAD/QXwgxDvixlPKGZG0pFAqFXfoN78P2tbtijnn9XjJs6AD0HhpfgV/TBL0GJ9/hI7tPFm6vi1BN7Pr8RNtMRHr3NIr3FRONRJGmBGH5qDud9BpivxPJgT3fMv/+/8f6pXkYUcmgI9M448YLGT7Vfip8OBjmrYc+YPnbX1FTFaR7v2xOvGwmx7VRCf5XJ93MuiX7MSISTRMMGJXGvZ/8lYzs/+1Z795De2IaJmWFFQQqqpGmidPtJD0njW59s0jv1j5Bp/Zi9cfree/JRezblo/L42Ty3PGcddMppLRTucuoY4ehaQIjGrslr8sdRRd70YSGKXUcThearuF0O+vPS9FEGNaX5qsPXkijGFn9LISXg4wiHUMQvosRrvFJ+VmcV8qbD77L2sUbMaIGg8cM4PTrT2L45KE29roxsmYhMvgWGEWg+cF9Ivh+YGVVKRTfI9w+V1w9HIDp5/9vlxgqFIcLZtmdEFyAFXgXmHovSH8EzTW8xXXthOjvB+YCxQBSynXADBt2FAqFwjYX/eZsnC5nzLFZPzi2QZ176zliwmAGjo5d1jD+hLFk90ouowPAm+Jh2lmxb5hy+mTFFQZsiWNOG0IkXBu8AJBgGpJoOMIZN7Q8CxyLqtIC/nH171nz6X6MqGV35zcV/PunT/Ddqthida3hkVue4ZMXP6emNmukcE8xL9z9BoueX2rb5o2Tf8TqRfswIpafpinZ+U05Pxr9E9s2DxcmzBlDWWE5VWVVyFodjEgowoF9xfQf2S/uedEZrHp/DU/8+nn2bcsHIByMsGzBSv55/WMY0cQzpa0lPSs1QRMWSVZPiaaBP8MEGSE1y/p8sntlIoRA0LyzzeyLrRIsaVYjK26H0FKQtV1+otuRlXchw2tb7WOgopr7f/QIX3+8vn6/d2zYzb9/+iTfrd6RzO4238Pql5CBJ63gBYBZhax5E1l5f5vsKhSHIxf95uyE42lpXSvAq1B8HzHLfgvBl7GCFwASjP1Qcj6mUdbi+rZyDKWUe5osap+7EIVCoWglfYb04v+e/Slp2QdTvHWHzvRzj+GGB66ybfe6f1zB6GOGH0wt1wRHnzyeS+8417bNs286lZnnH4vTdTDpbcjYAdz0/67B4bRXyWeGtpKeJWmYKa7pkNNbcmCnPRHPz195lrKiSLPlRlTy3hOv2LL53eodbFm1LebY+099QiTcfHstsX/HbratjV1CVFYU5r3HX07a5uHE1lXbSc3y4/IcnF0XwuqcUZBb2Ezcs7OQUrLwsY9iju3dmsfaOJ07kqWioqKZmG+tBwDk7bRede8LU+ZIMnNqAHB6nIyZMYr07un157vD6WDmBcdy3d+usFYKfQpGQQzTElnT+nNi2fyVlOQ3vykzDJP3nrAfHJRmNbJmQezB8ApkdKdt2wrF4cjL973V2S4oFIqWCL4dZyAElXe1uLqdO+c9tWUkUli5iTcBm23YUSgUijZxzGkTOea0iXz1wVrKiyuZeNI4MnLaNruSlpXKjQ9exYF9xZTkl9G9fw4Z3drWllJ36Fz4f2cy70cnkrejgNQsPz0Hdm+Tze/WHKBbH8juAVUVVvDClwqaBlt0IXTtAAAgAElEQVS/2sE8Gza3ro4daAD4bnWJPT+/jj+7HCivZv+2/KTFPD94Yv7BzJMYfPry55xyrb2uKYcD3329A6fbSc9B3YmEIhiGVUKi6xoleWWU5Jd1CfHIsqIKCvcUxx3f+tUOJs4Z1+btvPCHNxKOG1HJzX+X9BsKHh8EKmvIK76c1Kyc+rKwle+vprIk0OwaIqMb4xuObEHKKEK0fCuV6DxoUwaGsQ1kKP545BtwDLJvX6E4zKipqEk4/tbD73HG9ad0kDcKhaIpZngDkOh3a3WLNuwEMK4D/gn0AfYCHwI32rCjSIKBv1rY2S4oFF2WSXOPanebOX2yyemT3W72ivYW8+Ldb7Bj/W7SclI5+6ZTGT97jG17nhSdcDCK5oC0Js+qHr+9uvdEwo+eFHuigJ6UxEKFXn/ywpuZPRM/nPvT7WsrSBmG0BJkZKMl1uiegXC2rS2fYRgsfPg/LHl1BaYpOfqUsZz9i+twu+2JOHpSrM8sHIwQKAtgGiZun4uU9BQ0XesyAp5urwtNE5hxgk12vvtY9BvVN8bSg9vUdMjMKSEadmO4PETDYZa/tZqUjAymnjaRPoPKmXzcKqSsQTiDSDmjvouEEF7ihsqEC2idIGn9eSBrQFaBlCC8oPlbPEcSIlo41lsaVyj+xxCCBCVlMGjMgI5zRqFQNEdveG/d8GStK+X0tmgi6TtSKeUBKeUlUsoeUsruUspLpZTxp1gUCoXie866xRv58VG38uGzi9m2dierP17PHWfcyyO3PmPb5tEnj4g/dmryLVQBpsybGd/m3CNs2Zx40jh0R+yHvAGj+tK9f7ekbZ52w8U4XbHbbAkh+OGfr0naJoA0y5DltyGrHoLQZ8jgB8jy3yIDT9myBxCJhLl5ytU89ssP2PJVGVtXl/Pcn5dy/birqCpvuc4zFpNPOYqK4krydxZQWVpFoKKakvwy9m/PZ9CY/vgzUmz72574Ur0cOS3+cTr5lPYJPM67+sSE48ecXEl1eTXF+0vYtWkPyxbqrHjvGz558XPuufhOFv3ndmTwAytwVfUwsuw2pFn73bjiS3wJ94xWtXsDmHzKeDCLrBpfswJkJZiFYOxj0lx7OjhQ24pO7xNn0AWutgnlKhSHGyOPSfxbNea4UR3kiUKhiIWm9waRAc2mB6T157u0ZRvJblQI8WCMvz8JIc5M1pZCoVB8H3jgukcJB5uqoksWPvoRuzY2lRRqHSf/+Gb6j2jecmry3P6Mn2uvfGLM7LM5Zt7AZsv7HpHCqTfcbMtmek4aF9x2ZjORRH9GCpf81p6uiMPh4Id/OrlZ5wgEnHDxSPqNsBdsofp5iDb/PmTN28hIglKCBPz3jr+za3NVs+WFe0I8/NM/27KZ+KE5wdRjJ3DerWeQ1TOj2fLTrzsp6ZbEiXC4mgbJrM/I6TK59BZrjiUaMSjc4+DNJ3Ost8gg0ihj/mOCA3kNVjX2QvVzlhXXOIT31Bgb7Ae+S1rt35ip1Rw7t3nL5D6Dg8y7zF55Vh3C/9PmmRZCQ/hvQGitb0unUPwvMHjswM52QaFQtISIJ4ovwBErq7IxdkpIPMAI4NXa1+cCG4GrhRCzpJQ/s2FToVAo/ifZunoHRXtjJ6lJKXnzX+/ys0d+nLRdX2omtzz9EGvef4WNy9bhdOlMOGkmI4+bZ6sDC4CmaVx219+YdMq7fPX+p0RCUUZOHcvEeRficttPRZ9+zhSGjBvAFwtWUX6gkv4j+nDMGZPalClw3q1Xc+Rx43nklsfI311BWraHy39/PsedY6+2WUoTGUrQFSX0GTiTnyn//M34QpWrF+UmbQ9g1XtrSMtOxeNzU1VebZWQeF2kpPvY9c0eqsoCXSYLI6d3Fr996eeseOdrtq/bjT8jhamnTUha9yQR29bujNNCVdBvaJjPF6bRrY+PNUskX3+WTiQcpe8wiaASAGnCV58ITr7kYPBHhj6HlOsRQkekXAOuY6wMDVmDcI6xSotE60s/ZOgzLr5FMuF4yerFglAQRk6UTJwFTvcyIPlrQP1eOodB5r8guAhp7EZo3cBzAiJeZoZC8T/Mx88l7m6V++0++g9X54ZC0VmYZgnIPMAJRLEmXgRWXoUTql8Ed+JuenYCGEOB2VJa/cSEEA9j6WDMATbYsKdQKBRt4sNnF1OSV8oJl86gWzvqVrQHFUUVCceryqpt23a5fUw580qmtHP+28hppzJyWoxZ5zbQe0hPzr15EpiloPdDaG1/wB4xdQIPfPFIO3gHEAXZNEvmIFIGaF2xQGPCwQZNuuqej2sNhUP2uoXUtaN1eV2kagLTMHF5nAhNwzQlwepQmwIY0igGMx+0ngi97eeTN8XDuFmjkaYkq1dmuwYvAEr2WxkMQhMgQVr/AWDHJi+7t/pJy/YTqKjBSl6RSCkRHPz8K8sMQjURnB4nmtBqj4UodRoXwjkanKMbHQPSrAIjF7QMhN47sZPSOs9HTIARE2TMsbYgtEzwndfYPynB2G61f3UMQYjWtdetKK6kMPcAmT0zbLWOrqOsqJyiPcXk9M0ms3vbhJAVitZiRBI3RizYWaACGApFZ2JW1grV6LV/BgcDGFg6US1gJ4DRB0gB6nIhU4DeUkpDCJFAUlShUCjal3ce/YhHfvE04aDVivOp219i5NQjeGBpyy2YOoojp4/A5XHFKCGxmHhS27swdHWkUYSsehDqyjCEE9wnQMpVrerg0BEI4UI6hkB0e+xxhz2dgkFHZrP6kwKr7WadrdrJhj5DUxOtGpehEwbz8X+XcCCvhHCNdVwJIUjN8jNk3MCYJRutQZoBZOAhCH9ZKzIpkK4piBT7pQimaXL3xf9k1ftrMKLWg0VmzwxufeIGxh1vX/uhIUedOAbdoRMNR2OOG1GD0gLrlkVogpR0H5qugelBGpVEI1Eys0so2BVC6Brp2X7SeoyPm2EhpQHVTyGDH4G0rj3SOQrh/ylC7xFzHeEYhQyvib0DNo+tRMjwWmTg0YMtYLV08F2C8MTXCwnVhHjxnvl8/eE6DMNECMGR00Zw2e/OTyogVhMI8sKf32DNovWYpkTTBGNnjOKSO84jJU2JiioOLT0HdWPPlv2xBwVMPnlCxzqkUCgaoTkGYGppYBZjBS/qEIATXJNbtmFju/cBa4UQTwkhngbWAH8TQqQAH9uwp1AoFEmzbe1O/v3TJ+qDF2DNOG5avpXfzru7Ez1rjMfnYfbFsVPhuvXNYe6Vx3esQx2MlCay4g8HgxcAMoIMvg/VT3eaX7EQvh9ALH0JvQd4ZtmyOfX0mWi62UiZQgJCSI6aPdaWzRFThlJeUlkfvADr2K8orqR7/xzbJUSy6gEILT8o4S8lhL5EVv3dlj2Af1z7CF++81V98AKgNL+MP57/N8oOJM5Oai0ul4vMHq0L2khTEglZ1wzTTCFYbdJrYJihY6z5F2mYlBRUsn5Vgg5B1c8ia96tD14AENmErPgDtcmpzfGc1ER5vRahIXznt8r31iKjucjKew4GLwDMcmTVQ8jwqrjrPfen11n53hoMw8pMkVKy4fPNPPqL5MSGn7njJb7+aF199xnTlKxdvJEnf/V88jujUCTJ3KsSXKu7lkSQQvH9RetJ4+AFWCdoBFyntbx6stuTUj4JTAO2APOB24GtUsqAlPK2ZO0pFAqFHR762dNx2zOuWbQBw0icRtqR/OTBqzjrp6eSUtveU3fojJ05mvs//5Pth83DhvAqq/NCDGTwYysNv4sgXBMRqb8Cx+DaBQ5wH4dIuwshWm7rFYtPX9pEWqYgLdOoT+9PTTfI6iH5cuFeWzbXfLyBjO7p+DNS6gU9nS4n2b2zyN2yj0g40oKF5shoLoS/jj0YXoeM7kraZjgY5osFsR+YQzVhXvrL/KRtxqK6qoaayprmoq7x/KqJgIRgdRjDzGLC8Q6ktNbN35vK/P8cyZsP58dcV5rVyOCHsQ0b+RBeEXNIaKmItLvAfQyI2nPecQQi9Q6E88hW+d1qgu81Dq40QNYsiLn8wP4S1ixaH3Ns+/rd7Fi/u1WbLthdxPqlm2OObVm1jb1b48yMKxTtxNN3vJxwvKzMXvcnhULRPphmFIw9NG9DXpuBUfNcizaSzt0VQlwD3Az0BdYCU4HlwOxkbSkUbWXgrxZ2tguKTiJve+wHDIBo1KBgVyG9h7Rfl4O2cs09l3DVny+irLAcf0YKLo+rs13qGIyd8cdkGIw80Gx2DTkECNdkhGsy0qwG4UCItn1P+bsKKS/xAOBwRxGaJFDlIVAF7kp73Sf2bs1D1zWyemWS2TMDaUqrJAIIlFdTXlRBTrJaMEYLgqLGLnAMTMrkvm35cUunAHautydi2pTczVbQxuWxNB4MwyAaahLAFI27t/z88ev49MWlrP10Ix++msOi+SPQdZNwqO62qJhQTQi3t0kZiZkPMkG1rLELa46nOULvgUi9DSnDIKMI7dCUU0hjV/xBI3YgIm97QdyAMMDerfsZPHZAi9tuKUCxd2sefYe1oBeiULSBaChOFlQti55dxrk3zesgbxQKRTPMvSBrsEQ8nVCvR1Ub3De2tmjCTvHxzcBk4Esp5SwhxAjgDzbsKBQKhW1Ss/wU55Va2gINxBEFAqELMntn2ba9Yelmlry6nJKCMnoO7M6si6Yx9KhBbfJ3z6aVfPzsS+z5toi0LC/HnjWdo0+/rE02DwWBimqe/NXzfPXhOqKRKEPHD+KqP1/MwNE2hRe17Hq7gbIARtTE6XGSluW3gjia/e/pUGCGN0LV/RDdCsKNdB0PqT9Hs/mwmZrpp6ywHMMwiYQABJpmojs0fOn2bGZ0TwMgUBZo1IUkNcuPL9VLih0Bz7rvQVaDLLdm8IUTRLrVolNLXswzu3cmmq5hREwMw0CaEgTouoama2T3ti8Q2ZDu/bLRNI1IOEo0Eo2dJl4r7ll3feg7ojdZPTNBQlVZFYHyGkzTxONzk5rtJz07FafbCohIGYHgR8jwEjDLwTxgaUpwUBTTNA0O7M3nlf/3X7788C2GjM3myruuZcj48c1cEcIFcQJjUoYh+J7VBYUIwnkUeE5PTkw10TkVZ6zumGrmjympLKli4WMf8/n8FRw5bSSzLppGamZsTZT4pTwmmOWkeZ/CLHse4ToaPKepNq9dACklK99bw/IFq6gsCzBwVD9mX3Jcu7Y57kg0TSQMxk08yV7pnkKhaCe0blaGqwwBDbMFBeAGrXvLJmxsNiilDAIIIdxSyi3AcBt2FAqFwjYX/eZspNkgeAHWQ4qU9BrUA2/TmdNW8v5/PuHhW55m4/JvydtRwJpPNvDAjx9lxburbfu6aelC/nrlX1n1QS75u2rYurqEp+9cwMt3/ca2zUNBdVUNPzn611ZXl/xSKoorWf3xem6ZeSdbv9pmz6j7OIr21VC8r4RgIEQkFKG6vJr8XUUUHxjULl0u2gsz9CWUXgHhlWCWWRoCNS9DyYVWyqMNTrh0OpFwFCNqIKXV/cIwDCLhKFPnTbRlc+ppEyktKKM4r5RQtfWZVpUFyN9ZyKAx/fGmeJI36hhpBSyM/WAGrOwYM1Bb/qOBI/kyh7SsVAaM6kckHME0TGv/TUk0YmBEDC64rX3a52T1zCQ9J9US8UxU497k+nDMGZMo2ltMSX4ZoRrrc6wsrSJ/ZyGjp41A0zSkNJCVdyMDT0Bkq3VMyAgYewEru8SUJrmbd1O0N8ySt7xUlBisWVzIbSfczcZly1q9H1KGkRW/RwaescRko7nImreQ5f+HbKhn0QLCfVLSY/2G92HAqL6N/TElBbsLqSwLUFFSyd6tebz/1Cfcd+W/qSipjGln8NgB9B7SVMjUBGMf3XqWMGzMfojuQla/giz/JdKMbUfRcbzw5zd45ncvs3X1DvJ2FLD8na+474p/s/Xr2ILGXZ1ZlyVuvzhwRPt2QVIoFMmhaSkgMmkcvADrBzwI7itatmFju3uFEBnAm8BHQogFgCpqVCgUHUp2ryz8mc1nmjVd48gZI23ZrCiuZOHjzbWITVPyxgPvWLO7Nnj1by8SjTR/svrs9a3s+9Z+YKS9eekv8ynae6DZ8nAwzMO3JCfkV0fB7kqe+ks/aqobJ/wV7E3hybvSrVaPXYXKv8RupRrdDdX29j89J42UtOb6GR6fi2797AVvDuwrwe1zI5rop7i8Lqorg7ZsQtBqt9m01aaoTe+02eqz58DuOFzNkz0zuqfXi2m2ByWF5S2/icbXh8LcA3hTPY1KSwDcXhfV5bX7G/4SwuuaGOkOOMG0SoBK8wqpLBX8555eBKsP1vSGQ5JHb3m89TsR+gwiW5ovN0uR1Ynr+hsiXOMQvkuaCdIKz2zwnBJ3vavvvpju/XPqX1eWVmFETbr1zWr0GRXvL+WjZz6La+faey9rnF1jlpGZE+ba35s0OmSNPAi+1er9UrQ/uzftYdmClc2WR8JRXr//nU7wqO2sfn9dy29SKBSdi5kgdBB6ocXVky4hkVKeXfvP3wshPgXSgfeTtaNQKBRtYd2nG+k1uAfB6hDF+0owTUlKRgpZPdLZtX434VAEl9vZsqEGbFi6uVG3hIZUlgbYvnYXwycPTcrm/u/WUpAb/6Fy3aIP6TO8a7R1W/Xu2rhjrRXxa8q6xRvJ3ZbJI388lmFji/CnhyjYm8qubzOBAPu25dP3iM5PVTbNYGK9jtDH4L86abtrF39Dz9rjtOJAJVJK0rL8eFO9bFi6mQtuTT4LYd2nG/GlevEMdVNdWYNpmLi8Ljw+N3u37qesqJyMbunJGQ2vAyRo/UEGsGZGnCBqg4SRteCOre0Qj4qSSvZty2Pg6H5UlFRSUxlEd2hkdEvH4XKw9tONHDFhcHJ+xmD9kk1EghGEJqwylSYITeD2ueuvDzvX7SYcDLNu8Ub8GSl4/R6qK2uQpsTtc+P2utj05VaikSha+MvmGxQ6aH2AEPgu5vVHnuXDlx2Eg83nhHZtCmAYBrreVKysOTKOACgQVxw0HsJ3LriPh/ByIArOCQhH/4Tr5PTJ5s5Xf8GGpZvJ31nIJy8spaK4KqY46trF33Duz2MrxfcY0I3fv3Eb6z/bRGHuAbIzXmHsFBNHjMuxDK+0gi2KTmHd4k1xx/Z8u58D+0vIaUM5ZmdQ1zI5Hp++vJRZFybO0lAoFIcOM7Se5h1IGhD6tEUbdjQw6pFSxg/BKxQKxSGkbube5XaQ0c2FaRh4Uupq1uv+Y8+m9SJQOyPtgtoOFLayBRqsY0SjSNMEIXA4HCAEpmkmWLllivYWs2n5VpxuB+NmjiIl3Yb+QZ2rifLvbSZK1O1+OCRYtciFEdVweZy4U0TjN3Q6ZpN/G1j1mHrt/+19T3XHjMfnxtO/cVlTrIftZGxquoY/ht6FvY+0diUhQMTSJWjb95SWlUpaVmpji2089uvtNNhhIYR1HDdwVwCaZuDQre2FasJ89upy9m8vQEqJ7hCkpkvApK5VjDSDmNXvosk91vKmCatCgPAivOfwzcp3CAdjd9Npes2QZpUVjJAhcB2F0BsKWiY6/wLImoWWFolrEkK0fPsm9GzwttyOriGapjFu5mjGzRzNusUbqSwNxPEnsR1d1xk/22pFa5YtgJjJaxKMYmTNO6D3BudRCPE/3pWpi9Hi70+XuT63H9VV8YWFFQpFV6Dl606bAhgKhULRWYyZMZJPXviI4ryyBg+ClfhSXRx71nRbXT6suvcIZng/je64hQtfxmCGHDUwaZs9h44ju6eD/TsDDe4FJUYkjMOlM3bWCUnbBOvG8+V7F/D5/BX1D0kv3+vgnJ+dxszzj7Flc8KJY+N2ERhwpL264bEzRvLq317lwJ4iDOPg9+T2FjP86FH0HtrTlt32RtN8mFo/ML6j8Y9nFHCAe5Ytu2Onj2LXN3tijo2ZPsqWzTEzRrLohaUxx/qP6ENm9ySzLwCcY61gXawSGuEC51FJm7Q0MPqye1PsdrFjZ9rb/6aMmzkal8dJqDr2g4lpmlRXhKmuKKEwt5TULD/zH3yXQEU1pXnF5PSK4PHVfefW/0dOAkfkaTArQR6wetY3baXrnIgQGhPnDGfX5tgtaPsPT6nPvpDBjy0tjbrPOAB4ZkPKDQihWd1vwmuaWIhapRa4kIEnrUVaFqT9CuFILhssWcbOGMWujfGO3daX6QnXZKtNbyMi1n6JFGTgP9YivRek3Y7QOz8j6/vC2Bmj+PCZxTHHeg/pkXw3oy5AapafypL47bnnXW3vN1ehULQPmnssJjpxszBcx7Vso31dUigUio6h50BBoLys2Sx2TSBMWlrsm+6WyOiWypzzS2g6XSgIc9Y1lThdyZWkgDWjiQgjmkSUJZCWGSSt+0Bbvi55dTlL3/iy0QxvJBzllb8usF3uccnt55LZs3kXAafbyY/vs9cxJauXj0j1gQbBC4tQjYHXs8/6fLoKWiaxI/8G6PaU62ecfww9BzVX1M7ons5JVx5vy+awiUMYP6u5qKbT5eDsm061ZVNoKXFT+YXvQtvdIs65eR7OGBoY42aOTrocKxHxbTU576SkpsrKKvD5HTjdYYr2S8y6+ygZxe0JcvoPS63Xwg+4wMyj0c2W8CF8FwNwwW9uJLtX82uDwym49r4ras1uQwYebhYgksFPILjAeuGeDY4hjY2YhUC09tisW1aCrLjb6lhyCEl07M654vjWG/KcDnoTYU8jD5BWl5sGy2TlvbZ8Vdhj8NgBTJ7bPDipO3TOufnwbDX6g5+3jziwQqE4lCTK/mo5QK4yMBQKxWHJijffIKenpLIMApUgTXB7IS0TNq/YjmFE0fUkL3GRrzn9ygp6D4Cl7whKCgQ9+0tmnysZMXEPMprbYh15U/Z+t5W1SzR69IOsnCjBGoHTKTGlYNNXKbxyz6P86B93JucnsOT1GLX5WA9oS1//ksFjByRtMyXNx7++vIfHbn2WNZ98gxE1GDx2AFfd9QOGTbL3sLn6vddISbdS8wPlYBjgdFvfU/7uEkoLdpPZI3lf2xvTNCH6LVZrTIODP6669Rf8L3iPTdquL9XLL564nkXPLWXtpxswDZMjjxvJCZfOsJcpUctV91zM56+v4MuFX1NVFmDg6P7MuXwm/Uf0sW1TeE8HvadVqmDmgdYL4T3VanlpkyMmDOa2p27ko2c/Y8eG3aSk+5g6byLTz5vaTDzTLsHqENFwlG79sinJL7N0bCTES0ONhCSmaaJRSffeUFlWe+3IkhwxtoITzyunZ38JMt0qFdF6Wa1lhQe0TIRzDHjPri//SElN48EV/+LxW+5j9Se5RMMmA0dncOVdP2TUsbXZUMGP4qbjy+AHCO/ZVnvV9D9CzdvI8DIr+8Msqm051yRAYpZZ3XLcLc9U2cWX6uUXj1/Houc/Z80nG5CmdeyeeNmMpDRWhJYG6X+BmgWWzoesBNMHIsNqpdeQaC4ysgnhbJ/sHEXLXPHHCxk6fhDL3/6KytIqBozsy5zLZzJg1OHZreOl+99MOH4g7wA5vXISvkehUBw6zPBuEpaJhP4NXJvQhgpgKBSKw5LivBKEBmlZ1l9DAhUGoUA5vrQk01+NIgAmzoKJs2pr4mnwkGUWAckFMPK+24GUkJ/rIb9JFrUQULinODkfaynJK7U11hIZOWn839M/wTAMotEobre9drQHfclHCPCnW38NkSaU5e9pcwDDiBrojpZFEhNTDTJIXcBC0uibr50Jt0dKmo8zbpjL6dfPAWiXOn9d15l5wbHMvCD5oEoihGsywjWZaCSKw9k+twh9h/Xmh3ddRDQSwuFs2/EUi8qSKiLhKBnd08nono4RiZK7eU+jzj9C0KiEKxoK43JF6q8h0083OefHQTCK696CKQ004QChWS3fPKciPZeixRDkTM/O4dZn7ovrozSL4u9AgzEhvOC7AOG7ABnZiCy/o1XrHSpS0lM444a5nHHD3DbZEVo6pFyOSLkcGVqCrHwg/ps7YL8UB9E0jennTmX6uVM725V2oaYqlHD8u9W7yJmnAhgKRacRjl1y2eANLZpQAQyFQnFY0mtwLyB2bX1mdycef2bMsYQ4+mE9uZSCrDjYVlLLsFKd9b5Jmxx81Bg0XWBETeq0BQXWf4QQDDwyuYBIHb0G9+C7r3dQVlRBMBBCCGvGNKN7Or2G2NeV2LlhAw9e9wDb11dgmpIe/d1cdudZHH/R+fb8HDIIWI0RMTAME5AIIdAdOi6vTrcBw237uvSNFSx6fgmFuQdIy05l+jlTOPmq2baCGZrmx9T8mNFKjKiBaUpL/FHX0J06Qh9o209p7EdWP2/NmGMineMRvosRjrZ34GhPpJR88sLnLH55GcV5pWT1zGDm+cdywqXT21Tqs27RG7z3xFvkbqnC49OYcspQTr/pF8kHGOOQ0T0NX6qXTV9uJRK0WrPqjibBxyY49H1WBE1a2Ta9+uWDUQFANCIIBQX7d+1GCPCmOHD7/Mz/z2es+2ITQ48axLwfz2H4pCFx7TdF6H2RxOnyo8eZ6dZ7WcETGSfVNt56XZ2W/NbtXRMVCoC0LD8l+WVxx8fNUtk9CkWn4jq+hTe0LEbfhYqPFQqFovVMO+9ivCmxL2GzfjDF1gOXcI4BGQGzxApegPXaKAItHdG0jrsVZPfuQ9+hTmSDxgh1TVKcLpOzb0mcJhePMceNpGB3EcFAEJBIKQlUVFOQW8SkueNs2SzMzeWXJ/6J79aWY9ZqixTkhvjHj15h6Suv27I57sRz8PoNDMOg7hOQUhKNRBk2Pgt/Rjdbdt//zye8eM8bFOYeAKCiuJKFj3/M03e+bMseQEnZNCLhaP2+S8AwTEI1BhH9Kls2pVGMLP8thJZbD8tSQng1svz2GMKGncvr97/D6w+8Q3FtBk9Jfhnz//Uur/7tbds2v37vRR677QVyt1iiesFqk89e38qDP/4F0WikXfx2upzs2LC7PngBIM34wQuPT6Jp1B6OUdKzw0ycWQlAVYUgGrbLijIAACAASURBVJFUlFrXDykhUBFl16YqvllhBVy2rd3Jv3/yBN9+tb31TnpOscRQYyA8Z8RermWBK067R70vOLtG++VkEY5B4BwTe9B5pDWuUNjkvFsTa3f4fL4O8kShUMRCc2VhZbvGIfV3LdtoP3cUCoWi40jv1o8bHryB7n099cucLsHcy8cy+4qbbNmU0b2A1qTbQG1bSbMKacZpKZiAcDBM4T4H2T2iNCz59/lNPCkayxc07TrQOnI37yW9W1qjQI3u0MnskcG21Ttt2XzuDw9RXdV8ttc0Jf/9U+K64njs3pSH0HuT2SBOIQT07K+zd2cPopGY/RUTEqwOxVXO//qjdezfnm/L16f/Moh1X/bCbPDwG6x28OK/xrHyo/iq9gkJLgSzvPlyGUTWzLdn8xBQfqCCz15dHnNs6evLKS2MsQ+t4O2HF8aUfsjdUsX6RW/YstmUcDhMZXHj78c0BS63iabL+vNOCPD6TZzuuu/XoM/gED+5ex8uj8Q0Ye92N+u+SKGiuDZBVVrL/vWrPpTmHyzNMgyTdx/7qNU+Cr0XIvXXoDc8ETwI36UIz+z46/mvA/d0Gl08nMMRabcf1i1HReqt4GoSgHGNR6Te1jkOKf5n+M9v7QexFQpFRxGnAwlAsOV7A1VColAoDluGTDieOxfMYNe6pdRUVjBg7FTbM/oARL+xROX0PrXdAqKAs1ZAz7BEHpvedLfA+qWbCQYMggE/vrQwKf4o0YhGWbGPYA2sWLiaEy+dmbSrW7/eQVp2Kv7MFMI1YRACt9eFEIItK7dx8lXxH4risWXFvrhj+buCSdsD+HbVdqoDfmAkWb3KcLpCVFenUlLiA0Ls+y4vabG43Rv3EKyOX+e8ZeU2eidZRhMJR9ixYTc7NkzGnxFkzOR8qgNO1n3RC9CQru+Yfs6UpGwCyMj6BBtNMNbBbFuz0xK/jIFpSr77egdHnzI+KZsleTsp3BP/uPl2xVomzL0wKZuxeCVOhkg4pAOSzO4R0rJ9OF1BBDpGFK650yQrZx/9j6jESsXQ2f6Nh99d0Z+agE5qRpTegyKUFTnYs83S7UhJryat28Gyl+9qP7PWliwJ1zhwPmxdR2TICkQ0bc3adB3hRqT+HGlcBkYuaDlJCwl3RYSWiki7HWnkWR1J9F6qfaqiXYiGEgfFX7x3Phf98uwO8kahUDTFDH6R+A3RDS3aUAEMhUJxWKNpGoPHJx8AiImwsjmikWoCpUVEIwZOt4OUzB7ourt+PBnSMg/W8tVUuaipapxG7vUnfoCJh8vrIlgdQtM0PCmN/fL4Yqeqt2jTE/9BLNmGLrF8qaps3qLV7Ute1LFuHdMwCZRXEwlG0J06/owUdKeOx4ZN3aHjdDmIhCIU7DbJ3ZyO0DRS0qO4PC5bfgL1x0ywOkh1RQ0AXr8Hr99j63g6VLS0f24bx5TL608o4WD7M21Cj36JtDQEpYUuwiGd7n0kDofE5RGMngJOva5biQAkukNSE7DOgcoyB9+ucdGwykXTGpelON1ONP1gFsTOb3L56oN1hGvCDD96KONnH9ksuCGEBs6WW8Q12ws9B/SDwoMysglCy4AoOI8C15TDMiND6L0srQ+FooPoO0wdbwpFp+JoaYKpeVvyphx+v3YKhUJxqHBOJlBezP5t+yg/ECZQblBWGCJvWy7VVVFwjEja5LBJQ8nqFVtQVAjBGTf+f/bOO8yq6ur/n33O7WX6DDND7016UwRRFMXeYosxvkZjiiamxyRG88a8apKf0ahpahJNsxesERVRioAgIEjvA8wwvd657Zz9++NM5d47wz0MTffneXiAs+9eZ9976l57re+yp+4/5ZzxKdsmz01vpbyV066YlLJt5FR7gosTzhqLrid/1PQdXkzhgIK0bfYf1YdAlp/928uoOVBLY10TdZX17N9eRrQ5yrgzRqdtU9M0xswcSdmucqr2V9NY20RDdQNlO8upLa9nytzUv3eXOGdQua+K8t2VNNY00VjTREVJFeV7KjH1nq0gcjiMmDqEYE4gaZs/08eoU4albTOQlc+oqakjoqacd17aNpMx57rTu/1MqCFKqF4SCccYc4qB04UlzAu0qNIwaFQzeYWtHgvLWaF18D9kFnS+jiefPa6tFOwLD7zGb2/4A+89vZgl81bwt5/9h/934x9pbmw+rO+WDNn4J0tDJfwmMvw2suG3yPqfI6W9KCmF4rNE4aCutapmXvrZqLaiUJyoaI5BQBcLGN4ru7fRc8NRKBSKE5twcxNPPaB10kCwtmv8+7fNbeKO6fLtR27C6U70KM+5bhZDxtsTrJt74+ykK0kTzxzDxLNSCOR1w+U/+CZDxmUkbA9m69z26O22bGYXZHL5dy9om+i14s/w8sWfXmbLphACOpXFtGj9v92SqpquJT3GpmHgcNkLQfnovRzWLU8Ujduxwc3CecdPKT+H08GX7vgCzoO+p8Op86WffwGnq/sVkWR84Ue3kJmX+Nudc/04+o6aastmMgaPT51W0Xru1VQ48QfjTD/nQEuDj4Mrldx6z16crvZzQNMsJ0Ywx4PH3x5NVdAvj4tvmQvAphVbefc/ixL2u3vDXl7903y7XykpMrIMGU6ivRHbCM32hHYVis8S595wRurG1Nq+CoXiaCK6cDS6u0/XVSkkCoVC0cK6tx9lzeIAJdtcjJveSGZunKoyJ2uWBKmvdrB79XMMmnx12nYnnzOeR1bcy7/vfoFdn5YQzA5wybfmcsqFU2yP1Z/h4wd/u4WP3lzNhqWbcbgcTJwzljEzR9oueanrOvcveoxXH3mMhc+uIB4xGXvaIK75+a0Es3Nsj/X0q05l4Nj+fDjvI2or6uk3ojenXjqVzLxEZ8mhsGfTPhprmigaVEBjrZVC4nDq+LP8uDxO1r63nmnnp44mSYZpmqxbtJHCgQU01YZa0nMEvgwf3oCHFW+stuVsWv76WjavHMPQMRUMH1eBpkm2b8hl4+peFPT7lLOusxeBcyQYM3MkP3v6uyx+cTnleyop6JvHjMumUtDPvq5Mr4En8bPnHmbp8/9gxyc78Wd6OfmiuQydcmaPjbuprgmH00mvAfmU76lEdnRCCeuPpglM6aCsJMA7z5kMP2UiOtusz8gQmLWAZOLpbh56M8rTD+ns3JRFMNvDBd84j9zeI1nx5morPWTKEKZdMKktVWn56x+nHNuKN1dz5Q8v7rHvKiMLu2h7H+G7tsf2pVCciLz8yJupGyVU7K0kv8/x4zxWKD5vmGYtUInlhuioWaMBLgj9B9xdR6gqB4ZCoTjhKd9TQXNjmOIhhbZXigFC9ValhaoyFwtezEZKOkUONNVV2rbde3ARP3riVtv9k+FyO5l4zjhqDtThC3oYNyv91ImD0XWdS277Opfc9vUeGGE7/Uf2IZgZpb6ylIIBI/Bl2HNeAITqQ4AVNZCR4yMeDaM7XOhO69g31YXStmnEDSLNUTRNI5Dtx+1zoWlaW+SFHZsAoYZmpBRs+aSATz/KBglOj7PT9zieKOibx2W3dV2GMF0CWfmcfdP3e9RmR5obw5imJCM3iDfgpr6yloaaMPGYJUoqhEAIgabrhBp13nkul9lfuZTBQ1/C66y3tEi0dgdd7yHw3QdN9lXeRSA7h9yWFLBRpwzvvN+mMOW7K6g5UNvN2EzbTsUEZMdqK1Gs9BcXIMC0WSlHofgMEQ13XZ75wO4K5cBQKI4lZl2LOJaj5U+rUFbLc1LWd2tCOTAUCsUJy75tpfz7V1ZUA1i5+uf8zxmc9aXTbNkbPGkWsAlpSqSl7oeUEiEEThf0H3f8rJYD3H3V71j68goMw7r5/+6rf+baOy7nmp/YS804UtQc2M2/77qXjSsqkRJcbsGMS0dyyfd/jsORvsOp74jeOJwa5btLaKqLIFtyR7wBJ7nFRQwePyBtm06Xk34je/Ppks3UVTZgGtbk1+VxkVOUbcsmwOBxA9iyajs1ZXXEotaLtcPpIKsgk4ln2kv1UXQmpyibzLwga95bRzxq0qLMSWu8uERiGkDMQAiBw6nz5+//g4kz9nL+dRXkFmXjcLa+Dklqy+vZsUHjb79+FIARU4bwpTu/QE6h5cgwTZN5j/yXD57/kEizVcK1uSlMblE2urNz+tLAk/r1nPMCEM6RyOgqMCtaKiVhVU7ScsBlT/tGofgs0X9kbzYs25q0TWiCk05NX0RXoVD0IFpf0DLArKJzOVUBOMHZfbW/o6KBIYQYIIQ4IIRYKISY37Lth0KIxUKIfwsh7C+ZKhSKzyVNdU089M3H2pwX1rYQL/7+dRa9uNyWzb6jL6D/8Eib86IVKSVDxxlk5A09rDH3JH+47W8semFZm/MCIBqJ8cSdz/Dha6uO4cg6YxhxHvnGHWxYXtmmURGNSBY8vYGX77/blk1/ho9ARjWNteE25wVAc2OMaHNZ2mVZW+k3ojc1B+ranBcA0XCUmrJaRp86vIueqRl58lCq9te0OS8A4rE4VfurGTppsC2bis5omsb2NZuIRw3odO0m6plIKaGlmsj6jwop3yustJOW86i2op66qgYWvd5+Dm36aBsP3fJ4W5nZeY/8l7f/+T6RZsuB4MvyEYvEKN9T2WmXmiY496aeS5UBkM6JYJa3Oy8AZByMClsiwwrFZ40JXQhcS5s6VgqFoufQNA20Qjo7L8B6gMbAc3n3No7EwFLwtpTydCnl2UKIfOAMKeUM4BPgkqM4DoVC8Rlg6byVNNQ0JW1755/vd5rYHiofPP8hKxZkMWRMGLfH6u/PMBk6Nszbz2Z1cpYca+Y/uTDpdiklf739X0d3MF2wfuErlO5MXolh8UsbCdVXpW2zrqKE+spqMvNAb1nwFhoEs8DljrJlRRKRw0Ng2+qd5PXO6SC4KvAGveT2zmb1O93XJU/G6gXrye+b26nUrdvrIr9fHusXb7RlU9GZmooKasojKVoT7wOxSAwpJdGIg6cemcCnK7MJ1YeR0mT3Zpj3xGi2ru+s+1G+p5LVC9YTDkVY9MKyTm26rlHQPx+Hy0G4yaoE0ntoETf/9suMnm7P8ZUKEV0KWlFnAVLhtkqRxtb06L4UihOR/9z9fJftlZX2U0EVCsXhY5pxMEqAgwXXWyIwQv/o1sbRTCE5QwixCHgR2AIsbNn+DvBF4LmjOBaFQnGCU7J5X8q2ir1VhJvCeAPetGx+umQTkWYnC+cV4vbGyMwxKF3nZPMa6yb7yQcbGDDa3up+TxNuSjVhg8p91UdxJF1TsnFTyrZoRHJg50YGjpuRls39W9ZhGpLMHMjIBtOwqkWIFpd8yYYNDJs6Jy2b0UiMsl0V+DK8+DK8GHEDoYm28P+STanPt67Yu2U/Lo+Lgn55mIaJhLaysiWb99uyqejMslcXJ5ak6QppOTFcHhf1NR7mPXESzcY0pp1/Eo/9319TdivZtI9e/fMIhxKvPafLQX7fXM784kzmXD+LjJygna/S/dDjO0G4QC8G2bJ6JVpeAuM7jsg+FYoTie6iLF59+B1u+N/0xbgVCkUPYe4F2Qw46ezEaHmJM1K/N7ZytBwYpcAwIALMAzKAljpm1AHZyToJIW4Gbgbo1y91iTSF4kRkwO2vp/X5Xff1rLDeiU5XFSw8fjcuryttm3l9ctv+HWl2Ur6vc3ZbcTf15buidMcBFjy1mJJN+wjmBJh+8RQmzLavgaA7NOJRIyHSRAiBN+BJ0at7Qo3NPHnns3z05sfEYnGGTx7CDXdfTe+hiSVbD4XMPEsc0TTi1gReSoQm0HUHmq6RmV+cvs381rGYCAx0XWJ57nVAIyM/fYE2p8uBP8NLU70VLXJwKdbMfHuioxm5QaQpaaxtIlTfjETiDXgJZvvJsmkTrEiblfPXsuzVlTTVhRg4pj9nXD39sCqGnKgMmzgC6/inmri0nh/ttJXFlVY62uJ561i3ZCd15fUEsv0JWhZgnQPBnIBV0STFJCmvT25azgsZ3wXh16y/tRyEZw7C1UV1Iq3D65I4aIya/UpBCsXnhSlzxx3rISgUn2+0fOv5JeNYVUhan6c64ACt+3e4o5JCIqWMSCmbpJRx4DVgG5YTg5a/k0p4SykflVJOllJOzs///L2UKRSK1Ey/eAqalryo+ykXTEbXEycg3XHhN87GncLxkd0ri8ld5NZ2xeaV27nvuodY8vIK9mzax6dLN/PYj//FCw+8ZssewICT+iVNk5FScqZNEdNoOMq3T/4prz86n/KSSmrKaln22kpum3EH29bstGVz0nlXoOtR4lED05BIE8y4JBaNMXCUi5ziQWnbLB42gYGjXSBjWOrV0vpbxghkmow769K0bQohOOWi5BNHIQTTL7ZX8nbqeRM40FKpItIcIdocpa6ijrJd5Uw4y74D6z//9yJ/v+MpNi7fyp5N+3j/uaXcd93D7Fy/x7bNE5XB40fiTiPYSndobZE1FXurqC6rpaG6gdId5USjMUp3lhOPxDv1cbqdTD13Aln5mYyenlxrwu11MfmcQ58cyejHyLofIcMLrOiJ6Epk/b3I0H9S9hHus1K3eVK3KRSfF8Z3I4580ilKxFOhOJZomh/0/lhxDa3vcBLLmREF3w3d2ziiI2xBCNFxOeJULAfGrJb/nwUsS+ikUCgUXVA0qBdX335pwkr58MmDuegWe9VCPD4P3/nL1zpoIFj4gl5+8q9v2x7rs7+dRywaT9j+7n8WUbrzQJIe3eMNJo+yEMl9OofEU/e+RNmu8oTt4aYwf/7uk7ZsNlSVkpFt4HB1drZkZBuEQ3FM00zRMzVSGlx/u0Z+cWcBKF/Q5Ka7ojidYVtjveDrZzNyWmehVl3XuPKHF9NnWPqRIgB1FQ14/G4OjgBwe900VifXcOmOnet2s2TeioTt4VCEF35n3yl2IuN2m20pRIl0/u2NuHXONTc0E24Kk9s7p+0+klOYhe7QqK2oa/u8y+Pkxnu+SCDLD8C1d1xO7yGFnWx6fG5uvPda/Bm+QxqvlBLZ9FjLCtRBbc0vII3k9wXhGofwXZ14obung+eiQ9q3QvFZ5kCSZ5hCoTjOMJo5+NlsIcDY3m33o5VCMlMIcTeWq2WxlHK5EOIDIcRiYA/w4FEah0Kh+Awx49JpjJk5klXz19LcGGbY5MEMnZj+in5HZl52MuNOH80LD7zOgV3lDBjdl8u+cz4uT/opKQBlu8op3ZHaSbFmwXqKbkw/NWXHmt24fS7iMQOzpTqC7tTRHTofvbmam+69Nm2by7qoXrJ1tb38+jXv/JeaSi9Oj0FB7wgIEyOuU1PhZf9Ok/1bVtNnxKT0jMY3k1cY4o6/uln3YZj9OyXZBYIJp7nx+DWIfgSe9J1YLreTbz1yE9tW72TLyu14/G4mzhlLVn5m2rZaWbNgPVkFmfiz/DQ3NCOlxBvw4PK4WLNwPZd/94K0ba5esD5l2451u6mrrO8yxeqzxtqFy2ioE0gpCebEiUc0NA1icYg2J4/EmnHZNLau2oHL527TJAErdahwYAGRUIS5X5lNVn4Gk84e18kxkZEb5Cf/vo31izdRsmkfGblBJp09Dl8wjTAQYwekcFIgJUSXgze5Q0L4rgT3LIgutaKQXBMRjiGHvm+F4jNMV89bgCXzlnPqxdOO0mgUCsXBmPEDIMsAN1bUhUnHNGCaXwdv15G0R8WBIaV8A3jjoG2/Bn59NPavUCg+uwRz4PSLNoJZD570tQ+SkZET5Ia7e0bkqztBMdNIPwIBaEsf0R16WypNa1h8a7nHdGmLhpDt/xaahhDYquoC7d/PiOlUliWuTptG4gp097R8d6dg5BSNIWPj6E4Nj7/Fmy/tff9WhkwYyJAJAw/LRiutv6Oua9Yqv2zX17B77Nv7SZAh6/sKjyXueBh2T1SMmFWiVghBY40VPdXd6eoLenD7XOhJ0tAsHRkv5944G6creZV3TdMYe9ooxp42qm2blM0QXQUyDM4xCL0XuzeUULJ5P5l5GYyaPqxDaltXA4wjw/OR8W3gOQ/NmZiyIvRe3b7gKRSKRBpqQsd6CArF55yOJc87PoO1Du1dczSrkCgUCkWPYjY9CY0PtWghAKF/YjpHQ/bf0DT7QpY9Sa8B+eT1zklZGeSkmfbycQeM7svmj7ZhdJqsGjicOuPOOMmWzfFnjKFk0z7isY4PDwNd1xg4xp6Q8kmnnc6rf0lMdwDI7uWid7rRFwCOocQNL5Ulu4k2R9s3uxzk9cnDnT3R1liPBCedOoJ1izZSW17X5gQSCDJyg5x2+cn2bM4YwYL/vN2ygt/hWAk/fUaMI7tXVg+M/MRh4pyZuDwPEQ0fopNNwPwn36epLkRteR15vXNw+9ydPjJ8ypCUzotkyMgiZOOfLOcFEGoS/PVXfdm8pt1udq9Mbv7NdfQf1Rf0gZYgp1nT2ZB5AMwqSxMj8gGEnsR0TYXMv7Q5KBUKRWqyCzKpOVCXsn3u/5xxFEejUCgORnMUY2p5VjWSThiAA9yzu7dxREamUCgURxgz+ik0PtDuvACs+ojrof4nx2xcB6NpGpd++7ykgqNTzhlP/5F9bNmdPHd80koIUsLpV023ZXP2NdOTrlybpmTiWfaU2/uOmsrJ5w1I2C40uORb56Pr6fvRhXDx9rN9OjkvAOLROO+9GMTk+BF9LhrSi4aaxk4RLBJJY10TeX3tRQwNn9yLMVMPcl4Aut7ExTfGknf6jNN32KFpTwBkFljpNb4MLw6ng4qSqk5RKy6Pk4vT0NGR8T3Ixt+3OS8Ann4ANq/cC2a7RnnNgTr++N0niEZiCKEj/F8+yFA9mJVYr2Ytr2dSQmQ5NNxzyONRKD7P3HzPdSnbNP0wRKIUCkXPkUT/ySIO+rBuuysHhkKhODEJPQYyRah8ZDGmaSc14cgwYfYYbvvTzZx06ggycoP0GVbEVT+8mOt/eZVtmzvX7aF4SCEevxshBJqm4c/w0md4MWveTa2R0BWr3l5H3+HF+DN8aJqGEAKPz03xYCsU3i7X/vI+rvrRmfQZ6ieY7WDUtDy+9chNTDk/fZ0OgPKSSl7/Bzz36Fh2bc2mqdFFaUmQN58ewav/6MP6Rd3XED9arHxzDb365xPMCeJwOtAdOoEsP70G5LPm3XX2jEYWctOdMS69WVI0QJKRDWOnS75zv8nI8auR0p6I6YlKbWU9+3dIcosMPD5LzNPhlGTmGbSFpwoQmiCnKIuCPpbjSAhBfr9cgrkBnC4HmXlBJp89jh/87RYrSuJQiczvdC+qr4bVi1r2a3ZeCW6obmT1O59Y+3fPQmTcBa7xoGVa6UA4sPKAD97Hfw99PArF55gn734mZZtpShprG4/iaBQKxcGYRh3IitQfaPxdtzY+VykkA25/Pa3P77rv/CM0EoVCcdgYpanbZMQKzdaOn5X4oRMHHbbAaEeq9lfjDXiSVseo3Fdly2blvmocLgdFgxNFRatSpMAcCrruYNY132DWNd+wbaMj1aVW2P3OTbns3JSb0F613/5Ye5rK/dXoDp3sXplk98pMaLOFcQDdAWdeITnzCuikpyCj1qq/Xpiq92eOsp3lGIZBQ60VheFsydoIh8Dtg3Gnj+Zbj9zEL6+4P6Gvpmlk5mVw+lWn2hJUBZBG56oHNRUd/RmJjtSO6WTCNQ7hsqKbzIrzgRSTK6kmXQrFoVBf3cW1ImH7J7sZd9roozcghULRGWM37cKdSZDdVxL6XDkwFArFZwi9HzK2ESNuYBomUoKmCUvUUvdZ+eXHEasXrGP+kwvbqhacctEU5n7ljLTy7DtSOKCAbWt2UVdRT7gpjBACX9BLZkEGvQYU2LSZz6YVm8CsBrMRkCC8oOXQa4A9DQyAWLSZtx59gKWvrKOuKkbfoQHmXH8Ok869xpa9gn55aJpImkID2P7+ADK2ARl6FuLrQfgQrpnguxqhBbvvnGws/fPZtnon5SVVREMRJODyuMjvk8Mwuw4tvTemNKmvaKCxrgnTMHF7XWTkZuAN5oCWY88u8P5zH7LwmSVUlFSS1yeX06+czqwrpyMOpz7vEabv8GIcTgfhpua2EqlgRVg4PU76DOtNVkEmbq+LyEFpR6ZhUldRz3//voCFzyxh6MRBuH1ulsxbQU1ZLS6PiwlnjuF7j38dX6C9ykjH6/mcq0qYcV49GblBhBDkFoLDAfE4QOL1nfL81IvASIx0MgyTxnoHP7/odrIKMplx6TTmXD+rgyDo8Y809iNDT0N0BSDBNQXhuwah9z7WQ1N8xsguzCJU35y8UcDgHlxIUCgUNtCHYkUaJouUFqB1/1xQKSQKheKExHDdSKTZxIibbboNpimJRePUNkxB044f/+zSVz7isR//i90b9mKaktqKet7867s8+sN/2rY5ZuZIDuwqp7nRKs1pmiaNdU2U765k6nkTbNmccdk4HGKfVdEFk7YqF8Y+zrjSfpnGx7//A97468fUVsSQJuzZ3Mhff/oCi57+sy17OYXZjJuVfAWtcGABI08easuujH2KrL8LYp9YS+hmIzL8JrL+DqSMdm8gCSOmDWXf1lIiLc4LgGg4yv7tB2wLo0rnTPZtbaC+qgEzboKESChKRUkle/echBD2Sv6+9NAbPPOblzmwuwLTlJTvqeTZ//cKLz6YXvTi0caf4cOfYXRyXoBVOSfaHOXS75yDx+dm+sVTE9oP7K4gVN+M7tQxDJMl8z7ihQdfo3JfNVJKIs0Rlr22ku+cekdbv4Ov5w/n51JV2kjFXivyKZAJk89sOdpaZ0HV3KJsxp+RYvXXf5MlDtMBwzCJxwyWvtUb05RUl9Xyyp/e4sk7U4fJH29IowJZ91OILLYihGQMIkuRdT9Bpiolq1DY5Lo7L0/ZpmkagUAa5Y4VCkWPo+leEKnExiX4ftC9jZ4dkkKhUBwdVi1o4qk/jCUSbl+FlBI2r8nnr/cMP4Yj64xhGLz2l7eTtn26dDPbVu+0ZXff1lIy8oKdVsZ1XSezIIPta3bZAM4tGAAAIABJREFUsllUvJYbfx4ho0PwitMF511ncspZ9nQ1dqx+n3WLk09SXnt0IfFYxJbdL911BWNnjuz0/fuP6sM3H7zBdrUGGfpP8hKs8RKIvG/L5jv/+gChJ45H0wWLXlhuy+anH+7lid8Oo7qi/UXcNAVrlxXx93uErZK39dUNvPf04qRtC59ZQl1lva2xHg1isRhVZal1P16+//cAXPrtcznlgsltgrqhuhBIyO+Xi6ZpmKZJbYWlWWHEOp8H+7eXMf8f71vX85/nd2qrrfLy0t9OomK/5fAAuPJbOpPmDEdztKcN9R5axC0PfQWHM7lzVXOfDIHvWiVxsRKDYhGT1UuKef3fozp9duX8tezd2kUa3fFE+JUWp+hBmI3QPO/oj0fxmeaRb/89ZdvnrcS0QnHcIrt4p4g8123342eJUqFQKNJg0/KtrHq/L6sX9WbizL34M6KsX1FI1YEAsJdwKILnoNKIx4LSHeXUlqcu6bZx+VaGTBiYtt1NK7aRmZdBMDtAJBRBaAK3zxL03LR8K+fYKBUnY2sZcwqMmmKy9ROIRWDQSeAPArG1adsD2PThhynbGmri7Nu0iv5j0q+a4vV7+Prv/ofyPRWU7ignuzCLfiPsh6NLGYXYxtTtsbUIz5y07W5dtQNd19A0gWxJeRGaQAjBLpvCqJuWb+XA3gwev3cavQfU4w3EKCsJ0ljnBqqpLqsltyi9FKqtq3YcVD63HcMw2bJyO1Pm2ovsOdKsfut1zDgIQUIVHSFg5fwdADicDq676wou+Poc9mzaxzv//IDta3e1fTbcFGk7RsmcQMtfX8Wok4dSW5H44rVzUy5//uUpXP29Ycy4dBxux0hu/E2Ayv3V7NtSSmZ+BgNGdy8Mqvmvx/ReBeHXqCop4Xff2kl9TfIV403Lt9JnaFG3No81Mpr63iFja1NlQSsUtmisaeqy/b9PvKdKqSoUxxAzugGIYWlgHPysFRDrfnFHOTAUCsUJidNj5ZZHQmHeedaJNHXcvii+DInudKA7jo8AM5ena42L7tpT4fQ4CYciaLqGN9h5guPy2EshAMvhY8Qa6dW7HmlKBD4kWQhhzxnk9Fj9pJQtWiUSTdNaytkJnJ5DL3+ZjIJ++RT06wmxVh2Ewwpvl00gmwENtCDgsv39W1fbhRCIg0r46Q57GgbOtnNGsG9XZmK7O/1He3fnjP1zCqSMQfRDiG2yfk/3LISeKD5rl2B+eznaZFIdTk/7vUDKKFmZq8matJVP3ilj+5oYsUiMpro6ouG45QFJqvchcTtLcBrPW+eH8Cd+whQ0h4cgXFPatuUV55BXnJ4miaZ5wPcFms291Nc8nPJzdu8dh4s0GyGyEIz9oBeA+wyElngettFVSpPN60qhSEWyKVFHDhZTVigURxktI0VD67O3+2fb8fGGr1AoFGky+exx1FccoHRHKfVVIRpqwlTuq6Fs525GnjzQtjhmT1PQN4/+o/okbdM0waQ5Y23ZnTxnXMq2SWenbusK4Z5BTdl+ynYeoKGqmcaaMJUl1ZTv2k1cTLZlc+I5FyKlSSwSaxNcjcfiRCNxigZ4KB463pbdnkYIHZyTwNgLRplV/tKsgfgeS9TUNcOW3annTUzZNm7WqJRtXTG5i+M7fPJgMnLSFxwdMW0I/szkziR/hpeRp3Rflz0Z0qxB1n4f2fAgMvxfZOg5ZM2tyOY3bNlLxshpM/AFU09ZLv7WmdZYjEpk7XeQjQ8jw/9l4qnrqCvf1XYPCTdFkUikKdvSTFo6goxy3rW7yc9fSL9BpdZ5QudwdE0TTLR5PSej34jeFPTLS9rmcOqMn31Sj+3rUJHxbcjabyKb/mYdz6Z/IGu+gYx9krKPcKe+doTN60qhSMWAbrSFpp2b+p6sUCiOPJqjD4gAia5Gaf3xXtm9jSMxMIVCoTjSeDzlxKOJ5dKMuIGD1KkAx4Krf3wJ3oAnYfsFXzubvN6JZUAPhXNvOpOiQYnlTseeNorJ59hzYGxcUc3K9xKjAkp3a7z9b3tie25/Eb36Ja6yOp2SjHz7lU2ODC4OnpRaxJHYE3674VdXUZik6kR2QSbfePB/bNnsM6yYs798esL2YLafK394sS2bTpeTL/70soSoEF3XuOYnl+Fy23MIyqa/tkz2D97+ONLYb8tmMgaP9SC0RCdGQd8Yg8bPbtnnXyznVAteX4R4rEWstgWHA9peomj9d5yZF0pGtQRWXHWbxOuLWI6tDlzw9XPSjrboCiEE1/7sctxeV8L2y79zgS1H1eEgpUQ23N9SoahjQxjZ8IAVaZMMz3ngHJm43TkcvOf1/EAVn2u+8L0LU7a5vPYjyRQKRQ9iUxi9FZVColAoTkiWvfI6Ob3A64emBjBN8HjBnwkbPiwhHovgcB4f4cn9R/Xljme+x6Lnl1GyaR/BnADTL5nCkPHpa1+0Esjy88O/38Ly11axfskmXG4nE84ay8SzxtgWsfzwlXf5+N1Cho5rZOTERhxOk12bfKxblonHv53zb03f5qr5aykv7Ude3yocWhXxqInL46KxsRebVxtU7quy7cTpSaQ0IfYRaH0scalOKSQ+RPQ9cI1I267H5+GPK3/N079+mWWvrUIakglnjeHaOy7Hn2E/feaSb53LiGlDWPbaKppqQww4qS8zLz+ZjFz7k9oJs8fw038X8MHzy6goqSS/bx4zL59G8eBCW/akGYLostQfiLwHvmttjrad8j0VbFktKOgDQkSpr9ZxuiQ5hQb7dmTw0oOv86MnroPYx536LX/bIKdAw+c3aWpwYEoNjxecbqivluT2LiQQrOO868LM7lDYoP9w+NljJotea6Jkz3QycoOHfT2nYujEQfzs6e+y6Pll7NtaapVRvWzaIelp9DjxjZCqaohZB9GPwT0toUkIF2TcBZHFyJbzQbimgvs02xVzFIpUPPf/XkF00BzqSDwSo7aijqx8lUaiUBwrzPheoAsB96a/QuCGLm0oB4ZCoTghaay1hLq8AetPR6IRSTQcOiwHRlNdE7UV9eQUZeP1J0ZPpEt2QSZzv3IGFXurCGYHDmui2YrH52bWldOZdWX6IpjJaKoLIaVgy5ogW9Z0Hp9hJBd47I7W41Rfkwu0OCoaOu/zcBwY4VCEqv3VZOZlEMhK1CU4dOIgwyD0lvJefqx8TCvyQMpG22KDLo+LL991JdfdMcMqz6r37lQ9xS4jpg7F649SubeUEdOn4g8e/jlVNKgXF37jbKrLasnulXlYThZkyPq+qTAbUrelQfWBOqSU1FR4kNKDGTeIRgTNIcuRV11azoFty8jPNhAdypQ21llyF96AxBswO5Uw9folDyz9Bc7YHyGaKCiWlQ8X3hCGzDMQejZCS080NR3yinO49NvdRyr03LWQgmSVRDoiUx9PIVzgmY3wzO7hQSkUnQnVNwOWWLKUEqQVtYSw4qmqy2qUA0OhOJbE93TzgdRVxVpRDgyFQnFCMnjcUFa9kxiaDlA4wIsvaG9C0dzYzDO/nseqdz7BiBu4PE6mXzSFy75zfsryh91hmiZvPPYu7z29mObGMJomOOnUEVzz08vIzEslZnT0GTR2AJs+qk7aNvAkey98g8YNSNnmDXgoHJiYXnEoGHGDlx56gyUvryDSHEXXNcbPHsPVt19ia9IthAvpGGBVWzFrQMZbGryg5yMc9kvzyth6ZNPj7Q9tvRD8/2OtQttk66pV/Pb6B9m/03pZd7r+woyLB/H9J35t22Y0HOW5+19lxRsfE4vGcbocTJk7gSt+cCFurw1noJYDej4YFcnbD+M37cigcf3x+NzUVzW0VCFpXXkVCA12fbqRX12zgW/dV0Jhfy/BHOucGzgKVn/Q+tHODqXeg324vUEkw5EJDgzZco5EoO4HSCGQzkmIwDeOqCMjFUbc4OWH32TxS8t75FpIiWOo5eRJ5ZRy2NNJUSh6ksHjB1C+p7LTtlZHhtPtpP+oYxC9pFAo2nFMpUu5XX1wtyaUBoZCoTghmXrxF8nulTz8+Nwbz7Ft99Ef/pMV/12NEbciDqLhGAufXcrT971s2+brj77DG4+/Q3Oj5VU2Tcknizby8K2PY5rHT136mVddhz8zUQNDaDD3K5fasjli6hAGphBVm33NDHsTY6ww4QVPLSbSbOVRGobJqrfX8ufvPWnLHmBNwIyKducFWKkkRgXSaU/EVMb3IOt/1XnFwShDNvwaGdtgy2ZTQz0/O//Xbc4LgFhU8t5z23nwpp/Ysgnwj188y5KXVxCLxltsxln6ykc88fNnbNkTQkN4L0veqBeB+1S7Q+2Ey+XEiIdbSp92fCGyBDmDWRLTFCx5M4uasgYaa6zJzcnnuMnKM7BepDqf93NvPNf6h/ssONgpYVZbf0RLxIuUEF2JrP+FlYp0lHnu/ld59z+LEq+F7z7Ro/sRei7CnSKCwn0ywnG8adooPo/MuW5myjaJRNftVX9SKBQ9g+ZwAF1EjAZ/1L2NnhuOQqFQHD18wWy++9hdjD4lvy3yO7fIzZfvPJ8pF3zJls2d6/eweeX2pG3LX19FbUVd2jYjzRHee3px0rb92w+w7oPjR3A0M78v3330doZPahciLOzv5aZ7r2TUzPNt2RRC8M0Hb2DaeRNxuqwIlmBOgItvmct5Xz3Lls366gaWvvJR0rbta3exbc1OW3aJbwStF4j2MqUIH2j5iFgXWg5dEX49uViVlMhme06x5+79I6GG5BPlD17aRqQ5lLbN8j0VrF6wPmnb2vc/pXSnPRFX4TkHEfga6C1pQkJYk93Mu3tM/6ChppbGuhQCkkB5ifX3yveyeffFPMp2NQMSX4aL2x6ayOhTCtvuIXnFbq7/34uYdO411nC1ACLzbnBNaLFmWk4trRdoB+WuxUsgtrJHvtOh0lDTyNJ5K5K2bf9kN9tW27wWUuG/GeG9xLouAIQH4TkXEfhOz+5HobDJ77/xeMq2eCRONHp44oEKheLwMI0o0Jz6A02PdWtDpZAoFIoTlry+w7nlj3+isbaCcGMtOcWDbQtYAuzZkDwlBaxVzX1by9LOna0oqSLclFqsaPeGvYw7fXRaNo8kxcMmcNvjj1JftZ94JExO8aDDtunP8HH9/17FVT++hMbaJrILMhOqXaTD/m1lxGOpNTn2bNibtqCilFFrAqoFQQaAOKBZmhiAjG+3pYEh49tSN8aTO8u6Y8uqXSnbomHJ7k8/ZdjkKWnZ3LNpf0sEQ4r2jfsoGphY9eZQEJ5zwD0HzEoQPsTBE//DZPFL70EXY49G2o/cqvez+XhRFr9dcA/eYB965fq55U/QWHOASKiB7KJBCfcQoRcjMn6ONBuQ8c1Q938JKSdtxLfBYaQGpcu+raVdXwsb9zJkQs+JiwrhAP+XwXe1lUajZSHE8SGWrFAA1FcnVifryLJXVnLaF3pGN0qhUNjA2IT1jnVwGknLc9XY0q0J5cBQKBQnLFKaEF2CT3sfXzCECI9Ges5FaPZKGQa7EdYM5qQ/8Qpk+xFCpJwcZuTan8xV7K3iiTufZsPSLegOjWkXTOK6O7+Ax3f4oqMZucWHbaMjn37wKktenE9dVRP9hhcy64vXUDhojC1bbcdBRkHWtUQ4OEDLAOHt9jgmx2GtKstQy+S0c9lQIbJsjRWti36aPV2RrIIAUAnSCom2EAhhCdflFKZfNaS7c/twRWeF0EC3p3fSHQNGd50vKzRJLBpH0zQ0XcPt1nH5BiC09ol3ILsXgeyuHTRCC4I+ECkEELUqb8gICAeIDOv86eI8kdKAyEJkdDHICMI5DjznIjT7OjjdHZeOx3Xv1lLef2YppTsOkFOUxczLT2boxK4dlFJGIfxOiw6IgXBNBvfZCM0Huj2HlkJxJHE4HcQiqSOyBh2BakEKhSIN9D4k18CQ1nbR/buRSiFRKBQnJFJKZOPvkQ0PWOX7YpuQoReQtd9HGvtt2Rwzc0TKiVyfYUX0G9E7bZtZ+ZmMOjm5uJ3T7WTyOePTtglQsnkft0y9nfefXUrF3krKdpUz75E3uXXqTwiHuldwPpq89sg9/OG2J1nzfik719fz/gtbuPeaX7J52Vu27PUeUkT/4V4w9lqVEWQYZCMY+/H5m2xFtAihdV0hwWb1BOE+01ZbV1z+Pav0aKswnfXH0nvoN8xLXp/0ReqGThxIfp/k1WByi7MZPqV7Ua1jxchpY3F7U8fHZOfHkabEiBvEInHGn97XdoUioeeCo3/LuVdnnXumde4h68CdPP9eSgPZcB+y8Q8QXdtyv3oGWfcDpFGZtM+hUDy4MGVJVX+Gt+1a+OSDDfz6yw+zZN4Kdqzbzcr5a3nga39hwVPJ09usMUeR9XdZArSxdRDbgGz6B7L+J0iz61VuheJYceHX56RsE5qgz5CiozgahUJxMJqe0+KkSLawJ8H/ze5t9PioFAqF4mgQWw2RRYnbzTpkkz0hR6fLyVfv+xLeQOcIhqyCTG741TW2bAJ88Y7LKRyQf9C+HNxw99W2yx3+4ba/E6pP1Doo21XOf+550ZbNI0HFnk28+bePE7bHopJn7vuXLZtSmlz/4xKyCzo//NxeuPGOAzidTbbs4rsGnKM6bxMC4b8B4bA3gRfu6QhvEv0Q96ngOdeWTYc7jwGjnAlZDL6gpN+okbZsaprGjfdem+DAC2T5ueneaw8rNetokFsc6FgFtQ2Pz8Trbz9PsgskofBhVsuQYRIDWAVIPXUp0egSiK5K3G5UQvPThzWc//nlVeQWdRYa9fjc3HTfl3B5XBiGwdP3vdQmTNyReY+8SUNNCmdE+C2IbU7cHi+B5pcOa8wKxZEi1Jg6t16mqnqgUCiOLjJ1lBTG1m67qxQShUJxYhL9MHVbbCVSRm2JBA6ZMJBfvXo7K99aS9X+GooGFTBxzlicLmf3nVOQXZDJz57+LmsXbmDPxr1k5ASYcu4Egtn200c2f5RaW2H5ax/zlV990bbtnmT1/NdTyhOU7W5m3+aP6T18YnpG41soKKrhzr/DmkWwf6cgu0AyeTb4AkB0OXjSr0QjhBcy7obYJ9aKs/CCewbiMEPlhf9GSwMiuhQwwTkZ4bQ/if74nXWY9GPg2Ebi4QriMRNPwEPcKGT3plrqqxpspXz0G9GbX778I1bN/4TyPZXk981l0tnj8PiOb42D3Rv3UlUaRXdq+IMxIiGB5pBIQ+DxSXLyJd6gG8MIUF2Vz6dLdhKNxHC507+mpbEPjAOg9QXZBEQA3apIInSILAXfFxL7RVLfr2RkKSJwa9pjaaWgXz53Pv99Vr+7nv3bysguzGLK3PH4gl4AdqzdTW1FfdK+sWicdR9sZPrFiZopMro09ZijSxH+62yPWaE4Uix9ObnAMwAm7Nm8j37D04+mVCgUPYMZ2w2kWmgS0DwPAl1HYSgHhkKhOCGRbaUuW6oCYILwAM4WQb/Uwnbd4Q14mXFREMww6JkIh33nRSu6rlM0sIBYJEZGbhB/pu+w7Ekz9UpSspXWY0XHsRhxAyllmxYBQDyWWuA0Ndaxd7pg1OQqBo0I4fS48AYKsFbC41137wIhBNXV/di22sTjdzPy5BycPVB1LxwpYOOHozBNyYipvQnYlNSA9t/UMAIIZwCnEwyzXVcyHrP//d1ed9LJ7PFMa767rjsJNWiYhokMS6QUhJshuj6D4iGWLogQ0FTfwL9+8f8o6NeLc276EkjBxmVbaW4MM3TiQHIKsyndeYA9G/cRzAkwYuqQ9giU1nNLCBAB4GAnZKrfvqtjYv94teJ0OSkaVICUkuxemZ2iyLq7H6Q8X7q8jg5/zIrPJ1LGIbbWilZyjEDo6Wv2dIVpdB1lEarvovqBQqE48sjWNOfWMFJ50P+7f4dVDgyFQnFCIlwTkeE3wTwAsuVmJ4S1EuqeZa2m20AapciG+6ww6dZtzlGI4A8RNkUXw6EIT9zxFJ8sai+ZmluczU33fYn+I/vYstlvVB92rN2VtG3saaOSbj8WjJoxk3l/XEYsGusQiWGgCUFeHx99RtqYLDuGEY3A/m3biEVbH3hRKvc1UDQwD192mhEdLZimybO/mcfil5ZjtjiIAll+vvTzLxzWb7roxeW89PvXCYcsZ43T5eDcG89k7lfs6WqMmj48pXZB8eBe5BRmJ237rDJk/EACWT6qy2oPcuxZgmD+ljQt0zDYu6WEWMTgxYessrCP//RNCvr2wuH2t/VxedxEw+2lFnOLsrnpvmvpP6ov6H1Bz7NSP5LhnJR0s3BOQCZLIQFw2jtfW2mqD/H4j//VqQR04cACvvbbL9Orfz6DxvbHG/DQ3JiojaNpglHThycfs2siMkWlHHGYY1Z8PpGx9ZZulVnTvs09CxH4JkIc/kIBwOgZw1n+WmLaIoDu0BgxZUiP7EehUNhDcw3HxENiKdWW57frtO5t9PioFAqF4igg9cFg1rY7L8CKvDAbQMtP3bErm9JE1v9fJ+cFYInXNT5oe6zP/mZeJ+cFQNX+Gv74nb93miilw43/90UczkQfdCDbz3W/uMKWzSNB/oDxZGRHE9JIpJR4vC50PX0/uhAunry3roPzwsIwBC8/GiNm2JvAz3/yfT54YVmb8wKgsbaJv/7k31TsrbJlc+vHO3j6vpfanBdghe2/8qe3WPX2Wls2R04bmlQYVtMEF99iT1fjRMfvr08elSQlvoB1jpVu30s0bNB6ykkJoQbJ7o0HiEetKI6q0lq2rd5BfWW7lkVVaQ1/uO3vhEMRS+zVlyJ1wn0Kwjk0eZtnNjj6JW4XHoTvqkP+nsn4x13PdnJeAJTtLOcPt/0NwzBweVxc8LWzk/adefkp5BWnqNrkOS955RgtA7yXHdaYFZ8/pFmLrL+nk/MCgMj7EHqqx/YzdHLqyjpG3Oyx/SgUisOhCxeEI7kY9iH2VigUiuMXEV0IWiFo2SCcVv658IPeG2JrrRKr6RJbY1UTSEZ0rZX/niZNdU2snL8maVtDdSMr59ubxI47fTS/nPcjhk4YhNPlxONzM/HMsfx+8a/ILjiM/IQe5vn7X2XDqiCZuZJAhonTJcnKNXB5NFYtNNm3vTRtm5s+fItX/57FE/cVsnODh3CTRuluFy/+JZ+nHsrntYfuszXWD55LnvMfi8ZZ8tIKWzYXPrM0ZQndhc+k1hjoCiEEX7v/y1z0jXMo6JuLL+hl9CnDue1PNzNmpj0RzxOZ2sq9VJbGcDhlJ2FToUk0XdLcUInTrRMNGzic0JoNYhpYpWhNSc2BCkzDJFRnCeMeLGzZWNvEyres61i4ZyIy7gTnWKt0ql6M8H8ZEfheyjEK4UFk/ArhvRi0HNAC4D4VkXkvwmG/rGPl/mrWL9mUvG1fNZ8usUQ4z7j6VL766y8xaIwVjdF7aBHX3H4pV/7wotRj1jIRmfcgPHOtkr9aEOGZjci877B1YRSfQyILOoSOd0aG51sle3uAp+99ucv2yoq6HtmPQqGwhxmvJbUGBtB0T7c2VAqJQqE4IZFGKQgNRC5wUPlHs87SxRBpVvgwyrpv19MT/6ouqyUeS53PV7m3Oi17HRl72mgeWHS37f5Hg/3bygCNrZ8kS7+R7Fq/l96D0ytrt2ON5fRZtyzAumWJQqh7N6fvFImGoymFDgEqSuyVuqzYm7qf3agOsDQP5n5ltu00lM8S+zatxTAEugN0R6KzqP9wyZU/PpufXbil0/aOfqVYJE48ZrQ5m4y4gTQlQmv3iFSUtB8v4RqPcKVXAlloAfBfj/Bfn1a/rqjcW5XSQQadxzxh9hgmzB6Tln2h5UDgZgQ32x6jQgF0/XyVISt6Uk9eyjkd4pGu9VmWv/IR59941mHvR6FQ2CT+adftsvv3YuXA6IIBt79+rIegUChSIPTeGKZJfWUDTfUhTMPE7XOTmRfE7e9lVZBIF70YkDRUV9BQ04QRM3G6NYI5Gfgzc0FPv358TlE2DqdOQ00T9VUNRJuj6A6dQJafjNwg+X3tv7Dt21bKm4+/y4YPt+BwOZh41ljO++qZZOSkX4GilcbaJt547B1WvfMJsXCMkdOGcu5NZ9JnWLEte32HW/2MuGmJK0qJEALdoaM7dAaclL4GyJCJE4Dlqfc5Mv2xujwusgoyKd9dQV1lA5FQBKEJ/Bk+MvOD5PfLS9smQK9++ezdktyhUtDXns0TDRlbj2x+HmKbrFV89xng/YKtKkHJ6DtqErrjrxhx0dkr0eJ7iIRq+OVVD4OQSLNlo+zwAcDpduBw6gghMOIm0jTZu2V/h2s1QIHNc+BIktcnFyFESifG4dxfUrH89VUseGoxB3ZXkFucw+lXTmfGZdMQB9f1VSg60tXzUwuAZv+51RGnx0EsnNqJMe2iE0ukWKH4zOEY3XW76P65pVJIFArFCYmhn87+HTXUVzVgxKzV0nBjmAO7Kyk/MAUhbNzenGMp2VJLTVkD8YiJNCHabFK1r5aSrSD09CfG/gwfxYMLqSipIhKKIKUkHotTW1FHfVUDk84el/44sZwX99/4Jz5+dx3hUITG2iY+eP5DfvfVPxNqsKeyHg5F+N3Nf2bhs0tpqG4kHIqw+r313H/TnyjZnH76DMCl3zkfZHsFEqDtNyjon5t29AXAsGlz6JtCh82fITn/1h/bGuvoU4ZxYHcFzY3NmKaJETeor26goqSKky9ILs7YHbOunJ5yYnfGNafasnkiIaMrkfV3QfQTkFEwqpCh55H1d9tL80pCRk4hQ8eZJAqtgMMp2bfTRSQkyMmLWx9p+ZimW/8QAnKK8tF0DZfHZVXlaHEKtF6rdZUNTD7H3rV6JMkrzkmZNlTQN5eTZozo0f39928LePIXz1KyeT/RcIzSHQd46r6XeOGB13p0P4rPIO7ZKRcWhHtOjzk0r/zJpV225+XbE+NWKBQ9g+bIAtHFdRi4s3sbPTgehUKhOGp8vKCUpx4aRlND+0uPaQpWLyni37/ruoxaKipLtvDnOzMo3+futH3XFh/VsqqGAAAgAElEQVR//rmguak2bZumaVJXWY8v6KHjiq/D5cATcLN3cwrNjW548/F3OwlDtlK+p5IlL9vTa1g67yPKdpYnbI80R3njsXdt2azYU0lun9y2sqmtuNxOcgqzMQx7JV9/8cqvKD5IOiAjR/LzZ27A5fIk79QN5SVVBLL8dDxOuq4TzA2yc90eWzaHTBjItT+7HI+//Zxyup1ccuu5aYfzn4jI0L8SHQsAsU8hlqIqhw3KSnSKB0Y6HjrcPpPeA8PUVlj3iIpSB1l5sbZ2AXj8Ek/AQSxiOVPi8Thunxvd0X6+OpwOvEEPJZvsXatHmuvuuiJB1LX3kEK++eAN7eVfe4BQQzP//fuCpG0Ln1lCTbnSFlCkRmiZiIyfWRowbRsFwjMbfNf02H7++6i9Z5VCoTiKyFRR0hqYW1K0taNSSBQKxQnJhg83s3trDn/631PoP6wGtzfO3h1ZNNa5gd2EQxE8Pne3djqycelC6qqcPPGbfhQPaCYjJ05VmYuK/ZadHasWMfq0C9OyWbrjAHWVDeT1ySUWjRMNR9F1HbfPhRCCT5duZtDY/mnZBNjwYeob/Ialm5lz3ay0bW7syuaHm9O2Z/Xbgj/Dy4CT+tJY00QsGscX8OANemmqC7F3S6mtUrK9+o/k0fUv8PFbz7Jp2SqKh/Tn9Gu/aWuMALFojK0f7yC7MItgbpBIcwRN0/D43Qgh2LB0M6dcONmW7ekXT2HS2WPZtGIbpmEyfMoQfEF7ZX5PJKRZDfEuHD/R1eA6/HDuLSs/orZcUIuXogFhMnNixKIaGz7yUl/tR9NA08GIa1Tud5GZG6VXPx+aw00sng9ojJk5kqETB/Lc/a/hcOopr9UhE+wLbh4p/Bk+bn34RvZvL2P/9gPkFGbZuqd0x7bVO4mGY0nbTFOycdkWpqvwfEUXCOcoyH4UYutANoBjGCJZpZvDoLq0psv2955dyhlXTu/RfSoUikPHjO8DagEPEAcMrCUFDXBA9APgK13aUA4MhUJxQtJaQjQaNlmz2IU0HLh9Em8QdF3rtIJ6yDYdVh16KSV7tjiR0mHpNTgt3QanJ/2Vfaer9TYrcTrDOB1hwAHCAegd2tMcq8sBSSIw2tpsoDv1rvdng9bjpGkaGbmJOc4ut9OWXYBIuIlwQwMujxsjFqOx5gCBbHvVETRdQ3foGLE4TTVNNDWE0DQNQQaegMf292/F7XUzblY3eZ9pIo1KqwShrAfHcHBNQ4jUx/Do03I9IWluCBMJRdAcGv5MHw6Hw6oe1AN4/O1ivft3uNm/w00sBm3hGAdl8NRVucjt3QdN6G1VS/oO782E2WN46aE3rZG7HAnXZrrXaqixmefvf5Vtq3eSU5jFFT+8yFbK1KFSPLiQ4sGFR8y+o4v7A1iRRV0hY1shuhyQ4JqKcA7vwdEpThSE0MB17NKxsvJ7RmtDoVDYxYX1YO6Q0wm0JYYcwruBcmAoFIoTkolzxvLWE+9Rc6COthtgdQMur4uzrp2J05X+5GjsmRcg7nmdSCjW6ZZqxA1yizwMmnh62jYL+uXTZ2gOezeuATqsXprVCL0XE8+yl0Yw8ayxfPD8h0nbJs2x93I4ac5YPvlgQ8r92WHCWWOY94c3Mc3ENIKiQb0oGmTP4VC67RMe/ua91Fa0/6av/mUxN//2BkbNOC9te7quM2LaUF7/y9uWBkILjbVNBHMCTJpj7/sfKWR4AbLpj9CmI/EqOPpCxv8itOOjjK7QgkTNEVTufp9Yh5X7uop6cgqzCWb2jAZIv5GjyC7QqCprT0fSNDBafhpdt3QuhGb9XC6vhuboPBmfNGcseb1z6T+qD7s37E26n4lpnANbPt7BHRfcS6g+1LZtwX8Wc/0vr+Ly71yQxrc7fhg2eTDBbD8NNYnl79xeV0q9DSklNP0BGe6QftL8EtI9CxH4lj29IoUiBUMnDmTLyh0p2yec8dlP3VMojmc0Rz6mXgjGzg5bJRAFNPBc3L2NIzU4hUKhOJIEs/1Ew1E6e2/BiBk43fbEwDRnBm5foiaDpkM8LtoiNNLlyltrcXsPDr2WzP1iOfl97I31vK+embQqwuhThjNlbnrlHVuZdPY4xiYRBMzrncMFX5tjy2ZecQ4XfO3shO1ur4trbu9abK0rnvjZ7zo5LwCiYcnffvokkeYGWzZ3f7oHI5aoXt9U20R9tT2bRwJplCOb/tDBedFCvATZ9NixGVQKXn2ymPqDK6JJePd5P5UHeq5ChhGLdCqhqnVwWgjN2u5wgKYLCvp0Dlmfc92stsiFq350SSe9klbm3jCbwgGHHur+my8/3Ml5AZYezj/uevawyuceSxxOB1fffin6Qc4fTRNc8YOL8PpTRKhF3uvsvGjb/j5E3jkCI1V8nuk7PL1S5wqF4hhgpnqnMjmU+AoVgaFQKE5Ilr32MTlF2XgDlpaCaZp4fG4C2X42LtuCETcSXrS749U/vsWWNQEK+4bIzotgGtZEqGK/ly1rnHz42ipOSbMahTSqGDRsMz99FN6fJyjZAsEcmH6uZMREEyKLwJv+imxGTpAfPXErS15ewYalm3G4HEyaM44pc8en/b1b0TSNr/72Ola+tZZV89cSi8QYefIwTr10Kv4Mny2bAHO/MpuBY/qx5OUV1FXU03dEb2ZdOZ38PvYmsHs3fkTJlsakbaEGg7XvvMzUC69Ly2Y8HmfLyv/P3n2Hx1FdDx//3tmmVe9yt9x7l1ww2AZsDDaYbkooISG0hFASEgj5pZKEvCEJISGUUELooYVuDBhccJV773IvktXL1rnvHytLXmtXllarZp/P8/Dgnbtz90janZk9c++5u7A6rJh+E9PUgRmZFgNlKD58+nPOmjU2onijzv116MKYAJ5laLMCZcS3akihuKrcLHjvIMs/y2XUxAN07VVKdaWNDSs6sXNjOraUVREnxk606sv5FBy0EZ/sJSXdxFVlwWIzMSwaiwE+r0FMfDz9RmUx6+7r2ZZ3lD0b9xGfEseEWbkMOatuKkP2kO787LV7WfDWEvZu2k9CajwTLs2tVySzIdtW7eJwfv1iuAB+v593Hv+IOx67udk/d1sYdd4wfvqfdBa8tYQj+UdJ75bGpKvG03Nw97D7aHf4oora/RUqpn6CU4hIffXGNw2252/aR3YD71chRMsy/QWgT76zcZyCyqfBeWGDfUgCQwjRIZUXlaOUIjbRic1hxe/zB4p2KoWryo3H7cXZxC/yx4t/Hd4Xy+F99b+wHzsQ7oDbAF0GQFonuOL2EF86deSV+2MTnEy7cXJEBTvDsVgsjJsxmnEzRketT4ABuX0ZkBtm7dMmKi8qaLj9WNP/Tu4qDz6vL1DzxGrB0DWlE9Tx1wydMGkTZgPvGW2CrgTaPoFRXV6Nz+unotTBwk9612uP1u90x6otAFSU2KgIsVBQTKzitf2v1j4ekOuj6FAxcUmxxCXF1Xt+epdUrrhnZsTxHDvQ8AiLkiMde7WObv06c/3Prmj8Dg29X82mr+wkRENMf8PLM29bsV0SGEK0Jf8hAqOnQy8xf/y6uSEyhUQI0SFlD+mOq9LNno372LNpP/u3HWLXuj0cO1BEp+yM8MOZGzDyvKFh25QyGNVAe1iWLmDU/5JUy9r4O7sioOvAkVhtYU58QPbwptcAiUuMJTEtAdNv4nX78Lq9eNxefB4fWkOfEdnNiDjKbP3CtxkpYNSfWtQWkjISSc4Mv9Z7zyFNX30mlGk3X9Zge+fedcnIz1+ez8MX/4FfXfkYP73gt/zrJy9Tdiy604MGTRjQ4CiooWfXn6Z1WrM28H5tqE2ICMQmNrzC0+Rro1N7RwgRIctAjhf5Dt1+6gLPrZrAUErdr5RaVPPvB5RSi5RSryoVpVLkQogzxoCxfTm06wheT13NAtPUFB8txRpBAU+ACZfk0rlX6KKSgyf0p2u/pq8goJQDFRPmbq61J9iaNiVFQGJaFybOCv3Fp+/IZPpEUGwVYNT5w/B5/YGigzVMU+P3+bn8h00vDNpi7GeBJfR7UTkvazcrkRiGwfSbp4RsS++aSs70yGq1nCwlM4vE1PAJrZ+/8TAAn/9nPu898UntyA/T1Kz+agN/u+tf+P31a99EKjk9kdwLR4WONSuZGd87P2qv1REo56zQVeWVFeU8dbE2IZrinqdvC9tmsRo4HE1bXl0IEV2GxQ72c8K0WiHpl6fuI7ohhaeUcgAjav6dAZyrtT4bWAc0fPtECCFO8t4Tn2IYCmXUfXFRgMVqYX2YlTQa409f/YqBuf0wjMDh0WK1MGbaCH77wU8jD9Z5DSr2Ojhel0ApcIxHJf5SKvBH6MoHf820G4YRExv4/VltitzpPbjjb3+MuE+Py0tyZmLQe8pqt9KpVybbV4Wvat/alLKjEn8D9hxq1wE1klBxN6Ocl7RtcCeZPPssrrr/ktoldA1DMeSsAdz79G3NWkL3RB6PF1QCsQnBSQjDgIyufha+uw6f18cXr8wPuf+hXUdY+9XGqMRy3IOv3M3Zl4+rXQ1JKUXv4T15bN4va48tZwpl7YVKfBis2XUbrT1QCQ+jrH3aLC5xevr69UVh2wyLJarJSiFEZIzUp8A2CTjhhotKguSnMSynXg68NWtg3Aq8BPwGGAt8XbP9C+B64K1WjEUI0cFtX7UTZShsdmvgjnnNdDqlFOXFFRQfLSEls+nLSSanJ/LYV7+i8GARh3YdoefgbiSmNm/deKUUxF4NzkvBLACVgDISm9Xnmc5qtXH5j37JjLvKKTqwm8SMzsQlZUTcn8ft5cD2Q6R3TSO1cwruKg+GxcDhDKwSk79hX7RCjwplSUMl/gxtlgRqXhiZtNfBjOdddzaTr55A4YEinAkxzf48nWzX2j24q91ALDEJHpKS3Pj8NipKYygrhs3LtlN4oCjk8p/H5W/YF/FSwaFYrVYefPmHVFVUs3NNPlk90snsEfn7s6NTtuGo5L+g/UcAjWrEBaoQkdi5dk9tElqfsHy3MhQ+r489m/bRe1h2G0UnhDjOSPsXpr8CPIvA0hXD3vgljlslgVEzRWSy1vpJpdRvgGTgeIWOUiAlzH63AbcB9OjRozVCFUJ0EAkpgdEMpl/XFO3SKMPAYlFYbJZmrZqRv3EfC99ZStGhYjr1ymLS1ePDTi1pCqXsYDkzl3j78rWFfPTUXMqLK+jarzPf+r+r6D+6fmHHpnI4E+jct/lfPK02CzGxDlxVbgzDwBkfXEMlPrmBOianoLUX3AvQniWAibKNgZjzUarpdVpOtHHxVpZ+mEdFSSW9hvXknKvGk9JAzYm2ZLFayOrZMl/g0zqnoJTC7/Pjq4Yj5Y6a1/RhtVtJSE0gNtGJYSi8bh/lxRV4qj0YVgvxybHExMUQl1x3vCg6XMyCt5ayZ9M+ElLjOevSXAaOjaxWQ2y8k2FnWs2LBihL84+jQjQkLimWgv3HgpIXAGiNUoq0rtFbvlkIETltFqPcn6G9W8CIQ+NC2XMbtW9rjcC4EXjthMclwPGr+MSax/VorZ8FngXIyckJs2acEOJMNOv708mbuza44rjpx/T56ZfTB3uMPaJ+F723jNf/8F5tHYSteTtZ/P5yvvf/bmToxIHRCP2M8+dbn+KrN+qG9R7OP8qarzZw/7N3MHn2WW0YWR3DMBg3cwzz31ocsn3CrJyI+tXagy77LXjrpihozxpwfwmJv4l4udN3H/+YL15dUPt4a95OFr67lHueuo1uEdRq6cgyuqWRlBHLkfxiAkOxAvw+P6bf5Mr7ZpKYmkDPwd1Y+O6yoGNGVVkVyRlJjK1ZdWfPpn088f3nqK5w1T4nb+5aZtw6NSpLvgohWtbsB2bxx5v+UW+71oHkRlKUR4AJIZpO+w+gS38etEqVdi+BmOmo+NtPuX9rTcQcANyplJoDDAFygOPr/k0FlrZSHEKI00RiagJxSfVHWRhWC9lDIlsirbKsirf+/GFQEUcAr8fHa79/F9NseHk2Ud+ONbv5+s1v6m33+/w888B/2iCi8GZ9fzq9htUf7Tfrrgsjfk/h+iIoeVHLlw+u9yPqct/WA0HJi+MqS6t4+7EPIuqzoysrCE5eHKe1ZvuK5QB4XD4My8mXPYERW66ahMUbf/xfUPLiuE+f/5IjexpevlcI0fYWf5AXtq2iJPw0MiFE69GVL4ZcYlu7akZknEKrjMDQWtdWv1NKLdJa/1op9dOaFUn2Ao+3RhxCtKTsBz9u6xDOKCvnriOrZwaJaQmUFpRh+v04E5wkZSSyY9UuvB5vbQG9xlo3fxNetzdkW8nRUnauyadfFKY9nEk+enpuvYTQcWXHyln79UZGTBnSylGF5oyL4UfP3cmGRVvYlreTmLgYci8c2aypD9oTvqCcdn+Div1Wk/tcOXdd2LZtq3ZRVlQe9ToT7dmWpatwVYV+jykFH/zzc8bPms6BHYfo3DuLqrJqPK5AjZO4pFisNisrP1/H+EvGsGfT/pD9aK1Z+fk6Ztx6Zq0gIkRHs/yTVQ22b1+zm34je7VSNEKIk2mzCryrwz/B8w3YGh7x3JpFPAGoWXkErfUfgcjLxQshzmheTyDR4IyPqVevwO838ftMbE2cReI7YUlWT7UHn9ePzWHFVrNaQrjkRmMt+WglS95fTufeWcz+yaVYLO1jucuTeT1etuXtwuPy0G9072bVf/C4Gv6dVYW4290Uh3Yf4dDOI6RkJdFrWM9m9QWBqSTDJw1m+KTBze4LAB34+f1+P+XHKtBoElLjsVqtoD0RdXni0sGuSjem38ThtGOxBd5PXrcv3K5tRvsLwbcVjASwDo3q6jtlxYFZqCrMSqoVJdW897dP8Lg82GPsOGLtKKWwWA2sNivowKiWmDgHps+PYQ3xudSwb8t+Vs9bT5+R2S2aIPJ6vGxdsROv20v/Mb2JS4r889datHaBdz1oE2zDUEbkNYiEaA7z5NoXJ6looJivEKI1eANzugDwgXYBBignoBp1bdTqCQwhhIiGQeP7s/zT0Bnc3sN6EhPb9LXeB47rh9/np2DfMTyuugNoTFwMXft2oveI7IhiLSko5Y5RD1B0uK7czyu/fZu7//k9Lvz2uRH12VLWfLWB1//wbu2KDTa7lfO/NYlZd02PqL+Jl49jwdtLQrY5nA7GTG181ekTVZVX8+LDr7Nxydbabd36d+Z7f7yRjG7tp0ibso+kYO8KSo6W1o5EOXawmITUeDr1Py+iPgeN78dnL86j8EARPu/xZIUiLimWQeP7kdY5ZF3sNqG1DyqfQbvn1V2wWDIg/j7UKe6wNNboaZOw2p/C5wn9xcXnreKdv32Ez+PHYjWCppEYhoEGXFUu1i3YxOE9hTjjY0jJqiuG6qpwcexQMa5qN2vnb8JitXDuNRO5/J4ZgRWGomj1vPW8/of3aoe62+xWpt44mUvuuCCqrxNN2vU5uvLfoKsDG1QMxF6Pcl7cpnGJM1PPwd3YsWp36EYFo84d2roBCSGCKCMJbe0FnmWgy+uuDZQVjCywjTxlH2fWYuRCiNPG6KnD6Dm4W73tFquFSyL8sp3eNRXTr4OSFwCuShfO+JiIkiIAPxz/cFDyAgJ30R+//RlKCurPAWwrh3Yd4fmfvRa03KTX42POi/NY9N6yiPqceGlu2PoRM783NeJiqy//+q2g5AXA/m2HePKeF9pVrZLFczLI31QZNI1Ga82RPVV8/HJkd/Gzh3anoqTyhOQFgKaytIq0Tu0neQFA1Wto15cn3G0B/AXo8t+hzfKovITVaiWzW+jLGXuMSXGBrTbR4HX7akeoaA2uKjdetxeH04FSiqT0BMqLKig7FojN5/FRsP8Y9hhb7eff7/PzxasL+Or18NODInFw52FeePj1oHn6Xo+PT5//ksXvr4jqa0WL9m5EVzxVl7wA0C505QtoT/uMWZzehk4ZEL5RlgMQon0wMsEsD7420D4wS9HW/qfevQVDE0KIFmOz2/jhk7cy9VuTSEiNx2a3MmTCAO59+jYG5PSJqM8ty3dgtVtIyUrGareilMIeYyetSyrlxZUc3VfY5D73bN7PkT1HQ7aZfpN/3vNiRLG2hAVvL8Xv84ds++qN+oU4G+sv83/DlGsmEpvgxDAM0rukcssj1/Gd318fUX+FB4tYt2BTyLajewvZtGRbxLFG25t/ns8TD/Zi+RcpVFda8FQbrFmUxBMP9uLdJxqeqx1O3pw1JGclk5ieiMVqQSmFI9ZBZo80dm/ci6eZU52iRWsP2v156EazEtxfR+V1ig4XcygfUrK82BwmKFAGJKT48HoU2g/oQOIIRSDBpRRojcVqYLEaVJZWAYFVCjK6p9VMQbNi+jWJaQmkh1h68asQxWmbY8FbSxr4/EU3WRIt2hW+9pOulrpQovW9//hnDbYfPnCslSIRQoSitQ98G8HSBVQsgZO2FYwUMDJRnvpFyk8mU0iEEB2WM97JFffO5Ip7Z0alv6N7ClBKkZAaT0Jq8PKWWmuO7i0ks3t6k/rc8M3mBm/67Nt2MIJIW8aR/NCJFgj8biJlj7Hz4+fvinj/kxXsOxa2MCjUxNpOlrw9drCI0kIbbz7ZhTef7BLU5nCWRdTnkT2FGIYiOSOR5IzEoDZXpZuywrKQX7hbnVkaSFSE4w9dMLOptizfjtaa4qN1I6RMv6asKFC7Rild+345PhKjS+8sSo6WUlUeGDlwYv2b43V1/t8Xv+Df//cma+eHWEWGwFQgn9cXqKMRBQ2tctJuV0DxHwjfZjbQJkQL0aeogTHv5flc/+AVrRSNEKIeXQ5mRaDmhcVZv72h80oNSWAIITosd7Wbz/+zgOWfrqK6wkX/0b254JZz6Tmo/tSSxkivqZ1QUVJJRXFgiL7NbiMhLZ7YBCfpXVOb3OeQ8YHhrFrresNXlaHo3DsrolhbQnq3NFixI3xbhHw+H0/d9xKL31+Bq8JFWpcUrrh3JjNunRpZnF1TUUqFTWI0J9ZoS8lMorQgdKIiIcJCkBndwr8PHU57veRbmzGSAvUQdJhCrZZOUXmZfmP6oJTC9IeZOqRUUK0KpRSG1cBqr7sEOvHfAIlpCdhj7GHrqbgq3bir3Dx04SMkpicy8dJcplw7EcOIfGBrerc0tubtDNvWFOXFFXz24les+nI9fp+fIRMGcOF3ziWzR+Qr6oRkZAH7wrRF5+8rRFM0dG4AmHzVuFaMRghRj4oHIz6QxAilEecOmUIihOiQ/D4///jB83zy3BcUHiiisrSK1V9t4C+3PsXOtfkR9TlofD+8bh9Fh4rxuDyYfhN3tZvC/cdISImnU3Zmk/vMHtqD2ARnyLm3Wmvu/Ou3I4q1JUy6ajyGEboo4ZTZZ0Xc7wPn/ZrPXpxHeVE5Xo+Xw/lH+ee9/+bl374dUX8Z3dIYPCH0HMm0LikMPbt9jL4AuOzui8K2XfDtKRH1OW7maGLiQtdjGX9xDg5nZLVaok0pOyomTJJKxYAjsiKmJ8vomobDGb6Wis1uBUVt8c64pFgMwyA+OS6QzLAYxCUF3wWafPUElFKcfcVYrLbgVUmqyqo5urcQZSgqy6o5tOsIb//1I178+RvN+jka+vxNvrrxn7+q8mr+8r2nmff6IkqOllJeVMHSj1fyp1ue5Oje6I7kUDHhR7+pmBlRfS0hGuOiOxo+rnTt27WVIhFChKKUDeUId21gh5hTXxt0+BEY2Q/KHEtxZmjKez3/0ehMqWjPVs/bwM51e+pt93p8fPDkHO579o4m91l8pBTDCNTXOL5MKwRWy3BVuWuXYWwKn8+HxRY6V2yxWtiWt4uM9jDcH+jWvws3//oa3vjj/6iuWd7UYjGYPPssJl09IaI+l89ZxfZVu0K0aN7/x6dc99BlgSVFm+jmX1/Dv37yMttX11Wbz+qZwW1/urFZd8GjbeoNk9m+Kp85z3+J3x+ob2AYBmddmsv1D0U2jDkuKY47//JtXnj4NUoL6wphjjpvGFfc286+NMbeAGYxuE+oF2EkoxLuQxlJ4fdrouoGluM9vhyy1WYlMSuexIzAyBer3Ur20B5o06xdetEwFBMuyWX6LYHVgTJ7ZPCd313Pq4+8TWVZYLpJydFS4pNj6410Wfn5WqbecA49B4cuWnsq3Qd05cZfzubN//c/XJVuIPD5m3LNRCY34fO36N1lIaecVJZVM+eFr7jpV7Mjii8UZR8Bcd9BV71St/SdsqGcV6MckR0zhGiO9V9vaesQhBCnEntdzbXB/LptRhIq/l6UcerRzh0+gSGEODNtWLQ5bNv21btxV7ubfCd605JtWGxWOvXOxF3lCUwhcdhwOO143V52rMln8PhTV0c+0foFm3FXeXDE2vF7/fj9Zs0QdoXFYmHBW4uZeGluk/psSbkXjmL45MFsXLwNr9tL/5w+pGRG/kVz/n9DL6EKgdVdVs5dx7gZo5vcb3xyHPc9ewd7Nu/n4I7DpHZKpn9On6gvaxkNd/7lZq75ySzmvPgVfp/J1BvOoXOv5k0d6je6N498+BCbl26joqSK7KHdIxoh1NKUsqMSfoSOvQ68WwPDRm2jUCp6lx+nKnBpd9q56r6LmXDJGPrn9OXQriPs2bSf+JQ4Bo3vh+k32bR4G65KF31G9SK9S/DF08hzhzL4rAFs/GYLB3ce4f0nPw1b92LDoi0RJzAAxs0YzYgpQ9i0JPD5G5Dbh+SMpn3+Nn4T/gvchgbaIqWcF4NjCnjXAP7A39dIPNVuQrSIg9sPNdi+5KOVTLh4TCtFI4QIRSkbKuEedOxs8G6puTYYiVK2Ru0vCQwhRIdksVrCthmGqh0u3rQ+A/sopWqG6AcnQE4eSt4YdkfdwVgZBhYUKLDUjBI4ee59e+BwOhh9/rCo9HWqAof2mMadrMLpOahbxDVPWlNqp5SIR1yEY7FaGHr2oKj22RJM02TT0jJ2rfUQl+Qi98JqEtMiq/8RijM+RBGwE1itBilZyfj9gWKenXtnBdWesVgsjDmGUHEAACAASURBVJgypME+7A4bo84bRufeWXz8bJiVVWj4uNRYMbHN+/w1FMPxY1y0KSMeHGe3SN9CNE3DSezEtHZSI0gIgbJ0BkvnJu/XfsbZCiFEE4yeOjxs29CJA7HZm/7FeNg5gwLz5UNISk+gz4jsJvc5ZOJA4lPi8bp9+Lw+/H4/fp8fj9uL6dfM+O75Te6zI7no1vMJd0EZnxLHqPOikygR7VNlWRV/uuVJ/nnvi8x5cR7vPP4RD1/8B5Z9EtkSsqGMv3hMg1/abTE25rw4j7//4Dkev/0ZXFXuiF+rU3YmXfqEHj2jlGL01LZ/P49qIPkx+vzwx00hTgcDx/UN36hgyIQBrReMEKJFSAJDCNEhDZ7QnzHTRtTbnpASx2U/jKwOQEJKPJf+oH7RRYvFYPYDl0Z8dzV7cLeQVdFj4hxkD418uHlHMDC3LxMuqT9c17AYfPs317ZBRKI1vfXYB+zZFLxcqt/n55XfvEXhwaKovU7/3N5h25Izk2v/vX31bt7/x5xmvdY1P7ks5MihaTdOjv4qHxEYf8kY+o3qVW97Zvc0pn/n3DaISIjW871HbwjbltYlpRUjEUK0lPY3dlkIIRpBKcUtj1zL0LMHsuLT1VRXuug3ujeTZ5/VrJoN5113Nt0HdGHB20s5dqiIzr2ymHLNWXQfEFnl8pKCUsqLK+natxNFh0vwuL1YDIPEtASSs5JYMWcNU66ZGHG8HcHDr9/HR898zqfPf0l5cQVd+3biuocuZ/ikhofti47NVeVm9ZfrQ7b5/SbLPlrJzNumNft1qsqr0SZk9kzn2IEi/L7AcqrKAHuMncrSSpLS62oyLPt4JVfdf3HECcl+o3vz0Cv38PWbi9mzeR+JqQlMvGwsw85pH9N5bHYbP/jHd1nyQR6rvliP3+tjyMSBnHPVeOISY9s6PCFa1DuPf4yygvYFb1dWqC5zUVVRTewppp0JIdo3SWAIIToswzAYN2N0REUgG9JvdG/6jQ5/R7cpKoor0VrjTHDSNaH+RVPZsTDrYJ9mLr59GlNvnER1eTWJ6QlYLM2vFQDg8/ooL6ogLim2ySvEiJZVXV6N1+ML237iCirNUVlahd/nJyk9kaT0RPxeP8VHSqgqD6waYtYkNI5zVblxV3sCyxtHKKtnBtf85NJmxd1cFSWV+Ly+kEU+bXYbk66awKSrZCUQcWYpLSjDbreDHbQGtEbVLE/s9/spLSiTBIYQHZwkMIQQogVldE8jJs5RuyziyXoMOv3XpK8qr+adv3xE3tw1eD0+EtMSOP/6c5h20+SI+zRNkznPz+Pr/y6moqQSe4yNsReN5sr7ZjZ59RnRMhLTE0jOSKSkoCxke8/B0Sm+mpKVREJKHId2HaW0sAy/z4/pNzH9GqvNgt0ZnNjK7J6GMz4mKq/dFg7sOMTbf/6QrXk7AejarzOXfv9Chk4c2MaRCdH2+ozsyZbl2/F6fGizbuqmxWYhMTWejO7tY9lyIUTkpAaGEEK0IIfTwZTZoaeIdO6dxfDJg1s5otalteaf97zIko/yau/Glx0r572/f8Inz30Zcb//+/unfPTs51SUVALgcXlZ9N4y/vWTV6ISt2g+i8XC1BsmhWxL7ZRM7oUjo/I6VpuVrv27UHS4GL/PDwRW/NGmic/nr3e39YKbz22Xy+02RvHRUh6/49na5AXAge2HePr+f7N91a42jEyI9mHm7dPwuoOTFwB+r59OvbOwWuXerRAdnSQwhBCihV18xzQuvOW8mqVZA8u8Dp04kLv/8V0M4/Q+DG9dsYNd6/eEbPvy1QW4q5u+IkRlWRXz/7s4ZNumpdvI37ivyX2KlnHe9edw2Q8uIi6prvbCgJw+3PPUbVGb8uP3+zm06whJGYm1yycrpYhLjsMZH0N1pQuAhNR4rv7RLM66NDcqr9sWFry1hMrSqnrbTVMz54V5bRCREO3L3JfmY7Vb6i1+ZVgUR/ILME0z9I5CiA5D0pBCCNHCDMNg1l3TmX7LFAr2HSM+JS7kvPXT0c41+WHbqitcHN59lJ6Dm7YSy74tBxqsrbBrbT7ZQ07v1V06kgtunsK5103kSH4BcUmxpGQln3qnJig6VEJpQRlJ6YkkpCbg83gxLBastkCdlbEXjmLqjZPo1CsTq61jX/bsWpsftm33+r2tF4gQ7dSWpdsxLAYOpx3TNNFmoKCvYRhUllZSsK+QrJ6ZbR2mEKIZOvaZXAhxxtu4eCvLP1lFdUVgFZKzLsttdqV97dsNrrloswBl6Q4x01GWTs3qc8/m/bz627fZs3k/CSnxzLprerML7Lmr3Sz9aBWbl2zFYrMyZtpwRp43tF2N6ohPjmuwPTaCv9WJd/NDtp/iNVub1n7wfIP2LAXtR9nHgGMKSjVvBMK2lTtZ8kEeFSWV9B7ek4mXjyUxNSFKUUeXzW6jW/8uLdJ3bEIMhqHwejxUFB3DU+3BsCrikpNA29m+ahdV5dVROz6EorVm3YJN5H22Fk+1hwG5fZhwaS7OuJgTnmOCZznaswi0B2UbTlHJGL55bx37th4gOTOJiZeNbTD51tB7/1SfizPZ/u2HWPTuMo4dDKwsdfaV48jsnt7WYYkWEJcc+Bx4XV7ME6aR2Bw2rDYL8SnxbRWaEOIEpu8gVPwDfBtBxYHzCozYqxq1ryQwhBAd1huPvseCd5bWPt7wzRbmv7WY+569g7TOka33rl1foSv/UVO+HDSrwPUpJDyEso+IqM9ln6zi0RuewOvx1m7bsnw7Kz9fx33P3B5Rn5Wllfz19mc4uPNI7bbV89YzcsoQbv3jDe0miTF62nDe/dvHIUdM9BmRTUa3phdU6z6gK137debA9kP12mLiHIyY0n6WZ9Xaiy7/PXjW1m3zrADX55D0G5SKrBr+B//8jDkv1k0Z2Lh4K1//dzH3PnUbnXtnNTvujiQuKY7swWkseGc5pr/uC0vpsQIUBla7leKjpVE5PoSitealX77J8k9X125bv2gzC95Zyv3P3kFiWgJam+iKv4L7m9rn7Fydx5M/ew23pwsQGC2y+P0VXP2jS8IurTzu4jGs+Xpj6LaZY6L2M51Oln2yipd//d/aL7MbF29l/luLuf2xmxg8YUAbRyeibdb3L2TJB3n1tnvdXtK7pshSwkK0A6ZnHRR/D3R13UbvBkz3FxgpT59y//ZxhSuEEE20NW9nUPLiuKLDJfzviU8i6lObFejKZ2qTF3UNHnTlk4E7qBH4xw+fD0peHDfvtYVsi7Dw3ifPzQtKXhy35uuNrJy7NsQebSMhJZ4bfnE1FmvwsqnJmUnc+IvGZdpDuflXs0lIDb6TZrNb+fZvriUmth2tQuL+Mih5Ucu3E6rfj6jL/dsPBSUvjisvquCtxz6IqM+OzufagmHUfW5NE0wfoExM01O7vTnHh3DWL9wclLw47ujeQj7452eBB55lQckLgNf+qnBXecAsqt2mteadv35EaWHolVtGTB4ScuRW/9G9ueDbUyL/IU5T1ZUu3nj0vaA78QBej49XHnlH6iGchj5/aX7YtsP5R1sxEiFEWGW/CE5eHOdejFk995S7ywgMIUSH1NCX9DVfb8Tr8WKz25rWqWcFaE/oNn8h+LaArWmrhmxZsYPiwyUh27TWfPjPz/jRc3c2LU4gb+6a8G2frSX3wlFN7rOl5E4fSe/hPVn20UpKCsroPrAruReObFaioVv/LvzqnR+z/JPVHNhxmNROyYy/ZEy7qy2i3YsaaFuIir22yX029N7fmreTsqLydjuVpCWUFh5g75ZSOveEygrwVIOrKlDDTxlQVVqKPaburmvEx4cw8j4L/1lc+flabvi/qwLTRk5wYBcc3lNTZdCsACOjts3vN1n1xXrOvTb0KIxrf3oZ42aOZtXn6/B6fAw5awBDJg5oN6Ou2pP1Czbjrg59TC85WsqO1bvpP6ZPK0clWtLi/y0L26ZN2L1hD72G9mzFiIQQJzL9x8C/O/wTXO+A84IG+5AEhhCiQ/KccFHqrnJjmhqH045hMfD7/Ph9JrYmlxg4xYoY2tXkOI8v8wmBgR3a1CgFygh8eXFVNr1PCMzvDSfcBXtbSuucwozvTY1qn854J5NnnxXVPqOuofdMBO8nCH7v1+tSazwNvDdOR97qCiCQrIhL0MTGaYqOKnzewGdMn3SX3e/z4/P6o5bAaOjv4XV7A3f5T/pbe054qLUfV7kLi82CPSYQ06k+w72G9qDX0B6RB32GONXvsaG/neiY/CeNtjlZ0aESSWAI0ZZ0Vf2RzkHtp742knS9EKJDGjC2L+4qNwd3HObIngIK9hVyYPshSo6Ukj2ke2R3923Dw7cpB1gHNrnL4ZMH43A68Hn9eN1efF4fXo+vdp368ZfkND1OYEBu37BtA8f1i6hPEX2qofeULbKaKgPGhv/bZ/ZIj2p9h44gtWs/Mro6Ap+tms+Y1eZH11wgxcQHF3XNHto9qLhmczX0ees/pg+GYaBO+lt36xtItvg8PipKNQX7Czm8+wiHdx/F5/ExaLx8hqNh4Ni+KKVCttljbPQekd26AYkW131AA8WCFYyZFtlxVwgRHYa1Oxip4Z9gDz36MKiPKMYjhBCtpu/oXpQWluPz1hWH1FpTVlRBVs+MBvYMT1m6oGLOC93mvBxlNL34l91uI3tod0x/8F1grTUWm5VJV42PKNaLbj0fm6P+HeS0zimcc+W4iPoULcB5MRghlg1VTpTzioi6HHr2QPqE+OKllOKSOy4I+4XtdGUYBr1H9An6jDnjTKw2jcWiccQmnPBcxSV3TI/q64+/JCfkMcdqszDz9mmBB47z4YSVjGx2OOeSCvx+TUVpXbLV4/LgrvbQrX/nqMZ4psrolsZZl+aGbJt242RiEyIroivar7ufvDVsW0qn6C7hLISIUNx3IdS1ipEOsTedcndJYAghOqQVn64hrWsq8SnxNXO/FY5YB5k90tmatzPy4mxxd6HibgBLzRcSSzdU/J2o2NkRdVdVXo3P4yO1c0ptIUulFHGJsXTpncWqL9ZH1G+PgV25/9nbGTpxIIahsMfYmHBxDvc/d6dUWW9HlJGKSvo9OCaBsoGygH0sKun3KGu3iPo0DIPvP/Edpt4wifjkOJRS9BrWgzv+fPMZeXfR4/ay9hvIyk4hJTOwLSYWhoxVxCWn4PP4MQxF/9G9ufvJ7zEoyiOUYmId3Pfs7Zx9+ThiYh0YhmLg2H7c+/TttYkmZcShkh5BxUwFFYPfr0lMNUhKTwuM7kJhsVpISk8kNtHJuvmbohrjmey6hy7n8rtnkNYlMDKpU3YG1z10BTNvm9bGkYmWMPfFrzFsIb4YKfC5ffh89VfEEkK0LiPuRkh4uCaxr0DZwTEBUt/EME49QlJqYAghOqTDu49gsRikdkomtVMyWuvaO88lR0txV7lxxjf97ppSBjivQDmvCOozUkWHivF6fLVxmqYZVGzvSH5BxH33HNydux6/JSpxipajLJ1QCfcC90atz5hYB1fcM5Mr7pl5xv/9SwvKcFW6cVV2AjphizPRyqCkBJKzYMrss7j6x7Na9HeUmJrA9T+7gut/Fv64oYxUiL8LFX8X+evy+e/TTwHQuTf19mnOcUEEMwyDaTdNZtpNk8/4z8qZ4MD2Q9hsNqgZoOj1egOPCYxwKth3jM69zqylpoVoj4zY2RA7u951caP2baGYhBCiRaV1CZ4/d+JFaXxyHI4oLKUZjQvdpIxELJa6Q+3JB+njdwWbQy7Iz2xn+t8/ITU+aDqVUsGfsfSuqa36O2rMa6V0SsEw6p538j7ROC6I+s70z8qZIKN7WtDj48kLAKvdesbVCBKivYtkBS1JYAghOqSJl+VitVlCtp19+bh2s6RgQko8o84fFrItLtHJmAvOvCH/QkRTTKyDcTNGh20bG6atLaVkJjH07EEh2xJS4xl53tBWjkiI08PsB2aFPf+POncY9pgmL08mhGhn2scVvhBCNFF61zS+87vrccYHz5XLuWAEM2+L7nKdzXXdQ5cz8KRVQ5LSE7jzr7dEtlqKECLIlffNZOjE4FWC4pPjuP2xm4hPjguzV9u68RdX0Xdkr6BtyZlJ3PX4LVFb4lWIM03v4dnc8rvrsNqDZ8n3GtaDB176fhtFJYSIJqUbWoe1HcnJydF5eXn1tmc/+HEbRCPE6SX/0Zmt/ppKqZVa6yavIXryscBd7Wb9wi1UV7joN7oXnbIzoxpnNO3ZtI+9Ww6SmBrP0LMH1hb1FOJMF63jwf5tB9m9YR/xybEMPXtgh0gE7F6/h/3bDpGUkciQiQOwWOS4IM5c0ToWVJRU8MFTc6kormTcjNGMmDIkqnEKIVpeuOOBFPEUQnRoDqeDnChPw/C4PKz+cj3HDpXQuXcmwycNjkqyoefg7vQc3D0KEYqWtHNtPttX7sIR62D01GEkpSe2dUiikbr170K3/l3Ctmut2bx0G/kb95OQGs/oqcPqrdpjmiabl25nz6bAc8ZMG96iy232GtaTXsN6tlj/QpyJSo+VU3q0lPLiSgr2F7Z1OEKIKJIEhhBCnCB/4z6euu9Fyosra7dldEvjB3//Lhnd0hrYU3R0HreXfz3wMhuXbK3d9u7fPmb2A5dyzhXj2jAyEQ2VpZX844cvsGfT/tpt7z7+Ebc8ch3DJw0GoLy4gid/+AJ7txyofc57f/uY7/zuurA1K4QQ7ct/fv1f3v7zh7XLqS94ewmv/+E9/jz/NyRLQlqIDk9qYAghRA2f18czP34pKHkBULD/GC88/FobRSVay8fPfB6UvADw+/y88eh77N9+qI2iEtHyxqP/C0peALirPTz/0KuUFZUHnvOH94KSFwCuKjfPPfQqFSXBxwUhRPuzaclW3nrsg9rkxXFH9hTwx5v+3kZRCSGiSRIYQghRY8OiLZQWlods27NpP/u3HWzliERr0Vqz+P3lYduWvL+ilSMS0VRZWsmarzaEbPN6fKz4dA3lxRWsnb8x5HM8Li8r5qxpyRCFEFHwzuMfE66+36bFW6mqqG7liIQQ0SYJDCGEqFFaUNZge8kp2k8nrio3lWVVbR3GKZmmSXlxBT6vr1n9eD0+KsvqLmxNv4k26y6CS46WNqt/0Tq01pQXV+Bxe2u3eVwejuwpwO83w+5XeOAYh/MLMM3whc3lPSBE+1dypO5zqjVBx3G/z3/K87wQov2TGhhCCFGj24Dwxf8MQ9G1b6dWjKZtHN1XyLt//YgN32zBNDU9BnVl1p3TGTxhQFuHFkRrzRcvL2De6wspLSwnJs7BhEtyufQHF2J3NH3lCbvDRlbPDPI37KWkoAyv24tC4UyIISUrme4Du7bATyGiafEHK5j77684uu8YNruVwWcNwPSbbFqyDa/HR+H+Y8SnxAUV5PR7/RQfKWHOC/P46s1vKNx/jISUeJwhinbKe0CI9i97aHc2L9+O1+2FE/KRhsUgITWejO5Sy0qIjq5VRmAopYYqpRYrpRYqpV5UAQ8opRYppV5VSrX/dc6EEKe9PiOy6TM89GoAOdNHkpKV3MoRta7y4gr+etvTrFu4ufZO9N7NB/jnvS+yNW9nG0cX7IN/fsZ7f/+kdsqPq9LNV28s4vkHX424z35jelOwvyhw4QtoNFXl1RQeKGL0tOFRiVu0jEXvLeOV377N0X3HgMCUj0+f/5K5L32Nz+vDMBQOp53C/UVUlQdG2mhTc2RPAR6Xl5i4GAzDwB5jp2B/EdXlrqD+M7unMfJcWYZRiPZu5u3T8LqCkxcQGFWXmBaP1Sr3boXo6FrrU7xVa30WgFLqRSAHOFdrfbZS6qfAZcBbrRSLEOIk2Q9+3GJ95z86s8X6bgl3/OVmXvntO6xfuAnT1FhtFsZeNJrZP7m0rUNrcQvfWRayBohpauY8/yUDcvq0QVT1VZVXM+/1RSHb1i/azJ7N++k5qFuT+927eT/JWUmUFZZh1kw3sMfYSemUxMZvtpJ5bXqz4hYtwzRNPn3+y6BtlaVV+DyBaUXV5S5iE50kpiegtaaiuJLYBCcVJZVYbBbSOqegDAVAUkZghYLy4gqcCTEA9B/Thxt/eTVWm3zxEaK9e+XX4b9OHMkvaMVIhBAtpVXOxlpr7wkP3UB/4Ouax18A1yMJDCFEOxCXFMftj91E8dFSSo6UkNE9nfjkuLYOq1XsXLM7bNuONfmtF8gp7N28v3aURCg7V+9ucgLD6/Gyd/MBElPjSUiOw+P2YhgKW810lB2rd3PutRObFbdoGUWHiik+Elyfwl3lDvp3bKITpRTJmUmYfpMfPnkrn/9nPpuXbQ/a78Tn3Pv0baR2TiG9S2qr/BxCiObbtnJXbULyxPoXylB4PT52rc+n97DsNopOCBENrVbEUyk1Sym1AcgkkDg5XkWnFEgJs89tSqk8pVReQYFkTYUQrSclM4lew3qeMckLIKg2QP22mFaMpGGh6hM0pT0Ui9WCPSaQrFA10w1sJ9TSaOh3I9pWYPqHCtpmWIyQ/wZwOO30GZlNWgOJCUesgz4jsyV5IUQHExPnqP23MlTtf8clZyS1RVhCiChqtQSG1voDrfVQ4ADgAxJrmhKBkjD7PKu1ztFa52RkZLRSpEIIcWYaO2N02LZxM8a0YiQN6zmoG517Z4Vsi4l1MPK8oU3u0zAMcqePCts+bmb4341oW/HJcQwa3z9oW1xSbM2/FLG1/w4YM20ENruNsTPC/71zp4/EYrFEO1QhRAu77O4ZYdvSOqeQ2inkPVMhRAfSWkU8HSc8LAMswOSax1OBpa0RhxBCiPCGnTOISVeOr7c9e0h3Lvre+W0QUXg3/Wo2cYnBoyKsNgs3/vJqnHGRjRa59AcXhlxp5oKbptB3VK+I+hSt45qfXkZal7ovJnannaT0RJKzkrDZ62bLdu6dxWU/vAiAviN7ccHNU+r11aVPFpfdfVGLxyyEiL6Lb5/GwLH96m232a386p0H2iAiIUS0tVZFqguVUvfX/Hs78H9AZ6XUImAv8HgrxSGEEKIB1z54OTkXjmTl3HV4XB4Gje/PqPOGYrG2r7vRPQd141fvPsCSD1dycMdhUjolM2FWTrOG/Mcnx/GTl37Aqi/Wsy1vJzGxDnIvGkX2kO5RjFy0hPQuqTz8+n2smLOG/PV7iUuOZfzFY/B6fCz/ZDWuChf9xvRmzAXDsdnrpgZd9oOLGHXeUJZ/ugZXpYv+Y3ozelrwc4QQHcsTi3/Hx899znuPf4Kr0k3/3D788MlbZfqIEKcJpbU+9bPagZycHJ2Xl1dve0uuniCEaL5wq5AopVZqrXOa2l+4Y4EQouOS44EQAuRYIISoE+540Go1MIQQQgghhBBCCCEi1WFGYCilCoA9bR2HECJqemqtm1ydV44FQpyW5HgghAA5Fggh6oQ8HnSYBIYQQgghhBBCCCHOXDKFRAghhBBCCCGEEO2eJDCEEEIIIYQQQgjR7kkCQwghhBBCCCGEEO2eJDCEEEIIIYQQQgjR7kkCQwghhBBCCCGEEO2eJDCEEEIIIYQQQgjR7kkCQwghhBBCCCGEEO2eJDCEEEIIIYQQQgjR7kkCQwghhBBCCCGEEO2eJDCEEEIIIYQQQgjR7kkCQwghhBBCCCGEEO2eJDCEEEIIIYQQQgjR7lnbOoDGSk9P19nZ2W0dhhAiSlauXFmotc5o6n5yLBDi9CPHAyEEyLFACFEn3PGgwyQwsrOzycvLa5PXriyrwufxkZSeGPY52iwBLCgjodH9VpVX43V7a/vV2gdmKRgJKGWPOF5tVgJuUCkopSLu51R8Pi9lBfuJS07H4Qz9c3s9XipKqkhIicNqC7zd9m09gDPBSXqX1JD7mKZJaWE5sQkxOJyOFotftC2l1J5I9mvLY4EQomVE63jg8XjYsnQHWdkZZPUIXPM05pxYWbIPd3URqZ1HRBJGrZPP636/n7JjFcQlOrHHRH5e7wi0WQaYKCMZAJ/XR3lxJfHJsdjstrYNTnQY0bw2WPHZanat3cNl91yEwyHXk0K0N6bPB76FYOmGYetXrz3c8aDDJDDawpE9Bbz95w/ZtHQbWmu69Mli1l0XMnzS4NrnaO96dOV/wLcz8Ng2GBX3HZS1d9h+C/Yf4+0/f8jGxVswTU3nPpnc9BNF9+y1YJaDigHHuRB3c5MSGdp/FF35PHjzQGuwdIHYa1GOsyP/JYRgmiZfvvg4X76+nLJjPmx2Re70XlzxwEPEJqQA4HF7ef/vn7LkwzxcVW7ikmJJTI1n09JtlBdVoJSiS59O3PPU9xg8YUBt3/NeX8QXryyg5GgpNruVMdNGcOX9FxOXGBvVn0EIIcTp5ReX/ZEVc9bg9/kByOiayCP/zaJn3y0158TONefEc2r3Kchfwtt//BMblnnRJmR218y8dQK5lz7cpNc+uq+Qtx77gM1LtwXO672z6NKnEztW76K0sByb3UrO9JFcdf/FOOOdUf2525r27UBXvgDeLQD4VF8+eXUQi97fQ2VZNc74GCZeNpZZd02vvZEhREt65fdv89LP36x9/NyDr5KUmcjbh59vw6iEECcyC68H30pABx7jhOTHMWLOPeW+SmvdwuFFR05Ojm7Nu65lReX87rrHKS+qCNpuGIrvP/FdBo3rFzhpl/4MtC94ZxWLSv4zypJVr9/Ksip+d93jlBwtrd123qXbyZm8n4zuacEXNvYcjMSfNSpebVahS+8Df0G9NpXwAMoxoVH9NMbHT/6Bj59bWW9735HJ3P/icwA8+8B/WPP1xtq2koIyCvcfw2IxsNgstdsdTgf/zHuUrJ6ZfP6f+bz390/q9Zs9pDsPvPj9Fh1NIlqfUmql1jqnqfu19rFACNHymns8+PmsP7D8k9VBbVabn4Qkk3/ngSOm7vyhEn6EckyksmQfv599J8VH659bbv3dOYye+ZNGxVBZWhk4rxeU1W4rKyynpKCs5rweU7u9z/Ce/Oj5u5r6Y7Zb2n8QXfJj0K7awHE1PQAAIABJREFUba/9RbH4UwMsXUHV3fXOnT6SWx65ri3CFB1Ic48FW1Zs5+5xoa+dkzOTeOvwc82OUQjRPOaxG8C7PESLAelfY1g7AeGPB1LEM4xF7y6vl7wAME3NnOe/BEBXv1s/eQGgq8D1Uch+F7+/Iih54YzzMHLiQQBKC8uDn+zJQ/t2Ny5g99chkxeBON9qXB+N4Koq48vXVods27GmhK1LP+PAjkNByQuAkiOBn9nvN48n2gBwV7t5+Tdv4/V4mfufr0P2m79xH5uWbItK/EIIIU4vHo+HlXPXBW0zDI0CKkoN/vtE8HlaV/8XgGXvPh4yeQHw6QsLG/363/xvRVDyQpuasmPlgKassCzouTvX7WHL8u2N7rvdq/4gKHlRfBSWfKYADWZJ0FPz5q7l6L7CVg5QnGkeuvB3YdtKjpbidrtbMRohxMlMny8wWyB0K5Tce8o+JIERxs414RMHu9bVTMfxbg77HF0zlLJ+v/lBjzt1L8dqNQHwVHsI+nZ/itcIej1fA8/z5aN1daP6OZWDW1fjqjLDtu9cvYZda4OnK5mmic/rO+Fx8M+4Y/VujuQXUFlaFb7fk35vQgghBMDKuetrp40cp1TdeWbj8pOSFL59aLOSnWv3hu3zwE5wVR1r1OuffH7yur2YZuA86a72cvJI19PpfHbytcfuzaCPXyKckNgA0Fqze11E5Q2EaLSK4soG2+e/8U0rRSKECMm3HAj/XRL/jlN2IQmMMOKSwtdciD3ephoo2GnEN6rf6sq6wlbKYgAnXWg1siioaigWZQeiUzwsLjmt4fakhHo/o2EYQdM/Tp4J4oyPITax4TnBDf09hBBCnLk6Zze8YEH8yfW3lR2Unbik8OdFe4zGam/c+bfeOc9Sd2llGKre9Mf45LhG9dshnHTtEXviQ1X/EjNW6lmJFnbi5y+U3iOyWycQIURo1q6neELMKdolgRHW+IvHhG+bGWhTMVPCPkc5QhcgObnfw/sSKTwSOKHX+5JuxIF9bCOiBRwNxTIZpSxh25siq9dQsoeEXo3FaleMuehKhp4ziLiTEhLO+MA8WKUUygi+mJt+y7mkdkqh/+jQhU8tVgs505tXGV4IIcTpKXtoDxLTgr9Im2bNeUbBFSeVnFCOs1HKxtiLZ4TtM3daElZr4xL/42aODnpstVtrV9CKSw4+r9vsVkZPG96ofjuCk691+o+ElMzjjcHXColpCQye0L+VIhNnqvGXjG6wve/I8EX2hRAtz7D2BBpI5Mfdcuo+ohfO6WXwhAGcd1391Tt6D+vJRbeeH3gQczHYR9bf2TEZ7OfU3w70G92bC26eErTt41cHo1U8yRknnOyVDRV/D0o1btknZeuPip1dv8GaDbE3NqqPxrrhlz8kKT24krjFqrjxF5cRn5KF3WHj27+9DpujbnRJZo8MbHYrVltwIiXngpFceMt5AFz/8ytJyUoKajcMxbcevrLBJWyFEEKc2f7vv/cHnV+0Vvj9Bhdc62FwzgnnK2tPiL0ZgL5jv8VF3+5cr6+ufWDWfY80+rUH5PZl2o2Tg7aldUkhPiU+6NxlsRjc8H9XkZASeoRmh+Q4FxwTax8aBnz7IRNnfAIYdT97TKyDWx65Dos1OjdThAjn1+/+NOySxXc/+d1WjkYIEVLyk4RMQ1h6Y8Sf+nMqq5CcQv7GfeR9tgav28fAcX0ZMWUIhlH3C9faBO8a8OSBsoB9PMo25JT97t1ygLw5a3BVuRk4ti8jJvfE8C0E/z4wMiDmPJSR2uR4tW83uBeCrgTbsEA8KvrLllVXlrD8/dfYv3UPSRlJjL/satK7Ba/fW1pYxpIP8jh2qJjOvTIZdcEIvnjpa/I+W4sj1s5F3z2PCZfkBu3jqnKzYs4a9m7eT2JaAhMuGUN614anrYiOSVYhEUIcF43jQUlBKU98/zl2rs4nNsnJtx6+komXZIF7Qc05cWjNOdEW1MfeDR+w4qP3cFV66J8zmJEX3YvN3vRpHns272flZ2sD5/Vx/Rgwti+r5q5l75YDJGckMu7iMaR3afp5vSPQ3vXgWRYogGHPpaKyH0s/WsnRvYVkdEtj/CVjSExt3JQccWaL1rXB765/nAVvLUFrTWJ6Ao/N+xXZg7tHNVYhRORMXwGU3ge+rYEVq2K/ixEfPPoi3PFAEhhCiDYhCQwhxHFyPBBCgBwLhBB1ZBlVIYQQQgghhBBCdFjRn1sghBBtIPvBj5v0/PxHZ7ZQJEIIIYQQQoiWICMwhBBCCCGEEEII0e5JAkMIIYQQQgghhBDtniQwhBBCCCGEEEII0e5JAkMIIYQQQgghhBDtniQwhBBCCCGEEEII0e5JAkMIIYQQQgghhBDtniQwhBBCCCGEEEII0e5JAkMIIYQQQgghhBDtniQwhBBCCCGEEEII0e5JAkMIIYQQQgghhBDtnrWtAxCRO7TrCHNf+poda3bjjI9h7EWjmTx7Aja7rfY5W/N2Mu/VhRzceZjkjETOvnI842aMbvZr71izmy9eXsCB7YdISk9k4uVjGX/xGJRStc/R7m/Qrk/BLABLN7CNR/l3oL1rQTlQ9nPAeTFKxTQ7nhP5/T4WvfksSz5cTkWJh+whGUy7+Rp6Djurwf3WLdjE1298w9F9hWR0S2PKtRMZMXlIbbtpmnzzvxUs+WAF5UUV9BjYlak3TabX0B61z3FVufnq9UXkzV2Lx+Vh0Lj+TLtpMhnd0mqfU3y0lC9eXsDGbzajDIOR5w4lMS2evLlrKSssp1v/Lky9cRJ9RmTX7uNxefjqjW/I+2wNrio3A3L6csHNk8nskRG9X5wQQnRg907+ORsXbq19bIux8dquWSTGLAddBdbB7N09ni9e28aeTfuJT4ljwiU5nH3FOAyj7n7O7g17+eLlBezdvJ+E1HgmzMpl4mW5Qc85mdZuqP4Q7VkI2o2yDQfn5ShL5xb9mcPGY1aA60O0ewngRdlG18ST3ibxCNGatq7ayd25D6G1rt02cFw//r7k920YlRDiRGbVO1DxBJjHAAvYRkHyPzAs8afcV5344W5JSqmbgJsBC/At4I9Ad8ALXKu1Lmxo/5ycHJ2Xl9ficXYUezbt4/E7nsVd7QnaPmhcP77/xHcwDIPln67mP796E9MM/htP/dYkrrh3ZsSvverL9bzws1fr9Tv56rO45ieXAqCr3kBX/beuUXvAPABGKqikuu22AajEX6OUPeJ4Tvbcj+9h1ZcHgrZZbYo7/3oLgybOCLnPvNcW8vZfP6q3/fK7ZzDtpskAvPTLN1n2yaqgdovVwu2P3cTQiQPxerz89fZnyN+wL+g5cYlOfvT8XXTKzqT4aCl/uuVJSo6W1rYXHSrGXe0hq2cGhiVwgWwYilsfvYGR5w7F5/XxxF3PsWPN7qB+nfEx3P+vO+jat20ukJtLKbVSa53T1P3CHQuyH/y4Sf3kPxr5Z0AIEV3NPR7c3P8HHNxx5KRWjVIw54gJwOY8eOYXVnxmF1CO2mflTh/JLY9cB8CGb7bwzI//g9/nD+pp3IzR3Pzra0LGoLUHXfZL8G4NbjDiUYmPoKw9Qu7XUrSuRpf+DHx7ToonGZX0B5Qlq1XjEaIpmnssKCws5LrMO0M+p1PvLF7e8Y9mxyiEaB6z4jmoeAw4KQ+hkiF9MYbFEngY5njQKlNIlFJdgcla6/O11lOALMCjtZ4MvEggoSGa4H9//7Re8gJg87LtrF+wGb/Pz7t/+7hekgFg3usLKTxwLKLXNU2Tdx//KGS/899azKHdR9BmEbr67ZN2LALtD/wfs267dyu4F0YUSyjbV3xZL3kB4PNq3n38zZD7VFdU88FTn4Vs++jZz6ksq/r/7N13eBzF3cDx7+xV9WZJlnvv3TJgYwO2MabXkFBCCEmAJKRBet7U900hCSEJCSQhJIRQktBrMB1sbNxt3HuVJVm9X995/1jpTtckWZZlGf8+z8ODbnZ2dnbvvDP729lZDm47HBe8AAgFQzzTGvhY9cr6uOAFQHODh1f+8gYArz38TlTwwu8N0FTXTMAXoKm2OZxumppnfvcypmmy7o1NccELq95eXnogcb2FEOJ0kih4AaA1fOMKKxjxzJ8VwYDZercnYs1rG9m3ybrYf+a3L8cFLwBW/Xc9B7fFn98B8L0fH7wAMJvQLf86th3pCd434oMXAGYdeJ7p/foI0Ys+M/KupMvK98WeJ4QQJ0XzA8QFLwB0HTTf0+nqvTUHxmLAppR6Syn1B+AIkVpnA927mj5N+Tw+dq7dm3T5pve2cXBbCQ3VjQmXm6Zm09Lt3dp2ya4yasrrki7fvHQ7+DeANqMX6NaLcx0C7Y1e5F/Trbok3P57yYMhR/Y0U1WyOy59x+o9+L2BhOsEfAG2r9zd4fE6erCSowcr2bIseZ5NS7dZ9Vu2LSrd0+RJ+DdAdWktpXvK2bw0ep32tizfgWmaSZcLIcRH3YPffrTD5dvX2qkqg/KDrY84ag+xHadNS7dRccg6lyez6b3E52Id6GB0aGANvTXStU1Hbar2r+7FmgjR+zyNng6XP3f/q71UEyFEIqZ/b+S6MBHfm52W0VsBjELAqbVeCLQAcwGXUmo78AXg2UQrKaVuU0qtVUqtraxM3qk4HbWfayJ+YefrG0YXMiXcbmfLu1Nu9+rSHYnq11mdlepang6Xtx7v+HJUkr8jBXe07e4dbyGE+OhQts7Pgz3SdnXrfHsy5krvqJ7SZojTW2q6q/NMQog+rbda1nrgvda/3wbGAfVa6/HAj4FvJFpJa/2g1rpYa12cny+TFbZxpbgYf+bopMunL5jE0ImDyC7ISrjcMBRTzp3QrW0PHF1E3oCchMuUUkw9bwI4Z4KKmR9WpbX+3wYxk3Yq15ndqksi0xbOT7psyNh08gaOiksfd+Zo3KmJGzRXipMJs8cwbf7EhMsBBowspGBIPlPnT0per/OsZVPPi86TmhE5FikZ0celYEg/Bo7q32G5U86Z0OHEckII8VF3688/2eHyKXMC5PWHgSNbR0KoVGIv5KfNn0jBkHwGjEw+P8T0BYnPxcrZQRvmPKPXA80d1Uc5z+rFmgjR+zJy0jpcvvjmBb1UEyFEIoZzJKgOJup0J56vMKqMHqxPR1YAU1r/ngbsB2paP1cBia+0RVJXfvkiUtLj394xZd54JswZi81m49qvX5ZwpMXiT88nt3/iIERnDMPg2q9fjs1ui1u28MZ5FAzJRxnZqNTrYlbMtYIaRh5RPzvHJHDO7VZdEhkx/VzOunhYXLrDqbjm6zclXMed6uKqryT+x3Llly4iJT2FQWMGMO/q+I6fw2nnY3ddBsCsC6cxevrwuDwZuelccvsiABZ/+ryoAJDD5SAjNx2n20lGTuQfs91h42N3XYZSiukLJzHujPiAVVpWKpd9cXHCegshxOlk+JTYiTJbR70Z8IunrID6x76gsTuN1nYoYs7lsxg6YbCV567LcDjjX9A296ozGTRmQOKNO88Gx+T4dCMTlXrDse1IT3CfD/aR8em2PEj5WO/XR4he9NjhB5IuGzppcC/WRAiRVPqdJBwRaPSD1K90unpvvoXkHqAYK2BxA/AE0A/ravYWrXXySR2Qt5AkUllSzVuPL2P3+n2kprs54+IZzLlyFjZbJLiwf8sh3n7ifUr3lpNTkMXcq89kWgd39Lvq4PYS3nnifQ7vKiWrXyZzrzqDGedPicqj/evQ3iVgVoBtIDjnooI7rNeo4ka55oH7gh59AwlYE42uev4RPnhpOU11PoZPLGThp25kwJjpHa63Y/Vu3v3PCipLqskfmMu5nzg7aqSL1po1Szay4oU1NNY2MXT8IBbcMDeqUxvwB1j29ErWvv4hAV+AcWeMZsGN88hpNxqmsbaJd/61nC3Lt2PYDKbNn0xmvwzWvrqB+upGBo8dwIIb5jFk3MCocpc/t5o1r23E1+Jj7KzRLLhhLnlF3QtE9QXyFhIhRJueOB/86MpfsuKlteHpLVIy3Dx58GM41UrQLSj7JI4cOYt3/rODA1sPk5GdxuwrZnHGRdOjRkmU7C7j7ceXcXB7CRk56cxpzdMRrf3gfR3tfx+0B+WYCu7LULaTM3pUaw94l6B9K4Bg62tUL0UZp26bIU4PPXEuKD1UyS2jvoQZjMwRNvOCydy95Ic9V1EhxHExPW9A0y8hVGHd5HacBVm/w7BFrguTnQ96LYBxvCSAIcRHiwQwhBBtevp8IIQ4Ncm5QAjR5qS+RlUIIYQQQgghhBDieEgAQwghhBBCCCGEEH2eBDCEEEIIIYQQQgjR50kAQwghhBBCCCGEEH2eBDCEEEIIIYQQQgjR50kAQwghhBBCCCGEEH2eBDCEEEIIIYQQQgjR50kAQwghhBBCCCGEEH2eBDCEEEIIIYQQQgjR50kAQwghhBBCCCGEEH2eBDCEEEIIIYQQQgjR50kAQwghhBBCCCGEEH2e/WRX4GQ6sqeMD15cS2NNE0MnDOKsy4pJzUg5pjJCoRAfvrOVLe/vQBmK6Qsnk9s/m5UvraO+qoFBYwcw54pZpGWmRq2nA9vAtxStfSjHJHDNQylnh9s6erCSFS+soa6inkFjBjB53khe/MPf2bbqAClpTi7+3CLmffyaqHUaa5tY8cIaSveUk12QxdlXziS//0HwrwY0OM6g4uhwPnhhHbVH6yga2Z85VxSTmZtxTMchkarSGlY8v4bq0hr6Dy9g1kXT2bN+PztW78bpdlK8eCquVBerXl5Hc30LI6YOY/K8cWx4awuHtpeQmZfBWZfNpK6igQ1vbSYUDDHx7HEMGT+QVS+vo7KkmsKh+ZxxyQz2fXiQ7St34XA5mLloCuPOGH3c9fd7/Tx170use+1D7A47535iNhd9diGG0XHcb+favax7/UP8Xj/jzxzNjIWDsJvvoUNHULYCQvYFbFpaGfWbmThnLEqpcBneFh9rXt3A3o0HSMlI4axLZzB0wuBj3ofY38zsy4tJz0475nKEOBmGfeeVY8p/4O5LTlBNxKngtUde497PPYQZsj6PmFLAtPmjePOxNQT8IYaMzePrD3+d/ZvKOLj1MBm56Zx16Uz2bjzAkoffxtvkY+LccVz37StIz06PKnvPxv2seXUjPo+PMcUjKV48DafLEZXn4ObXqD34Mlp7cWcVM+qMK7DrlejgfpSRA64FKPuQ3joccRqqG1nxwhrK9h0ltyiHOVfMIn9Q3kmrj+g7SnaXsfKl1v7wxMHMvmwmKenH1h/uaxbZrwUz8vnKr17EHb/9zMmrkBAiihncB3XfgOB+UE5IuQ4j884urau01ie4ej2juLhYr127tsfKe+/JFTx5z4u03/+sfhl87c+3Uzg0v0tl+H0B7v/y39i9YX84ramuGV+zj7yBueG0jNx0vvrArQwY2R8A3fw3tCemY24fgsr8P5SROHCw6r/refQnT2KaVn1ttiZKd5fj86qofGddPJDvP/V7AA5uL+GPX3qI5gYPAIZhcvXntjJroRkOqKx7Bx75VTom/QGrrNSMFL70h88ybOKxXzC3+fDdrfzte48TDFg9yVDIpPpIDVn9MnClugCor2rADJrk9M8GIOALUFteR96AXGwOG1prakprcaW6SM+xLro9TV6aapvoNzAPZSjM1nIz8jJwp7nC259z+Sxu/P41UUGBY1FX1cDX5vwPVaU1Uemjp4/g1+/8CLs9cezviZ8/y/vPrQp/HjSyjhu/spOBo7IxDAO/Dx74no09WwtBRQIJMxZO5jM/vwHDMKitqOe3t/2ZqiPR2770tkVcfOv5Xd6H2N8MQFpWKl+5/3MMHjuwy+WcKEqpdVrr4mNdL9m5QC52P3rkOz19HO/54MdX/y/Ln9/UlS0xYHQBaZlWgKJ0bzm+Fj82eyQwnZGTzj3v/piBI4sAeOo3L/HOv9+PKmXAyEK+9ufbwwHhja98jezMFeHldkeIzNwgqVlDMGyOtp1EpX0B5e76ebyn7N98kD9+5e94mrzhNJvdxi0/vZ4ZCyf3en1E3/H2v97nmd++HNUfzi7I4mt/upWCIV3rD/eknugbLDKuTZhnwKhCHtn1x+OroBDiuJmeN6H+DiAmDmEMwih4O/wx2fngtHyEpLqslqd+Ex28AKivauRfP3+2y+W8/fiyqOBFKBiitryO5oYWmutbwumNNU08/rNnAND+D+ODFwDBQ9DyeMLtNNU188TPnom6EG2sLMfbQtw+rPzvEZY9/TwAj/7kyXDwAmDqnFJGjKuipqwW0zTxNMHj9xqYwRYwG8L5Who9/PPH/+nycYjl8/j450+eDAcvAOorG/A2e6kurQWs4E99ZQONtU14WztUNWV1eFt81JTXWfVo8NDc0EJNeR2hQAhtaqpLa/A0eWmobrTKrWrE0+ylurQm6t/AihfXsOm9bd3eh/u//Le44AXA7g37ePLXLyZcZ8v726OCF0ppLr1xGzrUQn2ldXzfeUaxZ5OGUAXtK7z+rc2semU9AE//5qW44AXAyw++Qcmu0i7VP9FvBqC5voVH//epLpUhhBCniuXPb+5iTk3Z3goAGmsaaWnwEAqGaN+UNtY28dtb/wJYI+pigxcApXuP8uIDrwGwb8OLUcELgMxcLw6nH29TWbtNa3TzX9Bmbdd3rAdorXnkx09GBS/A6rM89r9P4W3x9Wp9RN9RdaSaZ3/3clxfsq6inn/f/fxJqtXxWez8eNJlpXuO9mJNhBBJ1X+duOAFgFmC2finTlc/LQMYa5ZsjLuwa7Nr/T5qK+q7VM6q/66P+tzc4Ak3Au0DGAD7Nx+isqQa7XsvaXnatzRh+oa3NhPwB8OfFUGqyttWis//6l+XULKrlNK90SfqicXWZ21qWho8fLgc/G39Gd0Ylbf8QCUHtx1OWteObF62I66j1NJ6PIKBIL4WX/gzWMcq6A/i81idKE+TFzNktjuGmuaGFjzNVnrbOu3LDQVDcZ2w1THfz7HY+M7WpMuWPvVBwvRV/90Q9XnQiDoys31R9V31RtuIEBN0c1T+Na9uwNviY9N7ybe9OmYbycT+Ztor2VVG6d7yhMuEEOJU8/AP/0HCxjAJbULQH6C+KtLutbUtbXat24vfH+iwHVmzZANaa+pLXopKt9lNHE6rPMOI7gugQ+Bb3uW69oQDWw5Rcagq4TJvi48P303e5oiPto76wzvX7g3fLDqVmMGOzwV//0Him4VCiN5hBvcBnuQZWh7rtIzTMoDhjbm4PtblbWIv0nW7DlBsZwiskQ3olrj0SAFetI5fL3Y7GEESZGuXPxC/DuByRy5oTdPE09zu8YqubLeL4o6L1lENpGnqqONjmiam2X77Gm3qqDQdil+n/f8h/ph3t/4AAX8g6TJvc+K7VbG/G1dKZARKWz297WMWMce8pcmD3+snlOC3E87T2ME/+HY62/euliOEEH3dwS2HjnENTSgQRLe/cIu5A22GTPwtvg77Az6PHzNkomI6YsqIlKWUJi64EhO8PtE6aw+62ucRHz0d/Ta01sfVj+qrNr3b/dG5QogeEOzsJmrnowJPywDG6Jkjki7L6pdBwZB+XStn+vCoz21zOwC42/0N1twDA0YWohwTkxfoGIdS8V/JqJjtaNNNakbyCPPEOSMYPG5gXB0O782Oqt+oKe3KMNzR+5LiZOiEQcnr2oFR04dFfVZK4Up1Rv5OcUYdK1eqC4fLgWGzAWB32rE5bFH1d6W5Wj+rcP3b1gVQRLYRrseM5N9zZ4qGFyZdFvt9tBkZk156IJNQSEXVM+qYq+gJskZPH0Fmbgb9hyV/5rSj325X6gjgTnMxeOyALpUjhBB93Zf+8vljXEPhSkuJmjdJxUzOnNM/m/Ts9LjzensjJg/FZrdhS5kUlR70G2jTOveHQg7a2q0wR3T+E23oxME4YiYcbW/UjOT7KD7aOuorZBdkkT/4ozfJ6++W/exkV0GI05rhnkOHIQj7uM7L6LnqnDomzB7DiMlDEy678JYF2Oy2LpWz+Jb5UZ0Cd5oLV6oLw2aQkRs9g/niT8/H4XSAawHYCuILUwqVkvi5vRFThjJx9tiotKLhKVafKGaSyvQsG5/43hdwp7pYeOO8qGVr3h2Mz2cjJd2N0+1k8CiYMkcDBqjsqLwLrp/b7Rmoi4YXMnPR1Ki0rH4ZgCIjJx3DZpCamYLD5cDusJOenYZSqjUPZPXLtPYlJx3DZsOV6sKd5sLutJOWlYpSisxw3gyUUqTnpEV9b9n5mcy75sxu1R/gkz+8NuHbRpxuJzf9KPHkUPOuPoPsgqzw55YmJxveHwgqsk8XXK9xOAGVASry20nLSmX+9WcDcPGtixKWXzSikBnnd22ytUS/mTaLbjoXV4or4TIhhDjV9MsvwO7sencmLcsKducW5WAYBkopDFukLVVKce1dlwEw+/Ji8gbkxJVhGIqLb10IwIhZn8Hjad/mK5oarG3YnDE3RByTO76RcQKkZaay4Pq5CZdNnz+JgaOKerU+ou+YNHccwyYlnrC9K29d64u++IdPn+wqCCE64zg7yQIDsn7b6eqn7VtIPM1eXvjDq6xunXegYEg/Lrj5POZcPuuYytm36SAv3r+EXev3YRiKMcWjSM1MYduKnXhbfPQbmMuiT53HvKsjF9M6VIVueQT8K63nYe0jUanXo5wzkm7H7wvw4v1L+OCltXiavOQNyCG/qIlVr+6hviqIYShGTsnk63//JoPGRiJX7/x7OW89vpSa8jpSM1K44MahLLj6AEbImvAswCReeWwkH7y0n+YGDzmFWSy8YR4LbpiXrCpdEgwEeeXBN3n/uVU017eQXZDFmJkjOHqwkoPbSrDZbYw/czR2p52tK3YS8AUoGlHIkPGD2L/pABWHq3GnupgweyzeFi87Vu3GNDWjpg4jpyibHat201jbTGZeBuNmjaLySDX7Nx/CZjOYcs4ErvzKxcf9eri3nljGY//7FJUl1SilGDx2IHdb65XVAAAgAElEQVT8/hYmnp08MlhVWsNzv/8vm97bSihkMmzyID75dRv9i9aBWQtGBvv3nMmLfzPYvX4/hqGYMHssV33lYopGREZ9rH9zE/996E1K9x7F4bQzc9FUrvrqxWTkpCfddqxEv5nzbzyHcz8+57iOS0+Rt5Ccfo71OzpW8p2eunrifHBx6icIeEPtlipsDggFADSGTTF2ZhG5AwdTcagKV4qTUTNGsHP1bg5sOYxpmuQUZHH1nZdy1ZcvDpdSW1HPs797hY3vbCEUDDFk/EAuvf0CJrVrC2rKdnNo7c/JzNyOYZg0NhSSP/xMCoqOQOgoqBSUewGk3ohS0SMee4PWmrceX8bb/3qfuop60jJTOPvKM7nk9vOtmyvitOVp8vDcfa+yZskGfB4/BUP6ceEtCzjr0pknpT49cS64fcY32LfxYFyeN0yZxFyIvsKsvg0CS4m87zgdsv+M4T4jnCfZ+eC0DWC0CQVD+L1+3Gnubr9yE6yJsAxD4XQ7w+X6PH5S0pOXq7UfdBBlpHa9vqEQvpbocqtLj+DOyCAtIzPJdqznGF2pTmytj2lo7QN0uCPVVq47zdWjEXfTNPE2+6LK9TR7sTts4U5TMBAk4AuER3yE65viDI+q8PsCmCEz/OhIonK9LT4Mm4Gzg6Gy3VFX1YDDaQ+/erYrAv4AoWCkvlqb1vwnKjX8mFDsbyYRT5MnPFKluxL9ZvoCCWCcfiSAIZLpyfPBK399mVEzRjN2pjUKrbKsgoaj1YycNj6cx9PkwemOtDEtTR58LT5yCqJHI7YX8AcIBkKkpCUPQAR8zQQDPlLSrVepa62tOS9UCkp1bXTniZSo7RQCIv3h7o6+7Sk9eS547v5X2LxsOz/89zd6rH5CiJ5l+neAkY9hj7/xnOx80P2roo8Im93WIyfr2PkmbHYbqRkdl6uUE1Tyi9dEbLb4cvMGDOxkOypuHaVi6pug3J5gGEZcubGdP7vDHnWBnqi+sUGJROXGfgc9Jbtf4sBQRxxOh/WoSCulDFDRoye6Ut+e+G2eqO9WCCH6oktuvTTqc35RAflF0Y9uxp5bU9NTSO3kfGud1zsOkDtcaThcaeHPSqm4c//JlKjtFAJ6rj/cl1x1xyVcdYcEtoXoywxn53NexK1zAuohhBBCCCGEEEII0aMkgCGEEEIIIYQQQog+TwIYQgghhBBCCCGE6PMkgCGEEEIIIYQQQog+TwIYQgghhBBCCCGE6PMkgCGEEEIIIYQQQog+TwIYQgghhBBCCCGE6PMkgCGEEEIIIYQQQog+TwIYQgghhBBCCCGE6PPsJ7sCQgjxUTPsO68cU/4Dd19ygmoikjnR39GxlC/fvxBCCCFE18gIDCGEEEIIIYQQQvR5p/UIDK394F8Hug7so1H2Ud0qZ/+WQ7z52FJsdoOLPruQ/sNyW8utBdtwlGNct8qtrahn24qdKKWYNG8cmbkZ8fsQPATBbaBSwTkLpVLi8rzzr/fZsWYv/Yfnc8nti/A2etny/g601kyaO470nDS2Lt9J7dF6BowsZPSMEZQfqGD3un24Ul1MOXcCSsGmpdvxNHoYNX04A0b2Rwd2QGg/qFxwzkSp6J+TaZps+2AX1aW19B9ewJiZI9i+chfvP7sKd7qbS29fhDNF8eJ9D1NfVU/x4rOYdfGF3TpWXfHBS2vY+M428oqyufQLF6BNzab3thEMhBh/1mjyinLYsXoPFQcryR+cx7gzR6N0JQQ2Ag7r+Brx38HqJetZ9/pmcgoyueyLi0nLTI1arrVm19q9lB+opN/AXMafNZr6yga2rtiJYTOYPG88GTnpJ2y/hRDidPGJQZ+iptSDYYOfvPA1hk8awz9+8G8aa5tYcN1cFtwwj4PbDnNgawkZuelMnjcOuyMI/tWgPWCfiLIP5siOZdSVrceZks+w6VfgcKX1SP202QT+NYAfHFNQtqK4PM0NLWxeup2AP8j4M0fRb2Aeezbu58jucnIKs5h49lhsNlvcervX76N079FwHkO1tG4rAI7pKFtB5/XTml3r9lG+v4K8ATlMmD0Gw5B7XeLUc9+X/8pL978OgN1p41Xvv09yjYQQscy6/wHfW6DSIfuPGM6uXTOftgEMHdiGbvw1mPWRNMcUVMa3UEZqB2tG+8k197D29Y1orQHYsOQZ7vp9E0PHRjo72jEBlfEdlNH1i9SX/vQarz/yLqGQCYDNbuPS2xex+NPzrTJ1AN10H/iWR1ZSKZD+ZZTrLACO7C3juxf+jJqy2nCWh7//L/oNyMWd7gYgFAiiNdid9tZyNaFACMNQGHZba54QWutwHndqgFu/f4Qx0/wopayCjRzI/G44CFS2/yh/vusRKkuqASuYUX2kFk+jB1pXeefxf1Fz1CAUtBJeeWgLg8c8wa/fvY/0rOwuH6vO1FU18M0FP6Zs39Fw2qP/9zS5/bNJy7K+azMYwjQj+wiaKz5Tzvwr67E7WjuKygGpN6NSLgagqa6Jbyz4CSW7SsPl/udXL3DbPZ/ios8sAKD2aB0P3PkPjuwuC+cJ+oMopbC1lmt32LjijotYeOO8HttnIYQ4nbz81+f5/e2PhT+bIfjBpb8FFMqw2pjVr2zgt7f9hYFjijBs1kV5elaQz36vlNFTfNZ6ZpCqkloCzV7S7AoCsG/ZX0gp/D5DJi86rjpq79vo5gdB+yOJ7kWQdjtKWfVZ/vxqnrznRQK+QOt+mFYAQUVWySnM4vO/uZnBYwcC0FDTyJ/u/AcHt5WE8+Tm+7ntx0cYNCJoJSgF7ktQaZ9JWr/ainr+dOfDlOyKtFf5g/L4/L03UzS88Lj2XYjetMi4Nupz0B9ikXEtt//2Jj721ctPUq2EEG1M/0GouQCwrp/RNVBzOaZtPEb+C52u32thdaXUp5RSbyml3lVKDVRKLVJKvd36eWZv1QNAm83oxl9EBS8ACGxCN/+1y+U88qP/sOa1DeHghdNlcst3D+JvOUp9dUO7crehm//U5XLXvv4hr/797XDwAiAUDPHC/UvYsnyHldDy7+jgBYD2oJvuRYesC/X/veY3UcELM2TibfZRurccbWq01pQfqKR0bzktjR4AGqoaKd1bTlWptV4wEKR0bzlle48S9FsdoYuu20GKez/1le320axFN/wMrf2YphkVvACoPFxNfVUDgdYyUtL9VB5RBP2Ef7sAh3e18POPf6fLx6orfnrtvVHBC9M08bX4KN8X2aeKw9WU7i2nqbYJgKmzSxk7aQeVJVWRgnQA3fwQOrDNKve630UFLwAC/gB/vvMfHNlrdQAf+u7jUcGLprpmSveWc/RQZTgtGAjxzO9eZsfq3T2630IIcbr4/e2PJlmi0aZu+wtvi4+Sna3nbR2gqfoQf/5+gOZGK8nbWEJWbhXpmZEgg8vlwVf5EzyN1bGFd5kO7kM33x8dvAC09w3wvgTAwW2HeeLnz4aDFwDVpTUc2lFCY01TOK32aD1/uvMfBANW+/WPH/wnKniB9lJTdpg//49JKNiWptGel9He15PW8e/feyIqeAFQWVLNn+96BNM0k6wlRN/y8QGfS7rsL3cmO08IIXpVzcVEXQC2CW3HbOz832mvBDCUUgOBc7XWC7XW5wE1wO3AIq31eVrrdb1RjzD/MjCbkyxbbg3x7IK3nlgW9Xnq2Q2kZVq9haiLewD/SrRZS1cse/qDpMuWPvUBWpto3xuJM+gg+N5k19o9HNlTHrWoLSBimpq6ino8TV5CwRAATa2do6Za67h4m70E/UGa61rQWqPRNNU2k57lY+REqxPXVNeMbv/jM+vBv4LtK3dHBS8AmuuscrW2OpPpGX5MU6Ehugxg68pqmurrOjhCXVdVWsOudXuj0sygdRw01ggJn8eP32t1Khtb93/62VYHN+AN4GvxRa2vva/RUNPI9g92JdxmKBTiyV+9SMmuUvZvPhS1rKn1OAT9QbzN0eUue3plN/ZQCCFOb8/94Smihigkogn3lXweP9o0QTcAGp8H1rylMEMBbDYrmJ+SHqB958rh8LNv7ePdr6T3NdAJOmuA9i4BYNkzq8I3RMBqJzxNVjvRWBvdL6mrbODDd7dScbgqPvjdenOmrgo2x3Qn2rYV68ieMvZ+eCDhssqSaravlAC7ODXUltd3uPzdZ5d3uFwIcWKZ/iNAIHmG5t90WkZvjcBYDNhaR2D8AZgDmMCrSqlHlVIJHy5VSt2mlFqrlFpbWVmZKEv3hCqSL9NBMGu6VEzbxX6b3ILInZW2wECkXA1m1+7e1JQnv3ivLqsF3QIdBFl0qJJDO44QF9lq1zEK+AIEA5E6BgMhtKkJhaLT2u7wWJ+DZGZ7MZRVjhkyMUMxd2VCFVGjPsAa8WCakW1rTeSuEPHVDAU1FQcOJt2/Y1G272jcnaP2fciALxi9j60jMrJyvZG0QMx3aVZQfqAy6ljFqjhcRXVpfMAq5G9/fINRy6rLuhbgEkIIEfHmo0s7yREfOAj4g6AjHaiacggFI224YdOomJhI0Huk23XUZgd9GNPqk8S2ncFgiLa6h2LbIaCmrC5undY1I3mOxuxEknrUlHV806C6tGv9IiH6uv/84vmTXQUhTm/+VZ1k8HWyvPfmwCgEnFrrhUqpXwL9gSLgPOA2rNEY98aupLV+EHgQoLi4OPGti+6wDUq+TLnAlt+lYnIKsqIeBTha4gr/bXfEHFrlAKN/l8rtP6wg6cVs0fACa8JOIzdpoEXZBjF65kiUUlF3c9p/dqW6cDgjdXS4HChDYXfYWy+sFQ6nHYfLEZWntspNKKSw2TSG3YbNFhMDsw2m//DoicIMw8CwGeFghzIUNke7DDH9K4dTUTRqRMJ9O1ZDJwzCZrdFBZSUodCh1uOQ4ow7DgDVR1MZMLShNS36u1S2wQwaU4TdaQ8HPGINGTsg7jgA2F12Qi1WXRxOR9SyRPnFiSOvOhXio+ET376K/7v2gQ5yqLiP1nnfCdq6EdF/qMbmcGH6FSiNGVRoHb2eK717E32D1S5rNiZe2NonKRxWwI41e8LJVh0VoOPaIbDajMJhBRiGirpJgHKAtoLwhUNiuk5J+j+Fw/Lj+gztFY2QOTDER8MdD998sqsgxOnNeV4nGTqfNLu3RmDUA++1/v02MAx4X2sdav08vpfqYXHNtQIACSj3+Qnf5JHIpV+4IOrz5pUZ1FQ4UUBO/+hJKJXrvC5P4jn/hrkJ0w1DMf/6uShloFKSXEypFHCfz9Dxgxg5dVj0+nbr67bZDLL6ZeBOc7VesCsycq26tf0/NTMFm8NGWlYqhmGgDIP07DQ8zU62ruvfmjeNqI6hrRCcZzB6xgiGjB8Yte3M1nINw0ApaKhxYbdrFEQmAm0164JBpKTFv+2jOzJzM5h63sSotLagi1KKnP7ZON1O3KlW8Ckzz6rnmncHA1agx+l2RlZWNnBfRGp6CsUXTEu4TYfLwXXfvYrCoflMnDM2pj5W+c4UJ67USLmGoZh/3dnHsadCCHF6OueaBXT6CImKZElJc6MMA1QmYJCZA8XzwTDshEyr49Tc5Ixa3eNJY8SsG7pfSfeFVmAhUdXc1qSC5358dlRA3Wa3kZZp9UcyYt5CVjg0n4lnjyWnIIvpC6fEFJgFKPoP1Ywvjt3WZQnrUDC4H5PnJp79fcj4gYye0TM3FYQ40UbNHNbh8kmTJvVORYQQCRnOXDoMUmT+ovMyeq46HVoBtLWw04ASIkGLacD+XqoHAEo5UZk/BPvQdokGyr0QUj/V5XKu+vLFXHLr+eE3V4SCBg//YhSO1LGkZ6W1bQxc50LaZ7tc7oSzxnDD964Od1wAMnLSuOlHH48EJdxXolKujO4Q2QpRmT9AGVbw5MfPf4sRU4aGAwSGYZBTmMXIacNBKZRSDJ0wiAmzx+BOsy7gs/plMHHOOPoPs0ah2Ow2Rs8cwdjikeG3Zrz74ng8/llk5WVFtm0ficr8EUpZeT7/m5sZPX14eHH+4H4MHjMAZ4rVKfR5HfQb4CA1M/ITNAzF1HPy+eZjv+zyseqK7//7TibMHhs+DspQZOZlMHrG8PBM9ANHFzFxzjhSW1+BuvPDAnZsmUu/Qe0CMUYOKv3rKLu1X9/655eYfM6E8OzxABk56Xz38a+QU2B9B5/+v+uYPHd8eNspGSlMOGssQ8ZF7oJl5KZzy0+vZ+iEwT2630IIcbr469ZfkXBCsHZvIVFKkVOYxeBxred1ZWfA6Al86Z4cnNaLuUjJGk7l0ZE0NUQCGE2N/cgbfS9Od/dfpapsA1AZ3wFbXrtEFyr1BqvvgTX68tZffpLsgkjbWjSikGnzJ5GRE9n2sEmDueO+z4Rfb3rj969h+oLJGK37iXIxYupEvviLDMJvQFUpqLRbUK45Set48/9+ginnTIi6qTB6+nA+/xu5Yy1OHX9a8+uk8czfbvpR71ZGCJFY7jIgQVDfcQFGaudv/FLJhgv2NKXUPUAxUAXcANwBXAW0ADdorTt8wLK4uFivXbu2x+ulg3usCa/sw1FJRmV0pqmuiaVPr8TusHHOtbNxp7rRwX3WIx624aj2HZZj4PcF2LvxAErByGnD4h45ANBmIwR3gUoD+9i40QwAu9bvY8fKXQwYXUTxoqkE/Fa5WsOo6Va5R/aUUVteR/8RhfQbkEtzQwsHthzGleoMB0H2bz5ES6OHYRMHk56dhg5VQuggGHnhi/pYZfuOUl1aQ+GwAvIH5VFTXsuKF9aSkuHm3I/Pxm63s+zp56ktr2TG4nMZNHpMt45VVxzeeYQNb28hf1Ausy+bRSgUYu/GAwR8QUZOG4Y71UXFoUoqDlXRb1Ae/YcVoLUHAjtA2cE+HqXih/Ee2VvGutc3kds/m7lXnZlw2xWHq6g4WEnegFyKRhTi9/rZs/EANpvByGnD4h85Og0opdZprYs7zxkt2bngWB8JOVbH8giJPJ6S2On0HR1L+afL99+Rnjof/OyG/+P9ZzeRluXi6aOPEQqFeObel6k5WseFn57PsElDqK9q4PDOUjJy0hg6YTBamxDcYT12YR+DMtKpO7qf6pINuNPyGTiu515xbW1rO2gf2MclfG17KBRi34cH8XsD4bapuqyWsr3lZBdkMWjMgIRlV5XWUL7vKNmF2QwaXYTWwdZtBcExHqXcXapjZUk1Rw9UhNsrIXpTT50LKioquLH/HQCMO3Mkf/jg7p6rpBCiR5jNL0PL38DoBxn3YzijRz8mOx/0WgDjeJ2oAIYQ4uSQAEb3yj6VnU7fkQQwjk1Pnw+EEKcmORcIIdokOx/01iMkQgghhBBCCCGEEN12+o1bF0KIU5zc3RdCCCGEEKcjGYEhhBBCCCGEEEKIPk8CGEIIIYQQQgghhOjzJIAhhBBCCCGEEEKIPk8CGEIIIYQQQgghhOjzJIAhhBBCCCGEEEKIPk8CGEIIIYQQQgghhOjz5DWqQgghxCnkWF6jC/IqXSGEEEJ8dMgIDCGEEEIIIYQQQvR5EsAQQgghhBBCCCFEn3fKBjD8Xj8lu0qpq6wPp1UcqqR0bzmmaZ7EmiUW8Aco2V1G7dG6cNqutXtY/9Ym/P4AAJ4mDyW7SmmsbQJAa03Z/qOUH6hIWq7WAXTwADoUyaNDR600HWzN40EH96PNumTFxAkFQxzZU0ZVaU04raa8lpLdZQRa6+tt8VGyq5SG6sZwHtO/DtO3MvwdmMEjmL5lmMHK5PsQqrLqp/1drl9cGdpEBw+hQ2WR+pbt5/C21fi8za15fK3HoSZZMTQ3tFCyq5TmhpYubzvgD1Cyq5TaivqkeVoare+2ub65y+V2hTabW/cp+bZjBQNBSnaXUV1WGykn5jcjhBCnoo1vL+WX19/I3779/XCa6d+G6VuOaSY/r1cd3knJ9jUEAz4AmuqaWPvaRg5uLwnnsc6TB9u1rd5O25RE7aQOlbeW0/W+Snfa8Z6kQ0esNlZr67PZ1FqfxqTrJGpPdbAEHTwcLkeIE+mOs77NNQWf4b1nPzjZVRFCJGA2vodZfg5m5aeOab1Tbg4MrTWvPvQWb//rfVoaPSilGDiqPwFfgKOHqgDIK8rh8i8uZtaF009ybS1vPraU1x95l6Y66+I1Oz+Tg9tLqK9sAMCV4mTYpCEEAyECvgA2m8GA0UW01LeELzL7Dy/g49+4nHFnjA6Xqz2voD1PQ+vFq7YNthaEDlv/V5loW4H1WftAKbTzLFTa51FGRtL6LntmJa/89c1wh6toeCHKUJTuLQcgPTuVfgPzKN9fgbfFh2Eorvp8gHMvWoVBQ2spaZhGCuhq0BqUDdMxA7LvwzDSrPqGytFNf4LAZmsVIx1SrkKlXHVMx1f73kW3PA6hagCqKgv5171edq5rRGtIzTD41HfTmHRGALTVkdLO6ai0L6Bs/QArIPbkr19kzZINBPxB7A4bsxZP5+PfuhxXiivptpc8/A5vPfYezQ0eAMbNGsUN37+GfgNyASu48fRvXmbly2sJ+IPY7DZmLprCdd+5Cndq8nI73WcdgJZH0N43QftBGWjnbFTa7SgjPel67/5nOa/+/W0aa6wg2Yz5OVz/laOkuA9aGYwsSLkWlXJxt+smhBAnwzfmXciezSkE/QrUNravWMyd91ZRNNgKuqPcmO7LMbIiwY0jO9fzxE//yP4tVtuVmWsnryiD5a/4CQaCgGLaOel86wEH6elHrJWMbLRtAAT3g/ZYbatjJir98yjDOvebpsmLD7zG0qc+CLeTE+f05/qvHCIz40BrObmQegPKvSDpPmkdgpZH0d7X2rXjZ7a245k9fQjjtx/YjG5+CIKt/QpbAVrlQmgv6AAoG9o5F5V+G0qlAOD3BXjqnhdZ/d/14fZ05sJCrv38btyu0tZyiiDtMyjnzBO+D+L0c/PoOyjdG7mx99OP3ctPgTfMp05epYQQUczyMZEPoXLrsxqLUfhSp+ueciMwXv3b27z84Bu0NFoXjAFfgA9eXseGd7ZYF8pAdVkt//jhf9i6YufJrCoA7z25gmd//0o4eOH3+ln3xiaqDldD6w2I5gYPm5dt5+gBa5RCS5OX5c+tZvvKXeFyyvdX8Kc7/0HJLqvx19430M1/Cwcv0CHwLwX/MiBkpYX2gncJmK2jH7QG3wfoxp8nre+q/67nX3c/Fw5emCGTta9vZM2rGzCD1t2iwzvLWPr0B1Qdse46jZx4lNnnvkrI3/4uVA2YJdB2R1+HwL8Gam9rrYoX3fCjSPACwGxCNz+K9nT+w22j/WvQjfeFgxcBn8kf7jrIjjVV4Ts8U2dXkZm2nobKfZEV/RvQDT+yAgHAIz/8DyteXEPAb9U3GAjxwctrefj7/0667TcefY8XH1gSDl4A7Fizh99/4UH8Pqvcx//vGZY9uzJcbigYYvWrG3jo2491eR8Tan4Q7fmvFbwA0Cb4lqMb7066yvLnV/PkPS+Ggxfu1ACz579M9eHIiBnMenTzQ2jv28dXPyGE6EXfPGcxO9alWsELICUtxHVfrqSlIUCwte1Ce8HzJGbjvQA01pRz3xd+FQ5eAFSXedi+ppwho632LD0rwNWfXUvFvuWRERPBveB5EdpGPmoN/rXohp+E87zwxyW8/si7eFusER1mKMDmd1dw/zcPEh4katagm/6I9q1MvmMtD6M9L1rBi7Zt+VaiG352nEesczp4CN3w00jwAqw22/sKmK3HTIfA9x668Z5wlkd//CTLn18daU/9Lax6+X0e+WlkhCShMnTj3ejA7hO+H+L08tfv/jMqeNHeIuPaXq6NECKRqOBFe3onZuM/O13/lApgBPwB3vnXsqi0xtomtGkS9AdpafSG07XWvP6Pd3q7ilFM0+T1f74blVZTVofWGo0VHNBaY4as3kx9ldUhaKxuBDTeFh9+T+SxioA/yNtPvG+t73kuemO6wepI6CDoRuvvtg5G7JDTwE50YEvCOr/+SHR9m+tbCAVDhEIhmuqaMUMmTa2PuLQFOS78xE6UoTFN6z8rgNI2PDTmkYTgFszALvAthVDix0q053nrrlMXaM+zUZ83vOejqszWujCEzW5SPN8axdJQ0xw9ZDdUBv6VVByqZOO7WxOWv2npNsr2HY1LDwVDvPnY0oTrVJfWsva1jVSX1bL29Y0J82xbuYvDO490tnsJabMG7Uvy2w5sQwd2xK+jNW/E/BannFlGWrofMxiiuS760RbteaZbdRNCiN62c/2H7NmcGpV2xsJGMnKCmCbUVsQ8ruB5BtM0WfHMYzTWtmuj2rXHtLYVZy2qJS0zSChkUnu0HjBBt904iGlbg4fBvwZvi4+lT8cMWdcNgMmRvYptq2MWxbRj4XSzEe19I/FOB3ejA5sSL+sp3petURZhAdBNgI7vV/g3oIP7qSypZv1bm6OXmXWAZvNKRen+duk6hPa+cGLqLk5bT/6y45tghw4d6qWaCCESMRtLO87Q/NNOyzilAhhVR2qi7nYD+L2RxrX9xT7A4Z2dHKATrKm2ubXDE9G+jqbWaDPSsQoFQ2ito/fJG71Ph3aUgG6GUHn0xtruzoT/9hMOImg/EPOsbXBPXH0D/kDcxXr77fu9fgL+YHhkQzAQxAyZ9Osfufi1AhjttxXTcdQa/KusO1jJmLXxnaNkgvuiPh7a3X57mqzcICmpbfNxEH6+ObL+Xg7tKO3wedxDO+IDDbUV9eGRDIkc3lFKyc7S1uORpNzt3QtgEDwQ7lwnXr4vLsnT5KXicHVUWuGgyLPLPm8geoVQGVpH/1sTQoi+6I2/PkTAp6LSBo2KnOt93uhlmI1gVnNoe/SFjGnqcIvV1GBgd4QYNDJyY8QaTRGInH+1LzzyMyy0l6MHKvDF9Efat9GHdqm4dRIKHYoJIMToqB3tATq2nxC1v764/AT3cnjHkfj2tN2+H47tepzgfRAi1hP/kzhgKIToJYHfHncRp1QAIz07DcOIbvhttsguGPbo3cnITT4XQG9wp7txOKOnGWlfR4UCFdkfpUBQPY8AACAASURBVBRKKYz2+2SzRa2fmZsBym39155qvx1b639tywwgpsNkZMfV1+6wk5qREpVms9ui/m5/vJVSKEPh9TjapcWWGpcAtoFg5MSnh1dxgJGafHl7RlbUx4yY3fI02wjHEBQYtphpX4ysTn8nmXnx84WkZaVGHZtYGbnpnZbb7d9ngu8uenlWXJIrxRk350ZLkzP8d/vvFQCVAnR/jg4hhOgtI2YWxzU1TXXt2i5bzAW1soORQWbMOVgpFS7G4dQEA4qm+kg5dnts22qLb/RUNuk5ic7tkfUyc2Prk+Sc3tm5Ptl6PSWunY7tZ8TmT9Keqkje2Da6030UoofNvurMk10FIU5vjvOPu4hTKoCRkZPOlHMmRKWlZVsTQiqlSM+KvuidffmsXqtbIk6Xg+LF06LSsvIjF5eG3cAwrKAFEA4epLfuk81mIyUjOlAx+4pZKGVHuc6L3pjKINyDMzJBOSNBDpVJVO/OSAfn7Lj6KqWYfVlxVFpaVmrruoq07DTsTnv4QjgtKw2lFGvesSYPVdAafGnfsYn5iRm5GO4F4JqfKNphcc4JTwbWGeWK/kdw5iInNntr51DZ8DTb2L3J6lClZDiw2RztVraD61xGzxhOweC8hOXnDchh7KyRcekpaW5mLJyccB2bzeDMS2YwYspQikYUJsyTnZ/JxDljO9u9hJR9BNhHJF5oZIIz/ndvs9s44+IZUWmbV/UP321s+82Ft+Gej1Kn1OlBCHGauvhzN1M0LHpEwKo3MkBbAYmsvJiAgWMGhuFm9lVXRDVDyrCC8gAZ2RowWPWmdRGvUGQXZgP2SIBdxQS3lRNc88gryomacBuA1omzXSkw47yY1dyJO3PKNhAc4xLvtJEGrvh2vCfFtq/WzZPWwLeKmUDUyAXHDEZNH07BkH4xy6y8uYUwLmbOzrhtCHGcCoYk7s+1OffqE/vvRgjRMSNjcccZVOfXR6fcFcp1372KgaP6hz+npLvJKcym38BcjHZ3xKeeO5FFN51zMqoY5Zo7L2XYpMHhzxk5aWTmZWB32MMdJ7vDhjvVReGwfADSslPJLsii36DccHAD4Nxr5zCrLSCSdlN0x0a5wNYPbAVAawfDKLQ6Fa2zolv53Kj0r6NU4rvrl35+EWNmRi7YHS4HeQOyyS3Kxum2Lv5zB+SQ1S+TnEKrU/Lms2PYs7UAu9PeGiYxsO7U2ID2AYMUyPqV9aetEJX2pag7M9bBGIlK+2ySo5lAylXgjARdsgvsfOpbIewOG20/7zeezKepMZXcosjvBuVApX8VZeRiGAafu/uTcXeOMnLS+Nzdn8QwEv8zufYblzNk3MDo6jts3PSjj5NXZHV6P/vzG8jOj+7opWWmcOsvP9nhCI7OqIw7re+7PSMNlfFNlHImXOfKL1/EyKnDwp8rSjN4+7kx5PTPweFq9z05JkDqjd2umxBC9LZFnz6f9OzI3Enlh1w8+2A/svJCpKa3GzlgK4Jsqx0aMvFMrvnaubSP1TqcDnILNHu3WgGHw3tSeOkfReQW5eJsO08aBWDkg2rftjpRGV8Pv+Hrph9+jMKh+e2Wp+BMzeeW75mktm9qnNMh5Zqk+6XSvwK2mEC4cqPSv4GKHYnZw5RrNirl0uhEW//WfW/XrhnprW2PDaUUn/vFjdEjF1UG6Tm5fPYHJu2bU+VeCK7kb2ARojseP/DnpMs+/u3LerEmQoik3F9IuqgrbyFRp8q7uIuLi/XatWsBa3LMzUu3c3BbCek5acy6cBqeRg/r39xMwBdg4tnjGDFl6EmucYTWmm0f7GLvxgOkZqZQvHgqFYereemB1/A0eTnj4hnMv24OG97aQtm+o+QW5VC8eCqVh6vZvHQbSimmzp/IwFFFceUS+BCCW0GlgvMcwAT/+9brQu2T0PbxqMBqa94EWz9wzkMZaQnr2d6O1bvZtXYf7jQXMy+YimEzWLtkI54mL6OmD2NM8Ug2vbeNwztLyc7PpPjCaaS4NoHnGasO7kvAOQ2a/gahEnCMgtTPhF+hGt4HswZ871sTj9rHgmNGt+7868A2CGwEnOA6m4Zak7UvP01TXQPDJk9k0rmXYoQ2QGgPqCxwzUXFDF31e/2sff1DKg9Xkz84j5mLpnT4ClWwfotbl+9k/+ZDpGWlMuvCaXGPnAT8Ada/sYnyA5XkDcihePG043qFaniftR/8H1gTx9nyW7/bjh+90VqzfdVu9qzfT0q6m+ILp5Gd54/6zeCYEhU4O1GUUuu01sWd54zW/lzQ3rDvvNIj9UrmwN2XdDnvsdblWMo+1vKPtewTSb6j3i+7O+WfDD1xPqg6Wsrvb7mVphoTh1szfdECPvGtK6Hln9bkk8654L46LihdfWQP6159AU9TC2NmzWT0mRfw/H2vsmP1HrLyM/nYXZfSfwjgWw54wDEF7BPAv9qaw8HIBte5ca8nD4VCfPjuNg7vOBJuJ1PTGlrL8YJjGsoxsdN91DrQOnfUAbDltZ7re+8RWR08ZLU1aOuGgW2ItQ+hI1ZwxTU3btSk3xdg3esfUnGoivxBecy8YApOhzVxNgDOM6zRhELE6Km+wVfP/T7blllvIzRsBk9WPUhWVvwjtkKIk8NsbITm2VjzNgKOhRh5f4rKk+x8cEoGMIQQpz4JYHSv7GMtvy9dvMp31Ptld6f8k6GnzwdCiFOTnAuEEG2SnQ/siTILIYQ4PX0UL46FEEIIIcRHwyk3B4YQQgghhBBCCCFOPxLAEEIIIYQQQgghRJ8nAQwhhBBCCCGEEEL0eRLAEEIIIYQQQgghRJ8nAQwhhBBCCCGEEEL0eRLAEEIIIYQQQgghRJ8nAQwhhBBCCCGEEEL0eRLAEEIIIYQQQgghRJ8nAQwhhBBCCCGEEEL0eRLAEEIIIYQQQgghRJ8nAQwhhBBCCCGEEEL0efaTXYHu2LZyF6//410ObjtMek4acy6fxaJPnYvdcXy7s3PtXl77+9vs33KI9Ow0zrp0Jhd8+jwcTkfSdSoOVfLfv77FluU7UIZi+oLJXPS5heQUZCVdp6muktce/CNrXt+Jr8VkzMx8Lr7tRoZOnhPZx+Wv8tC3/s7+bSEARk+1c9cDMygoOgxmDdiHsX/32Sx5tIK9Gw/gTndzxkXTufAzC3CnupJuu6q0hlcfeotN720FYMo547niVkhPeR/MKrAN4eDe0Sx5eD271lfhSjWYPn80hnsaG97eibfJy8ipwxg5bRhbl++gZHcZWfmZnH3lGSy8cR6GcXwxsT/f9SuW/H0tnmaNzQZjZ/bjZ0t+RWpGRjjPl876LrvX7cMMmRg2g5HThzF9/mS2r9qFw+Vg2nkTcaW62PD2Zppqmxk+aQgXfPo8xp0x+pjq8uG7W/nb/zzBoW0lGDaDsbNG8qX7PsvA0UXhPDqwGd3yDAR3gpGJcs2HlKtRynlcx2HPhv289vA77P3wACkZbs68eCaLbzkPV0ry71YIIU5nl2Vci7dZt0vRTJtv0FLrIeiHnEIbE+acTU1ldrj/MHPRNIKBAOvf2IS32cfoGSMYUzySzUu3cXB7CRm56cy5fBbn33ROVB9D+9ehPc9BcC8Y2VRVnc0fvnGYbSt2EQyEGDi6iDvumcL4aVshuB+MHPzmObz0SAprX9tEwBtg/JmjufDmfAYOWtqaJxflXgTuy1EqeVvaUNPIqw+9zfo3PyTgDzJx9lguvvV8ikYUnsCjK8Sp4+5P38db/1wWl/6G+dRJqI0QIhGz8lII7WqXYkD23zHcc5Ku00ZprTvN1BOUUp8CbgZswI1a6yNKqWuA32mtB3e2fnFxsV67di3r39rM37/3OKYZXe9JZ4/ji7+7pdv127xsO3/5xiNx5Y6bNYov3/85lFJx61QcruLXn/4jzQ2eqPS8ohy+9ciXyMhJj1vH523mnpu+yJE9zVHpDqfiaw9+leFT57JrzVt8Z/H9+H2RbV7/1aOcsbCBwWOLcLrT2P0h3P8dg6CZD0ZmON/wyUO468HPY7Pb4rZdW1HPr27+A/VVjeG0C67dyYx55fQfXoDdbmffFh/3fVMRDDoAG1pDRQlobaNw+BCUMmiqa6a2vI78wf1wp0UuqGctnsYtP70+yRHu3C9v+gFvPbE9Lj0738mT5U8AcN2g26gurY3LY3faGT55CFprKg9XEwqE6D+8AGVYx9AwFLf+8iamnjexS3XZtHQrP7ziVwQDwaj0tMxU/rj6bvIH5aH9a9CNd0PsvyHHFFTmjxL+Zrpi28pd/OlrDxMKmVHpo6YN52t/ue24g0R9hVJqnda6+FjXazsXxPp/9u47PI7ifOD4d/aaei+WLdty77133MEGGzC999AJgSSE0BIIBEIgCSWNEn703qup7rjg3rvcZKt3Xdv5/XHSSSfdqSFLAr+f5+FB2tmZffd0vpl9b3Y2446PWySuUPb9eW6j921qLE1pu6ntH8+2m9q+/I1av+3mtN8WfuznwZyws3C7gu+T2sWJvbKrMgxNdEIsTlc62tQc3Z+NxWqQ3DkJgNKiMvIO55PSJQlHjS8DBk/qx7WPXQaAdn6HLv67v6ykAK6bZlCYawHlS3KMnFrABbccpkO3FKJiIzFNk6P7stmwIp4PX6zsh8wi7PZsbn3cpHPPGgE7JmNE/zLouZQVl/PIZU9yLDMnYHtYpINfP3eDJDHET96P/SxYvXA9v5v9QMj9JIkhRNszj84AnRm8MG4lRlgcEPrzoFWuhJRSnYApWuvpWuuTtNaHKovOAg40th2tNe8/+WmdJAPApqXb2LFmd7NjfC9Eu9tW7WLL8h1BasDnz39TJ3kBkHskn0VvLg9aZ+X7L9VJXgC4XZqPnn4JgOfv/G9A8iKtq5ORU4sxTUXOoaMAfPCcwuPBNxuD6rj3bsxk7debgh7765cXByQv4pPLGDLuMKbHpDi3BICP/mficSvQHkBTXgLOcnBVeCkryENrTWF2EVprCrILA9pf9fk6Du44HPTYDXE6K/j2jW1BywqyXbzxyDOs/OyHoMkLAI/LQ0WpE2eZk4rSCtwuNyUF1a+zaWree/LTRsfz3J2v1klegG9w+8K9rwOgy16qm7wAcG8A99pGH6u2D576rE7yAmDXur1s+G5Ls9sVQoifq1DJC4DsQ9UzJ0xTgenrR0qLynBVuCgvqaCi1Onr344V+fu5mjYs3squtXvR2vR99tfwyuNQmAvgBTSGoZl70TG01uQeyvMdq7AMt9NNv2HHSE0vAjSYebgq4JMXaiW7nYvQnr1Bz2XpuyvrJC8AKkqdfPLMV6FfBCFOEPUlL4QQ7USo5AVAwSkNVm+tr3JnAxal1FdKqSeUUhal1FxgIVD3Si2E7AM5ZB/MDVm+een2ZgWXl5XPkT1HQ5ZvWRa83a0rgic2ADaHqrM8eHIBYNvqHLxeD7s3BI7E+g4v8/9cUWpSUQZ7t1QNeLygnYHHXho8EbClVrzd+uRR1Up5SQUel2bHupq34WjKa+RaykvKcVW48Xp8t7W4yl2YtS60Ny0JfuyGfPvKx3g9oWcDfffG97z+8Pv1tpF3JJ/ykooa8VYElB/dn03OodDvn5r2bzkYsmzz0u1obw546sm9uX9o1HFqK84vIXPboZDlod5XQghxorp1yi/qLTe9gUOdwjwLUE5FaXXfWVFagcfl8SeuK8p8CY2aNi/bDt5M8Ab2I+uX1mhfm6RlVBCT4AbA7XSj0QH9Ufd+eZX9tq8v3bo6yGw9V/AkeH19wOZlzet/hTiR/Grq3W0dghAnNLP45Qb2aPharbUSGKmAXWs9HSgD5uO7neSl+ioppa5RSq1WSq3Ozs7G0sAaFzZH6LUq6mO1199uqHKLre5tGv5YmlHHYlUoZWCpVdXjrh7cKAWGBQJvjw0c/ISK11rr2F5vjXYNhTLAYg0csNW8C0Ipat0WoercJtHcv0FkbES95Ta7BXsDbSsjMJ5gt3A09LeuYlhC/9Ow2qqnCYfWvDUwrDZLvbeehHpfCSHEiSqxnjWnglEK0JZa/Vvd/kPV6lttdiuouv2Qrdammn02le3UbNvrMajZb1uCdW1BjuOPIYTm9r9CnEhSuia3dQhCnNgsCQ3s0PAt+E1KYCilDKVUwytr1FUIfFf589dAP2CZ1rqeSZ+gtf6P1nqk1npkcnIyiWnxZAwIvVzGiJmDmxEaxCRE02tYt9DtzhoSfPuM4NsBhs8IHsuIWRNC1hk2tTOGYTB4YnTA9g3LozB9X9QQEWPF7oCBY6oSDTZQgQs7hnodase0c2OyP4kREROOxaoYMqHqtgkFGETUCCUiNhp7mM2/qGl4VJh/jQnwrTMxbPrAkOdXn4lnnoI9PPTb8azb53Pd3+tf4yS5cxIRMeHV8db4GaDH4K7EJTduoNt/XO+QZWNOHYEy4sDWP3QDjub8M4HwqHD6jQm92OjwZr7HhRDi5+quNx+pt9weHng7YFySF5SdiJjqxHlETDhWuxV7mC/5HB4VVmcMNXzmYJSlE1gzArZPPK3GTERlcPRAGEcP+PplR7jd3z6AqRXb1ydX9tu+vnT4lFqzD5UC+7ig5zJsxqCQ5zl8uvQPQjTkjv/d3NYhCHFCMyIauEXE0vBDF5qUwNBam8Bfm1Kn0jKgqmcdim8hz3lKqc+AAUqpRt+wdu5v5vsGFrXMvHjKj1q86uzb5xFZ64IXYOq5E+jSt1PQOjMvnULHHnWP2WtYN8afPiponUHTzmDoSR3rbI9PsTHvpusB+MXf7iMuqXpAU5hr5ZOXErHaNEnpvljOuEYTHa/AEphJHjt3RMinbZx07viABFBZiZ1vP+iJPdzmX3B0/lU24pJM/wyDsAiIjIGIGDvh0b4FVeI7xGG1W4lPDUwGnHrtbBI6xAc9dmNccs+sgIRIlV7DE5h45il06dOJARP6BK0bEROO1WbBHmYnOiGK8KgwImsOTqPDOfvX8xsdy/V/v5youMg62ztkpHDR3QsAUJFXglF3oVYVPgdl7Vlne2MtuPXUoMeecPpoeg4NnWgTQogTVUp6qNkHmtT06gRDWLiJy+Prg8MiHUTERBCTGO2fvRDfIQ6bw0ZcrVkdsy+bSoeMFABU5DWgqschp18DGX3B92A3Xx/21r/S8LitpHTx9dGRMeGER4ex9LMMCvMqxxqWZBI7KOZeUmvmY/iFKEtS0LMZM2c4/cfWTbCndEnilKumhXgNhDhx1LdIp80hs1iFaBdsM0MWGckfNVi9Of+Sv6h8esg7upGPMNFar1NKlSulvgVygAu01n8EUEot0Vrf1diDd+3fmTtf+SXfvbGM/ZsPEJ0Qxbh5oxgwPviFbWOl9+7oa/fN5ezdsJ+ouEjGnDqCwZNDf8seGRPB7c/dwLL3VrJpyTYMi8HQaQMZM3d4yEevWixWrnz0MX749DVWfboMZ5mbPqN7MuncS4lO6ABAQlo3nlzzD166+wHWfXsMZSgiE/rTYfC5WGyrwcwltVcGd742hcXv7mPnmt2ER4czes5whk0LPQPCEe7gl//+Bcs/WM36bzeD1vQcO5+kvlEY+hsws0nq1oXfvjyCJW99wY7VewiLtDHi5PFYwgay6tP1lBeX03NYN/qN683arzZxcPsh32NUzxhDn5E9ftTf4JzfXEXX/t34729fIudQKWERVmZeOporH6pejf1vix/g+bte4f2nP8dZ6iQsKoz5159Mx54d2LJ8BzaHlZGzhhAWFcbKj3+gpKCUjIFdmHLOuCYlV9K6pfL06of5v/veYNOSbVisFsacOpwL7jyTsAjfwFVZu0HsY1DxKbrGY1SVPXjyqtHH7p7Kna/cwqI3V7B73V4iYyMYPWc4Q6c2b3aLEEL83L2c+Sq/nXkDP3yVRVUSweaAs2/pz47VG/C4TZI6JjD98is5sK2EfZsyiYqPYuxpI3CVu1j16VrfY1RHdGfAhD78sHAj+7f4xhjj54+i/7jqMYay9YW4qs/+3VgdcTy+eDpvPLabJe+sxFXhovPA8aQOnEJYwkq0Zw8Y8ST1nUbviZrisvW4yl30Hdubiad3JdL+jW/RTiMBFTYLZQv9WW+xWrju8ctY+elafli4wfcY1fF9mHDGaCKi634JI8SJ6KUjT3JR2o0B21K7JfPS7qfbKCIhRE1G4lOYRX+Hsn9SvRxmIkaH4A/BqK3Jj1FVShUDkYAHqMA3UtBa65h6K/5IoR6dKIT4aZLHqDav7aa2354e0Sl/o9Zvuzntt4WW/jwQQvw0yWeBEKJKqM+DJs/A0FpHN7yXEEIIIYQQQgghRMtp1s1gSql4oBfgvwlUa72opYISQgghhBBCCCGEqKnJCQyl1FXALUA6sA4YCywHZPUoIYQQQgghhBBCHBdNegpJpVuAUcB+rfVUYBiQ3aJRCSGEEEIIIYQQQtTQnARGhda6AkAp5dBabwN+3CNAhBBCCCGEEEIIIerRnDUwDiql4oD3gIVKqXzgcMuGJYQQQgghhBBCCFGtOU8hOaPyx/uUUt8AscBnLRqVEEIIIYQQQgghRA3NfQrJRKCX1vp5pVQy0AnY26KRCSGEEEIIIYQQQlRq8hoYSql7gd8Cv6vcZANeasmghBBCCCGEEEIIIWpqziKeZwDzgFIArfVhILolgxJCCCGEEEIIIYSoqTkJDJfWWgMaQCkV2bIhCSGEEEIIIYQQQgRqTgLjDaXUv4E4pdTVwJfAf1s2LCGEEEIIIYQQQohqzVnE04kvaVEE9AHu0VovbNGo2khRbjGL317Bng37iYqLZOxpI+k3plfAPhuXbOX1h9/j6P5sEjrEc/ZtpzFy9tCAfb55fSkf/fMLivKK6dynExfdvYDugzPqPfaxzGwWvbWCI3uOkpAWz/BZQ1j85nLWfrkRFIw6ZSgTTk1m5Uefk59dTHqvNCafez5Jnfv429Bas3HxVlZ+8gMVpU56jehOv9HxrProXQ7tziI+OZqJZ59OtyETA469b8NSFr/5HvnHiujYPZWRc2azfcVydq7ZhT3czug5kxg8fQGGUZ3vyj2Sz6I3l3Nwx2HikmMZe+oQcvYvZd3XqzBNzaDJQ+jUfwbff7SB3MN5dOiWyuSzBpOSsg7t3gDKgbJPBPtYlKpuN/9oAYveWkHm1oPEJEYzfv4oeg3vHhDvwZ1HWPzWCrIP5pLaNZnJZ40lrXtqo/7GVTweD68/8j5L312J1+1l0OT+XHTPWcQlxTSpHW3mQ8XnaM92UDEoxzSUfUi9dVwVLlZ8tIZNS7ZhGIph0wcxcvZQLFZLjXYLoGIh2rMVVCTKMRVlHx7QTu33zMQzx9C1X3qT4hdCiJ+Lv1/3R/atX4bSCgyNx+3gpHOGodxLsDvcFBakM2T2LezZkM++zQeITohi7KkjcJa7WPXpWn+/2aV/Om8/9hH7txwgKi6SudfMYMZFUwKOVZC1m/1rn0V7dqNVLAldF9B10OxWOU+tTXAtRTuXAm6UbRg5ucNY9OZaDu/OIj41jkkLxtC1f+cffSy3q5zVH70W0LePmX8hjvCm3TmstQuci9Gu7wFQ9pFoS3eU8xu0eQhldISwk1HWLj86ZiEAZhpnB/zec0QG/1z1lzaKRghRm1n8DJT+FfD6NqiOGKnfNqqu8t0N0nhKqQeA84AfgOeAz3VTG2mGkSNH6tWrVx+39o/sOcrfrv03xfmlAdtnXjyFM26eA8D7T37Ks3e+gmma/nKlFGffPo9L7j0HgMd/8W++enlRQBtWm5Xbn7ueiWeMCXrsLSt28O/bXsDt8gC+i+uD2w4DYFh8F/e9BhVSlG/gCLOBUgDYwxQ3/ONaeo2aDsBL97/Fsg9W+duNjc8m/2guSllRlXWUgnNun8aUC64HYPFr/+K1R76k6i8YFuHG6/bgrLBjWKovqMec0pWLH/gLhmGwZ8N+nrzpWSrKnL4YDS+RETsoyLH4L8JLCqG0yEJK184ow0JkjJOLbllHzyERhEeGVZ+8YwIq6laUMti/5QD/uOEZyksqAl6fedfN5uQrpgGw6vN1vHDPa5hm9VvOarNw5UMXMmTKgKCvb20ul5tbxv2eA9sPBWyPTojmb4v/SGrXlEa1oz2Z6KJ7wCwK2K7CT0dFXhK0TnlJOX+79j8c2H44YHvf0b24/m+XYbVZ0d7D6MK7wCyo1e4cVORVQN33DIBhKM7/3ZlMOH10o+Jva0qpNVrrkU2tF+qzIOOOj1skrlD2/Xluo/dtaixNabup7R/PtpvavvyNWr/t5rTfFn7s58FvZ1xG0bFsnM7qhLgC+o8qYe7Fef5tBTlWXn1yGG5PIgC5h/Ox2izEJvuS16WFZRzdl43FZqnqagGYtGAcv33hRgAObVtM2eHfYbO5AmIpKp/H4Fl3NfUUmkRrL7r4YXBVfwZWlFWwbXUFL/9jCOWldsA3Njnvt6czacHYZh/L5Szjnzf8ku1r8gK2d+kTxc3/fZyI6PhGxuxCF90H7m3VG80S0IVgScM/GVgZqKhfoRzjmx2z+OlribFB7eRFlfAYBx8UyHMHhGhrZt7t4PogSInC6LC9+rcQnwdNvoVEa30X0At4FrgM2KmUelAp1aOpbbUnb/zl/TrJC4CFL37HwR2HcVW4+L8/vBGQvADfrIe3H/+Igpwidq3by9evLK7Thsft4Z+/+l/Q45qmySt/ejvgQjTnQB4etxeP2wsaImNcFOcrtNa+bZVcFZpXHngegK3f7wxIXmhtUl6Ug8el8daoozW8/bdvKMw5RHFeFm8+/hU100/hkU5Kiww8nup4AL7/dD9bFn8EwMt/etufvABISDpIbhZ4PV601pheyD8GznIvRTnHAJg0Zy+x8WXkHclHU+OAzqXgWgnAqw+9Wyd5AfDhv74g51AuznInrz70TkDywvf6ennlT2/jcXvq1A3mtYferZO8ACjOK+aJG59tVBsAuvTZOskLAF3+HtqzJ2idhf+3qE7yAmDbyp0se39VZbvP1Ule+Nr9BO3eQhJjFwAAIABJREFUFvQ9A2Camjce/YDSorJGn4MQQvwcuEqOBiQvAFCaLasj2bPF4d8Ul+Rh/CzfhXR5SQWlhaUU5hThdroBOHYgB9M0A/pNgMVvr2DLct+gKm/Pn+skLwCiwz4k99DWljytupyLApIXoMk7UkBCcgnjZu6v3qo1bz32IcX5Jc0+1LI3n6+TvADI3F7CV88/3fiGKj4OTF5oE3Q26HIwCwO269J/+WZrCNFMoZIXAOVFzpBlQohWFDR5AaAxs+c3WL05a2BQOeMiq/I/DxAPvKWUeqQ57bW1orxitq/eHbJ89efr+eqVJTjLg3eqXo+XT/77JR/96wtCTUYpzC5i45K6A5s9G/aTlxV4sVpeXO7/2TRN0rpWUHXN7vUGJlCOZlZwYMtK1nyxPmB7TFwBJYXVbdTkcWvWf/kh67/8EI+rZrya7EOVdxWZoM3AAdyaz7/j8O4sjuw5GhhvSfUFs+k1KS/FnxQpLfIlJPoO9SUyvG4vzrLA11G7lnDsQA6Z2+omFcA3EFv9xQY2L9tBRWnwzqc4v5Ttq0L/DWuqShQEs2X5jka1oc1CcG8MvYNzSdDNaxauD7odYM0X69FmGbjXhm7XtTToe6aK2+lm/bebQ9cXQoifmSdvepDC/MA7YmtMnmDbD4FrjQ8aUwLaTVlRdV9bVlRORVmFP3FRu98EzYf/WkhO5kaio7ODxqGU5tCWd5p9Ho2hXUsDfneWu/FUJrP7DT8WUOZ2eVj3TfP7gzULf6inrPGJGt+tLjWVg64cX+haCRazBNzrGt22EE31l6ueausQhDihmeVf1r+Dd1v95TRjDQyl1M3ApUAO8Azwa621W/kWMtgJ/KapbbY1t7P+b+7dTneQwUygitKKOhfmtZWXlNfZ5qpw19mmA3MKGEoTOByr3UYZrorAYxuWuoOvmm24yisCbhEBUAaYXhWihi/WYPHWynOgzZo/+x5YY7WZtbbVrOAK2m5Nbqcbd0X9r2/t16C+tkLxerwhywI0+A1R8PL6YnQ53b569d2R1ajXqnEzUYQQ4ucg/2iQhEKNzsvrCew/DYsGwxvwhYNv9mDtvimwHVeFC7er/hlu2nucv+HVge3XPAerrW7/VV9/1xBXRej+0OVsZF8JdfvLmoMEgoytZAaGOI6O7Q+egBRCtBJP8C+sqzW8MkVzZmAkAWdqrWdrrd/UWrsBtNYmcGoz2mtzCR3iSO2aHLK875heTFow1r8eRW1KKU46Zzzj5oW+Zc8R7mDoSQPrbO8+uCthEY6AbfZwW3XbFoP8HLv/d8MIHIhFxljoPGA0fWstNlpcEIe9slllKGonQPqNn0zfcZMCtmlTkdih8uJXgaECz7ff2EGk904jOj7w26yImJrxGYTVKA6PtAOK/Tvi/bE4IuwB9ZVtCGndU4hLiSWUfmN60XtUzzrnX8Vmt9ZZ7DNkW2N7hyzr3Kdjo9pQlmSwdAq9gy34Qp59R/cKuh2g35jeKCMOrBn1tDs46HvGH5dS9BvTM3R9IYT4mbn1mT8QEVU7k179Y+eegbcm7tsWDjqMsMjqz9GwyDDCo8L8fYxSqs73BmPmjiAlYxgVFaGfHh+fPrl5J9FIqlbfYg+3oSrHJvu2J9TZv/ZC5E3Rb0zoPrXf6Hr6v1rqLGytwvG/uCqiVpkFbI1bz0qI5vjLwvvaOgQhTmhG9KUN7NHw+krNWQPjHq31/hBlx/nmz+NDKcW862cHvTjuNawbAyb0ITk9kfHzRgWtP2hyf7oPzmDyWePo0jf4UyBOuWo69jB7ne1hEQ5mXz41YFtSpwSUUhgWA6Ug+3A4CSleFL4FQWuae/Vk7I4IRp08lPTeaf7tpmklqZNvYFDz6RYAo2Z1plOf4XTsNZQxp3QNKDMsFgyLxmI1qLmCWcfuEYyadwFWm5VTfzEroE5JcRrhkRrDMFCGwmqDqFgwLIqYJN9CaUs/y8DjMYhJjA54mgmWThA2DYvFwmnXBrZbZeCEvvQc1o34lFimnDMh6D7TL5xMVFzoQWVNl/7hXCKiw+tst1gtXPbH8xrVBoCKuCDgNfKz9QPbiKB1Tr5iGuFRYXW2x6XEctK54+tv19oL7GODvmeqjD11BCldQifjhBDi5yYqJgbDUAH5Bl/+QpGa7mbg2OrbFEyv4ruPfP1eREwENoeNsEhfMkMpRWyyL5FusQX2mx17dGDWJVOwWO1ox7lB4ygo6EWXgTNa8MyCCDsZLNULTRvKIDYpGpfLwvIvAvvzMXOGN/kJXTVNvfhyYpPqTtQNj7Iw68rLGt9Q2DwwanxBoaxgxPmSFUZcwK4q7DSU0bjFQYUI5vdv/bKtQxBCNESlhS6Le7/B6s1aA+PnaNi0QVz3+OX0HNoNi8UgOiGKWZecxPV/v9x/wX3Hizdz+k1ziE6IRilFZGwEsy+fxgMf3uFv57FFf2DSmWMJiwxDKUV8hzgu/cO5XPXQhSGPPfuyqVxy7zl06pWGYSjSe3fkvDvOoPeIHhgWA4vFglcNZebFQ0ns6EAZ0Ll3FJc/cAYnXXQDADa7jVv+eQ1Tz51AZGwEFquFpIwZLLh1Bl36RKMMSEyzM/+6sVz8p+qlSi66/2FOv2EciWl2lAFRCYmcedMIBozvgMWqiIi2MOWsPvzy2b/iqJxaMWnBWK566EK69k/HMBSRcR2Zcem5jD+tO/YwhdWumHZuGpfeO5+OPX37YO1Nsec3xHY8qXLQEokKm42KfQClfMmEcaeN5JpHLiZjYGcsFoO4lFjmXj2Dqx+5yB/vWb86lXNun0dKlyQMQ5HWPZXzf3cm865v/OPrktMT+cvX99F/bB+sNiuGYdClbzq/f+2XdR6JWx/lGIeK/r0vYaEMMGJ9TyCJuTvg0bA1pXZN5vZnr2fYtEHY7FbCIhyMnTuC2565jphE32PplH0kKvoe37dQ/nbnoWLvQylL0PdMUqcEzrh5DhfetaDR8QshxM/FP1Z9TESMIibeg6E0jjCT+CQ3vYeAq8KKaSr2bI1hzYrTSew8BYvFID4lhgvvXMCCW0/195sTTh/N2bedRnJ6IkopwiIcjJ83ir8tfcB/rH6Tr8Vtv5WiojRMrSgvj6So7BQGzPrPcT9PZUShYh9Ehc3wzWRQVmI6TKOc32OP6o1hKBI7xnPGTXO4+N7Qixk2RmxyZ2577g+MPrmLv28fNrUjtz17B6nd6s4oDRmzJRkV+yA4JoOy+/4LnwfRvwZLhi9hb+mEiro65BO8hGisk86cwJTzxwUtW2i+2crRCCGCMVK/A6P2Y7MVxP0XI6xDg/Wb/BjVtnK8H6MqhGhd8hjV5rXd1Pbb0yM65W/U+m03p/220NKfB0KInyb5LBBCVGmxx6gKIYQQQgghhBBCtDZJYAghhBBCCCGEEKLdkwSGEEIIIYQQQggh2j1JYAghhBBCCCGEEKLdkwSGEEIIIYQQQggh2j1JYAghhBBCCCGEEKLdkwSGEEIIIYQQQggh2j1JYAghhBBCCCGEEKLdkwSGEEIIIYQQQggh2j1JYAghhBBCCCGEEKLdkwSGEEIIIYQQQggh2j1JYAghhBBCCCGEEKLds7Z1AM2htWbL8h3s33yA6IQohs8cTGRMRJvFc3DnETYt2YbFYjBk6gBSOic1uQ2vx8v677aQtecoCWnxDJ8xCHuYvcntFGYfYM0n71JRWkavUaPoOfIkcK0C734wksAxnqP7i9nw3Ra01gye0p/I2AjeeuxDsjNz6T6kK/NvOgW73VbvcdxuFx8++Rw7Vu0kqVMCZ/3mKuKSUwP2KS0q4+3HPuTInmNkDOzMGTfPafCcvF4PWxZ9ROaWrcQkxjNiztlExCQG7KO1C1zLwJsFlnSwj0GpwHi1WQKuJWAWgrUP2jqYrSt2BrxnyksqWPvVRrxuL/3H96FL30514tGePeD6AZQV7ONQltQ6+7SW/GOFrP1yA85yF31G9aT74K519jmy5ygbFm1BKcXQqQNI6ZLcBpEKIUTrKy8v55pBN5KdWYTNbnDDk5fSrX9vnr/nNcqKyhkzdzgX3HlmnfFDQdY+Nn71LKa3lOSukxh12oV12jY9u6H0JdBl4DgJHLPZsmw7mVsPEZ0YzchZgwmPCm/9k66ktRdcK8GbWdnXT0CpsONyLI/bw7pvNnNsfzaJnRIYNn0QdketPli7KuM5CJY0X/+pmj6m+SnQnt3gWtsuxgmi2u/m/onVn64DwLAafO56vY0jEkLUZmafDt5dgA2ifocRdU6j6imt9fGNrIWMHDlSr169mpKCUp665Tn2bznoL7OH2bjiTxcweHL/Vo3JNE1efuBtln+42r9NKcXsy6Yy7/rZjW4n+2AuT970LNkHc/3bouIiue6xS+k2qO5FaiiLX/83bzz6JV6P728aE+/myrty6DYoCUNZAHjrnw6+fT8FKgc2xXkl5GUVoFR1O5Gxkdz/4R30Ht496HH2btzInac8QHG+17/NalNc99g8Zl9xMQDLP1zFXy5/GleFy79PRHQ4973za/qP6xO03cKcQzx53Z0c2lXq3+YIN7jyoYsYOGUeANqzF110P5gF1RUtiajoe1DWzr59nCvQJX8D7Tt2SSE8fVcsmbuSqJp0VFpYhsVqISzS4W9mxMwhXHb/uVgsFrQ20SX/AOei6uMohQo/BxVxbtD4j6fv3ljGW499iNdr+rcNnNCXqx+5CJvdhtaa1/78HovfWVEjXMW08yey4NZTWz3exlBKrdFaj2xqvarPgtoy7vi4ReIKZd+f5zZ636bG0pS2m9r+8Wy7qe3L36j1225O+23hx34efPa/T/jrFc8FaxllVHZwGiw2C536pPmT9D367eSU8/disVaPhQ7uSWHseW8QGRsHgFn0IJS/DpXjpaJ8g6fu7s6h/T2o6lPCIhxc+dCFDBgfvH87nrT3KLroj+A9Ur3RiEJF/w5l69eix8rad4ynbn6O3CP5/m3RCVFc/7fL6dov3RePJxNdfD94q8c0GPGomLtQ1m4tGk9b0tqLLvk7OJdUb1QKFX4+KuKstgvsJ64lxgYzjbOD7nPxfWdzyT2Nu0ASQhw/pnMF5F9St0AlYKQGXMsE/TxotVtIlFKXKKW+Ukp9q5SaoJRarJRapJR6RanKq+tGeO3P7wYkLwBcFW6e/d3LFOUVt3jc9VnyzvcByQvwzQ757Pmv2bBoS6Pbef6uVwOSFwAlBaX85zcv4nF7GtXG4R1ree2Rhf7kBcDci49is5ZQeDQLgNVfw7fvuH0zF9CYHpNjmTm4nW7MGhfGpYWlPHje4yGP9eD5DwckLwA8bs0/f/UBBdlHcVW4+OtV/wpIXgCUFZfz4AV/D9nuK/f9KSB5AeAsN3nmjhcpKcj2JRWKHw5MXgB4c9EljwKgzTx0yeP+5AXAG08oMrcVgzcbgIpSJ7mH88g+kIPpqT6PNQvX8/XLlQORio8DkxcAWqPLXke71oU8h+Mhc9sh3nj0g4DkBcCmpdv45L9fAbD8w9UByQvwvRe/emUxaxaub7VYhRCiLfz1iudDlGi06esXtdZ4XB4O7/Rd6EdF5zD3gt0YRuBna3r3Y3z3fzcDYFZ8B2XVyQuAV/+eyKFdbvAc8m+rKHPyzB0vUVpU1oJn1Ti65O+ByQsAswRd/LBvJkQLevZ3LwckL8D3Rch/f/MipmlWxvPXwOQFgJmPLn4ErQNf65+0ig8DkxdQOU54Be2SfretXJhxbciyF+97sxUjEUKElH9p8O06D7PgoQart0oCQynVCZiitZ6utT4J2AKcprWeDOwF5jSmndLCUtZ/uzlomdvlYdWnrXthufS9lSHLltVTVtPBnUfYt/lA0LLCnGI2Lt7auFjeeYea44L4ZBede5QDUFJYgUaz9JOqaRZe0KXkZxdSNQPH6wkcVOQczgv6Wu9cs4YjeyuCxuBxa97+y7N8/N8vqSgNvk9BdiHLP1pTd/uxTDYtPxa0jqtCs/qjN8C9AbzB98FzAO3eCs7vQLv9m0uLYN2Sqm/fSgGTkgJfkkRrTWlheUAzS9/73ldWsTD4cQDtDF12PCx7byWhZkote39l5T6rQtZf+m7j3otCCPFT9N2bXwMNzCatUeyu8OD1euk7JBNU3XKATp234HG7oOzFgMLCXAubVlbeKqJLAuo4y12s/rx1L1y15wC4twUvNIt8t3G0kL2bMjm0KytoWV5WAVuW7/D1w57gYxq8R8G9scXiaWv1jxO+bMVIRE3HMnPrLf/69SX1lgshji/TuYt6++yKFxtso7VmYMwGLJUzMJ4AirTWVV+jewBvsEpKqWuUUquVUquzs7Mpyiup8y10TYXZRS0eeH0K6jleQU7jYmko5qKcxs0qKax1vKjY6pkb2gva9FKYW+M+Ee3B4w76svsd3nO0zras3fvqrZNzOI/szJx698naW7fd4pws6vtipjAnD8y8etvFrLtPcQGY/tPUoL14a8y6qPkz1Pib1neshuJoYfW9z4rzS/F6vHX+/jUV5rbuzCQhhGhNqz9b2+Q6XpeX6LjQsxPCIj2UFRXW+bwvyjfQZs2sR2DH1drjkEb1iy2koXMrzC5q1XjanJlfT9nP6Dx/Zr59fWlbhyDEic21vIEd6r8+hdZLYKQCdq31dKAMmA+glOoIzAC+CFZJa/0frfVIrfXI5ORkEjsmEBEdepGs9D4dWz7yenTuHfp4nXvXXRAymI49O2AYKmR5Y8+pc+/OAb/nZtnxen3tWu0Kw7CS3rNGtks5CK+x/oNSgTEoZTBwQt17efuMHYVhCR1vn1E96Teud8hypRSDJtW9Jzepa28c4aHfjp379oKG7p21dgNL4D4JqRARXfWbAcqKPax6sTFbWODCY/6/qTUj5GGUJfjaIMdL53reA2ndU7FYLXTqlRa6fj3vUyGE+Kmbf1Mj1vlRgT9b7BaOHowMuXt+dgQxiclg7RGwPbmjB0d4VdLCQu1hVOe+rfx5a+0C9d2F24JrTqT3TqszVggo79Oxcf30z0V94wRr644TRONd8Vjrr2MmhKjBPr+BHRp+MEdrJTAKge8qf/4a6KeUcgAvAFdrrRu10IPdYWPKOeODliV1SmDY9IEtEWujzbh4ctDkg81uZer5ExrVRnxKLKNOHha0rMfgrvQYktGodsYtuIDI2OpBTFmJlY3fxwAQk+i7gp+2QGNYAOUAFU5MYjQWm6+OxRr4Vug7uied+9RNwqR06cLgicGfbBGTYGXu9Vcw6cyxIZ/E0mNIBj2H1h3AhEfGMenM4IuwpnQOY/CMBb7Fv+xDgu6DYyzK0hEcE8FSHZ/dAVPmVyZujDhAERUfhTIMrDYrETGBCbGZl54EgAo/I/hxlB3CG3XHU4uZcMbokIm7WZXxhnovWqwWpl4w8XiGJ4QQbarn0D5YbKEvrP0qd4mIicBisbBzS3fKii2+7bWqF5fN8P0QdYPvc79SWIRm0tzKWW1GfECd1K7JDJ7SuouJKyMe5TgpeKG1J8o2qMWOldQpkWHTgo+z+o7qSdd+6b5+2D4meAP2Yah6Lvp/auodJ4Sd0rrBCL85182qtzwjI6N1AhFCBGU4YoB6ntoV94+G22i5cOq1DBhc+fNQfOte/Ad4Wmvd+NUugbnXzGD2ZVMJi6iePdB7eHdufvpqbA08+rOl9R3di0v/eB7xqbH+bSldkvjFXy+lY48OjW7ngt+fyYTTR2Oz+55qaxiKoScN4NrHQixwEkRMYkdufvo2uvSJ8m9b/kUHXOYQohJ83whl9FNcfX9vEtOrZ0AMnNCHjj1SMSxG5bENBk7sxx/e/03IY9399qMMOyklYCZGx+7h/PmLe7DZfAO9hxfeQ8aAzv5vawzDoO/oXvzpkztDtjv/1ruYccEA7GHV7fYZkcDN//ojVqvvb6uibgfHBPyPTVEWcExBRfkWXFPKjor5A9iqB5GnXGJn9iVDcET5/iY2u5WRMwczcFI/f3yxSdFcdPdZ/ifZKPsIVNRNgQNUSydUzO9RlsbNrmkpccmx3PTklaT3rp5lER0fyTm3z2PMnOEA9BzajSsfupDEtOp4k9MTueaRi/0rwwshxM/VuwX/h2Gtm8QwrIb/KSQWi0HHnh3oUfkIarcrkm8/mUH2oRj//uWlNvbunMrMq/5YWT8DYh8Bo/px3vOuLGf6+d1xRFV/tvYd1ZObn7oKi6XRa5K3nMirUWGzoOpx4kqBfSQq5vctfqiL7z2bcaeOxGL1nadhKIZPH8RVD1/k30dF3wyOKdUzQ5TyPdY16lctHk9bUvZRqKgba40T0lExd/sSOaJN3PrU1cSlxgQtW2jKIp5CtAdGh/VAkEdr20/HCJvUYP1We4yqUupRYCSQAzwBfARUreb4d631u/XVr/3oxIoyJ1l7jxEVH0lSx4TjFHXjmKbJwR1HMCwGnXp2qHeKZX1KC0s5diCXuJRY4lNiG64QQtaejZQXF9Kp73Dsjgi0WQLew2AkoiyJmKbJ4V1ZaK3p1CsNwzDYt/kAB3ccpveI7qR0CT7DoracgwfYumINHXt0pcew4LNI9m89yIFth+g1vBupXVMa1W5ZUS5Ze7YQk5hCUufgj6TTZr7vqSKWFJQRF3wfbxaYxWBNR6nwoO+Zw7uzcDvdpPfpGHTgqbUXvHsBG8ra+EfaHi9H9h7FVe6iY88OQRN2pmlyaOcRlFJ06lX/dN+2Jo9RbV7bTW2/PT2iU/5Grd92c9pvCy31efD9J8t55U9v0G1gF37571sBWPz2CnIO5zP1vPHEJccG7Qu2rfiWiuJ8eo+dTkR08Isf07nSt3CnfQyGEUl5STlZ+7KJTohq83EIUKevP56K80vIOZRHQoc4YpOCv17aLPAtvG1JRtWarfJzUj1OsKOsXdo6nJ+8lhwbXJhxLYU5RVz/xBXMuXxGi8UohGgZpnMDFD8Glq4Y8X+oUx7q88DaKtEBWuvba22KDrpjI4VFOMgY0LnhHVuBYRh06fvjv5WPjI2kW2zoe3Ibq0P3wCmjyogCo3pdCsMwSK+1LkLGgM5Nfj2T0jsz6az663Ttl97kGQARMYl0H1p/9k0Z8XWm79bZx9IBLNUzYYK9ZxqaKaOUBaw9G4i49aR1S6233DCMoLf+CCHEiWDMnHGMmTMuYNukBWMDfg/WF/Qde1KDbRuO0QG/h0eF021g+7lgrd3XH0/R8VFEx0fVu48y4ipv3fx5a2/jBFHt5X3/ausQhBD1MByDwfG/ptdr+VCEEEIIIYQQQgghWlarzcAQQgghRPv2c7w9RQghhBA/HzIDQwghhBBCCCGEEO2eJDCEEEIIIYQQQgjR7kkCQwghhBBCCCGEEO2eJDCEEEIIIYQQQgjR7kkCQwghhBBCCCGEEO2eJDCEEEIIIYQQQgjR7kkCQwghhBBCCCGEEO2eJDCEEEIIIYQQQgjR7kkCQwghhBBCCCGEEO2eta0DaGuHd2dRlFtMp15pRMdHtWks+ccKObrvGAkd4kjpkhx0n41LtnJo5xEGjO9D5z6dGtVuQXYhWXuPEZ8aR2rXZLweL3s3ZYKGboO6YLFa2LJ8O5nbDtF3dC8yBnTmaGY2nz//DbFJ0Zx67SwsFkuddotyizm8O4vY5BjSuqXi9XrZt+kAptckY2BnbHZbs16HwpxDHN6+ntjkNDr2HobH42HZe6twuzyMmzeSiKjwOnVKCkpY8dEaHBEOxs0bidXa8Fvb5XLx/hOf4Sxzcuq1s4hLjiXncB45B3NJSk8kqWMCWrvAsxOwgLU3ShloTyaY+WDNQBmxlBaVcWDbIaLiIknv3bFZ5yyEEOLHe+EPr/PuPz4lIS2e5zY9DsD/7n2N3MN5nHHTHLoPzqAkP4uc/asJi06lQ49RQdspzi/h0M4jxCRG07FHB7TW4N0NuhysPVEqnKK8Yg7vyiI2KYa07qlB2/HvU9lPaq3Zt/kArgo3GQPScYQ7/H1pXEosHTJSGnWeWpvg2QF4Kvsme7Ner/bG6/Gyd2MmKOg20Dc+aYzsg7nkHs4jpUsSCR3ij3OU4qdipvVsMGHYrME88tndbR2OEKIW07UXnB+A0QEj8txG1zthExjHMrP53z2vs2/zAQCsNgvj54/m7NtPC3qxfjw5y528/MDb/PDlBkxTA9B3VE8uu/88YhKjAdi7KZP7z/4rxw7kAKCUos/Invzxw98GvaAHcDndvPrgO6z+fB1erwlAfEosznIXZcXlAFjtVnIP5VGYU+RvVxkKZ5nLN2AD/vPrF7ns/vM4+7Z5ALhdbl5/+H2+/3iNv9245BjcLg+lhWUARMdHMv/GUxg/L/jgMBiP28kbD97Lio9343H7jm1YHBzZZ8VZ4QHgyZtsnHLldK555GJ/vSduepavXl6Ex+XbJyI6nCsevICTL58W8lj/veMl3vnbx3g9XgBe/OObJHdOIjYpBq01SinmXaGZduZBLIbvtcKIQmPzJS8AU1t479neLP5Q43b52knvncZlfzyPjj06NPq8hRBC/DjHjh3jwg43+H8vzS9lpnF2wD5fPP815/+ygEmnlmKxeKkoho3bUkkd+AApXYcA4PV6efPRD1n2/ko8bt/n+qgZYZx30yEc9jwANA5WfpPBq48r/z7dBnXhsj+eR3J6IgAet4fXH36fFR+v8fcziWnxuF0einKLAQiLcBCbFEPOoVx/X9pjSAaX3X8eiWmhL8K1ay269F/gzfZtMKIg/DxU+Jwf9Rq2tVWfreWtxz+iOK8EgJjEaM761WmMnDUkZJ2SglJeuOd1Ni/fDoBhKIZMGcBF955NeGRYq8Qt2p+qxEWVtV9sYKZxNi8deZLU1ODJRiFE6zG9XshfAJ5tgO+azyz+E0T/vlGJjBPyFhK3y80TNz7rT14AeNxeFr21nPef+KzV43np/rdZ/cV6f/ICYNuqXfzzV//zxebxcOecB/3JCwCtNdtW7eTe+Y+EbPe1h97l+09+8A+MnGVO1n+3hd3/NcHZAAAgAElEQVTr9wFgmiY7V+8m+2AuuvLYznIX5cUVmN7qT363y8Mzd7zMpqXbAHyDuw9WVbdb7mLDoq3s+mGvv05xfikvP/A2W7/f2ejX4d1H72fJe7v8yYvSYtixzklJQVl1LE43Hzz9Ge8+8QkArzz0Dp8//7U/eQFQVlzO0798ni2VA5ravntzGW89+oF/UAlgejVH92WTte8YAD0HHqPPgG/IOZjp20Gb4N4M7tWAC4DPXjT5+vXtuCuO+ts5uOMI/7jhGSrKnI0+byGEED9OzeRFKKdelsPYmdl43OX+bdExR8nd/ktcFaUAvP/EZyx6a7k/MREV62TC9C/IydzsT+oXHD1KWoevGDAy09/O3o2Z/OOG/+Jx+/qitx77iKXvr/T3M26Xh3XfbGLbyp1VYzUO785i1edrKaq8YAfYvX4fT938LKZZ4+qrBu05gC7+c3XyAsAsQZc+g3Yua/A1aK92/rCHF+593Z+8AN8sz//d/ap/zBLMv2//P3/yAsA0NWu/2cQLd792PMMV7dhNE34XkLyo6aK0G1s3GCFEcAWXgGcr/g4RACcU34fp2t1g9RMygfHDlxvJPZIftGzJu9+36sVnXlY+a7/aELRs/5aD7PxhDx//ayHFecVB99m+ahdH9h6ts70ot5hVn60N2OYbGGjcTjcVJRUU55X4kxBej+//ukYSpeZ7SmvNv297gdLCUr7/eE1AuyVV7brclBeXB9T56qVFoU49QFlxPkvfD0w45B/zxaC19mXqavjw6c8B+PSZr4K2Z3pNXn3o3aBl/7vn9YB/LlWDUsA/eBp1ki+55Sx14nK6QJeCdoPWYBbgdsG376nKgxVTs7csyi1m1aeBr70QQvwcZdzxcZP+Ox7WLdrY4D42h8mkuYWAr5+r2QeEh5eye+VLVJQ5WfLu9wH1ho47jN3hxev2UlZUjmmalBT4kh1V/USV3MP5rPtmM6VFZSz/YFVAWUl+KVprPC4PZSXlmF7TP2OxakZGlax92Wxasi34iVR86uuLgtAVH9T7GrRnX728OOBLnCqmqfn65cVB6+zdlBkyubFh8VaOZWYHLRM/b9uW76q3fMOGhj8vhBDHj+ktB3eo6yQTiu9vsI0TMoFxeFdWyLKKMid5IZIbx8ORPceCdtpVDu/KYte6fSHLTdNk24q6sxyy9h3zJyequJzugJ8rylz+37XWeGslCQKHeHA0M4djmTm4a8x2qNtuYNnh3aFf65pyD+zE5Qw8nrs6PHStb6Pyj/oGooXZRSHbPLKnbmIHID+rIGSdqgROclppdRwVHqpmXfh2clGUB2X+MacJOvC8D9XzHhNCCNFyXrn/7Qb3SUj2EBZR3Y/U7lOcJbvIO5Jf5wuM5I7VMwLcTjcel8ffTyQkl2NYAts5vCuLnIO5dfpJd41+0l3hxu3y+JPn7lr9ZlU7wWhvZtDtAHj2hy5r50L11wCHQowjjuwOXQekHxbBPXnp820dghAnNu9OwBu63Gy4LzshExgJHeJCllkshn/dibaOBSAuNZaUrsEX9PRRdOpTd+HIuJTYOtusNmvAz3Z79e9KBWs5cGNMQhRxKbGoWjtbbZagPwPEN3B+/rZTOqFqvRtrLkVS+5gRsb51P8KjQ9/jGpsUE3R7REzwNUNqHqeowFEdh81CwHIxykpkDFhrrlGqap13at3XXwghRMubdPa4BvcpLrDg9VT3I8qo1Y85UohJjMZiCeyIivKr+xiLzeLrDyqrlpbYMb2B+8d3iCMmKQajdvu1+kmr1UJVQ7X7TainDzGSgm9vqKydiw8yZqmSkBp8HBHXQD/b0PhKnJhOvW5mW4cgxInN0gUIcuFZRSU22MQJmcAYefJQwiIcQcuGThtEVFxkq8WS1j2VHkMygpbFp8YycGJfzrxlDvaw4CuMd+rZgd7Du9fZntI5ib6jegZsqzovi9VCeHQYsSkx/gt2w2LBYrEEJjJqvbcuuPNM4lPjGDihb2C7lU9vMSwGEdGByYFJZ44NGndtsUmdGDK5Y61tVXEoDEvgerNTzvINWEMvEqqYf+PJQUvmXT87cM8aJx0e5Rusrlvmi8XqsBIWYQcVhf+fi4olLAJGz6icMaKigOoBqM1uZeypI0LEJYQQoiWdds3sBvcpK7HwwyLflxNKqYAEvcdjpduIi4mKi2TotEEB9dYvT8PUvsWtI2MjsFgshFcunF3VT1QJjwpj5OwhxKfEMnBiv4CyyLhIQPn6yZgILDYL4VG+cUjtMUdUXCTDpgfGUUWFzQp5jiqs4dehvZq4IPRYYdJZwcv6ju7pXzS1ti59O9G1f+cWiU38tETGR9RbPu+aU1opEiFEMIYlDiwZIUoVRN/WcBstGtFPRGRMBNf85WIia30T32NIBufdcXqrx3PFn86nY4/AVZHjUmK59q+XYrFYiIyJ4Nb//KJOEiM+JZZ73gz9R77kD+eQ3jvN/3tETDgpXZJJ6ZKEUgrDMOjQLQVHuB3D4hvMWewWlKECLuoVcNI545l2wSQALrx7AV37p/vLw6PCSO2aTGrXZP+3WoahmHb+xCZdyF9wz+/pNrB61kRsAsSnKGwOa41kimLQ5P5c9fBFANzwxBX0HdUroB3DMJh79XQmnxX8W7nzfntG3cGhAkeEnY69fE8PWbukEzs39yA5PclXqCxg6eT7T/neN2deq+kzIg4s1TNkwiIdXP3wRSFnfwghhGh5N//rqgb3eeOpZHZvisAeVj19zu2yY8TdQWSc77P/vDtOD/hSIScrim8/GERSeiqG4RsyJabFkXWoJ8u/6OrfLzI2gl88eon/yRcX3rWAjIHVF9COcDsde6SS1i3V308mpsWT1iOV6MTqR7hHJ0Rx7V8vDfmlhbL1R0VeUWfWnwqbAWE/3aeQjJw1hFmXnhQwc8UwFCdfPo1h04IncwzD4BePXlLniS2pXZO56s8XHtd4Rfv1Xu4LIcsuvOfMVoxECBFSwougas+iUxB2Doaj4VmVJ+xjVPuO7sWfPr6Tdd9spiS/hC790uk5rFubxBKfGsfvX72VrSt2cGTPMRLS4hg8uX/A888nnTmWEbOG8MGTn3EsM4feo3pUdvahc1BxybH87qVb2L5qF4d2ZhGfGsvgKf0pyi1m46KtAAyc1I/I2HA+eOpzsvYeo+fwbpx8xTQ+f/4bvn19KZExEVx6/3l07VedsIhJiOa3L9zE9lW7OLjjCLHJMQw5qT8lBWVs+G4L2jQZOLEvSZ0angJUU1R8Kr9+8Tl2rFxI5ubNxCQlMmTmmRzbX8Cnz3yFy+lh+oUT6Temt7+O1Wrl0W/uY8OizXz35goc4Xbm/mIGnXqk1XMkePjzu9m+Zhcv3/82znIXp14zkwlnjGbTkm1kH8gluXMiAyf2RekscP0Aygb20aAiwLUCzCLCY3twyzP92btxP3s3ZhIZG8GQqQNDzu4RQghxfJx2zWxOu2Y256ZfQ96RfAyLwa/+ey3dBnXlqZufpayonMnnjGPubWdzYPNXFGb9gNWRQLdxZ+GIqB5ERcZEcNsz17Fr7V4ytx4kKj6KoVMHYLO7wLUcdAUW2yDGnpdBYu89ZG49RExiFEOmDsTuqE6MRMdH8Zvnb2THmt0c2HbY3086y1ys+2YTrnIX/cb2Jq17ap2+1Ga3BTtFPxV+Kjgmgut734Ke9mEoS6fj9tq2ltNvPIVJC8ayabFvfDJocj8SOoR+nCxAxx4duO/dX7Np8TZyDuWRmpFM/3G96x0biZ+/heab3DX/Qb7/sHKhQAMWet5s26CEEH6GJRlSV2KWvgQVX4IRB9G3Y1jTG64MqJpPYGjPRo4cqVevXt3WYQghWohSao3WemRT64X6LDheTziosu/Pcxu9b1NjaUrbTW3/eLbd1Pblb9T6bTe1/bb6G7X054EQ4qdJPguEEFVCfR5IiloIIYQQQgghhBDtniQwhBBCCCGEEEII0e5JAkMIIYQQQgghhBDtniQwhBBCCCGEEEII0e5JAkMIIYQQQgghhBDtniQwhBBCCCGEEEII0e5JAkMIIYQQQgghhBDtniQwhBBCCCGEEEII0e5JAkMIIYQQQgghhBDtniQwhBBCCCGEEEII0e5JAkMIIYQQQgghhBDt3gmXwPC4PXi9Xv/vpmlSVlJebx3TNPG4PQHbatcpLymnvNzZpFi0NtHaHbDN7XKjtfb/7vV4A+INxu1yY5pmg/vUbLduLBq3yx2yPJSykvKAYzudTpzlZQH7uJxltasF8Hg8Df4NvF4vXk/g6+ByNj3e2oL9DY6X5ry+Qgghmmbdoo0cO3bM/3tJcREHd2UG7FO7TwzWD9Xu30zTbJHP8cb0Oy3VN3m93jrjl7aktRet20884udt3boNvPLwW20dhhCiHqYnG9NT0aQ61uMUS7uzd1MmHz79OdtX78awGPQb3Yv92w6yfdUuPC4PMYnRzP3FTC68c4G/Tv6xQt5/4lN++GoDHreXHkO6UlZczobvtlJeUo4j3EHHXqkc2plFSX4pAHEpsdz81FVMPGNMyFi0mYcufQlcS0G70ba+LPpkEF+/nknOoTyiE6IYNLEvBdlFbPt+JwADJ/Zj/o0nk9Yt1d/O5mXb+fg/C9m3+QD2MBujZg9j/o0nExUX6d9n8Tvf89VL33HsQC7R8ZFMOH0Mc66ejtXm+9N7PV4+eeYrlrz7PcV5JSSnJzL9wklMPmtcva/nc3e9whf/+5aSglJsdhuDJiaTc/Do/7N333FWVPf/x19nbt1e6UhvKiAdFFCwYYuxxhZjTIyJxhrTE03ySzHJN71oYmxJjDGaGDUCKoKgKCggVUBEmvSybN+99fz+uLt39+7eu81ld4H38/Ew2Tln5szn3rnMmfnMzBl2bKoEC70H+5h6YT4b3jlAaVGYgl5eZl59Kmd+5vZ4Gwd3F/HLzz/A+iWbiIQj5HXP4ZI7LuDyuy6Kz7P/o4M8/4eXWLPoPaJRy7AJgynsk897b26k+EAp+T1zmXHVVM66bjrGmJb8FGLbILIPW/kEBJeCjWA9ozDp12A8I1rcRovWYy2v/XMxrz31Jof2HCanMIvTrziVWTfOxHGOu/yhiMgRc0HatYTqJba9/giXfP4gk88pxeuzvDTbz7JFw3B849i1eS/+DB8nTRnG+qUfsHnlFqKRKAW98+l/Ul82r9xK6aEy3F43J04exuBT+rP2jQ2EgmEGjerPRV86hxGThrYqPhs5hK38OwSX1PT9J9X0OyfXm+dAzTxLwYaxnpNr5jmpVes6uOsQz/1+LqsXrScaiTJ8wmA+cessBo7s16p22osN74j1uaEVsWnPOEz6pzHu/p0Sjxzbbp38dT5YtjU+/di3/gXAvOgznRWSiDQQLboNgvOB2AXqqMmB3EdwfKObXbbDEhjGmM8ANwAu4DrgWuCTwHbgs/YIXgbfsXEXv/nSQ/EDm1AwxIt/mUckGMbj84CB0kNl/PMnz1JVVs1N919HRWklv/rCgxzafTjezhvPvk1laRUerxvjGCpLK9m4dHPs8zmxk+fi/SX86Kpf8/NX72P06Y0POKytwpZ8FyJ742VzHnmfOX/fBK7eYNIo2nOYp//vBdIy/RT0yQdgzevr+XD1Nr759zso6JXHusUb+NM9fyUajV1BClaHePP5d9iydjvf/PvteLweXn78NZ7/40vx9ZQdruClxxawb9t+vvDz6wF4/L5/sWLe6vg8B3Ye4qmfPUdFSSXnf/6spN/n7297mJcffy0+7fVVsOq1UiJhE/8esnMO8fqzpXh8bhyXi0N7gvz714soKyrmk3fdSzAY4u7p93J4X3G8ncP7S3jsu08Rqg5x9TcvpeRgKb/6wp8oPVQWn2fJC8uoKKmk54DuePweivYW8+xvZ1NyoJTL765LfDTFRkuwJd+BaFFdYWgttnQj5PwY4x7SonZa4vk/vMQrf1sYny45WMb//vQKB3cVcf19V7bbekREjmezvFcRDdfdLWGMJb97mO2b/Ey9oASAASOq6T9sDY/eXwKcSHlJBf/+9YvYqMXjix0O7dt2gN2b9+LyuHB7XIQCId59dTWrF77HgJEnALBl7Xb+eMejfPl3n2txEsNGy7Gl34FI3Z0hhNZjS38A2T/AeE7ERstq5jlYb573aub5fxjP8Batq7SojF/e9CAlB+v6zo3LNvPhl7bz1Udu4YThfVrUTnuxkb3Y0u9CtLyuMLgCG9oAuT/HuHp3aDxybHvyZ/9OSF7Ud45zpZIYIl1A9NCXILQgsdCWwOGriBYuxnEXNLl8h1wCNsb0Ac6w1p5lrZ0BBIGZ1tppwBrgkiO5/pcemZ9wVab0UDnhYBgLRCKJj1689OgCgsEQbz23LCF5EawOUlkau7209lGGcKjeIw31ns6IRqP88c7HkgdTvSAheVFVDq8+XXPnQM0JdemhcqLRKBWllYQDdbdaVpRUsuDJxQC8+Od58eRFfXu27GP5y6sJVAUSkgz1rXxtHTs37WbX5j0JyYv65v1tEdWVjR+JqSyvYsE/FyeU+dMChMMGC1gLud2CHN7vwUKjW1cX/HMN5cUH+O9vZickL+pYnvvDXAAWPb0kIXkRDoUpL67EWktJvXKARc+8RWlRYllK1S8nJi/iqw5hK59uWRstUF5cwWtPLU5at/TFFRzcdajd1iUicrzav39/QvICID0rgsdn2b3Nx5b30uLlxoFzPrUPgMN7S4hGolhriUZi/Wlt/177/7H6WP9TWi8hEIlEmfOX+S0PMjAvMXlRy4axVTX9TvUricmL+DwhbFXLT7oWPb0kIXlRKxQI8dKjyY8LjqiqFxKTF7VsJVQ93/HxyDGt9m4LEenCQgtTVESg5J5mF++oe9hnAS5jzHxjzO+BScDCmrpXgSlHcuWbVmxJmK4qrRuToeHYEdUV1Wx4axObln+YUF5RUn+ZxomDhuNL7P5wb6N5AGxobcL0to0QrM0T2NjzP4F6iYOGSYRNyzdTVVHNjo27krYP8P6yzezYsIvqitRjcmx8ZzOblm9JWV9dGWDbuh2NylfOX5eQDAIoL3XVm7IU9gjH8zm2wdAcoaBl68o3WfXaupTrLi+uYNt7HzXaBoHKILWZooafLRyK8OGqbSnbrM+GUq+bpupaacua7YSCyZ/1tdY2+f2LiEjL/PbzDzcq86fV9cnbN/kT6voNrSYSKqO6vO6ZWxuNxpMYsYLY8YGt199XlCaO57R51dZGYzOl0nS/EzsusOG1zc7TEg37zsS6zS1up7009dkbHhOJHGm3Tv56Z4cgclyLVr8FNDF2Y7j5c7GOSmD0ALzW2rOASiAXKK2pKwHyki1kjLnZGLPcGLP8wIEDbV65L92b2K6r7mMnGzUhKz+j0TKOq5mvqkFDbo8rxXxpCZO+hMlYI7WPYTT8Oza/D7fHhcudon3An+HHl+5rMlx/hq/RZ0zWTkOZOemNylzuxORNwpijSb5gX0YG/vTGbccXMYaMnHS8ad5G5bUcp3HDzX3muoZSr7vh9vk4fGlNf7/Nff8iItK8XoN6Niqrf23C60s8UAqHDMbxJvavxtDcMEoN+x2v39P8sUG8/ab6ndp+tYn+pxV9k7+JvjBZv37EmSb65nbsc0VaYtBojbsi0qncjfvsRJ5mm+ioBEYJsKjm7wXAACC7ZjobSPYsAdbah6y1E6y1E7p169bmlU88b2zCdG637PjfDQ8+uvUtYNDoAUw8P3GZrPzM+MGOq2aZ+gc7psGZ+tiZI5PGYnynJ0wPOBEKetUGkwVARk2SwDgO6VmJnfvE88bg8XoYk6L92nn6jehDz4Hdk9Z7vG7GnjmSU2acHBsDJInu/Qrpf1LfRuWnzDiZnHrfH0Bmduzg0BBLMuzc4sfjiSU1Gh7w5fXwMnj8TC68+eyU8fcZ2otufQuY1GAb+DP98e2VkZ2YSMkpzGL4hMEp26yv4TZIrJveojZaYuj4QeR2z0lal5bpZ+S09h0wVETkeHTbbz/XqKyirC7Jf+KEioS61W9m4rh8ZOdnxcscl4NxTDxRbgw4jpNwjJDbLXF/Pv6cU1o8eLTxnZG6zju9Zp726ZsmnDcmdd2s1HVHSpOfvR37XJGW+OpfvtzZIYgc1xz3IJpM2Kd9qvk22i+cJr0F1A4pOgb4CKjt0c4Glh7Jlc+6cWbCybg/w09WfmajgxOv38tdf/oiAKNPP4kpF46P1zmOQ2GffBzHid/94PLWHCAZEu40yC7I4mt/r3vbRn3GOxbjP6deu/Dpr0ZjV+Od2ICdmXmZ+DP8FPTMTbhCdPKpw5l2WeztJpfddSGFNQN81nfWtdMZfMoAAD793SvwZyRe+XAcw9XfvJSMnAwystO59tuXNUoy+NN9XH/flSkPzG77/U14vHWJj/170snOi8S/g1DAweOP4nKBu958Xr/hM9+/AZfLzYRZYzjt4omN2vZn+PnKQ7FtMOmCsYyZUTc6u+MY8nvl4U/3kV1Yd+Dp8Xm4/r4rm7wrJYH3NPBNbVzuHghpV7SsjRZwHIfPfO9KvP7EJJHL7eLT916BL62Fd4yIiEiTRk1PHOAyWO1QdtjF9AuLKexZ9yjfob0e3pg7DIDswizSMv243K74BQm3140x4K4Z1NNxxY4TsvMz8dXrT3v078Ynbzuv5QF6JoJvRuNy9wmQfnXsb+8USJbEcA+AtKtavKqJ541JehGl/0l9OfezSWI40vzngjfJqPKekeC/oOPjkWNaU4N0ZhVkpKwTkQ6U/UOS3qbv9MTJvrvZxTvkLSTW2lXGmCpjzELgILE3kPQyxiwGdgC/OZLrT8vw85W/fInlL69m/Vvv4/K4GHf2aMoOl/Pin16hrKicgaP68+l7L6dH/9hdC8YYrv/elYw/9xSWv7yKUCDEiMlDKeydx79/PZu9W/dR0Dufi289l1f+uojVi9bjGMPki8Zzy29uwOtN/XiAybwldhIdfB1rqxl22mjufXYMb/53NXu37COvZy5TPjGePVv2s2bRehzHcMrMkYyZeXL81Zt53XP41j/u5O0XV/DBii2kZaUx6YKxDBtfdxfCoNH9ue+Ze3jzv++we/NecnvkMvWSifQeXHfrzuQLxtFvRB/efO4dDu8tptfgnky9dBJ5Ke4cADj1ovH84e37+ceP/8OODTvJ6ZbNJ245lx3rlvDW8yuJRi0TZ03m9E+dzvLZczi0u4geA7oz7cprKOhT94aPbz95F6//ewmzH3qVipIKho0fzDXfvoxufWMjzzqOw00/+zRr39jAyvlriYQijJw2ghNG9OHt2e9ycOchuvfvxtRLJ1HYu3EyJ+X3bxzI/Ar4TscG3gKCGM848E3HmPZ9rGPEpKHc+3RsG+zbtp+C3vlMu2wS3fu1/Y4iERFJ9KtFP+IfP/kPj3/3qXiZy9+DkpJ+vP3qKvzpUXZ+mMeMz97L5V+pYtu6HWTmZXLqxRPYtHwzr/w1NnD1yacO4+zPnMELf3yZrWu3k5WfycW3ziKvRy7L5q6iujLA8ImDmXTBuCYf1WjIGAOZt4NvGjawGAhgPKeA7wxMzSMWsXnuBN/0mnmCGM9Y8J3eqr7JcRw+/9PrWLd4I+++uoZIKMLJU0cw/tzRCRcfOooxXsi6F4JLscG3AYvxTgbvFIzpsJfhyXFkXvQZznES3/Q24fzR3D/73k6KSETqc9IvJuoeFhuwM7ITjAf8F+Pk3Nei5U3DwSe7qgkTJtjly5d3dhgi0k6MMSustRNau1yqfcGAb85ul7hS2fbTC1s8b2tjaU3brW3/SLbd2va1jTq+7da231nbqL33ByJydNK+QERqpdofdNQjJCIiIiIiIiIibaYEhoiIiIiIiIh0eUpgiIiIiIiIiEiXpwSGiIiIiIiIiHR5SmCIiIiIiIiISJenBIaIiIiIiIiIdHlKYIiIiIiIiIhIl6cEhoiIiIiIiIh0eUpgiIiIiIiIiEiXpwSGiIiIiIiIiHR5SmCIiIiIiIiISJenBIaIiIiIiIiIdHnuzg6gPYSCId787zusmLeGcCjMSacOZ8ZVp5GVl9nkcjawBBt4FaIlGPdg8F+EcZ/Q5DJb125n4b/eYt/2AxT0ymPiBePYvXkv697cgGMcxp41ipxuWbwzZyWlh8roO6w3Z147jd6De8bbCFYH+ev3n2bpC8sJBkMMOWUg539uKO+9sYi9W4vI75nJGZ86k+Fjo9jQcsBgvJPAfx7GpMXbKdp7mAVPLmbzqq2kZfiZdME4Jl84Dsf5eHmpbe99xOP3PsWHa7bh9XqYcvE4Thz7ASteXktVeZRBo3M56/rLyC/YiQ1vAycffKdjooewwaVgoxjvBDauGc7iZ1dzaM9heg7ozoyrpzLg5Ka/34aC1UEWP/s2K+evJRKJcvJpsW2bkZMRn8dGi6F6Nja4CowX453KL279kKUvvkuwOkRBn3y+/MtZTDjjI2x4Mzg5GN+ZGN/UJtddXlzBwn+9xfol7+Nyuxh71iimXTYZr8/Tlq81rqK0kkVPL0n4zUy7bBK+NN/HaldE5Hj2l2/8gv/+dgmhoME40Gewnxt+9CVWL1hPVUU1Q8cOovzACraueZPyEgevz5LXI4N7/jQVql8FGwDPCMi8C8czrMl17dmyjwX/XMxHG3eRlZ/JlE9MoLK0iuUvrSQYCDF8whAK+uTxwh9fZu/WfWTkZnDuDTO47M4LP/bnjEYroOzXEHwdbBi8Y9i971rmP/khe7bsI7d7DtMvn8LJpw1vsh1rA1D9Cja4BGwY4x0P/gswTtbHjlGkM23evJlbhn0roazHwG488eEDnRSRiDQULfoaBJ+vV+KHnIU4afnNLnvUJzBCwRB/uO0RPli5NV62ff1O3pnzLl95+BbyuuckXc5WPIKtml03Hd4CgUWQfS/Gc3LSZd6Zu5K/ff9fRKMWgK3rdjD74VfJzs8ksyZZ8u78NVRXBujRrxvGMezYuItlL63kliNEjLkAACAASURBVF9/lhGThhIOh7njtO+wc9PueLsHct7goa8uwu314DgOB3aWMH7q/ZQUeMku7BGLL/Q+BBZDzo8wJo09W/bxq5v/REVJZbyd95d/yHtvbuTz91+HMaZN3+f6Je/z3U/8lGB1MLZeG6Fo61/5x5w0jBNr0+XaTcm2+8hwdcPrzwG7Gaqew5o0cAoAmP+vzTz7Jy+4+wAudmzcxfJXVvHZH17DhHNPaVEswUCI3936MFvWbo+XbXvvI96Zu5J7Hr6F7IIsbOQgtvRbEDkUn+ezYzeyZ7sDNd+Bz7MVX+S7FG3PI69HbuxzBVdCaD0m8wtJ111ysJRffP4BDu0+HC/7cPU2Vs5fyx0P3ITH27YkRtnhcn5504Ps33EwXrZl7XZWzFvNnQ9+QUkMEZE2+P5l9/Dmc9uB2H7fRmHnB9X8+OrfMHTcYAA+WreQUDBIJOwDYv344f0B/nn/s1xzR1msocBiCC4jmvsgjm9C0nW9v/xDHrjzUULBcGxd1vLaU2/i9XnI7RE75ljz+gYO7jyE2+vGGDi8v4RHv/Mk697cyH1P39PmzxmNVsChyyFSdwwRqZxHpp3HtrXT2PdRNjs27mLN6+u5+NbzOO/GmUnbsTaALf0ehDbVlYU3x46Dcn6CcXLbHKNIZyoqKmqUvADYt/UAF2V9mhfLnuiEqESkvuiBT0FkVYPSaiiZAmmbki5T31H/CMnSF99NSF7UOrTnMHMfnp90GRvenpC8qKsIYiseSbpMKBjimV++EE9eAJQeKiMcDHN4XwnRSJRQMEzpoXKCVUHKiyvqLRvm6f+LZZj+86sXE5IXxokSjYSJRiFcczA0ceZhCnsGKT5QTiQcrAsivBWq5gLw7G9nJyQvar07fy3rlzS/4VN54K7H4skLgAkzSti6wY/FYq0FLGdfUYRjLMX7DsRmsiVgq8EWAyHKS+CFRwwQgmhxvK1o1PLML54nHAq3KJa3nnsnIXlR68DOQ7z82GuxiaqnEpIXzz8Ce7YbwIKNbasrb9mP1xdNSEYA2Oq5scRVEnMfWdBofoglMZa8sLxF8SfzyuMLE5IXtba99xFv/vedNrcrInI8iyUvkrCwe/MOADzeKsIhU5u7wOUC48Cbc3LZ9n69pL8NQNlPUq7r6f97Pp68AKgsq6K6oprSojLCgTDWWg7tLsJaSyQUSVj2nTkr2fB22/toyv+QkLywQCQUxucPc8VNaxJmnf3nVyg+UJK8nep5CcmLuMheqHq27fGJdLKrun0xZV2gItCBkYhISo2SF3Wi+2Y0u/hRn8BYtWBt6rrX1iWvCC5J3WB4Gzayt1HxpuVbGiUMqsqqgdjVl6ryaqpKq6g9MqosrUqYd++2A+zZso83n1+WUN6rXxXVVbHNYGuSI8PHlMcqLVSVJR582OASgtVBNixNfQC0cn7q76QpwWCI7et3JpRl5dRLoFjo3idEXrcQANUVlmg0CLYmWWMt2HLWLYVwqGaZaEVCe2WHK9icJOHU2s+xsmbb2sDShPKXnki88yS/R4i+g2PbKRqNUlWeuF1S/RZWNvW7WpDid9UCKX+TtH27iYgcz5793V+brK8sDRMJ7Kb0cOym09rLEI6r7oLEqsXpiQtFPiQabXyRYM/WfezZsi+hrKpef19ZVkVVWRXRSBSI9TuJLHMfXtBkvE0KvpHYWtTW5urpN7Q4oS4SibJm0fqkzdgmjoOaqhPp8mzT1b+/8y8dE4eIJBUtf7rpGezupus5BhIYNpp6T1V7ANFYJEV56vpkbVmbuG6bsNdsHFckHCEaTmzbJNkCycrqxxaN2oQ7QVoSa4vUOxCq1XAtxmnwmZMskHi8lux7aFl8TX3GSPx7TGyr4bGi0yDeRm3a5L+Fpn5XkXBzv5/UIk1sm0ijA10REWlOpLoFV1VN02c10UiDxy5t/H8Si5P0DQk9v7XNnkBFIm3vQxp10vUke3I09fFAE/2NVV8kx67ifaWdHYLI8c1WNT9PM476BMbIaSNaX+dJ/lwrAK5e4PRuVDx0/CD86YnjE6Rl+gEwxpCW4YtPx+rSEuYt6JVH7yE9GX/umITyvR/58fpiByS1Y0x8+F7NAJUG/JmJg2kZ7wT86T6Gjh2Y8iOMOv3E1J+vCV6/l96DeySUVVfVGybFwP6dXsqKY2W+NIPL5QNT78qVk8FJE8Bx1U3Xl5bpZ8jYAS2Kp6ltO2pazWf0jk8oP/3ixIO7g3u87NvpjYXvGDKyG1xl8yb/LYyc2sTvanrbvt/m2o1/JhERabErv35zk/W+dBcubx8ys2OJg9rzfButO+M/eVKDuy3cfXEa9F8APQd2p6B3XkJZQt+flUZadhpOTX+ebFDtmddMazLeJnknJUwax8QTF7u3ZSfWGZOyHzWe8UnLIXacIXKsuvepto9BIyIfn5N1QzNz5DVTfwwkME67ZFKjk26AjOw0LrjprKTLGM9wSPYGCmMwGTckHQDTn+7joi+ek1CWXZiFy+UiOz8Lx+3C6/eSkZOO2+smM6/uwMdxDJfcfj6O43DVNz5Jfq+6DRMJufCnxw6o3J5YYuCd+XmUlbjJykvD7ak7MMLVHfyxEcw/edv5eJK8DWPYuEGMPuOkpJ+7JW788bW43K749Duv5tF3cABD7GDIWsPC5/PAQE73ms9hcsB4wMkGvOR2g7OusIALTOJAYBffMqvFA1VOv3wKPQd0a1SelZfBeZ+LDUxm0j6VkCS57h7IKQQw8ctRzz3SjWjUkNutwYCuvikYT/Lv6rzPn0lmbuOD116DejDtssktij+ZWTfOJCu/8dtxuvcr5PQrT21zuyIix7MRk1IPOnnCiP41f7lxjI1nMMLh2A0No08tZ/iYencdGBdkJj/JcRyHy+68MJ6gAMjITo/1/9npeP0ejDHkdo/F43InHmaNmDiUCee0bCDrpLJuh3oDbBrA5XYRDjk893jiAOQzPnUahX0KkrfjPy92waYhJwfSLmt7fCKd7MFN96esc9xH/WmPyLHB6Zu6Lmdus4sf9W8h8af7uPuhLzHvb4t499U1hIOx16ie+9kZdD+hMOVyJvNucA/HBuZDtATcQzBpl6Q8oQU489rpFPTOZ8GTb7B3+wEK++Tzqa9dwoEdB1m7eAMut8NZ10wnMz+Dd19dQ/GBUvoN78PZnzmD4RNio6CnZ6bxuyU/4U93P87KBesIB8MY71iu+Eov3n9nFXu2lpLfM4tqziZ/gA8begdwMN7JkHYpxomdhA8a3Z+vPXorLz++kA/e3UJ6lp9J54/jzOum43K5Un6G5px60Xi+95+v8rfvPc1H7+/G7XXjyx3PhZ//kBWv7qKq3OLPLMCbfwn+vBIIb4m9ecR7Q+x7DC0FG+GS2ybSe+QwXn92PQd3FdFrQHfOvG46o09veXIlPSuNr/zllti2nb+GSDjKyKkjOPezMyjsHXvFjnGfADk/g6pnY28WMV7+unYS37t2KxuWfEg4HGHnlt7s3H0xI6YWQb3XqOK/IOW6u59QyNcf/zIvP7aQ95a8j8vtMO6s0Zx7wxmkZfhTLtecgl55fP3x23jl8YXx38zYmaM454YzGt8dIiIiLfL7pQ9z7yfu4p25O+OPEmbnOVz9ratYv3QrVeXVDB13NcW7F1C0awclRW58/ijd+sJn7h0DzgqwIXAPgMw7cJp4zfbYM0dx54M3M+9vi9i+YSc5hVlcfteFVFVUs2LeGgKVASadP5b0rDTm/+MNDuw8RHpWGtMun8xNP73uY31Ox8knmv90bJDR0HKwEVxpw9m/5wr82fvIyt9Lfs9cpl8+hdMunpiyHeNkQs79UPXfxNeopl2GcTW+KCRytBgyZAj3PHYLv7zxwYRyT5qHORVPdlJUIlKf030B0QOXQKT+OE0Gcp5p0WtUTaMxDLqoCRMm2OXL2/72BxHpWowxK6y1rb5XOdW+YMA3k7xZqB1t++mFLZ63tbG0pu3Wtn8k225t+9pGHd92a9vvrG3U3vsDETk6aV8gIrVS7Q90L5WIiIiIiIiIdHlKYIiIiIiIiIhIl6cEhoiIiIiIiIh0eUpgiIiIiIiIiEiXpwSGiIiIiIiIiHR5SmCIiIiIiIiISJenBIaIiIiIiIiIdHlKYIiIiIiIiIhIl6cEhoiIiIiIiIh0eUpgiIiIiIiIiEiXpwSGiIiIiIiIiHR5SmCIiIiIiIiISJfn7uwAOoq1lg1vf8D6t97H7XEz7pzRZOVl8Pbsdyk9VEb/k09g3Nmj8Hg9TbZTVVHNO3NWsm/bfgr75DP5wnFk5GQkzHN4f0ms3YOl9B3em9FnnMTsP89j7RsbyMzN4JO3nUdutxyWvbSKQFWQ4RMHM2TcQFa8vJo9W/aR3yuPyReOIzNzOwSXAQa8k8HVAwKvQbQIXAPANx1jfAnr3rN5Dctmv0B1RTVDJ4xm+GkX8e68tezevJe8HrlMvmgcmWmLoHouYMF/AU7aRQltRKNR1r6+gU0rtuBP9zLhvDH0Gtij2e94y8pFrJr/GtFIlFFnTKXPiGm8PftdDu0uoufA7pxy1sk8+f/+w/JX1uBN83D5XRdy9qfPaLbdJf9bxiuPLyISjnDaJRM594YZOE7rcm/BQCUrX3qabes+ICs/m8kXX05BnyEJ81SWVfHOnHfZt/0g3fsVMvG8U9ixdhHvvbk09ps59xwi0Qz+88snKD5QzrDxA7jiG7eQkZXdqlgAolXPQvU8wAX+i3HSzk2oj4QjrF74Hh+u2kZ6dhoTzx9L9xMKW72ezmQjuyHwOtgKcI8A7xSMcXV2WCJyjFr71hJe+v13CVQZXB5LVuEAPJln8L8HXyYSjtBrcE9+ueirlO94ACe6jqjNJbPvl1mzOMK/fv48gYoAp5w5ki/94nLcob9DeC2YAsj4HI5nWLvEaMPbILAYCIDnFEKRkSyft5ad7+8mt3sOky8cR1bWaqh8BgiC9wxIu6rVfZ4kKi+uSDgemXj+WNIy/J0d1nHJRvZCYCHYcnCfCN7JGNP+pyMXpl9LsDoUn/7xnG8z6byx7b4eEWmbaPl7UH4FEIkVOKfgdH+mRcseFwmMUDDEQ1/9O+8teT9e9t/fzyEUCJHbPSdeNucvr3Lng18gv2de0na2b9jJH+94lPLiinjZi3+ex5d+dQPDxg8G4N1X1/D4fU8RDsU2RrA6yJ4t+wCDMbFlFj2zhMzcdLrVnJC+/PhrlB8uJ69HLsYxGGOh4hecdl4Yf20HW/kk2CpwCoCahiqfgpz/h3H1jrXzl1/w/ANL47HNe2IjZcXPktuzD47LjeNEGXDCN0kfXo7j1LQRWEy08m+Q9ySO46aqoprf3/Yw29Z9FG9n7qMLuPiWWZz3uTOTfi/RaJR/3Pd1lszeVvddPrqeqoq/ktezLxhDMBjko2t2Y6M2Hv7PP/tHnv3tHB5Y9rOk7QJ86/wfsfaNDfHpd+ev4bnfz+U3i3+IP71lBx9Fe7byu5vvZf/O6rr4HlnGdd++kFMvuxGArWu388Bdj1FRWgWA40SY98gvOLzfYGoOHOc/uZxtG73UfoDVrx9g9sMruH/utxg8tmWdYjRaDUVXQ3hLXWHgdaJV4yD3URzHoexwOb/78sPs+mBPXbwPz+fKez7BjKumtmg9nc1W/Q9b8Vi9ktngHgDZP8A4WZ0Vlogcox7+6t2sf2sjJUVp8TJj9pGd9ySBytg+p3T/Nta+cAkjJ1WSlmkB2P3+F5j3cC82r4wdC1SVbWPmrD8yYlwEV22+NTCXaMYXcDJv+1gx2oonsFXPxqdDJc+x6g3LU78dQTgUW1m37B8wcvJBXPE++i2o/DvRgqdxnMyPtf7j1fvLNvPnr/6N6spAvGz2Q/O4/Q830XdY706M7Phjq+ZgKx8Ba2tK5oD7hJpjg9x2W885zpWNyr5zwU8YOnEgD7z983Zbj4i0TfTw/RB4rEHhaqJ7h+H03NTs8h2S0jfGDDDG7DPGLDTGvGKMcYwxTxhjFhljXjXGHNFLy/OfeCMheREJRyjac5jSQ2VUlVXFyw/sPMQ/f/LfpG1Ya3nsu/9MSF4AVFcGeOTbTxIOhSkvruCv3/tXPHkBsG/7AUKBMJFQGIid7EfCEUoOllFVHjuhPrS7iPLiCooPlAIwZuouhpy0l4O7irDWgo1AZB9Ei8GW1a08WoQt/x0A29a8mZC8ADi0D8pLwhTv2wfABdetp+/AYsLBMLb+jKGNUP5LAF7448sJyYtaLzz4MlvX7Uj63Sx78YmE5IWNwsE9UHY4SFnRAQD2fLCPaCQa+zz1bF65lSd+mDzb9swvX0hIXtTauWk3f7zjsSRLJPfUj/4vIXkBEI3AP34ym6I9W4lGozz6nX/GkxcA+YU72L8zQqhmW7rcYbZt8GKj1Ot4obIsys+u/1WLY6HsJ4nJi1rBd6HyIQD+8+sXE5IXEPv9PfPL/7Fn676Wr6uT2PCOBsmLGuFtyctFRD6mHe+to6Qo8Q6vaNRQUuShe5/Y/v/q2/fTvU+I/btjd1qWHXYIBQzX3rmXnPzYldrr79lDRlaYon3RuoashYqHiYY/bHN8Nrg6IXkBcGj3YXr03s/UWdsAOO3crYwYu5dIwz46shNKv9vmdR/PQsEQj3z7yYTkBUDZ4Qoe/c6TnRTV8clGdmErHk44hgIg/BG24tF2W88s76dS1n2wbGu7rUdEPoaGyYt6ovs+0eziHXlP4jxr7Qxr7bnAGCBorT0DeAy47kiueMn/lidMV5RUxk+kK0oqE+rWL91EycHSRm18uHob+3ccTNp+WVE57731PiteWU0oGI6Xh8NhApVBAKJRi7UQjdTtuIv3lxCoChIKxA6cKmqSI6Mm7Y0tE4lSWVYVuwWfmoOpaL0EBkBoEzayi7dfmJ1QHAxAsOacvaI0ANYy9rTdANiathNUzyMajfL2nBVJPyPAkheWJy1f+sLihOmqiliCAKC8OPb91r+NjwZ919xHFiRtd/4/3kgZy7KXVqasq6/k4C7eW7o/aV00Asv+9x82Lf+QQ3sOJ9SVHqr58qwlGo0SDkbifW6D8Nm9tYrt773XongILExdVz2bQFWAd19dk7TaWsvbL77bsvV0pkDy7QlA8E2sDXZcLCJyzPv3//2K3dsSH6fExu5gsBZ6DwyRlRvm5Ik1FyAsREJQWhxLeDgumHRWKXmFIYaNiSWyK8tN4r7eRqHi8TbHaAOvJUyHgiGCVbF94ciaPn/K2Ttqw2vcRweWtHndx7O1b2xsdOGp1t5tB9i6dnsHR3Qcq34tdV1wKdZWpa5vhWi44VFaoid/9u92WY+ItE20fE7TM9j3m66nYxMYM40xbxhj7gZ2UXcemAscSraAMeZmY8xyY8zyAwcOtHnFZYfLE6brHxhEwokHCdZaKksb70TLDyfvAGtVFFc26iSjDdrG2oTMcyQcTYglGo2ChbSMupP9WH2kXiP1/66dqYSyhuuuN5uNWqyN4ktLsmxcJZFwhOqKQMo5KlIcBJSXJC4TqbeaaCRKJNLUeiFQmXydlWWpO7NAVctOgiuLD8XumkihvLgsnmRJaL/+DRsWwqFGsyQo2r23RfFgq1PXRSsIVAYT7uBpKNWBWFdio40TgHWVoaa/AxGRVtr9wapGF3XrM8aSnhnF1DviCQVNQt+QkR0hMzdMbdrCRmncd0SL2x6kTdwv1u/7a/t8f3pdR9P48yjx2xapjltqJev/5QixZU3UhWOPSXeARU+91SHrEZEUQh//32BHJTD2AMOAmcDZQE/AZ4zZANwCPJtsIWvtQ9baCdbaCd26dWvzygeN6p8w7U3zxv/21fsbIDM3g8K++Y3a6H/yCXXjRiQxYOQJDBjZL6HM7XUnDLxljMHUayMtw4fX78HUDI7h9XvBwO5tdYNC+tK8YOqN9WAajPtgfODqz8CRgxKKPT7iY254fA7GcXFgd91go7XrjHP1x+P10HdYr9SfcVS/pOUDT+6ZMO1Lq/+3G5fLlfC5G+ozNPk6+5/YN+UyPfq37PdQ2G8oWXmph3oZOPpEBpzct9H3kZNfN20cQ1p66nV4vIZhUya2KB5cfVLXuQeRlZ9JQe/kY7AADEyxDboS4xmeutLVE4zGwBCR9nPJ1+4lPbNBtqHeLj1Y7XBwr4ey4rpHTPwZFo+3LkuwbaOfvTu8VJbH5nF7odG4mZ7xbY7RuBMHAfX4PPF+sbbP37O9ru9vdLzhND+QtjTWVJ/pcjn0O7GJPlnal7uJgXBd3cC03xgYTfnB3K93yHpEJAXPN5uZIfU5Y60OSWBYawPW2gprbRh4ERgJlFhrTwS+D3z1SK5/1o0zEw4G0jL9eHweHMchKz9xUKyzrpue9E0ked1zmHLRhKTtjz79JHoP7slJpw6j/0l1J92O45BdGGvf5XaBAcflxOvyeubicrvIyI0lFnIKYyd277x2ApGIwZ/pr0lqpNX850CDQY6M/3yMk8GUS68lt1td3C4XZNbMmlMYG5xs7lMjsFGD45jEgyPjgsxbATjvc2cl/Yy53bI57ZPJT9LP/Myn8frr2vP6IC0jliTJLoglg7ILar5nQ8Lv0nEcbv3djUnbvf6+K3B7GicfjDFc9bVPJl2mIY83jbOvm5S0rtfANEaffTmFfQqYOGtM4jo8hRgTi88YQyjsjR8gN0x2TPvkoJa/iSTjZhIuA8ZX6IbML2OM4bwbkw+WWtA7j4nnjUla16X4TgdX96RVJu2KxskzEZGPod/QoZwwuOGdXRYDZGZH2LgynUjYMP/fseSw2xNLXOT3CGMM7N3hZc2SLEJBF6/9NzZPTn6DWyCcfEj/dNuD9M+CegMY1x5/WGDpq7GLLHOeGkE45CTpow1kfK7t6z6O9R3Wm5FTRyStO/XiieQUtv4tYtJGvumxt+klYdIuwyQ7NmqDGdee1mR9z549m6wXkSPLycykySRF5q+bb6P9wknNmIRLrlOBIUBRzfRBIKfRQu1o6LhB3Px/n6HXoB618TDpgnHMuHoqXn/spD8rP5NL77iAWZ+dmbKda751KWd/+nT8GbFnbb1+D6dfPoXP/fiaeLu3/e5zTJw1JpawAAaO7Mf4c04hIyd2Cd9xHPqN6MO0SyfF5xk2fhDn3jCDgt6xk/2iAwVsev8aCk6o92YL76ng/wSYmrsonExM+qfiB1QZOd2486F7OXFSYfzOi8GjPJx7/Si6nRC7wrB1Y2+WL7kUl69eB+IUQPb3cXzTARh31ihu+MFV8bsAjDGcOHkod/3pZjKyk9+G0HPQKG7/45cZcFLdZh41LZ2zrh1HRl6snf4nncDwSUMSkkMZOel8/fEvM3z8kEZtAgybMIRv/O02CnvX3RGTXZDFF35+PTOvmZZ0mWTO+fxXuOyOaWTnx5IhjgvGzOjNHQ/9FLc7Fs91917OmddMw58e27bVVb2Zesk4+g7Lin8P48/OoP+J/vj36/Ubzrx6CPc8nvotKg05aedD5jfAqXeXhas7ZP8Uxxvb3lMvmcQ137qMvB6xfxaOYxg17UTu/vMXYwmtLs6YNEz2D8A7vu42IFchJvNLGH/y5IyIyMfx9X8tYPgplfUSzdB7QICi/YZIuOZNUv/J56V/9qDHCbF5PB4IhXJ49P6hRKOxfdXcf/Rkx9ahZOXV9HfGgGcE5P8Vx2n7i9uMk4fJ/iF4RsbLcnuOYP+hz7Bv9wkAHNqbx5uvXYPLV+/uQycbMu7ASU89MKE07fP3X8u0Syfj8cX6+7RMP+dcfwZXfb1lF0KkfRjjjf0b8E6oOzZw8jGZN2P8s9ptPd954m7ScpK/pW5etGWvaBSRI8vp+T5Jkxju03EyL2h2edPwrRBHgjHmAuCHQABYDHwbeAooJJZEudFa2+Tw3hMmTLDLlycfRLI1Du05jNvjimfdK0orqSqrIq9Hbjyh0JxgIETJgVKy8jPjJ7wNVZZVUVFSSV6PHNweN+FwmC2rtpHbPYfu/WKPP5QWlRGsDlHQKw9jTNJ2beQgYDCugth0tDL2LK2TjzHJT2ZLD+0mUFFKfp8huFxuQsEQxftLyczLiL/3PBr6ALAp320fjUYp2luML81LVl7LX91WtGcrNhqhoE8sKVFdGaCsqJycbtl4fR4ikQgrXl1LTmFmysRFMts37CQUCDFkzMAWL9NQOBTg8J6tpGUXkJmb/BGUYHWQkoNlZBdk4kuLbYODOz/A7fWR2z12K+r+HTs4vHcvA0aNxNfUsyXNiIY2gvHguAcnr49GKdpzmLRMPxk5GUnn6epstBRsJTjdG11dMcassNYmv62pCan2BQO+OTvJ3O1n208vbPG8rY2lNW23tv0j2XZr29c26vi2W9t+Z22j9tof7PjgA57/xY/pNWQ0V3ztKwAs+Ndidm7cxWX3XEhmZiaRYAUlB1biy+hDRm6sT9m8aitF+0oYM/MkvF5v7JXX4Q/A6Ybjbt8rtjZaDDZQs180dX10bjppmbFnMKPhbRCtwPGe3K7rPp41PB6RzmOjZbEB6o/wscEXx36F3Vv288NFX2fMmNEfL2gRaXfR8r1Q9RXwjMXJ/Vqj+lT7g7ZfTmgFa+0coOGQo1d0xLobKuiVOL5ARnZ6yjsLUvH6PHTrW9DkPOlZaaRn1Q0G4Xa7GTYh8YQ9Oz9xLIBk7RpX4htmjZMONB1vdkFvKKh7t7nH27hdxzO0yTYcx0m486Gl8nslJhj86b6EJI/L5WLSrNY/BtHUeBgt5fb46NYv+a2ktbx+b6PvqrBv4nfVvV8/uvf7+GNROJ6mY3Ech8I+Tf/OujrjZAO6RVdEOka/oUO5/c+PJ5SdeVXiHXsubwb5fRLLGibHHccP3lFHJEbT4FHQpH20e8ARWffxrOHxiHQe42QBR348rD+vbMVr7kWkwzmZPSGz9a+07pA7MNqDMeYAoPddiRw7+ltrWz06r/YFIsck7Q9EBLQvEJE6SfcHR00CQ0RERERERESOXx31GlURhgy8jwAAIABJREFUERERERERkTZTAkNEREREREREujwlMERERERERESky1MCQ0RERERERES6PCUwRERERERERKTLUwJDRERERERERLo8JTBEREREREREpMtTAkNEREREREREujwlMERERERERESky1MCQ0RERERERES6PCUwRERERERERKTLUwJDRERERERERLo8d2cH0FKFhYV2wIABnR2GiLSTFStWHLTWdmvtctoXiBx7tD8QEdC+QETqpNofHDUJjAEDBrB8+fLODkNE2okxZntbltO+QOTYo/2BiID2BSJSJ9X+4KhJYBxvrLUQWgOhZYAB72SMZ2RnhyXS5djwdggsAlsJnpHgnYIx2rWJiIiIiHRF1gYhuARCG8Bkgu8MjPuEFi2ro/wuyNoItuznEFxWV1g1G+ubjsm8E2M0dIkIgK38N7byybqC6lfAPQCyf4BxsjotLhERERERacxGi7Gl34PwR3WFVc9CxmcxaRc3u7zOhLui6rmJyYtagTcg8FrHxyPSBdnw5sTkRa3wNqj8W4fHIyIiIiIiTbMVjyUmL+Llj2PDO5pdXgmMLsgGFjZRpwSGCADVqf8t2MAbWBvuwGBERERERKQp1lbHHh1JpYnz4FpKYHRFtqxtdSLHEdvkv5Ng7D8REREREekabDU0dZGxBee6SmB0Re4RKatME3Uix5Mm/y24T8A46R0XjIiIiIiINM3kgKtn6voWnOsqgdEFmbRLwHiSVPigBQObiBwX/DPB1T1plUm7soODERERERGRphhjUh+nu3qCb3qzbSiB0QUZ90BM9n3gHlJX6BmByf4BxtWn8wIT6UKMScNk/z/wTgBjYoWuXpisOzG+aZ0bnIiIiIiINGL8MzGZX667EGkc8E3BZP8QY7zNLq/XqHZRxnMyJvfn2GhxbNrJ7eSIRLoe4+qOyf42NloONgBOPqY2mSEiIiIiIl2O8Z8FvpkQPQzGj3EyWrysEhhdnBIXIs0zTiaQ2dlhiIiIiIhICxjjgKug1cspgSEiIiLHlAHfnN3iebf99MIjGImIiIi0pw4ZA8MYM8AYs88Ys9AY80pN2deMMYuNMf8wJtmIlSIiIiIiIiIiMR05iOc8a+0Ma+25xphuwExr7TRgDXBJB8YhIiIiIiIiIkeZjkxgzDTGvGGMuRuYBCysKX8VmJJsAWPMzcaY5caY5QcOHOigMEVERERERESkq+moBMYeYBgwEzgbmACU1tSVAHnJFrLWPmStnWCtndCtW7cOCVREREREREREup4OGcTTWhsAAgDGmBeJJS/61FRnA8UdEYeIiIiIiIiIHJ06ahDPrHqTU4HNwBk102cDSzsiDhERERERERE5OnXUIyTTjTErjDFvAbuttW8DrxtjFgNjgOc6KA4REREREREROQp11CMkc4A5Dcp+BvysI9YvIiIiIiIiIke3jnwLiYiIiIiIiIhImyiBISIiIiIiIiJdnhIYIiIiIiIiItLlKYEhIiIiIiIiIl2eEhgiIiIiIiIi0uUpgSEiIiIiIiIiXZ4SGCIiIiIiIiLS5SmBISIiIiIiIiJdnruzA5Bj18Fdh3jzuWUc2l1Ez4E9mHrJRHIKszs7LBGRY8KOjbtY+r/llBdXMGh0fyZfNJ60DH9nhyUi0um2rtvB27PfpaqsiiFjBzLpgrH40nydHZaItAMlMOSIWPXaOh79zpOEQ5F42at/X8Stv72RIWMGdmJkIiJHv3l/W8R/fz8nPr38ldXM+/si7v7zFynsU9CJkYmIdK7//ekV5j4yPz697OVVzP/H69z5py+S1z2nEyMTkfagR0ik3QWqAvz9/z2TkLwAqK4M8Nfv/YtoNNpJkYmIHP32btvPc3+Y26j88L4S/vXz5zshIhGRrmH7+o8Skhe19n90iGd/M7sTIhKR9qYEhrS7tW9spKq8Omndod2H2bJ6ewdHJCJy7Fg2dyXW2qR1G5Zuory4ooMjEhHpGt6Zuypl3arX1hGsDnZgNCJyJCiBIe0uVfKipfUiIpJaU/vQaNRSXRnowGhERLqO6ib2j5FwhGB1qAOjEZEjQQkMaXdDxg5IWefxuhk4ql/HBSMicowZOn5QyrqC3nnk98ztwGhERLqOIeNSj7PWe3APMnMzOjAaETkSlMCQdtdrYA8mnHtK0roZV01V5yEi8jGMPuMk+o3ok7TugpvOxnHUtYvI8WnCrFPoNahHo3JjDBfefE4nRCQi7U1HOXJE3PCDqzjvxjPjyYrc7jlcevsFXHL7+Z0cmYjI0c3lcnH7H29i6iWT8Po9APQa1IMbf3g1p35iQidHJyLSeTxeD3c++AWmXDgejzf2ssW+w3px00+vY+yZozo5OhFpD3qNqhwRLreLi2+dxUVfOodAZQBfuk9XBUVE2klGdjrXfedyrv7GJQQDIdIy/J0dkohIl5Cdn8Vnvv8prv3OZYSCYe0fRY4xSmDIEeU4DmmZaZ0dhojIMcnldpHmdnV2GCIiXY7b48bt0amOyLFGl8RFREREREREpMtTAkNEREREREREujwlMERERERERESky1MCQ0RERERERES6PCUwRERERERERKTLUwJDRERERERERLo8JTBEREREREREpMtTAkNEREREREREujwlMERERERERESky1MCQ0RERERERES6PCUwRERERERERKTLUwJDRERERERERLo8JTBEREREREREpMtTAkNEREREREREujwlMERERERERESky1MCQ0RERERERES6PCUwRERERERERKTLUwJDRERERERERLq8Dk1gGGO+YoxZXPP314wxi40x/zDGeDoyDhERERERERE5unRYAsMY4wNOqfm7GzDTWjsNWANc0lFxiIiIiIiIiMjRpyPvwLgJ+GvN35OAhTV/vwpM6cA4REREREREROQo0yEJjJpHRM6w1i6oKcoFSmv+LgHyUix3szFmuTFm+YEDBzogUhERERERERHpijrqDozrgSfrTRcD2TV/Z9dMN2KtfchaO8FaO6Fbt25HOEQRERERERER6ao6KoExHLjFGPMScDIwATijpu5sYGkHxSEiIiIiIiIiRyF3R6zEWvuN2r+NMYuttT8wxnyj5o0kO4DfdEQcIiIiIiIiInJ06pAERn01bx7BWvsz4GcdvX4REREREREROfp05FtIRERERERERETaRAkMEREREREREenylMAQERERERERkS6v1QkMY8xgY4yv5u8Zxpg7jDG57R+aiIiIiIiIiEhMW+7A+A8QMcYMAR4BBgJPtmtUIiIiIiIiIiL1tCWBEbXWhoFLgd9Ya+8GerVvWCIiIiIiIiIiddqSwAgZY64BbgBerCnztF9IIiIiIiIiIiKJ2pLAuBE4FfixtXarMWYg8ET7hiUiIiIiIiIiUsfd2gWsteuNMd8A+tVMbwV+2t6BiYiIiIiIiIjUastbSD4BrAJeqpkeY4x5ob0DExERERERERGp1ZZHSL4PTAKKAay1q4i9iURERERERERE5IhoSwIjbK0taVBm2yMYEREREREREZFkWj0GBrDOGHMt4DLGDAXuAN5q37BEREREREREROq05Q6M24GTgQDwT6AUuKs9gxJpjo0cwIbWYSOHOjsUkWOOtRYb3owNrcfaYGeHIyIiIiLHGButxIbew4a3t2q5tryFpBL4Ts1/Ih3KRsux5b+H4LJYgTFY7xRMxpcxTnrnBidyDLChddjyByGyJ1bgZELapzBpF3VuYCIiIiJy1LPWQtVT2KoXwAZiZe4BmMw7MO4BzS7f4gSGMeZ/NDHWhbX24pa2JdJWtuznEFpXr8BCYAnWhjDZ3+68wESOATayF1v6I6h/10W0HFvxKDi5GN+0zgtORERERI5+1c9hK59JLAtvw5Z+H3L/gHEym1y8NY+Q/AL4JbAVqAL+UvNfObCuieVE2oUNf5iYvKgvuBwb3tmxAYkca6pfSkxe1GOrnuvgYERERETkWGJtFFv1YvLKaCkEFjbbRovvwLDWLgIwxvzQWnt6var/GWNeb2k7Im3W3PNRke3g7tsxsYgcg2xkW+rKpupERERERJpjSyB6OHV9C4432zKIZzdjzKDaCWPMQKBbG9oRaR2n4OPVi0iTjFOYurKpOhERERGR5pgMMP7U9S043mxLAuNuYKExZqExZiHwGnBnG9oRaR3PKHD1TF7n7o/xjOjYeESONb5zUlYZ37kdGIiIiIiIHGuM8WJ8M1NUusB3ZrNttDqBYa19CRhKLGlxJzDcWvtKa9sRaS1jHEzWN8HVIDPn6oHJ+lrnBCVyDDGe4ZiMz8U6kPp80yHtk50TlIiIiIgcOzKuB+8piWXGi8m8C+Pq3uzirX6NqjHGA3wRqB0HY6Ex5s/W2lBr2xJpLePuB7kPxF6jGt0Lrj7gGY9peMIlIm1i0i4C31QILo0N6OkZg3H37+ywREREROQYYIwfk/09bGgjhDeAyQTvqc2+faRWqxMYwIOAB3igZvr6mrKb2tCWSKsZ4wbfqZ0dhsgxyzh54D+/s8MQERERkWOU8YyANgwB0JYExkRrbf17PhYYY1a3oR0RERERERERkRZpyyCe/5+9+46Po7oaPv47s0Wr7t57wb1i44J7obdASCihtxRCHhJISCDlSXlSCIS8oYcSWiAkQOi92Ljb2Ma9N9ybrLqrLXPfP0aSJWtXZbXaleTz/XyEvfdq7xyj3dHsmXvPjYhI3/IHZTuSRBIXklJKKaWUUkoppVRV8czAuBP4VES2AQL0BK5LaFRKKaWUUkoppZRSldQ7gWGM+VhE+gMDcBIYG4wxpQmPTCmllFJKKaWUUqpMPDMwAE4FepU9f4SIYIx5NmFRKaWUUkoppZRSSlUSzzaqzwF9gZUcr31hAE1gKKWUUkoppZRSqlHEMwNjDDDYGGMSHYxSSimllFJKKaVUNPHsQrIG6JToQJRSSimllFJKKaViiWcGRjtgnYgsASqKdxpjLkhYVEoppZRSSimllFKVxJPA+FWig1BKKaWUUkoppZSqSTzbqM6pqV9EFhpjJsQfklJKKaWUUkoppVRV8dTAqI2vEcZUSimllFJKKaXUSawxEhi6O4lSSimllFJKKaUSKp4aGC3Gl5+tZcHrSyk4WkjPwd2ZftnpdOzZPtVhKaXqyLZtlr67gsXvrCBQHKD/6D5Mu+x0WnfITXVoSimllEoB27ZZ+t5KlryzHH9RgL4jejH98tNp06l1qkNTSiVAYyQwpFqDyFDgcSACbAGuB+4ALgR2AtcaY0KNEEtM/7n/TT55cV7F453rdrP47S/43v+7nn4jeyczFKVUHIwxPH33i3zx0aqKth1rv2Lhm8u4/fFb6Ny7YwqjU0oppVQqPPOLf7H0/ZUVj3es/YpFby3j9se/TZe+nVIYmVIqEeq9hEREbhWRmlKYV0Vp22iMmWiMmVz2eAww3RgzCVgFXFTfOBpi79b9VZIX5Ur9Qf795zeSGYpSKk5rF2yskrwoV3SsmNf/9m4KIlJKKaVUKm1YsrlK8qJccYGfVx94OwURKaUSLZ4aGJ2ApSLysoicJSJVZlwYY9ac+IQTZleUAqcAn5U9/ggYH0cccVvx8eqYfV9t3Muh3UeSGI1SKh41vY/XzN9AsDSpk7qUUkoplWIrPq72MaTC+sWbCZSUJjEapVRjqHcCwxhzD9AfeBK4FtgsIv8nIn1rep6IXCAia4AOOEtXCsq68oGoMzpE5GYRWSYiyw4dOlTfUGOKhO1a+iMJO5ZSqnHU9D61bYOxa36fK6WUUqplqenawBij1/hKtQBx7UJijDHA/rKvME4C4j8i8qcanvOGMWYosKfsOTllXTnAsRjPedwYM8YYM6Z9+8QV1xw6aWDMvg492mkhT6WagaGTBsXsO2V0H9LS05IYjVJKKaVSraZr/D7DepKZk5HEaJRSjSGeGhi3icgXwJ+A+cAwY8x3gFOBS2I8p/IniQLABUwtezwLWFTfOBqiz/CejJ45rFq7ZQkX3Xo2J6yKUUo1QaNmDI1acNeT5uGC752VgoiUUkoplUrDpw7mlFOrTwr3eN1ceKteGyjVEsSzC0lb4GJjzM7KjcYYW0TOi/Gcs0Tkh2V/3wz8HOgsIvOAXcADccTRINf97nL6DO/J/NeXUpRXRI+B3Tjj2mn0H90n2aEopeLgcru49W/X8+Gzc1j63gr8Rc42qmdeN53uA7qmOjyllFJKJZllWXz3r9fx8fNzWfzOcvyFfvqN7M0Z106j5+DuqQ5PKZUA9UpgiIgFXGKM+WW0fmPM+hjtrwOvn9D8x7KvlHC5XMy4YjIzrphc+zcrpZokr8/LuTfP5tybZ6c6FKWUUko1Ad40D2ffMJOzb5iZ6lCUUo2gXgmMslkWX4pID2PMrsYKSqnGFiwNMe/VxSz/cBXhcJjB4wcw7bKJ5LTJTnVoStVZwdFCPntpAesWbcTtdjN69nAmXzIOj9eT6tCUUkqplLBtm0VvfcGSd1bgL/LTd0QvZlw5mXZd2qQ6NKVUAsSzhKQzsFZElgDF5Y3GmAsSFpVSjSgUDPHgrU+yZeX2irZd6/ew5N3l/OjJ79K6Q24Ko1OqbvIO5vPn6x8i70B+Rdu21TtZ+ckavv/QDZrEUEopddIxxvD03S/yxUerKtq+2riXJe+u4H8evZlup3RJYXRKqUSIZxeS/wXOA34N3FfpS6lmYdFby6skL8od3X+Md/7+UQoiUqr+3nn8oyrJi3JbVm5n8dvLUxCRUkoplVrrF2+ukrwoV1Lo59W/vpOCiJRSiVbvBIYxZk60r8YITqnG8OWna2L2rfxkdRIjUSp+Kz+N/Vpd+Uns17hSSinVUtX0+2/j0i34iwNJjEYp1Rji2UZ1vIgsFZEiEQmKSEREChojOKUagx2xY/fZJomRKBW/ml6rNb3GlVJKqZbK1PC70RhTY79SqnmIZwnJg8DlONuhpgM3lrUp1SwMnTwoZt+wGvqUakqGTarhdTxlcBIjUUoppZqGoZMHxuzrO6IXGdnpSYxGKdUY4klgYIzZAriMMRFjzNPAtIRGpVQjmnjhWLr271ytPTM3g3Nu1C23VPNwzk0zycypfiHWtX9nJlwwJgURKaWUUqk1bPIgBo7tV63d43Vz0ffPTkFESqlEi2cXkhIR8QIrReRPwD4gM7FhKdV4fBlp3P7YLXz03FyWf/QlwdIwQyYO4IxrptG+W9tUh6dUnXTo0Z4fP/N9PnjmM9Yu2Ig3zc3oWSOYddUUfBlpqQ5PKaWUSjrLsvjOA9fy6YvzWfzOcvyFfvqO7M0Z10yl+4CuqQ5PKZUA8SQwrgJcwK3A7UB34JJEBqVUY8vITueC757JBd89M9WhKBW39t3acuXdevpVSimlynm8Hs64ZhpnXDMt1aEopRpBvRMYxpidZX/142ypqpRSSimllFJKKdWo6pzAEJHVQMzSvcaY4QmJSCmllFJKKaWUUuoE9ZmBcV6jRaGUUkoppZRSSilVgzonMCotHVFKKaWUUkoppZRKqvosISkk+hISAYwxJidhUSlVT0XHitm8fBser5sBp/XD4/WkOiSlkqI4v5hNX+hrXymllCoXiUTYtGwb/kI/vYf3pHWH3FSHpJRKkPrMwMhuzECUitebj37AR8/NIRQMA5DVKpPLf/o1Rs0YluLIlGpcbz/+IR8881nFaz8zN4PLfnIRp84ekeLIlFJKqdTYvHwbT//8JY4dzAfAsoSJF57GZXddhGVZKY5OKdVQdX4Xi0hO2Z9ton01XohKxTb/v0t498mPKz7AgTMb46mf/ZM9W/alMDKlGteCN5by9t8/qvLaL84v4R8/f4ndm/amMDKllFIqNQqOFPLI7f+oSF4A2LZh3muLefeJj1MYmVIqUeqThvxn2Z9fAMvK/vyi0mOlku6zf82P2h6J2Mz996IkR6NU8tT02p/z74VJjkYppZRKvQWvLyVQUhq1b+5/FhKJRJIckVIq0eqzhOS8sj97N144StXPgZ2HYvYd3BW7T6nm7sDOwzH7DtbwvlBKKaVaqpquCwvziikp8JPdOiuJESmlEq0+26gCICI3GGOerPTYBdxjjPnfhEammhVjF0HgbUxwMRBBPGMg/XzEatWox23fvR37th2I2teuW9tGPbZSqdShe1v2bNkfta8pvvZN6VxM4EOw88DdG0m/EHH3S3VYSimlWpD23dvF7MvMSScjOz2J0SilYjHh3RB4AxNaD1YWkjYN0mbhpBZqFk8lm5ki8o6IdBaRYcAiQAt8nsSMXYwpuBtT8i8I74DwVxj/a5j8uzD2sUY99tRLJ0RttyxhytfHN+qxlUqlKZdOjNpuWcKUGO+LVDHFT2IKH4DQWojshdL5mPyfYoJLUx2aUkqpFmTCBWPw+qLvxjX5kgm43LV/OFJKNS4T2ozJ/zEm8BFE9kBoI6boMUzhnzEm2qanVdU7gWGMuQJ4BlgNvA38jzHmjnpHrlqOwHsQ/qp6e+Qg+F9r1ENP+foEZn1rCi7X8ZdyepaPq3/1TboP6Nqox1YqlSZfPI7ZV02tcjHmy0zjql9cSs9B3VIYWVUm/BXG/3aUjgim+Kk6/aJSSiml6qJ1h1xuvvdqstscXyYiIow7ZzTn3jwrhZEppcqZkmfABKp3BBdDaFWtz49nCUl/4AfAK8Ag4CoRWWGMKanvWKplcJaNxO6TzOsa9fgX/+BcZlwxiY1Lt+L1eRg84RTS0tMa9ZhKNQVfu+0cZl45mfWLN+NJc177vowm9tqvaZZF5ABEdoBbSysppZRKjMHjT+F3b/2UdQs34S/003dkL9p1bXpLK5U6GRm7CELrYn9DcBF4R9Q4Rr0TGMCbwPeMMR+LiAA/BJYCQ+IY66RlTBBCX4IpBc9gxGq8nWhNeCdEdoLVFtyDcX5sCT1C7C67CFM6F1zdEHefBB/3uFbtcxl3zuhGG1+pRDCRPRDeClYrcA9FpOH70ee0zW7ir31z/E8TAMIgaYA3hTHVzESOQHg9iA88IxCJPh1ZKaVU0+T2uBk+ZXCqw1BK1cQuBHMM8IDVHsRFjZ8ry8STwDjNGFMAYJy5v/eJyBtxjHPSMsGlmKIHnR8agFjgOxsyrk9ocsHYRZii+yD45fFGd3fI/jHiStzyCvGehglvPeHgIbD3g6Q7a98B4xmMZN+JWLkJO7ZSzYExAUzhX52pceVcHSH7zkZN7DUJ3rFQ8hRE9jvnhXJWJriHgqtn6mI7gTE2FD+JKX0fjO00WjmQdSviHZPa4JRSSimlmjmxsjCuvlD6DlDpujByGKwO4D2t1jHiqYFRICJDReQbInK1iFwNNK2KcU2YiezFFN57PHkBYGxnjXjgzcQeq+ihqskLcApsFvwWYxK4D7bvbCcxUnFgA/Y+wHbuNJcLrcMU/iVxx1WquSh+omryAiByAFPwG4yJvl99i+Hq5CQuKicvAOwSsFolZBZKwgT+iwm8ezx5AWAXYArvxUSi7/iilFJKKaXqIbyGKsmLcvYhjKl9fkW9rxxF5JfA38q+pgN/Ai6o7zgnrcAHYMJRu0zgnYQdxkQOQWhJ9M7IAQh9kbBjiZWF5PwOybjUSWRYWSAZ4OpKtUk+oVWYaAU/lWqhTPkyqmjsfCidn9yAki24ECQLrI4g6SAe57GrK0S2N/pORfVh/DHOwSYEgQ+TG4xSSimlVAtjR4qc+mecuOqg7HHhH2odI55bX18HZgL7jTHXASOAJlY1rukykb2xOyMHEzczwt7vzISIeaw9iTlOGbGykIzLsVr9Fcm8Eqx2xFyhZOudTHUSsQ/GTFo6/fuSF0sqRMr+fVa2k7Rw9XRmZYjPmekQOZTa+MoYUwr20dj9Lf3npJRSSinV2CLbgPKZrlLpq4yp/XornhoYfmOMLSJhEckBDgItfBF34oirS+zSJK4OiCRof2qrE4jETmIksAZG9WN3jtp8eB989LKwcfU7pGXO59TZI5h+2el4fU2jmN+a+Rv47KX5HNh1iPbd2jHzisEMHrkeE1wO4ka8E8B3AWJlpDpU1ZxYHUDcsZMYMd4vqWJMAPxvYYLzwQQRzwhIvxBxdYxvQFcN/z6xwNU+vnETTCQNY7Xh4K6jfPSysPlLIS3dMHYmTLnQ4E1vWj+n5mT35n189Nwctq3aSWZuBuPPPZVJl4zD5UrQ7zullKokGAjy2J3PseSd5QT9Qbqd0oWrf/UNRkzT/QaUSjlXH5w5FDFu2kvt11vxJDCWiUgr4O/AF0AREGOtgqrGdwYE3on6YUZ85yTsMOJqj/GcVn3dPTjFAz2nJuxY1XiGO0tJKi0V2b8T7r/doqQoHVwlQAm7N+1jzbwN3PbwjXi8qa3y/+lL8/n3fcdr0Yb8+zDH/kH+nnRy2mQDOEtfgksh97eIpKcqVNXMiJUFaVMwgU+qd1q5kHZ68oOKwZggpuBXENp0vC2yD4LzIff3iKtL/Qf1TgDrWbDzovRNQirXyUmxvXsm85db38RfXN4i7N4Ka5da3ProzLh+YZ7stqzczt9ufZJQqbPW9fCeo+xct5tNy7Zy05+uSnF0SqmWJhwOc9vEu9m96fiM543LtvCLi/7Ij5/5PqdfODaF0SmlLFcWtqsXRLZG/4bsu2ofo74HNcZ81xhzzBjzKDAbuKZsKQkAIqLpzRqIqwuSfacznbqi0ULSzwXf+Yk9Vtb3wDuyaqO7O5JzT+JmekQ7rgiS/TNw96poe+sfQkmRz0meVLL1yx0se/+EQqNJ5i/y88bD71Vpm3jGDrJyguQfKsC2KxX0C2+HwEdJjlA1e5k3gndc1TZXRyTn54g0oRV4pZ9WSV5UsAsxJS/FNaSIF8n5efWZGN4xSNYtcY3ZWF5/IoLff+IuSS42rerEys+axlKX5ubVB96uSF5UtuLTNWxYsjkFESmlWrK3H/2wSvKiXCQc4cm7nk9BREqpajzDgCg3r60OiNSw7LpMg24oGWN2RGl+DhjdkHH1yVJzAAAgAElEQVRbOvGOhdZ/h9CXYALgGYJYbRJ/HCsLyfkFJrwDIjuduhTuwQndqjXmsV0dkVb3Y0IbwD7AmmVvgCv6y23VnHVMOD91WxRuWLKFUn+wSlu/IYcBMLbBXxQgM+f4shETXIKkJzbZpFo2ER+S8xNMeLez9k9ywTOsae3AAZjgstidwfgn2om7F7R60Kk6bR8FV2/E3SPu8RpDJBxh/aJNznlSWgEBQJyCxAir5qxlzBkjUhxl81JwtJAda2MXbV41Zx0DT+ufxIiUUi3d/NeXxuw7+NVhju7Po02n1kmMSClVmbGLILwF3APAFDoF7cXtLLnGcq43vTWnEhpjRmzjfzpuYoyd7yQjsMAzCrEya32OiBe8yZnGJu5eVWZDJJN4BgIDQd4n1lqnJORTahQtoWMqvYyr9550L3GVIOLuBu5uqQ6jBjW9thv2uheRsox7MyBuIKtqU6pPVM2dCTkJe7EqkkL6/1QplWh6XlGqGREfWAZwcfw6s/b3cGPc/qth64uWx5S8hMm7CVP4AKbwfkzejZjA+6kOq8kZPmVwzL5UF1UacFo/fBlVp/FvWdMOALEEX5avSp94T0tabEolk9SUVD1xCUwL43K7GDJxYMz+VJ+nmqOcNtn0HtrN2bo7ssvZkSeyH8I7wfgZrv9PlVIJdvrXYl+jdezZXmdfKJViYmWBZyDYB5zrgchBZ8e6yA4wJXW63mxa85ebGVP6Oabk5aoFOU0ppugxTGhd6gJrgi747plktao+M6X/qN6cmuJp2emZPi783llV2hZ80JPC/DRatc/Bsiq9Tdx9wTcryREqlSRpU51fKieycpGMy5IfT5JdeOtZZGRXL9A78LT+jJiuH7bjcfG3I3i9RSe0Rhg96QCnjG6XkpiUUi3XebfMpsfA6jMdXW4XN/7hyhREpJSqRtqCXVy1zUTAFGNctS8xbowlJMHav6VlMIF3auh7F/HEnnVwsunQoz0/efb7fPLC56xfspm0dC9jzhjJlEvH4/akvrb/1G9MpEPP9nz20nwOfnWY9t3a4ml3JTld12NCywE34p0I6ecg4qt1PKWaIxEv5PwSAu9iSucBIWcbVd8FiKvlf9js0rcTdz33fT56/nM2fbEVX0YaY88axWTd8jMuxgTp3W8pdz5o8/F/hO3rhMwcw7jZhgln207R2PSLUh2mUqoFsSyLBxb8lqd+9k8WvrGMUn8pPQZ245pffYMhp8eeZaeUSg5jQhD+ElzdwBxzlpdigWSD5CCln0LGJTWOUe9PjuIsLrsS6GOM+bWI9AA6GWOWOEGZ8XH8W5qnyP4a+vYlL45mom3n1lx6xwWpDiOmQeP6M2jciQXlTkO4JiXxKJUKImmQfhFykn6wbNe1LZf95OT8tyecnQ/GT+de8K07DNVWmOrvSaVUI/B6PXz7z9fw7T/r9ZtSTY4pArsIxAvSoXq/Xfu1QTy3vh8GbGAG8GugEHgFOPk2VnZ1cS7QYvXVw7pFm/j0n/PYt/0AbTq1ZvLXxzP2zJG1P7ESY4IQeBtTOgeMH9xDkPSvIe7u9RpHKXXyCpSU8skLn/PFR6sIBoIMGncKs6+eSvtubVMdWlKY8G4IvIYJrQXxIWlTwHeeMztF1Y+VC1Zm9Wmi5VxdkxuPUuqkEA6F+exfC1jy7nL8hQH6jujF7Gum0rVf59qfrJRqXJINVhZEjoB9DGfXt/IZGLl1+gwdTwJjnDFmtIisADDG5EktV3YiMg74C842FMuMMbeLyJ3AhcBO4FpjTPWN4ps48Z2LCa2P0iGI75w6j7Pg9aW88LtXMMa5O3V0/zG2rNzO3i37q9VmiMWYCKbgdxBafbwx8hkmuAhyf424+9U5HqXUySlYGuKv33mcnet2V7TNe20xKz5exR1PfY+OPdunMLrGZ8LbMPn3lE1nLG97HoLLIedXiKR+uVtzIuKFtNkY/3+jdGZA2vTkB6WUatFs2+axHz3L2oUbK9qO7Mtj5WdruO2hm+gzvGcKo1NKibgxnpEQfBZnTkQZUwpWEOOdXus+JPEU8QyJiIuyuaAi0r7q0aPaCcwwxkwGOojIZGC6MWYSsApolvN1JW0iknmVMwWmnJWJZN5atn1o7ULBEK/97Z2K5EVlHz77GXkHY8zwOFFwUdXkRTkTwJT8s25jKKVOaovfXl4leVGuuMDPW499mIKIksuUvFAleVEhtM45x6r6y7gC8c2oul+2qz2Scw9iZacuLqVUi7Rm3oYqyYtywUCI1x98NwURKaWqiRx0ZmFUvjYQN0g2Etle69PjuZ30/4DXcBIRvwO+DtxT0xOMMZWLRYSB4cBnZY8/Aq4A/h1HLCkn6V+DtNkQWgPiAs9wZw15HW1duYPi/JKofbZtWDNvA5Mvrn07GRNcFrszuBJjgjoFWilVo1Vz1sbum9uyd1YyJgTBFbH7g4uRtElJjKhlEHFD1q2Q/k0IbwIrG9xDEdFN0JRSibd6bpSZ0WU2r9hOSaE/6m5TSqnkMHZR2fVAB6BNpSKe6YBAcDF4R9c4Rr0TGMaYF0TkC2CmcxQuMsbEPltUIiLDgXbAMZzlJAD5QNRNmUXkZuBmgB49at9SJVXEyoK0+GqXilXzRZxlHc9MGWNDaCXYR8Ddu+7LQqS2iTiNwxjDxqVbOLI3j859OqZ82p4J7yx7w7QCzyidDq7UCawazkcuV8M+cEYiYdZ//jbHDh6k++Ch9Bw6oUHjJZ4450pjwIScOkJYzlIHsQDdhaQhxNUeXC17CZJSKvWk7LrZ2AZ/UQDbtknLSMPjdSMiFf1KqSbAlDpFPXEBac5kgDpcb8WzC8lfgX8ZYx6q5/PaAA8C3wBOBcqrd+XgJDSqMcY8DjwOMGbMmOprLFqAviN6kt0mi8KjRdX6XG4XwyYPAsCEt2MK/+hMuSljPEOQ7J8gVhbiHe8U74zGc2rSZ18c3HWIR3/0DPt3HKpo6zWkO7f8+Wpy2+UkNRZjApjC+6HyLBWrNWTfgXgGJTUWpZqyEdOHsHpe9Hz08Knxbwu9c/UCHr/zQfIOlO+y/T4DTm3Djff9jszcpvGhVsSNcY+G0nfBLqRixwxxgbRHvLXPhFNKKZVaI6YN4cPn5nBk71HsSPkKdyEjJ52J548hPdOX0viUOtmJlYVxD4DAOzgFPMsdAqsT1OF6K55basuBe0Rki4jcKyJjag3UudX9PHBn2XKSpcDUsu5ZwEm7uNjtcfP1H55fZaZFuXNvmkVO22yMCTkFOislLwAIrcUUPej83TsWvFF+FFYWkvGtRog8Ntu2efj2f1RJXgDsWPsVT9/9YlJjAaD4qarJCwA7D1P4f840JqUUAGPPGkn/Ub2rtee0zea8b58R15ilgWIe/sH/q5S8cGz84igv/vo3cY3ZaFztwBRTZbtPEwGKMC7dzUkppZq6zn074i/0V0peABgCRQFadcxNWVxKqUoi+4HSExoN2Icwklnr0+NZQvIM8EzZjIpLgD+KSA9jTP8annYpzjarfxRnOcNPgbkiMg/YBTxQ3zhakrFnjqRt59Z8+uI89m47QJtOrZh66QSGTiqbHRBcDPbR6E8OLcVEDiOudpD9Yyj9CFM6F0wx4hkGvvMRV8fk/WOADYs3c3DX4ah9m5ZvY9+2A3Tuk5yYjF0ce2aKXQylcyG97jvGKNWSebwevve3G5j3yiKWffglodIwg8b1Z/rlk2jdIb4LvxXvvkxhXjh632d7yT/0FbntU58cMMZAcAm4ujnbY5sSwHJqNkgOUvoJuK9OdZhKKaVqsPD1pbTp3BpvupfiYyXYto0vI43sNlmsnb+BSDiCy61LApVKFdsugfAqwINTUcLGqUrhAiwofgJa/aHGMRpSBKAfMBDoBdRY3c0Y8yJw4q33hcAfG3D8FqXP8J6xa0ScOPOiMmPAPgiudk5NB99ZiK9uW682liN782rsP7T7SNISGNh5znr2mP0HkhOHUs2EN83DjCsmM+OKyQkZ7/CefTH7jA1H9+5sEgkMCDnnCzxgtavWa+z9tW7rpZRSKrXKr0GzWmWS1arqndziAj8lhX6yW2elIjSlFDifW00IJ2kRJRVh76l1iHhqYPwRuBjYCrwM/MYYE7WGhUoQV5fYfWKBq1PyYqmDDj2jr2kPlYYoOFLIs796mazWmYw5YySzrpqCL6Puu7bUm6stSJpTJCZqf9fo7UqphOjYuyfOysPq3B6hXfc6FiNuZCJejKsdRKLPHpMGnCsK84r48Nk5rPhkNcY2DD19IGdcO402naLWr1ZKKRWnDj3bY2xD4dEiigtKsCNOEc+ctlm07dKGjBzdgUSplLI61fLZrPZNH+KpgbEdmGCMOcsY85QmL5LAOxZiLQPxTkKsNsmNpxannNqHbqd0rtIWDIQ4sOMQdsSmpNDPwV2HeeeJj/h/3/07oWANMyQaSCQd8c2K3mnlQlpi7jIrpaIbOftiWneMXkR47Bk9yW7TdBKw4jsvRofX2S47DsUFJdx34yN89PxcjuzN4+j+Y8x9ZRH3XvcQR/fXPFtNKaVU/Yw/fzRH9+dx7FA+odIQkXCEkoISDuw4xODxp+By6fIRpVLJsnyQNjV6p3gg85bax6jrwURkYNlflwA9RGR05a+6jqPqT8SF5Pwc3L2qdqSNR7Jq/yEnm4jwnfuvpfew41vf5h8uIC3dS9suVZMtO9Z+xdJ3VzZuQBlXQdq0qtvJurogOb9ARDPxSjUmjzedWx+8iy59MiraxILRM7rwjbt/mcLIovCdj6Rf5PwCLWe1QbJ/irg6xDXknJcXRq0JlH+4kA+eiVGfRymlVFx2rt1NVqtM3J7jk8xFhKxWmRzecySFkSmlKuT8AbynlW1TX8bKgpzfYrlrX1ZcnyUkPwRuBu6L0meAGfUYS9WTuLogre7HhLc4U5zdPRFX59qfmCKtO7bizqe+x1cb93Bkbx6P/ugZLFf0fNmqueuYeOHYRotFxItk34aJXA7hbc7MC/cARHRFu1LJ0LnfcO555Vm2rfyc/IP76DZwOO17DKz9iUkmIpB5NaRfCOGNID5wD0UknsmKjjUxtqUFWP35Oi77yUVxj62UUqqqNZ9vIC0jjS79OlFaUuosIUn3YrldbP1yJyWFfjKy9eaVUqlkWW5o8wR2eBsE5oKrDaSd47TXQZ0TGMaYm8v+erYxpvKmrYiIbqqcJOLuB+6msWa8LroP6Er3AV1JS/cSCkbficAVI7FRHyZyAEJfOlO9PWMQq3qBJnG1B1f0+hxKVWaMDaGVYB8CVzfEMyTVIbUIfUY2jyVbYuU6dwYSwLJin99iJXWVUkrFR6zjN6fSTqixJiJV+pVSqSW4wd0ZJAvErv0JZeLZhWQBcOKSkWhtSlUYMW0Iyz74MmrfyBlD4x7XGAPFf8eUvu/syAJOEiPzxti1L5SqgQnvwhT+HiLHd6gx7j5Izs+aXL0Z1fSNmD6Ebat3Ru0bNX1YkqNRSqmWbeSMocx7bXHUvgFj+5GeqfdclUo1Y4KYogegdNHxRisLsm5DvGNqfX59amB0EpFTgXQRGVWp/sU0IKOWp6uT3AXfPZOcttnV2geN68/oWcPjHzjwNibw3vHkBYAJYoofcZbbKFUPxtjVkhcAhLdhCv+SmqBUszb5kvH0GFR9B5MO3dsy6+opKYhIKaVarkHj+nNqlOvKjOx0Lv7BOSmISClVTckLVZMXAHYRpvBeTKT2WjX1mYFxJnAt0A24v1J7IfCzeoyjTkLturblrue+z2cvLWDDks14fV7GnDmCiReOxeWOvyK0CbwXo8NA4H3Iaj7LbVQTEFpePXlR0bcWE96FuHtE71cqCl9GGv/z6C3Me3UxKz9ZQyQSYdikQUz95kQyczT3r5RSiSQiXPe7yxk8cQCL316Ov8hPv5G9mX7FJNp10VmUSqWaMSFM4OMYnSEo/QQyLq1xjPrUwHgGeEZELjHGvFKfQJUCaNU+l4u+fzZwduIGtQ/F7DKRg+hKR1UvkYM199uHAE1gqPrxZaQx61tTmPUtnXGhlFKNzbIsJpw/hgnn1z4VXSmVZKYITEnsfjvGjcRK6pzAEJFvGWOeB3qJyA+rxWLM/VGephLEGMO8Vxfz+auLyD9UQNd+nZl11RQGTxiQ6tBSy9UVwjuidomrW3JjUc2fu5bXjKv6UgClUiEcCvPpi/NZ+NYyio8V03toD864djp9hvdMdWhKKZVSwdIQHz8/l8XvLMdfFKDfiF6cce00eg6ufXtGpVQjkxxnR0g7P3p/HT6/1WcJSWbZn9W3d1CN7sXfv1alKNGGpVvYuGwrV/3iUsafd2oKI0st8Z2PKfpblA43+M5KfkCqeXMPA3ev6Ekx71jE1SnZESlVjTGGv//4eVZX2qJ11efrWbtgI9++/1qGTDzJE9tKqZOWbds8/IOn2fTF1oq2FZ+uYc38Ddz6txvoP7pPCqNTSom4wHc2puSl6p1WJqTNqHWMOhfxNMY8Vvbn/0b7qkfcqp72bT8QtaKyMYbXH3qXSDiSgqiaBvFNRzKuBKm0VZbVGsm+E3Frpl3Vj4gg2XeDZ2DVDu8YJOu21ASl1Ak2Lt1SJXlRLhKx+e+D76YgIqWUahpWzVlXJXlRLhQM8/qDMeqmKaWSK/3rSPq5zg3ncq5OSPbPESun1qfXextVEfkT8FvAD7wHjAD+p2x5iWoEa+ZtiNmXf7iQXet303vYyTttWDIuAd/ZEF7vbKHqHuxk95SKg7jaIrn/hwnvBPsguLohrs6pDkupCjX9TtizeR9H9+fRplPrJEaklFJNQ03nx22rd1JcUKIFlJVKMRELMm+A9EsgvBkkC9wDEalb9cJ6JzCAM4wxPxaRrwG7gUuBT4GTIoFhjGH94s0c3HmItl3aMOT0AVhWnSeyxKW2XTqsBuzikWwmsgeCX4L4wHsaYiVmRZJYGeBN/FIaE9kPoZWA21lCYOUm/BiqaRJ3TyCxicFtq3aya/1uctpmM2zKIDxeT0LHV02XsY9BcAlgwDMKcXWIe6zafic0aGcnYzvnvMhecHV2YpXG/R2nlFKJUn7+M7ahpNCPsW3S0tPw+DyISIPOj0qpxDKh1RCYC65ccHVEpG47BcWTwCi/4j4HeNEYc7Su2ZLm7si+PB65/Wn2bj1eHbVd1zZ85y/X0rl3x0Y77sjpQ3j1gbewbVOtr13XNvQY2PQLCxpjQ/FDmMCnxxvFC5k3I77a1zolmzEGip/ABCpNxxY3ZFyNpJ+XusBUs1RcUMLjdzzL5hXbK9qyW2dy0x+vot+o3imMTCWD8b+GKfknmLLlfiLguwDJvCau8UbOGMqHz82J2tdnWE9y29U+/TJqnJH9mILfQWTP8UZXZ8i5R2chKaWahVEzh/LBM59yZG8etm1XtGdkpzPp4nH4MtJqeLZSKhlsuwiOXgXhSsu9ip/GzvoBVubVtT4/ntsqb4rIBmAM8LGItAcCcYzT7Dz50xeqJC8ADu85ymN3PFvlJJlobTq15pybZldr93jdfPPHF9V5uk1K+f9bNXkBYIKY4ocw4e3Rn5NKpR9UTV4AmDCm+ClMaG1qYlLN1kt/+G+V5AVAYV4xj/7oGQIlpSmKSiWDCa7AFD93PHkBYAzG/3r1c2Id9R7ag0lfG1et3ZeZxqV3nB9vqJjCP1VNXgBE9mEK/xD3mEoplUztu7fDX1xa7bo8UFJKVqvMGM9SSiVV/h1VkxcAJgRFf8EObar16fWegWGMuUtE/ggUGGMiIlIMXFjfcZqbXRv2sGPtV1H7Du46zMalWxk0rn+jHf+cG2fSc3A35r+2mLyD+XTr34Vpl02ka7/mcVfMlMYonGQMBD6ArFuSG1AtTCB2oScTeB/xDEliNKo5KzhayMpPVkftKyn0s/S9lUy+uPqHUdUymMD7NfS9h/imxzXuFT+7mIGn9WPhm8soOlZMn2E9mXbZ6bTv1ja+OEMbYm5JTfgrTGitnveUUk3ewjeW0bpjLmnpXorzS7AjNmkZaWS3yWTtgo2EQ2HcnngmoCulEsG2iyG4NHqniUDx36HVvTWOEU8RTw9wFTCl7M7/HODR+o7T3Bzdl9eg/kQYMnFAs9wezxgDkcOx++1DNLk5JJFDsfvsGvqUOkHB4UIikdgztPL2H0tiNCrpajpfNPBcMnrWcEbPGt6gMY7HEvsc7fTreU8p1fQd3ZeHiJCZm0FmbtVinf6iAP6iANmtE1N/TSkVB/uQM9siZv/+WoeIJwX5CE4djIfLHl9V1nZjHGM1G5371Fzjorb++jq89yjvPflJxVZ5I6cN5awbZtC6w/EikjvXfcW7T37C1pXb8WX5OO3s0ZxxzVTS0pvW+j4Rwbi6QWR39H5Xj4q/m+CXGP9rENkK0grxzQTf+cnfVcTdHUIbo3ZVjjceh3Yf4b0nP2HN/PWIZTFqxjDOvmEGOW2zGzSuSgxjDPNeW8Lnryzk6L5jdOrVgRlXTmb0zGFxjde2S2s8aR5CpdFP1p37xF/MsTEYuwj8/8GUzgOC4BmBpF+KuON/3du2zWcvzWfef5dQcLiQbqd0YfbVU5tlQrbeXN3JP7Cd9/8prPxcsG0YOs5w5pWG9j0bdi5JKFe3Wvp1W2qlVNPXuU9HgsEQ+7ceoNQfBAMul0Wrzq3pNbhbtaSGUirJrC7OZg7GD4SB8hqPLufL1bf2IeI47FhjzDXGmE/Kvq4DxsYxTrPSsWd7hp4+MGpf72E96DM8cbsVHN2fx73XPcSCN5ZSeLSIwqNFfP7qIv58/UMUHC0EYMuK7dx/06OsmruO4gI/R/bm8e6TH/PgrU8SCUdqOULySfoFMTq84DsDAFM6D1PwvxBaBXYxRPZgip/FFN2fxEjLwvLFitflbNkap8N7jnDvdQ+x8K1lFOYVU3CkkDn/XsC91z9E0bHiuMdVifOf+9/ixd+/yu5N+ygp9LNt9U6euOt5PnlxXlzjpWelM/GC6KfINp1aMXLG0IaEm1DG+DEFd2P8b4B9FOwiKJ2Pyb8LE2t5QR0884t/8Z+/vMX+7QcpKfSz6YutPPw/T7P4neWJC76JKi6dyf3/YzH3DaEgD4ryYdEHwp+/b3E0b1qqw6sg7l7giTGbwzMYcdd+QaGUUqk2avZwvlq3h9KSYMXnokjE5sjuI6RleBt950ClVM0sywuekUAQsHHeqAYnmRGBjJtrHyOO40ZEpOJKRkT6OEdr+a79zWUMnzK4StHMgWP7ccufa6+WWh8fPjuXwqNF1drzDuTz6YvzAXj9ofcIBcPVvmfrqp2s/HRNQuNJBPHNQjKucDJu5VwdkOyfIq5OGGNjSp6L/uTShZg6FHRJJEmbgGReD1IpU2+1QbJ/7Fzox+n9pz+Lmqg4sjePOS8viHtclRiH9x5lzsvzo/a9/fiHlPrjK7h5ye3nMvGCsbhcx0+5PQZ15baHbmxaW6kGPoFwlFo/JoDxvxzXkLs27GHp+yurD2kMrz/0HpFIy/71MeeV/Rw51BHnzkI5i+Ki9nzwz6OpCisqyb6j+nbU3lFI9o9TE5BSStXT879+GRNl1z4EVnzc9K6PlTopSQ5IFlQpIuACV2fERJ+xX1k8S0juBD4VkW1lj3sB18UxTrOTkZ3Ot++7hsN7jnBg52HadW1Dx57tE36cdQujL10AWLtgA2deN52tX+6I+T1r5m3g1NkjEh5XQ0nG18F3DoQ3gaSBewAiZR/oIrtrrjsRWg6eU5ITaBlJPw98syG0wdlC1T2wwUtZ1tb4s93IuTdX321GJc/6RZujblcMztrZrV/uZPD4+r8O3R433/r51zn/O2ewZ/M+ctpm0+2ULg0NN+FMqIYZEcEv4hpz3YLYr/ljB/PZs3l/s9gKOl5rF2xwfkm7M8GUbdglPkBYM39DSmM7kVhZSM7dmMg+iOwDVyfE1fRep0opFcsXH6wCcZYvU/nXuUCgOMDO9bvpOaiWJXNKqUZj7AKIbANXTzBBMEWAC6yyMgnBL8B7Wo1jxJPAmA88Bswse/wYsDCOcZqk7at3sn31LjJbZTJi2pCo+0W369qWdl3jq/ReFy5P7A/JLrcLy2VhWRLzg5bb2/Dqyia8A0JrnAtt73hAILgYTDF4BiHufnGNK1YGeEdG6agt5gT8m0LrIbwFrBzwjkek9lohImngPZ4MMqbU+f9g54O7L+IZXK8YXO7Yk55c7iTX+VDVlP983O4wU87bTtuOxezZnsuCD3oCFu4a3pt1kdsuh9x2OQmI9DgT2QPBFSAe8I5DrFYNGK2m91l878Hy17XLbdN/2CGycoIc2J3FV1tbO6M24P+pMYYNS7awb+t+WnVsxfApg5pcdXl3xftaQNKr9jXw9dRYxNUZXM1jhyullKqs4jrLOP8xgFS6y+vLbFp14pQ6+VS69rGPginEWRSSBpaPulxvxnOl9yxQAPym7PHlwHPApXGM1WQESkp5/I5n2bB0S0VbepaP635zGUMnDUpqLKNmDOPdJz+O2jd65nC8aR6Gnj6QVZ+vj/49DahKb0wIU/QAlFbKSZn7y2b4HF/+YbxjkOw7EPHGfazKxNUF4+4J4Z3RvyFtYtxjG7sYU/h7CK073mg9CVk/Qrx1n6liQqswhX926gKUt3kGItk/Q6y6VbQePXM4Hz43J3pfonYTUHEbPmUwQ8ce5sofLMabdnxpw1mXbeT5v86i78heqQvuBMbYUPwoJvDR8UZ5EjKudmYPxUHSJmBibG0lcb4HR80cytK3X+Sia1eTkXW8kOneXTnM+2AGXfp2imvcvIP5PHL70+zetK+irVX7HG6575omdXdt1MxhbF0V/bw2eqa+55VSKpGmX3Y6L/z2VWcHvDIGAwZy2mbRsUfiZ04rpepOrEyMqy+UvkGVaVL2FrCz6vSZL54aGAOMMTcaYz4t+7oZSO7c/kbwyv1vVUlegDNl/O93vVBRODNZZl45ma79ql/U9xzcjSmXjgfgokJcg0oAACAASURBVNvOIbt1ZrXvGXvmSAaeFt/sCAD8/z4heRGCyB7ni+Dx9uAyKHk+/uNEIZk3OUtLTmxPv7hB05hN8RNVkxcAdhGm8A8Yu24/W2MXYQr+UCV5AUBoA6b4sTrHcsY1U+nUu/quE32H9+T0r9U8XUo1vowcD9fftbJK8gIgMzvILT9fhsvVhO6YB96umrwAMGFM8VPObKN4eCeDd1T1dld7SL8sriHbdsngxnt2V0leAHTtVcgN9xTENSbAMz9/qUryAuDYoQIe+9EzhEPV6wOlyqSLx9FnWPUiz516d2D21VNSEJFSSrVc595yBi5P9I83Y86MMgNYKZV8wc+ousarXBEmvKvWp8czA2OFiIw3xiwCEJFxOMtKmq1ASSlL3o2+9jtUGmLxW8uZffXUpMWTkZ3Oj578LvNfW8KqOWsRy2Lk9CFMvHAsXp8z46FTrw789IUfMOflhWz+Yhvp2T5OO2c0p84eXqXIaH0YYzCBD05oLICyzDV2AVjtjncFPoaMqxBJTBFC8QyGVvdB4B1MuHwb1VnIiUXl6sHYRRCM8fI0pVA6B+pyt7p07vH16ycKLsTYBYhV+9KAzNxM7nzqu8x7dQlr5q3HclmMnDGMCReMwZvWhIo5nqz8L+H1hrBtN5GIDcYgIlhuC0vysEsXYaWNT3WUAJjA+zX0fYB46j9zTMQF2T+F0jmY4OdggohnJPjOqtPrO6rSBbTr7MWf046ivGIi4Qhen5fsNpl40jZjIkcQV/2W5B3YeYhNy7dF7Tt2qIDVn69n1Iz4tr1NNK/Py22P3MTCN5ax8pPV2BGboZMGMeni00jPSq99AKWUUnW28PWl9BjcjUO7jlCcX4IxBk+am/bd23Fo9xHCoXCTW2qo1MnEDh8Ckx/7Gwr/BOk17/gYzzt4HHC1iJSnR3oA60VkNWCMMc1uTmzRseKoO3qUO7r/WBKjcfgy0ph55WRmXjk55ve0ap/Lhd87K4FHDTlJispM5f8vJ/w/Mn4wJSC5CYtAXF0g80biS8FEYR874d9wYn8NhUOrfN/h2H3GBjvPqa1RB+lZ6cy+empSk2Kqjsp24LAswbKizLYIb4MmksCo8TVZ19d1FCJu8M1EfDNr/+a6KIslPdNHeqavap8xzvrHeiYw8g7UfE7OS8E5uybeNA9TL53A1EsnpDoUpZRq0fIO5ONyuaLOdg0Ul+IvCpDdum7LfpVSjSBcy86SpvbZufEkMBL5iblJyG2XTWZOOsUF/qj9XaIs52guVnyymo+en8veLftp1SGXyZeMZ9o3J0bdB1vEi3F1hMiBSo3eSjN8Tqh3YbUGyQbARI6A/2VMcCFgwDMGyfiGUwwulVztncJ5JvrPFlf1qd1RuXvE7hMfuKr/olTNkHcY+P8VvU8EPE1odx9XD6cobRRS19d1MtQUi3jiKhbZsVeHGgsZN+ScbcJbMCUvlxUxTkfSpkL6JYhVfcleXeUdzOfdJz5mxSerMbZh6KSBnHPjTDroWmyllEqozn07Yts2BYcLKc4vwbYNaelecttl07FnezJzM1IdolInN/cInOKK0a/hsGq/hqt3DQxjzM6avuo7XlPg8XqY+o3To/bltM1m7FnNc83c3P8s5O8/eZ7tq3dR6g9yYOch/nP/m7zw21diPkfSLzyhIQfEBWJVm2Eg6echYmHsY5iCuzCBD50aEXaxMwU9/y5M5WRICoikIb4zo3dabSBtUt0G8k4EV7uoXeKbjYhOBW8JrPQLqiyTqsJ9CpZ3SHIDqkG192pFhwd8NU+9SyrvaeCK/stI0mbUuQBuZa075MbcKrrbKZ0ZMDa+OkAmtAGTf7dT48cEwM7D+P+LKfglxgRrHyCKgqOF3HfDw8x7bTHF+SWUFPpZ8u4K7r3+YQ7tPhLXmEoppaIbd+5o8vYfo+BIIZFwBGPbBIoDHNx1mIGn9Y96A08plTyWOwukY+xvyP6/2sdIYDzN2jk3zWTmFZPxVNqCtNspnbntoRujbqXa1IWCId589IOofQvfXMb+HQej9onvLCTjcmdWATjJC/dw8IyiYsKOeJGMS8B3kfM48DZEolyI24Xgf7WB/5IEyLgS8Z3tfLAr5+6D5PyqzruoiHiRnF+Bu2+lRreTHMm4KrHxqtRq8xS4uh9/LAKegdC67sVak0HSTkcyr4fKMwNc7ZHsnyDuprMLh4gLyfkleAZUanQhvlmQeV3c4155zyWMPXMklnV8wdkpp/bluw9cF38doJJ/OoWLTxTeBqXz4hpzzr8WRF2GWJxfwgf/+CyuMZVSSkW3Zfl2sttk4alUV0xEyGmTzb7tqb2pppQq4x5D1IUg0gGRGLPmKz898RE1T5Zlccnt53HW9dP5auNeslpl0u2U+He+SLWda3dTnF8Ss3/dgo106hV92YNkXOoUtgxvBskAV19EBBPe4syucPetctfUBFfEPI4JLU9cPYs4ibgg6ybI+AaEd4DVCnHXf4q9uLogre7FhHc6tTXcPRGrVeIDVilluXtB+7exgysgvBU8w7E8TXOjJUk/D3yzy9YTesB9CiJNLy8tro5I7u+dytJ2Hrh7IFbrBo3p9Xm57reXc9Ft53Bgx0HadGrVoCUZxgSdZSOx+kPLEd+Meo+7blHstZ5rF26s93hKKaViW7tgI16fl859OhIMBLEjBq/Pg+Wy2LluN8UFJWTm6DISpVLF2AVgdoB7INglTi008YCrbFZGaEWt9eY0gXGCzNxMBp7Wv17PObDzEGvnb6jYLaR1x9R/qHV7Y/9ou/XJo/+Af2LnfwLpF2N5q1frF0kHT9V6rOKOMS071i4kxga7EON/DVzdwHNqSj9ciZUL3obXMHCSH02oxsBJzg7vhZLnwBSCdxpW+qyEjGt5R0XfUjROxhgIrYLINpBWkDYBEV/tT6yFSBp4msaOG7URdw+cus+J07pDLq07JKKQsMuZcWYiMfrj2yXI7XaKwUZCEUoKnbsK6Vk+3F53lRl/SimlGq78vGpHbIKBEHbERiwhLd2LZQkudxPaDl2pk1Ll92AxEAQizu6Qkhb7c2UlevXUAMYY/vWn1/n8lUXOhxPglb+8yTk3zeacGxNUwT9OPQZ1pV3XNhzec7RK+w0/WczgMQfw+jzgB/yvYqdNwmr9UNzHEu9ETOiEO4mmFOx9YLIxxc85ba7OkPMLxFXDuiel6sEu+hsUP3X8Q6f/Dezi3tDmn1gNKLqYaMYuwBT8tmrRzZKnIOsOJAFJNdVwIi6MdxyULojenxa9TlJtRs8ewfKPV5N/uJDyglV5B4Ts1pmcee20OKNVSikVzejZw3nv6U85svdoxbU5ODthTf3GxGa5LFyplkSsTIz7FAj8F7CdRgNECp0bfN6JtY7R9OYaNyPz/7uEuf9ZWOUEaduGtx77gLULUjs12LIsLrvra1Xu8M26eBODxxzA7XFVWtZhoPRz7KJH4j+Y70ynRkDFkAbs/YALKi+xiOzDFP45/uMoVYkd/AKKn6h+xzy8HfLvTE1QMZjiR6vvGGIXYwr/iLGLUhOUqkYyrnJ2VzpR2ungOTWuMVt3zCFQUkrVatuGQEkpWbqVn1JKJVSbTq0oLSmtcm0OEAyESM9q+KxHpVQCBJdQkbyozORjIkert59AExgNMO+1xTH7Pn9lURIjiW7w+FP46Qs/YOqlE+k/qjdTLziIx+vG5YryY/e/HvdxygtcStYtzhINV2ewcsHVlarThIDwVkx4W9zHUqpC8dNOsiya4GJsO5zceGIw9jEIxjhXmAAE4ysOqRJPXB2RVvchGd8EzxDwjkGyf4hk3R53YdBFb35Bh+7taNOpNb5MH75MH606tqJjr/Yse39lgv8FSil1clvw+jJadcylXde2pGelk5aRRk67HDr16cDaBRsJh5rGtYFSJys7cgzsvVCtSmLZ46K/1DqGLiFpgLwD+TH7jh2K3ZdMnXp14Js/drZbtA8+D3aMi3BT0KDjiHjBdybiOxMTeB9TtD/2N9tHgD4NOp5S2Idi95mQUyzSir+oY8LYebETLVD2flBNhVitIOObCN9MyHh5B/MRS8hqnUlW66rLmmr6HaKUUqr+jh3MR0TIyEknI6fqFveBklL8RQGydfabUqkT2cXxWalRPpea2mdgnDQJjD1b9vHO4x+xbtEm3B4Xw6cOIT3bx8pP1lB4tIgeg7px5rXTGDppUJ3H7NqvMxuWbI7Z1+S4OoN9YqLCADaYUuwj3wRXdyT9QiRtcgOOU0OBSxFwJbaInzpJufpiQuuJhCPYERsMSFmBLsudDVbbVEfosDo6RYlMafT+mt4vtdi9aS/vPvExaxduxON1M2rmcM67ZTY5bbPjHrM5MaWfY/yvQ+QrsNo62xr7zm9SO7F07deZrzbujdHXKcnRKKVUy9a1f2ci4QgHdx6mpNCPMQaP102bzq3pPrArmbm6A4lSKeUaiDNDP0LV5bUAAlavWodIylWeiHQRkeUiEhARd1nbnSIyT0ReEKlDudEG2LftAPfd+AgrPl1DqT9IcYGfNx5+j+d//R8O7zlKKBhm65c7eOSHz7D0vdhbgp5o1rcmR51W7Pa4mH5ZfAXfGlXG9U4CoYoIEHaWfJgQhLdhCv+C8b8Z92HEMxA8A6J3esdrEU+VEBHvDZSWGCJhG2PKUnG2IRQMk5c/CctqGh9ixcpAfLOjd7o6grfmraJi2b15H/ff9CgrPl1DMBCiuMDPvNcWc9+Nj1TsdtGSGf+bmMK/QHibc+6K7McUPwPFD6c6tCqmX3561Kr3IsLMb01JQURKKdVynTp7GLs37aO4oKSiDkYoGObAzkO07dK6yVwbKHWyslxecPWnevKiTPbdtY+R2JBiOgrMBBYBiEh7YLoxZhKwCrioMQ/+zhMfEyg+fvezfApZOBSmKK+4ot0YwxuPvI9tRykqEsXgCQP41s+/Tnab41PR2nZu/f/ZO+8oOYqrbz/VPTlt1EqrnJCEUA6AACEEiiaaaBMcCCYbjPPngLH9GrBJBgwYE4zJWUhkIRASQignlKVVDpt3ZnfyTNf3R28azcyGVlpQP+foHG3X1O2a7p7urlv3/i7X3XsV3Qd0PXRf4BChOKeD+1YQDSF1GiBBKdJVX5shw68hs60YtwHh/Q3YmoneCQH28QjPrYZtmpg0Z/lcP8/+YyTBWlvjtmRSsGJBV569u/fRG1gmXD9AOKanloayDkL47qTep9tuPvjPJ/XikKmU765kwduLjY70G4GUUWTo1cxtkU+Rid1HeETZ6TGwG9fdeyX5XZrusd58D1f98RIGnzzgKI7MxMTE5NvH+09/ipQybYFRVRVWfvb1URqViYlJCmpf4EBRXQFKV4RsIUW8niOSQiKljACRZjeTE4G59f//BLgceP1w7X/Dok0pf0fqIo3/Dwcj+Aqbwq0r91ZTuqOc4j5tixIYd+4YTpw+kh3rdqOoCj2P79ahvbuK51o01w8g+hnElkJ8CRn9WFoQEhvBOszQfoSSg/D9DpksBa0C1GKEkn9wgzcxacb6rzaxdmkxf7y6mEEj9uPOibN+eSdCtQ5gB5FQtMOUSxPCAp7rwPU9PfdPyUWo3Q7K5vpFmdPXANYt3MjkH0w4KPsdmsQmkKHs7fGVYOl+5MbTCsNOH8yQ0waxY91upCbpNbh7xqgMExMTE5ODY/ns1SiKQLFb0DQJ6M4MIQTV+2so311Jp+4dJMXUxOQYRGp+0HaApT/IGMgawNpUBS6+EuzjWrRxtDQwcoEGMQY/kKFuHQghfgL8BKBnT+O6CRZb6tdscKQIAQ5nTBfZE47GyASrvX0ZLapFpe8w43nsbUXKCEQX6qJ/lt5gHWUo11tRbOCcihQCGV/WwidtLbRlGF9yf31ZnCRYxyAsPfR0kRZSRqRWB7EFoNWC9XiwDGb9V5vYuX4PvkIvoyYNO2yT0OrSGlbMWUMinmTwuAEdMmrGJDtWm/47dbpjON1JrBaN3IIooVoHqqqgWjqWI1FKqZdSTWwFJQ+p5COEs/WOWbDaLURCUWKRGJFgFCEETq8Ti1Vt9z3sm4f+/TQN1i6CvdsEeUWSEePBZgdE++5dByLjGyG+Vn8u2E/RhT0PEiHL6d1vJXrangBMLSATExOTQ43V0fB80EgmNJASRVVQLSpCiMZ2ExOTo4VVn4RLqQvyyzp0TQwrKJ42vcMdLQdGDdCw/Oir/zsNKeWTwJMAY8aMaUHGv2VGTx7Opy83lSp0+ZxE6qrJLQhjtaug1UdkCDu9ho6hsGvHixSQ8bXI2ntBq2vaaOkB3j8iVIOeZNtJIJ7W88cPRC0CS9vDm2XoRWT4rWbVFp4HxxRwX5+1/KCMLkDWPaJ734BANTz++1x2lRTQEBXyxgOzuPaeKw95qPXHz81l5mMf1nvnYcajH3DitJH84K5LO3QEjUkTo6cMw7/3HaZcsgmLpSnta9uGfLaVXNbo4OgISK0GGfirrtfQQPAZ8P4SYRthyObIM4fx9sPvEQw0RSJUl/rJLfIxesrwgx1yx8YygMryQh77dRWluxo2Ct58HK7/i0K/004yZFbKGLL2HxBr5tgN/Rfc1yIcUwwPV4ZeQobfPOD+OBncNxguz2piYmJiks6kK09n+Ser0RJN7wVaMkkinqT3kB7kFvqO4uhMTEyE4kIqfSA6s9nWOGjbQXOBrXUdyaM1U1sCNMQ3T6JeG+NwMf2aMynu2xQF4M2D71xVgWoRKGrTIXC6olx6S+3hHIohpIymOy8AErt0B4BBhJKDcP8oQ4MV4b6xzdEdMrYYGWr+cl6/PfIxROdk7pMsR9Y91Oi8AHjlIcGuTQE95aSeSDDKU79+IWWSdrBsWraVGY9+0Oi8aGDxhyuY8+L8Q7Yfk8PLwFEOLr5hV4rzAqD/UD+X3hrM0uvoIOueSHVeAMgwsvbvehSSAdy5LmKRA52PkmgoSk6nb/cLmhAK//tHT0p3pU7+gwH4z5+LSSQMqsyHXkl1XgDIBDL4b2RihyGTMrYEGXojw/1xNkQ/NjZOExMTE5OMdO3XGbQMa54SnB7jUY8mJiaHkNjcLA0hZHxTlrYmjlQVEqsQ4hNgOPAR0AeYJ4T4AhgBzDic+3fnuPnlszdz8c/O4fiTjmPaFSqX3mLh9gcFoydKBo6UTL5M8tt/a/Tqu0rPzelIxBamOy8aiK/WUzcMIhzTETl3IxwTwToU4Twbkfsgwtb2FVwZyf4SLiMfZW6IfgYy2fhnoArWfFU/GdHqaK5MGwlFWfrRqjaPpzVaEjhcMOPbLX74rSL6KYVdcynsXoDT68ThtpPTyUeXPkW47F8hm11fRxOpVddrzWRqjEDMmNNs2exVdOlTRF7nXBxuB06Pk4Ku+XTqXsjCd7Ls71vCvpJStq6pBbWHXkFJOEHxgtqNWr+D1Z+va7dNKSUy+km2RsjW1prdyGxDbSYmJiYm7efNB9/D6rBisaoIRde+UFQFm8PKjrW7CNV9+6t0mZh0ZLREOchA9g/UPdCqjSMl4hlHj7RoziLg3iOxfwCHy86Zl4/nzMvHI4P/Q4a3MmgUDBp1gJdWStD8+ktxRyFZ1XK7VgVqF8PmhXUgWAdiOJBZa2F82dq0ypQ/A9UgGxfSpe7caFadwV/ewoXeTmrKsjuoWmoz6VhIrRIQuLxOXN4DVlVkSHcOCPdRGVsKWk3a6ntqeyu/7yzUlPkRisCb70mphNTQ9m3GX1F/PxBWEIXp7YbuF/HsjmL0683QPfKAe11qm7Fzb3Jo6f2b99r1+e33nH2YRmJiYnKwVO/Xs9JVq4pqTRVLTiaS+MsDuMxIDBOTo0eilQgL2Xo2xNHSwDi6WHpnbxNOXf/hMFK6o4yHbvwP67/aRDKepEufIq764yWcfnEWxdUWx2sFte1q+2U7y5n1xGxWz1sHUjJswgmce8Nkinp2at+XaI7aCxLbM7dZ+rRpe2Ex2J0QDQOoIFIfOt0HFLd5OLFIjPefmsPCWUsJ1gTpfUJPpl09kSGnHV9vqyubV2zL2Lf7ce0T8pSx5cjwG3rFFuFD2CeC61KEaCoNJCMfIyPvQXI3KEXs2zue955LsnbBBhRVYfgZJ3DujVMpKM6oZWuSBWHpjYx+kblRLQJhLI0gkUjw6C1P8+U7SwjXRfDme5h01elc/dfLjQ1U7aKLQcpIlvbehsx2P64rG5ZswV8eIBKMIITuzMkp8tF94MFVOOnoFPftjKoqDBq5l7Fn7KKwOEhtjZ2VX3Zl8ac96WZAkFcIG1ItZuvKfbz/vGDzaoHDBWMmSs7+ocTTJcu9rDW7lt7IROb7Tdb7YxuQMgKh15DRz/SVDMtAhPMiRPPy1R2EdQs38uEzn1GyejvuHBcnnT2a71w3qcNUCTIxMfn20HtID9Yv2kQ8mkjZLhSBr8BLpx5mBRITk6OKZTS6mHmWxT2l9XfYY1Ot0DYua2UM4ZieMvk81ASqarl9/B9ZM28diVgCKSX7Skq57+rH+Py1LzN3so4AS+YqJ8J+JkJpW7571f5q7r/2cZbNXkU8GiceS7Bs9iruu+ZxKvYaXwkUznPTHA56g0A4z8/cyT6hqVwO4HDBaefUX8hKLjRb6yzqWcjwM05o01iklDx2+3/5+Lm51FbVoWmSkjU7ePyO51g2W09DmXDpOGxZVKgn/7DtpSdldKEuzBjf0Bi5I8MzkIG/IOvDSWTolXr9g10gJfu3lfLA9W+xcs5nxGMJouEYiz9Ywf3XPk6gsuPpr3Ro7JN0teIMCOcFhsUR/9+0v/HJC/MI1YaRUhKorOWth97jH1f/y5A9IZwIx9TMjWqxLqZrgBETT6B0RznhOn2cmqZR5w9SvrOCsdOMCYN+U8gp9HHZTwVnX76eoq51KEKSkxdhwtklfP+nexg4pp8hu1s2nMbDv1LYuEKgJSFUC/NmCv75CxsxYbAsreOclGiy5gjHeYZMSqkhA39FhmfoEYNSQnwDMvB/yGiW58hRYtXctTx2+7NsWbkNTZPUVgf55IV5/OvWp9E0rXUDJiYmJu3gO9edlea8AJCaxGq3YLEcm2u3JiYdBcXiANGCI9H719ZtHMLxfGMQworw/Qmsg5tttOmTbZfBVdY28tLf3qK2Kn2iqmkaz//59Yx9hFAQvj9A82oFwqpPitxXt3nfc16YT211urhhXU2QOS8YF68Ulr4I769TnUJKPsJzO8I6NHMf4aw/B03VRc671sbE7w3D6myKBhkwuh+3PnotqiWDgyQD6xZuYtOyrWnbpZTMevwjpJQU9ezETQ/9mM69mvbjzfdwxe8uYviEtjlKQK+8kpH4eogvQ2p1+gSjGR+/LAgHAS0AsukBW1PmZ+6rHWvi0dERig/huyt1FVtxI1xXIhzTDNlct3Aj677KHNr2xVuLqC7LWDCpdVxXIpznpJaGsg5B+O5EZJnctkbJ6h3kFeWgqk2/DavNSm7nXDYu2WJsnN8QpIwwbsp23LluvRRXPQ6Pg5Mm1yETu1ronZ13n/GT1JqqIAEgbOzd2YWlH203ZFNY+mS+P3pva5fWUArxZRDPrPMhQy/qJXs7CDMf/yhNMBlg6+odrJm3/iiMyMTE5NvMP65+LGtbxR4zbc/EpEMgbJAxMdcCidZrexyzbkihdkbk/FUXwNRqQO2OyLKaeyhp6YVt//ZyEolERu+wUPIRvj8ik6WgVYPaDaF427Xv9Ys3Z29b1Lria0sI2xiwjoLkNkCC2rfVKibC0gORcw8yuQe0WiyWXlzyaydn3xRm/7YyfAUeCru1L9RvfZbJJ0DZrkoq91ZR2K2AAaP7cecbv2D3pr3EYwl6DurWZicJ6FVUSO7N/oF4vehosyorABuWN/uxyjCIpnO4ftEmzrspy0q9SUaEpQ8i935kYqeue2HpgxDGw9LnvfkV2ULakokkX76zhLOvm9z+cQpVdzY6L61PJcpFHIRuDcCGxVvw5Lkbq5EIATaH7iDZsGgzU3808aDsd2gSmxBEKCjOI7fIRyKWQLWqTffO+Cq9zHQ7iMfibFm5TY8AU3JARgGl0em0YdFmTr3gREPDFbbR9ffHEl3sx9JXvyaMEl+dvS25T6+rfphTIdtCdZmffSWlWdvXL9rc5ug6ExMTk7awc/2e7I0SVnz2NSMnDjlyAzIxMUlBS+yq1wCzo79zJ9AXjurfi2JfAj9p0cYx68AACAZCLJ+9nUBlLb1OgMHjBqAohzcoxWbPnLoAoKii1f0LtXPW9JdW992QNiFj+mQPqYscClvWlIoGAlW1LPt4NeHaMP1G9Gbg2P7pYxMKWJpCt2urq3jzH0+yb1sZvYf05MI7foLdma5LINRujdcsgMvrpO+wzCkzALFonBVz1lCxu5KiXp0YMfEErDZr/Xe0Ze0HYD3g+Hc3kCuvD7rl/YCt/t8BW5vPrQ9IcWjp2jBpGWHpeUjsON316WNSj4qSUj9NDeWWHe6DSy8TigeUQQc7TACsDiuRUBQhBHZn6rV24HX+7UP/IUkpCVTUEg1Fsdgt5BXl6o7IVn+f6SiqgsWqkognAaHrljTjYI+pECLl/nhw6N9PS8aJhqpAJlEsLuyuHPSxt//7NyBlDGJf6Q5atRhs4xAG7dkcVoQQaJpGNBQlGoqhqAounxPVopr3PBMTk0OOqggOLDDeHE9eBxD3NjE5lhEu/eVaSqAhlVRDd2K07R3mmHVgrJ63jmd//zLRcNMKefcBxdzyyDX48tsX2dAezrjsFDavKMnYNmBUv8PqQBk9eTg7167Sc6YbqQLFx5gp07P2WzhrKS/f/Vb9i73OcSP7cONDP84qwrbgrXe4/9oXiEX11ewFM3fy9sNfcudbt3HCqaca/g67Nu7hX7c9m6IXkVuUw62PXENx386MmTqcD5/9NGPf40b2IaewbXohrSGUHKR1WPaVUPv4+hKPuXqETz2jz5B89LJAX9lNdeaMmfrt1i34JvCd6ybx+v2z9PrCWgAAIABJREFUiEVTI2dEQsOd42LCpVmEdo8CYyYP57NXF2Ru+7ZfS5bjCNa5KS1ZSzLZpKNQUxagU48u5Oaf3G6Tqqoy8syhLPloZcb2MVMNpnscDuynEal4CqGVoQj0KEzNT9hfgdU3DZuSa8isTOxABv6SWh1F+R/4fo8wIDjq9rnoP6I3X7y9KOVZW1PmJ79rHqM70jE1MTH5VnDyeWOZ+0rmZ6NQBceNMC6ebGJicvAoagGa0gOSm0mNek4CFnBc1LqNwzW4jkxdTZCn/99LKS9UALs37ePVe2Zk6XVoOPemqRw3qm/adpfPxS2PXnNY9336+Rr9Bqfn8PcZ5GfCBemCR6BXLXnxr2+kOC8ANq/YxoxHPsjYJxoO8cD1Tc6LBkJ1Gndf/ojB0esr4k/+6vk0scuaMj9P/VbXo+jarwvTfnxmWl9PrptLf5VFUNQgwn01ZBBQFc4L9NQGYUF4btQrxdQz6VJJ9/6AWkjzn9+gE49j3HljDun4TNpPXucc8jqnl1CWUtL9uOIOJf41/dqzKO6bHo017PTBHWuyfRgQQuGB2xqqFjUhNcmTd1oJh4yt7F9w6/SM1YDGnTOG408ekKHH0cFfCcEaf3r6qEiweUULqW0tIKVE1v49vbSrVo2s/XujMHF7sbvtJGKpzw8pJYloAk+uuRJqYmJyaGkpglcxKO5tYmJyiBEN6SMHounRn63Qcd7GjyCLP1hBPJo5wGzV52upra7Dm3d49DAUReH+uXfx9sPvM/fVBcTCcQaPG8AVf7iYwq75h2WfDVjlp9xyr2Tpp5LVC/Sb+LBTJWPOBCufApPS+nw5c2lGATaAxe8v56Kfnd2YvtHAe48/RzSUuU9NRYKF78xi3Pnntnv8GxZtpnJvdca2fSWlbF21nX7De3PeTVMZOLZffRnVEL2H9OC0C086ZNEXDQhLT8h9CCIfIRObQHgR9okpwnzCNhZyH4DIh8jkHlyFRfz82TNZ/HE1X89fj6IqjDhzCGOmDG+XBofJ4WHN/PV48z0oqkJ1qZ9kPIHVbiW/Sy6JRJLdm/YaTzs6xHhy3fzy2ZtZ9O4yvl6wAZvdyshJwxg1aehhT4U72qyau5YlczRK1vbjlKlVFPeKUl1u5avZeeza4qTHIx9w+W8vbLfdvM65/OaFn/LljCVsWroVh9vO2OkjGTr+eMNVbQ4HO1e9gM9pIx5VcLrjKKokFlUJ11lx2rcQrqvC6Wnn8ySxTtfPyESyFOJroJ2io5FQlI2Lt1Dct4jamhCxUBRFVXDnunG47Sx6dxnfuS79uWNiYmJilFfvzb4QmUxo7N9ZTpeenbJ+xsTE5PCiaUFdEwwbuv5Fw5xRBSwQehZsD7Zo45h0YPjLA1nbNE1SW9W6A0PTNMOTBEVRuOj2c7jo9nMM9ZdSSxHIPPDvTGiaBlo1VhuMmwbjph3gYNDSHQMN+eXZiISiRILRNAdG+e7yFseyr2R3i+3Z8LcwFkg9rwPH9s+o03Ew5y0TQskF12UZdXQbP6N2A/c1jZ+xA+MvhPEXGiufaXL4aLje3Tku3Dnpei015YEO48AAcLjsTLj0FE6/ZNxhmWBLKQ+5XS2ZQFEP7tGzb5suDFm538as59LFUCsPQmne7XMx+QcTmHTV6R3KadEcLV4JTohFLcSiqcdSUTRC/tL2OzAyPANSkK20ZyBcGyYeS6BYVHIKvUBqemZNC89iExMTEyMcGF19INvW7DAdGCYmRxOtHGQSPRLdhu7AaPa+pVW0auKYdGD0GNQta5vT46CwW/YXv3lvLOTTl7+gbGcFBcV5nH7xOM66cvxhX/GUMgHhN5CR2XpIr6UnWIdDYgvE1yOFC2GfAK7vp1RTWThrKZ+8MI99JaVccv0+RpwWwFfoRRw45bY0pbVsXl7Ce/+ezeYV2wjXhgnVRsgt8jUKGTaQ3yU34yRv8CnDmPXvdG2I3oPCTLu8irMumolWOQ9hPw1clyOU9JDtTPQYmH3iKISg+4DsIUcrP/uaj/77GTvW7caT62bcuWM4+yeTWhX9NDm2aLg3BCpqqa2uI5lIYrVZ8eZ78BV46HZc62FtR5Id63bx7hOzWb9oE6rVwqizhnLezdPIK0pPg2krNeV+Zj3+MctmryIRSzBwbH/OuX4yfYZmD8ttjXg0yLrP7sZhmY/dHqa2thO23EsYeMqPDNkbetrxCKEgtQR6zqSG/vBTQagMPmWg4bHK+Fpk6BWIr0UKB8J+ev196tBGcB0Mdu8gIHPZ5WjUQfdOvdtvVG2lj5qe+tgavkIvvgJvWtpfAy09i01MTEyMkFvko2J3dif28ImDj+BoTExM0lC6g3CCDKC/wzUgACuorYvdf7vjjLMw8swhFPUszNg24ZJTsk5qZz72Ea/cO4OynbpnqHJfNW8/8j6v3vvOYRtrA7LuAWTotaZVsvgGqH0IYgvrPxBCRj5ABv6oq8gDs5//nOf//HpjGbv57xdRtb82fXVSKAjHeQBsXLqVh29+ik3LS5BSYnPaCNeGKd1RjjwglWTyVRMyOm7GX3wBRd1Tj2HvQWFu+ssehp4Uxe5ygowiI3OQ/t8jtVCbjkH3AV0ZnCUPfcQZJ1CUxaO+6P3lPPmr59mxTo/8qKsJMvv5z3nstmeRMnOqi8mxSd9hvZCapKbcTzKh31TjsThV+6vJLco9KMfAoWbH+t088JN/s3bhRjRNEo/GWfT+ch649nGC/qAhm6HaMA9c9wQLZy0lFomjaZL1izbz0A1Psu3rnYbHuvbjn5Dj/hi7XRet8HrLsScfY+1n/zRkr9txxQwa4wPiNClY66W48jsrnHX5eEN2ZXw1MnAnxNfWb4ggIx8jA39Ayqghm4eDfideRTicOUowqk3GYnO226awdAfb2MyNttGGKv2oqpr1XOQW5TB22rdcbNbExOSIc+tj2fXkbE4bLlf6wpuJicmRQ1EsoBSR6rwA/T0uBq6rWrdxOAbW0VEtKj/917UMapZi4HDZmfLDMzjnhskZ+wT9Qea8OC9j24IZi6jYazxkuTVkYgtEv0rdqFWh13qspukFHkhsh+gCYpEYHz6TWo1j/y4fM/47hL3bJLEGDRC1COH5JcKqr1i++8THjRM30EsLFvUqRCiCoF93NLh9Ti64ZToTLj0l65jvmX0PvQd7GiOCpl9ehSdHoetx3VM/mNwH0cxVQzJxzd2XM3rycNT6aBDVonLy2aP5wV2XZvy8pmm8+8THGds2LS9h/Veb2rxvk28/pTvKkeilfBsuXiEEnlw3tdV1hOvCLfY/knzw1JyMWj6V+6qZ/+YiQzYXvL2YigzpF/FYgvf/84khm7vWfkZu7uaMbZbkWyRi7T+mUoa587+VDDlZ0uhDFdDjOLj7tSAyYczZIkOvQCaxysQuiM41ZPNwYHO46XT8I/j9PRq3xeNW/MFpDJn0O8N2hfd2sE8AUa/HIxSwj0d47zBsc9JVp3PO9VNweprK0vYb3pvbH78uaxUrExMTE6O89WBmgXmARCyzYL2JickRJpntPU1A8LFWux+TKSQA+V3y+Olj11Gxt4pgTZDOvYtafJnasnI78Sw3Pk2TbFq6lcLzDpMIZ3xN6t9SAxmp/38SZAxE08uhjK9mx+ZehOsiaaa2ri1k69oCrvztWL3qhdqrUT8jEU+wddX2tD5Wu5UuvYsYMKYf3711OsX9umCzt6zyX9SzJ48u+S87169j1/pNjDn56TStjObjFc626YE4PU6u+dvlBKpqqdpXQ0HXvBb1Sip2V1K5L3vu9obFWxg8zni4ucm3i41LtqAogsLuBSQTSRLxJBabBVVViEfjlKzeyQkHkZ5wKNm0dGvWtg2LtzDt6vRqPK2xcckWQ20tUb17Hr4sC152e5jSkqV0G9TOiInEZlzuGH97Fcr3Sjavgm59oFdD1GF8NbQzYkDKmB7Zlq09vhrhmNq+cR5GCnucQGGPN6ncs55IbQXFPYdhdx1chJAQToT3NqT2Q0iWg9pJ1/k5KJuC71x7FmddcRr7t5Xh8rno1L3goGyamJiYZGPLipKsbVpSY93CjeZ7n4nJUUSLrQEipJdSqye+pFUbx6wDo4HCrvltqv5hd2bXSoiGY6z87Gv85QFGnDmE4j7ppQ0PjkyOFUGjamtzoTmZgMRWbHwMMqznGGXom6QnwpJaC1tRFaw2S1ZHTUGXPHoN1lf8kokkqz5fx+6Ne8np5GPM1OG4D5ilSBmmR5899OgVRIY09EiRDEE/wt6sjwaxxZDcCiJPX/1TvGldfPlefPnp2xvtaDUQ/QKLVgYyCCJzub6WzqvJsUfz60G1qGmVYewu49eLpmms/nwdO9fvIafQq/9mcoyXkbQ6rERCUbr29tNnUBXJuMKGVZ2oqXDhMDjOxu8vw01RXsILSo7h34qithyua3Ua0ZZocth26qr/S6GZQ7ftWPSSxzIOMlTvJBYgPCCsiIz30qNLoKqWVZ9XUFcTou+wvQw+xXtI9JiEkgsH6bg4ELvT3vj8MDExMTlcWFqp6JbTqePoGZmYHJOoDYsYmdL4BZnnvakc8w6MtnLc6L7kdvKlqKZLKanaV024NsKaeev5+osNzHriYyZdeToX3nb2odu5/RS9pIysT+0Qij4hl3UgbDSeaM1fr9wap3uP3RQVK5TtcdbX021yclhtFkaeOSRtN4qiMPKsoSz+YEXGYYypz1euLq3h4ZufonRHU7WRdx79gGvvuaLRqy3jG5C1fwOtrmlsMgRqV3TF2SaE/XS9j1aFDNylh2s3EPofeH+ulyNtIzK6AFn3MMg4uR7oN1iw9WtH/XFoerAJIcwcbJMUhk0YjM1hJRZJT80oKM5rsb58S9SU+3nklqcb9WgA3n7kfa7+v8sZdroxQbETpw3Fa3+SAUOb1JrHn13Clx/1ZsD47xuyOXrqCFZ88pH+e20kAFo5o6d8z5DN7kMvpnrjGwiR/qCqre1E717tK80JgOU4UDvr5T0PRFjBdnK7TQqhIK0nQvh1/V7VSBUohWAzpqtxuFj68Sqev+u1FIdzz+O7cesj1xyUY8zExMTkm8ykqybw5oPvZmyzOW1069+xxLhNTI41FLUrGl4gUyUyCc7W3zePSQ0MI6iqyhW/vxirrcnnU1cdJOQPk1+ch1CaHASfvDCPVXPXHrJ9CyUX4T5AlEgp0CMXlCL9bxnVnReKD4QTIeCKOzTsjjBolU3dFMHFPz8PT27mF9wLbp2esQrLqeefyPEnHQfA83e9nuK8AL2k6lO/eZFwMIKUcWTtP5qcFwAiH1DSJxz2CWAdo3+FusdTnRcAMoasvR+ptVxCtfHjyUpk3T/1VdR6LrtV4vZGIZlaluec6ydnFf40OTZxepx8/7cXoiipYW02h5Urfn+R4dXtl/7vrRTnBUAsEueZ371kWHDz7B+GOGGsP2WbACZdXMroMwyZZNjJJYwavzdte5ceQb7zvYWGbOZ27kMwfkna9njMRn7f3xiyKYRAeG5Jid6qb0C4r0upxNQ+w3Z0YdDm1ItKZYgEO1pUl9bw3J2vpkXL7Vy/h1f/fvhFpU1MTEw6Ki2VqO/ar+gIjsTExCQ7LejRtCHi1YzAaAcnnDKQ3796B/Pf+Ir928tYPW8dDrcdawY9iC/fWcLwM044ZPsWjmlg6Q+R2UitCmHpjbSehkisRMbXQmITkATRFK7dbyj87j8aX7wXYu/egeR1zuO0C0+kx8DspetyO+Xw2xd+ysKZS9m0dCt2l50TvzOqMe+/Ym8VG7LkwkdCUZbPXs0p02VTtZTGL2DRy+bIWv17qJ31yAvrGIQQSK0KYssyD0rGIDofnN9p/UBFP9fTaJrRtQ/87imNBe/VsWP7qXjzcxl33hj6j+iTxYjJscxJ3xlF9wHFfPHWYir3VlHctzPjLz65Talmmagp97P2y40Z22KROEs/WtWiIG42bGI+nXt3IuQPEa6LIITA5XPh8jogNgfs7Y8uUiKv8qNfVzDmjCBL57qJRwXHjw5z4llBHM6adttrYMhZv2DPhpMo2/IqQlYhrP3oNfJqcjsb/w0K6wmQ+0+IfIRM7kQoheCYkpYa11ak1CC+CNT6+5QMA4qeQiNcEJkDnvaXEj0cLHpveYrYcnNWfPo14bowTk/HS3kxMTExOdy8+dB72F024tEEWrJelFno0cdlOyoJ1YVxmfdHE5OjhpbYD7RQgTL4KLjTF76a861yYGiahpQSVW05/+1g6NS9gAtv19NDfj31L9RW1WX8nL8yQCKewGJt3yE+sI+m6TdfRVEQlv7g6d+YDCIArL0QzvPRau8HLT0UJ78znHd1lKjzuzjdeW0ag9Pj5MzLxzPh0lNQVAXRTGOjtjI1EkJqSYTSdLz9FX6klsWrJhQQOQjXJekpIZo/c5/GHaULccZj8XRh0AyfA/DlwfQrNUTe2Qj1wIT5o4OUSUA0iqiaGEf/ncRQFCPaB+l061/MZb86/5DYqqsOppTrlZpMidjyV7QtuigNrRpFKHhyPbhz3Q01UxrbDCH9KAoMGxdm2LgDqoPIg6vA0m3QeLoNGk8kFMHhOjTnSahF4L6KpIF7bToJ0IKACiIXia/+3qcfUymrs8lNtRktmUBKDdVycNo7LV0zyUSSoD900A6MjPdXExMTkw6Ovz7V22q3kKx3YDRUrksmk/jLA6YDw8TkaJLILkIP6BIJrfCtcGBU7qvmnUc/YOVnX5NMaAwc049zb5xCn6HG8tXbSs+B3Vi7MHVlVdMkgYoAddVBfnrK7yju25kpPzyDk74zqkVb895YyJwX51O+uxJfgZehpx2Pv8LPuoWbEEIwdPzxnHfzNLr0zhz+Jix9kdEFaduf/jN88rqVYO0t2Bw2xkwZzh1P3dDiBGLD4s289+/ZbF29A6vdypgpwzn/lmn48r107l2EzWFl39ZdBCpDJBP6ZKznAI2Lbwxx2qS1EJS6AKCST5oQixCgZlghVYt14T2ZXjlFb9dXPjVN45Pn5zH3tS+pKfNTUJzHxO+dysTvn6ZPNtQWVkgVr57LfpSRia3I0EsQXwkoSNuJCNcVHcax8k1CS+yBwO/0qhMygaZ2Bte1KO7LjvbQGunUowCHy87erfvxlwdIJjWEEDi9Drr0LqLHIIPn3dKXaGAV/vIAkWAUhMDldZBblIPV2c+YTbVvehpXA8J4ulWoLsx9P36MFZ+uIR6N4833Mv3qifzgTwd3nua/tYg5L86jbGcF3nwP4y88menXnJkmvtoWhLAh1e74y9ZRva+GeDyBEAKHy05Rr07YXQaPKVCxcw27V92PL2c9AvD7+9J50E8pPm6cIXstXTPefA95nY0JcEopmfPifOa+uoCq/TXkdc7hjEtP5awrxx8ScVATExOTw02fYb34+ssNaImmktgNy2q5RTl06mFWQTIxOapYRpNSjOJA1NYFv7/xbyS11XXcf+3jLP14FYl4EiklG5Zs4aEbnmTHuiwv4oeIyT+ckJYrX7G7ktqqusZqBftKSnnuzlf5/LUvs9p578nZvHLvDMp361oVlXurePXvM/j89YVomiSZ1Fg5dy33X/t49pKg9kmgpJbQe/BngneeVggG9NMci8T4cuYSfnHmXVnHsmHxZv7102fYunoHAPFonIWzlvLgT/5NNBzF5XWCVkV1aZBkQr/wcvJjXH/nNvoM2ofTYwXsICUk9wCx1B3YTkeo6U4EIRwIZxbhU0sPsJ0IwKv3vsOMRz+gpkyP2KjcV80bD77L2/98v/44nKqL+2VAOM5FiKNbdUQmdiL9f4DYCv0YySREFyL9v0MmK1s3YNKIptVB1fchtrwpbShZCnV/Qwu+fHQH1wy7047FZqFqf03japCUklAgzL6SUoZNMCbiWVp6GqU7KnTnhW6UUCDM7s3VhGJnGBus52ZdBDMTriuN2QR+dtofWPzBcuJRXV+itqqW1+6byWO3P2PY5ofPfMrLd79F2c6Kept1vP/UJ/z3j68atrl8QT/Kd1YQj+vXk5SScDDCpqWl1IbbLwwKUFO6jcpNt5Cbuw5FSISQ5OZuJbj7F5RtyyyY3Bpjpo6goGvmiLqzLh9vyIED8OaD7/LWP9+jar+eLlRd6uftR97n9ftmGbJnYmJicqQZNXlIivOiOXU1dVgs34q1WxOTbyyKxQG0sNDi/XPrNg7dcI4O899c1DiZbU48luDDZz47rPseMLof19x9ReOLZDQUJZlI0qlHYZouxvtPzyERT0+tCNeFmf385ynbAlV1aJpG0B8iEW3qE/SH+OzlLzKORShehO8usOpaFYFqmD9TQS8NmHqat3+9k6Ufrcxo571/z26cZDWndEc5i99fQbA2wPa15SgWGqPVz7igBm+u7jyqq67Pk1c6A+6m1BBhRTimIDw3ZtwvAM7vI1wXpYq32EYhfHcihErlvmoWzFiUsevc1xYQqKpFCBvC9yewDmtqVNwI12XgvCj7vo8QMvxW5igTzQ+R9478gL7JBJ8ELYMug5QQ/M+RH08WYpEYW1aUNIawNqAogkQ8kfU33RrvPFHGuy8MIlDTFOVUttfDS4+cwPy3txmyqVgHge9eXSS4AeEE99Uonh8Zsvn5a1+yZ8u+jG2fvDCfSChL1FULREJRPn5ubsa2ZbNXsXfr/nbbBHj0ji28+WQxdf6mF9wdG1088v968Mq9nxqyuWP5E9jt6ek3FkucPV8/ZsimzW7ltsd/wqCx/Ru3uX1Ozr1hCpN/MMGQTX9FIKujff6bC6nO8Jw1MTEx6Wj85dIHs7YlYpm1g0xMTI40LaQlh19qtfc33g25KYugJMDGFtoOFSPPHMrwM05g/7Yy5r72JV+8lXmCXVtVx96tpfQclCqgue3rXWklG6MNK6roL+oee9Npauk7CUtPRM7dyGQpSz/4gkTyLbIlbS+YsZgxU1NF/hLxRGPkRSY2LN6CIkpJxCUWC0hVnysOGhVu3E+dvw5PXh4IVY+EEF5Ezp2gFLVaGUAIBVxXgPNCSO7T9TLUponUluUlaFrmcKNEPMnWldsZeeZQhNoZkfMnZLJCF+NTuyIOrFZwtIivydok42sOOsf+mCK2JHubVoGmVaEoxoQ3DyWr568nFomjWlUUi4KUeiZVg7bMoveWM+nK9k86Ny0rIRLqwvrlnSnsEiSRUKgu10V8NyzewrSrzzQ0XsU5CZyT0GIr9HKi1rEoivHIpa/eW561LRaJsWLOGsad2/YyyQA71u4iEopmbd+weAtd+3Vpl81QXZiKvVVU7M1n4cd5dOkRJRJWqCrVv/ua+evaZa8BRcvez+kw/owq7JrPTx+7jurSGoL+EEW9OmHLICjdVras2JbReQ16auTmZSWcOH2kYfsmJiYmR4LmC3+ZePnet/n+r797hEZjYmJyIFpsDRClaZLaMLer/zvewvt9Pd9IB0YymWT15+vYsXY3pTsr0BJJlAwhsw1pHIebHWt38fUXG9i3ZT/xaDxjVRIAR4bxOFzpE+sGgT+pSUK1YZKJJC6vE6vDit3Z8kS8YtdGlr4/kw1LS9GSSRRF1cVNNYkQAkVVQIDT2xTlEKoNs+SDFVTtqyESjGJ32VKEOwG8uRGOH7EWl6OpnxASISAa0T9rtWm4PHG9bKtwEwk5WDLXRnVgI136VDNq8jC2rd7BhsVbsDttjJo8jE5dBcTmgVYL1uPBOhohnGBp0rJYNXctHzw9h/LdVdRVB3HlOAn5w7rgqc2Cy+dEURTsztTjq6eqHH3NixREC+KFzdo0TWPdlxvZsnI7bp+LMdNGkFeUmiIkZQyiCyC5C9QisI1HKJnL4zb1kaxbuInNy0twehyMnTaC/C5tE3ftcNQfr1hMUlcj0TRwusDlU+qFUY07rbREOYSeguResBwH7mtQWjm22fDl6f00TUNLasikBIHu0FAUw2KLNqeNSCiKlILyfanOwUz3mvawafF8tix+CWSMgp5bOen8Hxi25fLq50lKdEV4qevmKPURKZ689pc8tdffNxtSceLROBarisvnQlGVjPfV1rA5rCiKQjKZoGvvMB5fHE8OJOPgr7K1eu/Nhib1c2GxJrE7E/o9M2whHlNJJg9OJHPXxj28+eC7BCrrGHLaIM67earh8Gh7K8fsYJ6niUSMtXP+RcnKNbhy3Yw952ryu5nOEBMTkyNP9wHFR3sIJibHNo2L0wcuSkt0J0br71vfOAdGoLKWh2/+D3u3lgJ6WkXVvmoKuubj8qVOBMZOPbwvSJqm8ezvX2HZ7FWAHgWwr6SsXkQtdbLZa3B3inqmi+D1HtKDgq55VO5t0rZw+ZxU7o2gJTVEXYRIMIK/IoAn18Nlv8xeGeHT/z3Cm//8HFm/iKYlIR5LIhBNTq5EEpvDygW3TAP0iI5///J/jXn0QX+Qqn3VFPUsxGLTL4/h4/Yw+eLNdOlVgMPtIKdAw1/ZFA6//HMPo0+vxZOTxOa0gVbN1q9DPPGnYsJhAcpcNE3yyC1P4fI5Gx0825b/l+/dsp+cwvoJTHiG7rjw3YlQvADc+d2/Nx5fKSWxSBwhBDZH04u/vyxAn2G9GNgsnLqjIuzjkaHXsrYBBAMhHr31aXas293YNvOxD/neb77LqRfoWiAysRsZ+JMultpo4AXw/VYvL5mBcDDCv259hpI1TVE2sx7/iEt+fp6hMp5HHed3qdm7hJpKAVK/wOtqwFalkd+tPy6DDgctPBMCd4Gsj4yKfg6hF9FyH0Gxn9huewPG9Meb76Vid6rGiZZMoFpVzrt5qqFxjp06gjkvzc/YNmaa8XvfB4/+hL4DltBvYMODZRlz/v0i4y5/HZfX12575944lQ+f+ZR4rNmqWBJEQqOgax5DTzu+3TZ7De5OToGX9Ys2p6Tm1ZQFKO7bmeET21/C2mKxMHBMN5KR1dRUqlRF9HucwxWhc/cwk646vd02ASyeiXhzn8TlbYq0c/tiREIWAqFphmwC/O+u13iCVcStAAAgAElEQVT9vll6+Vdg8QfLefOhd7nv0z9R3CezDlBLDDqxP958T8bKWu4cF4PHDTA0ztrKrTx6w+3s2tywpZxZT/6By389ilMubT3P1cTExKQ9dOnbmf0lpVnbx3/XmJ6RiYnJoUFRu6LhBjJVG5Hgurx1G4d8VIeZl+9+u9F5Afpk3+l1Urm3imSiKbet+4Bipl1jLIS6rXz+2sLGyTWAxaqS1zmH2qo6QrVNuT1un5Pv//bCjDYUReGqP16SEj2gKApSk1isakoKSCKeyLoKtmvdYt58qMl5AWCzAVIiD/Bw5RT6yOnkIxaN89RvXmgSAURXaEZAxR59YlzQOcjkizfjy3fhcOsrqTf9dS8Wa5PNSEjBYpWoFlAEJOLw1F87Ea6Dhi8QqAjgrwg02s3JDzP10o34y2uIhpuFgidKkEFd2O/tR95POb6gh91LTZJoNhnSNA1VVRpXdDs0zvPBmmEiYBsD9rMAeOuh91KcFwDJpFYvWFgOgKx7MNV5ASBDyNq/65EZGXjnkQ9SnBegh4a/dt/MrBoFHZmPX/Xy7v/y0hy4pbtt/OEKY1ElmhaEwF+anBcNyDD4f2VwpPq9IRvePGOOlmnXnJlxJWn0pGGMmjTUkM2FM56j78DFIFIPave+pcx97ueGbHbqUYA3S5RF7yE9DdkUQqBY1bSUB02TSCmx2oz55oedksRflXqupIRAtcrxY4yVfh104lDsrvR7k80uGHSysfNUsno7r983s9F50YC/PMDdVz5syKbFauHK31+cduwsVpUr/3Cx4ZKqb9zz/5o5L3S0pODFe1ZQti29cpaJiYnJwWCxfgPeBU1MjnlaKJUa25y9rZ5vVARGbXVdWh6yEIKCrnlE6px07tmJHgO7csKpgxgzbcRB5QO3hS9npufoePM92F02fcXq5IH0GNSN0757Ir4Cb1Y7A0b34w+v3cEXby1mX0kpqz5fi91lIx5NEK7TBe6cXgcur5NF7y3n5HPGpNlYOGMmstmcIx4FTYLVDskEWG1WVKtKblEO7hwXKz9bi6IqBAOpIiqqRaVLnyKCNSH6j+zDqVM2NkZeAKCFGDfFzz/fDfHyw13YucnO9MursdrAagNQ+HpJHrV+u66DIeuAXII1ofpxxYmFYww5cT9K/SSpriaYGp4dW4CUP+Hj/85NGZus178QQiAluLxOLDYLnlw34WCELSu2cdyoFsqodgCEcILvzxBdgIwvBVSEbRzYTkIIhVgkllVgVdMkC2ct47zrB0Aii0ijVqtrQ9hPTdmcTCRZ9H5mLQIpJQtnLuXiO849mK92xHnjvpns3FDMygUeTp4cwOHS2Lzayeczc4mG6wiHozjbG/YfehlkFm0FrQot8imKo32O0e1rd1Fd6sfmsDZWShLUp5CoCq/f/y63PHx1+8YJuH0ufvHMzSz9cCVrF2zAarcyavIwhpw2yHDJS//ut8kfmLmtqPNqQzaXz15Nftc87G47NWV+kokkNoeNguJcasr8VOytorBr+7RK9mzZR/X+Gor7dqauJqinkFhU3HluLDYLq+auZWw7o1CSyQQr5+7A5rCRjCfRNA0QqBYF1aKycMYseg1tf6SSSMzDldOHSLAKLVELSBTVg8OXj5ALgUvabfONB95Fysx6QNtW7zR0TAGGjj+e3796B1+8tYiynRV06lHAad89MWP0YFsI1e1nxdwgmcSYpAaL3nmOc28/Nb2jiYmJiUF2b2x5QearD5dx8rTRR2g0JiYmB6JF17f8gejrQPaKmfANc2DU1QQzijgKIXB6nQybMJjzb5lmeKWovQQqaxv/3zApQQhsDhvFfTvzo79cljIxj0ViWGyWjJOL/C55nHfTVKLhKH+59AGq9tdgc9hweuwgQFH0VcHa6sweq0BVbcrfyaSeR6QooNgkPQYVI5SmlcXq0pqsEQuKouDN9zD5BxMYPCyKiDVftddXpnv0j/Orh3cBtvpt+nlJJOzUVObTGNwj9Qlb85XSZFLD7W2KEojHNGIRsDUscMoEaCGCgVDjZ6SUTS/s9e/Chd1Ta3lXlwVIJpKGSwhmIxYNoSgqFuuhEQIVwgaOiQjHxJTtmqbhrwikhtofQG1VXebKGymG0qsFRMMxouHMkRkAddXBlm0aRF8hjh8WEdWgX78+vl7kZd0SD6oqiccbrmlJzf4anO0NpU82RXdpEqIhidMtMra3FT11RNd9sNotTSl+9WSqotRWbHYrp5w/lrHTBqMoFtSDvEZttuzXQfPfbHtouE+6c1y4c1y6Hk+z8tPBmmC7J9sNaQ4Wq0puJ1+azUxpEK2RiEcJBzWEEI3pcynfo8rgb0SrAQQOdwGJeAFSNjh7yfhbbQs15YHUDc2uKSk1qvbXGHJgAHTqXsAFt04nFolhc6TrIbWHcM1eEvHs/WuNHlMTExMTgyyeZTowTEyOKrGlrXyg9WpB3ygHRmG3fNw+Z1rUQCKWoKbMz/tPzeHTl79gwKi+nHvTVPoN731Yx9Pr+O4s/2Q1NeWBxjQMp8eOoirUVtXxs9P/SOdeneg/sg+7Nuxh54Y92J02xkwdwQW3TsftczXa+uyVBXz68nwq91bjr6glFg4RCoSIR/WJv82hUNi9gF7Hp0df6GPpxfI5u/XJPxpWmz5RllJgtSmNzotYJI6/3M8b988CoYcc53bKwdpMUyJcF6G2spZ/3fo0J00q45yrqsgtyql3DLjQ35Rl/T99UrN/p5V3ni3k60W5RCMKNeWQWwg2p71RsyIWiQECm93K/l1ewsMEn77lZcNyD8mkQrd+kulXSEZMyAclj+4DulK5R08NaggNp76Cg6VZdE0kFCVQHuCpXz/P8y47IyYO4fxbphl+gW9g6/K5zHz0RTavqEZRYdj4rlxw+40U9Wp/zn5LxCIx3n1iNgtnLaHOH6JybzVOtx13bnpqQa/B3cHSWy+NKzNXDMDSL22T0+OgqEcBZbsqM3SAnsd3y7jdKFILQOgFZHQeyBjS0hfhugxha1+1iZboMbAb0VA55/2onNFn1GK1SXZutvP+84VsWp1rSAcA21gSta/wwQtuFnyQS51fJb8ozunn+Zl0SQSMaGCM7YfFaiEWiZFMaI2OOFXVV/aPP/m49o+znp1ff0L19sfJydmFpikEAsfTY/RvKOiaJYyiFSLRHkC5nnrWTBRaCEHZnlyMXPm9BndHahJ/eYA6fxAtqWG1WfEVeCjomk/n3kXtttm1fxcsVpWaMj+ByjoS8QSKquLNdZPTyUvPwd3bbdPucNOlt5P92zOX9uo12GC6i6U/+0q2MfNpwZqvBEgYOEpy7o8lvYam/1bbwsAx/Vg9dy2JRFIXRkU/R6pFweF20NvA9wfdifrJ8/OY+9qX1JT5ye3k4/RLTmHKDycYiurJ6TKEnAKJvzKzE6Pn4N6GxvlNo/dv2l4ie/s9Zx/GkZiYmPz4nu8f7SGYmBzb2M6H4F9a+EDrqdXfqEQxq83KWVekCqklE0lKd5QTjyYalec3LS/h4Zv+w/a1uw7reIZPPIGynRVEghEaJvSBqjpqygJNQpVrdvLy3W+z9suNgL4SvmDGYv5545ON4nOznviY1++f2Sjk6fFF8FfUNTovAGIRjX0lFfQZmllE7+QLL8WbEwP0PqoKHl8SkPgKdHHTeCxB2Y5ytKTE5rRhc9jQNEnpjvJGTYlwXYTyXZVY7VYUi8qqhZ3Yv1M/xpqmgWJFV4dtWvKrKlN58BfdWbPQjZQWbPWLwKW7IR7Xc98bUmg8OS5Uq8rapUX8995OrJjvJpHQnSt7tgqe+rPCsgVjEULhgpunkYgnG6NuGlYCpQRfvm43GopSvrMCxaJisVlIJpIsm72KB657groa46t7O9Z8yT9v/BebV+jnREvCyrl7eeDqu/CXH9rr6omf/49PXpxHMBBGCIHdZaNyX3XaKnJe5xzGTh+JUPIR9XoZaVhPQFjTJ69CCKb8aGKGDpBT6OXkczM7xowgZQwZ+AMy8gk06HEkSpCBu5HRrw7Zfm6477vcdu8uTp4SwGrTr5Gex0W5/k97OOdqYxUTFOcUnvlbPh+9UkCdX78uq8qszHiqkJce8qJkcA61hi/fS+8hPRrTRxpIJjWEIjj/FmNCjrvXf0684g/k5OjXo6Jo5OaupXztDdRW7TVks99J1xOqVVLS0ZB6+lYgNMmQzcGnDCRcFyZQVds42Y7H4lTuq6ZLnyJDFUN8+V7yi/Oo2l/TeB/Vkkn8lQES8ST9R/QxNNapP8r8u/LmWTjloisN2az2n86Dd6is/lIg9SIsbFgm+OcvFPbuGW/I5kU/PwcpZePxBD1KLRFPMvDE/tgcxq7/N+6fxYxHP2iMCqopDzDzsQ959d53DNmzWGxMujyz+GdBF8mY8+4wZNfExMQkGxOvaDktzevNntJtYmJy+FHsPjKlljbi+WvrNg7dcI4MU388kXNvmIK7vuJIsCaEzWGlqGdhyrGIxxJ8+PScwzqWjYu3kN81rzHcWNM0kHpuu+7UAH9FAJAEKmpTJi+7N+1jxadfE6oNM+fFeSl2o6EaVFUvUdqAUEC1SD58KvOLpMe5ltvug35DmtIPeg6MM+H8GPmd9VCcYHUQh8dBpx5NqReduhfg8DgaU1OCNUF8+R7yOucCEItaeOWxEZSs8+gh+1JDD9xx0nDAP3s7j2DAAqg0OFAKiy24vB5qa/Tx5Bb5OP3iU+g7vBcAgaoYJesLsNg8zUKUVVAKeO+/AaSU7NtWRlHPwpSUEEVVcPmcjceyrjqIO8dNQddU0caaMj9fvLUo47FqCx8+9RKJWHq6UqAqwbyX/2fY7oFsXl7ChsWpYjXePA+5nXMJ+kON33PgmH7c/sT1TZM997UI59nQkJohVLCPR3h/nXVfp5w3lst+eX6KHstxI/tw+xPX4/IaK+WZkegCSGR28sjwK4dsN5bksww5KYSiNJ0nIcDl1fjeTV8bsvn8nX9i6WdetObBLVJ3YM2flcOSD2a322YinsDqsOLJcaWE49vsVjr37sTWFdsNjbViy5OoanqYncMRZNuS/xiyuX1diFcfH86uLU3XQ02Flbef7s+OTV0M2SxZtQO7y15fuln//oqi4CvwUr67kngs3rKBDISDEfzlAbz5HkRjZIDA5XMhVIXdm42J0p50/o/4/q8nkVPYFJzYb1gOtz3xa7z5xr7/3Ne3E6wrBtHcqWAlFu/CJy/tMWRzy7JtdOlTlCK4KYTAm+/BarPU63e0j+oyP/PfzOxgXDBjEVX7qzO2tcZZ1zzABTf2wZ3T4IiGQaNVfvrE73C4Clrp3THp/Zv32vXPxMTkyPHZi6Y4sIlJxyezjhcA4Sda7f2NSiEB/SVt+jVncdaVp1Oxu5Ln//x6WsWGBjYtKzmsY9m0rASX14nT4yARS1BbHaSu3hEQCUZJJpLEo/rLeTKZJB5NpJT/3LR0K26fk1gk9QXeX4muXaFIJAIhaHRmlKw5oPJEPTL+NV16W/nZQ1aqSuNEQpLOPWyoFkEkFKIqehP//sUrVO5N7a+oCoXd8snvnMN1f7+Ke656JCWXHKC63MUrj41kx46+XPGbQVD3ECDq01XCbFrt0SfQAEoOCB+KxUZBN/DkebjtsWvJL87D6XagaRql28t576lPWD67XhSwPu0FYQUEZTsrqCkPsGnZVrz5Hrz5HiLBKFLT6idBekrEHf+5gXuufDitEkHz4zvtamOVaDYtL8/edgivq01Lt2bc7sv34M1zc/X/XU7/kX3IK0otyyuEFdzX6KWGkmWg5DeWnm2JCZeewmkXnkTpjnKcHkejo+qQkmjBeZDYidRq2zTW1rCKVbi8Gi6v9v/Zu+84uep6/+Ov7zlnyvbdbDa9U5NAaAEEpEhRwY6KXuDaRUUverEXVK4Xr8rP3jsgqNgQUFEEBKQTWuihJSSETdnN9unn+/vjzOyUnZndmSRmIu/n48GD7Jwzn/nOmZlTPuf7/XxJjBnSaUNzq49xANLER3qJttZ20bn+4VVYC6lk/ndnLeM9Em75zcUcevJJNcXsfWYzowNjzFoyk3Q6TWIsRSjsjt8lX7PqqbqmAG5trVDIlWB/UI81q55iaGAGV1w0g2jTIK6bZmykC4uDt6H8d3UqMV3PpWdeN5l0hkzaxwu7OI7D6OAYG5/sZeGy+TXFXPfwehKxJF0zO+noaSedzIwX28y95ry9Js7QMhVHv/m9HPGGd7L56YeItLTRPXf7pmdec89TYJrAnZ+d3caOJzPW3FP/No00R1i4fD7JeJJ0MkO0JRi+uPW5/rpqYDx1/9qK+1Lftzx531oOO7m+2X1e+p5vcexbB9nyzO00t89k2tydO8W5iEglF77ru3z0J+/f1c0QecHyx66tvkKm8vltzm6XwIBg2Mjqmx5hXba6fyaVwS0zTWG0pXrX5IEtg9x9zf0M9Q2zcPl8DnzJcrxQ8SZ5/ulN3PP31STjSZa+aG/2PWzP8buo0ZYIQ33DDG8bYXTbKKlkMB7ZcR0cx2QTAbl6EeCUJAaizZHx2T0y6TTbereQjKdw3fzwjPEQkL2wsvz+639i3t6zOfD4ZTx22zU8ec8DLD9kDUuWxvHCUabNLC5iGm0JMXf+XNq6WiYkMCAYNvLQE7185PjPk4glmbFgOoNbhhkbHsMYh2mzOvDCHrdc+QRbN2zgLecO0tLZwf03e2x4sp3+zYZMGoYHLEP9w1g7Qtu0FjpmdBMbiXHH1fcwa/EMDjpxPy773z9w9zX3MTo4FozXbo0yOjiGn/GJNEVo6Qyz7KCtNHuXE408B2RIxX0SowksFsd1CEVCxEZi3H7VKpLxFI7rkIgliY/GMcbQ3N5E1wyf/Q97DDv6c/D2zs7yUfzZ2vSzkLwlSKCED8SEVhRsM5fYSIb4GMSzRfSb28BxYdN6n99//U8sWDaPg16yFNfeA+nHwbRD5FiMO73q965QU2vlqRmNMSxcOndC8qJ4nSbwFo7//fTqday++RGMMRz4kuVlLwxdz2XOHvkLe5t5HhI3B1OFhpZD6BCM2Y7OWaa5yjKn5E50/Xw/2HYDW12GtjlgDUNhn5kL0hgDoVCVdlTgeh4Q9BqyNujyH/z+gt9wpLn2xEs0+xn7mQQzZqxh7uIxhgc8nn50EU64YIafGqXTYTwvTSqZws9k62p4Ll7Ixdr6innm2jLUN8yzm4NeYy0d0DWrk5aO2rdnEDNoi2uG2GO/Z2jrSLJpQxvPb9gDTKiu9z/+u7FJWpvXYVqTWOsSi83H0jLpvr8a30/jJC5nVs/tYJrxk6fjhOu/4M71mhrYMsTQliEswdS502Z3Ealj+AzkP6eRbUNs2dCP9S2RZo9ZS+bgeV5dw3Ly+yIf/BGC+kZhcFoBZ7u2qbUZwuYR5i54HswINjMP49Y3s4kUq7WHh2psyAvd0sO2LyktItspNFmdrsknY9jtEhgDWwb51tk/pndtcId8dGiM/o3b6J7dRXPJCXa1afRWXfsAl3z+ctKpfBfsmQt7OOd77x6/YPzTD//OX35y3fjy6y69mX0P3ZP3fv1thCMhDj5xf3700V+M96DIFZl0HEP37C4cx6GpLUpsOEa4KTyhsv2hJx/E/H3mEI74PHr/s+PThM5elCY2Gs7OahKs62cgnQZMmOt/+U+8UII/fHUtQ/0uxhgev2uM0z/4HF0z2mjrLimKFz4SYzwOfflBE+qC9D6zKV8JPlvYft3DxT1ann96MwChaIjnnjAs2SvBzVclGBttwhjDyABs2mDxM4bcEJK+jUP0bRxmxoLpXP/Lf5JMpvjCac8FF1omv62MY8Z7pbjTt3HGB55lyfIQIX+IlcfAbVe7DA945EY7DWwZxGYs02Z1ccOvbmGwb5jBzYM4rjPec2Sv/Z7ljA9tYvYePdhYNovnzYf2z2Oc4A6iHb0EG/tj/k3G/ogNH4Rp+zjGhDnkxH341VceJFZQRmMgqG1IIu5y/S//SXNrkkjqUfZdGcHLDXOJ/Qpa3oeJVqhRUeLgk1Zwxbf+UvbO58Jl86Y8faHv+1z82cu5u2AK1r9d9A+OfPWhnPGZ11ecScDGrsKOXpR/IHYVhPaGts9inPouWAkfA7EKJ9Xhw3fYjCROyxtY/+TXs9+7QDLhsP6JMBl/GssXl68XU80r3v9Rnjn7iySTBQkcCwZDa0eat/zf/9Ucc/qcabRPS3DKm25lxtz8TB7p1FYu+8Y8DnnpJ2qOCTA0fBD4NxQ9lk6lyaQzNE+vr67GYScfxJXfvYbEWL6dydggg1uGOeMzr68r5sEnreDGy77Bae97Ci+U6zLYR1/vc1x/9QnMXFj7heyCpfPomh5joPcp+jc7BDtKH89bS3t3Bwe+ZL+62ur7/dD3Zsj05h+MX4vf9CqcjsnHZJZz6MkHce0lN5JK5If39cWSDGwe4mXvKF+XZtKYLz+QH3/8IuKj+d57Y0NJnr5/HUe+5kBayxQAnsw+h+5BW5fD8JZnyO3HAfD7aJ22pO5is9YfxA6dD+m1+QfH95P11VUREamkqS1CbLjCdOjAK8962b+wNSJSygktwy+4wT9B9NWTx9ixTdr5fv1/V4wnL4BgCEdbE33PbyOTzicjFiydy8vfWX74wFDf8ITkBcCmdVv41Rf/AMDjq54qSl7kPHb3k/z1p8FFw9MPrCs6KTXGgAm62+aSGl0zO4g0R+ieVdz19uR3nsCCfedijGHjU88W1cfYtD5Ea0e+bdYGyYtQ2GH63ODOeXvHs/Q9748XsNvwVDOr/tHJts3DJOMFV93ubEzLWwA4+vWHs+xF+YJqiXiypmnsbPYi++KvzKJ/szf+3h0vVXQRWfAMRrYFxeCef7K3aAaG8TV8O/45nHb2RqbPToyPlwlHIZOy2SEmQdIjk8wW9cy+nMlu71yMzu4Ub/rARozjFw+FSa/HjgZ1AWzy3uLkRU7yPsg+Hm5bSSaTzwBaC+kUWOvguMGF/Qmve4L2jn76ny8YG2597Oj3sFOcbrOzp4M3fuTVExIMrZ0tnPHpqV8w3nrFXUXJi5zbrrqbu665r+xzbPrp4uRFTmoNjF065dcuZUJ7YZpeN3GBOwPT/Pa645b6xZdHuO43E7vJb9kY4vy319fVfZ/DD6OpzRTV1QDwQhYv3Epre31Dbg4/9s6i5EUu5hkf2sBDt95VV8wfnjfK+icnJoPuur6V33678hCoau77x4Nlp9v1Mz63XXl3XTGHtvbyxvc8XZC8CHTPSnHoi+sbq2yMYWzgaWJjxYewdNowMjBIfKTOqWkHP1GcvADAQuwq/PgNZZ8ymduuurvoOJGTSWfqrtPz6J33FCUv8iz3XPtAXTFdz+HMD2/GCxUnU72Qz5kf2YxXppfjVNjRnxUnLyCY5Wb0+9gJ21pEZPtUS16ISKOoUgPDH5702btVD4yh/mEeuvWxoseMMXTP6SI+2szsxTOZv+9c9jtqHw4+aQWhcKhsnLuuuW9C8iLnkdsfZ6hvmNuvqnyyfsefVvHqs1/GXdfcRyji4Wf88Z4FjuOMJzEOPmF/5u87l4NPXMHDtz7GU/evpbm9mcNfcTBLVgTd/u+4+s+MDWUIhYNeFtaCMQ6jQ4aZ8xJ4kSYymSbGRlw6Z/ZgjMGYDFufy17UZ6cXNcZwwxU9rFndwivf0cR+Rx+BCe0fDGkwQddgL+Rx9jffzgM3PswDNz7MdZfeXPE9lpNJZ3Bch5FBl7ERh9b2DD3zWuh7fjioFQATvo+x7El2MlZwsl2yjvUtsxZ6LDs0jut5xEcT+L7PXde6dM+GllHL2KhLKuli/WAYydhwjI6edsaGYnhhFz9jiTSFOeqUISJNLsZxGB0Ypamwe3ryTqw/gk1ULu5qEzdgmk9j1bWPMHPxAsYGtxEbGSMZ97EWHNdldHCMjp4Qe68ILhLjo3Ey6Uy+2Ki1kLgRmt80pe16zBuOYMkBi7j9qlUMbhlk/r5zOfI1h9LW1Tql50NwkVTJ7Vet4vBTDp64IFH5gswmboSWd2JMfRctpuU/IbwSEjdi7SjGWwbRlwRDXnaQf/z6NtLJHh68s4VDXzJMtMXnidVN3H1DO4mYQ39vP9Nm1VYH4MGbHyWRWo4XXk9be//4V3VocCaxxCzWPbqBhUtrm6LyTz/6NfsePFZ2mRey3H35tznm9bXfEVr/2ABfO3c+Bx8zwvJDR0kmDffd3MYjq5qJtt5bczyAa39+Y362H/LTFmNMxVpDk7nl8q9x8BHlayssXjrMfdffwEEn1Far5obLLmewv3yvomTC8Pv/9w3e/dUv1RTT99OQvKfyCmOXQbT2mjp3XL0qu03zU9OabPe6R29/osozK/v5py+vuCwR81lzz0PsfUiNvVDSD7H8kG2c9zO47S+Gzc9Bzxw46hWW7lkDkFoN4QNrCmltDJK3V1qY3U++ubZ2iohsh2984Id86Dvv2dXNEHnB8uOrqq+QvAb4etVVdqsExujg2PiUmhDcFTSOwRhDU2uUFccu49QPVh/fGR9LjE8RB2Ctj/V9HDc3k4ilb+M2hvry2R/r26D+Qrba/bbNAzy9eu34TCPB8IWgbkXuRroXNZz8ziOZuXgBnudx3JuO4oCXLKepNUq0Obiofn7tOtbcHdwtMwZcL5fAADBs3hjl5LcvZO7SF3PtRbePt8W4CdIF+YBEzBIKG1wv6Ilx+9/nMn/lu+hqC+4Wp1IpNj29hq55C2lpaeWg4/dnr4MXV6w4X0lQzNBmt71hsM8l0tZMJh0ULi3XGcj6jG+n8ceyF0Y5vu8zc34Yzwu2byJuuf1ay9beYFs0tUI46jEyEiEZD+4Op5NpkvFk8B0wBtcztHS10DN3oOBz8tn8fIZlK/OJheeeeJgZ3VvHv/ib1sPwAOy5f/B3fKyPDc88zeDWIYxxaOnsJhRtZbh/lHQquAhNp9KEvLFsrZLgTT/5oM/cxS6t2Rv0Y0P9pMaG6Gw2zjcAACAASURBVJweDGNIJpNsXd/H9PndhMNBDYjNGzYSCofomtHDvL1m88YPv2rCNu/v7aWprYWmlqD2wthIjGQ8NR7XWkt8ND5hytVCQ/3DxEZiRFuiRT09rF/lLrWNE4yBrz/hYEJLIbS02kRJ2yXX4+rJB5t55tEo4ahPbCS/S9uw5vmaExi57eiE5zMwPIvkWIxwc9P4tMgj22qfmvf5NWvYM1umxFpIxg2hsMXJfi2jTfXdLQqmzXS46/p27rq+eLhMblrkWsVHs23JjwYb7xFV2MOtJv5AQbss8TGH5nZ/vCZQ/3Prag65dUM+mZJMQCoBoQjjUziPbKtnxox4tshmNm4yg+OY8f0Str5eHclYivHxgMUZjPEedLWKDVefueXZR5+pPYHhB2MIu2fBcaf5PHJ7hmVHuLS15oowDVR5cqWYo+M96AC29vq0tBmaWnJjI+uIKSKyHW75/Z1KYIjsStVuFgFFw1gr2K0SGNPnTqO1s4XetZsZ6hsmnUxjHIeW9iY6Z3SwcFnlO6NP3vcMV373rzz1wFpiIzFGto3gOHHGhlNY3+KFHSJNTfgZj6+8/TuMDowGM19YS2wkAVickENyLEUmleE9B3607Ou4IZ/mFp+t6xK8fd+P4YUMs/foJDbiMtQ3jOM4LFoW5on7BkmnoKUtjbUu6ZTBL7g+2OegUYa3eVzz82fw/WcAg+uFScbSWGCfA9M8uybK2EhxF+pwBNY+1svVP34/XTM66Orp54n702TSwcwm3XMcIq2z2byub3yGlLxc8dDCNETx5WcqmS5avHntVlo6fFLJynfq1z+2ceLLlPx93039DJyV4nNvX8TYcHEsxwHfHwaKuxTlanWEoz6uB/3P9bL6tjFa23y+/YkFJBP5bTNncYKtG8OkkhfymnduxTiWq37WQ6bg+iHSbEnEABtMReqFsnVHCoa9hMKWkYEED94yzOYNGX7x1Zk8fn9zUOwRiLZYZs4zbNpwM9beQufMDkIhj95nNpPJBBdtC5dG2LQuwdhwBgy0dXmc/fU3c8KZrx1/nd986Vv88bu3MdSfxnEM8/ZuwdLNhjWbsdanc2YHBx63H/29AwxsHmRwSzD1bGdP+/jFke9bBrcMMtQ3zIdf8nm653Rx4hnHcOxpRwJgvL2wiQpd+N15O7S3xM7Q0tlM2Otnwd5JBrZ6ZDKG9s44mQw8dm8rK45ZXnPMRfvNJ5WMs+6hjUXDnYxr2OOARczbu/aZLU54y5sZfPIKnljdxJ1/72DbVo9wxGe/w0Y55lUDjI3NqTkmgBf2KiYqOqbXN8vLzMU9DN+bS4YF7z+ozWPqnmq3uesARocf5/rfTeOZR5tIJgxtnRmWHTrC4SeOsN9xtddBOORlL+XvF9/I2sfCFO+jLDMWJNnz4NqLbjpOK77TSf/zWxns90inXQwQjlqmz87Q3LNvzTEBps3uYuNTvRTu+HLbtLW79loVAAuX97D65gpTsBrD4a84uvag3h4Mj8AZyyERc8kV0Yo0+Vz2sKW9q44aGM40cKZxyf9t4i+/8BgddHFcWLI8zce/D7OX1VdXQ0SkXhfedt6uboLIC1v4TTD21SorTF7cfZcmMIwxXwdWAvdaaz842fqhcIj5+87hkdsfH3/M+j4jA6N4IY8Vxy4r+7y1D6/n2x/4SXDxDUSaw4z0DTAyaMbrJCRiPmNDo7R3B4Upw00RNq3bGrxuJNhMscF42fiFPM8yMpi/AE8lLc8+ui1brDJMJBrn0bti48tHhz1a2jMk4/nnLF46yrNPNOGnyQ5LgUQMUolkts6GYfMGb0LyAoK7keFoEMuwgcdW5Wd8yGRg83ofeI5Ic4RQxCMxVnz31wtlSKcK4+ZOus14V/LSOhbGZChfMXbq995TCYePv2lJ2Voavl85Ts+cJIN9HplUMMHFPTe1cPvf2oqSJNNmpHh+bb5WwN8v72B0eOLwosRY/nWaW1OMjRT+PCyhMKRT2eSANXz5nHkM9RXHiY8a1j1uiDQFQ4k2rd0ynhcyxtDWGeeZhzP59lkY7k9z4dsvo2tmNwefdDS//MLX+eWX8omFdNryxH0jYEaJRMNgoG9DP9defCM987vpmN5OtDXK5nVbSCfTTJ/XDcDWDX0kxhLMWhwUde3buI3LL7yS2Eg8mF42cjzErgR/4t1q03xqxW3eKF579v7cc8319G3KfwZDAx4GOPJl9dyBh4XL5rP2oecmDnPKWNY+uJaO6bUXBl2yYm8+8v6Z9G/OtzOZcLj3n22sfTzKR39b21CHnGoTxSxcUdu0pDkrjl7Ck/eWmybY4oZKM49Tc8rZ5/K5U25i83P59z884HLn3zvY9GyY866sva17HLh/meQFBNMwR3jVB95bV1t//32fFUfmf/cWSMQNT6yOEHP24sjXVn5uJU44SfmxnpZ0ovzQosm860vv4pwjzy+/0EJbV+21Wow7m9P2NBP2t4mYw2l7Wv6Wnlt7TOPwrY93ce2l+d+jn4EnV3t86BTLxU+spKm+SXikAWlGFNkdLF68eFc3QeQFzYl2TtLHYvIh+LusiKcx5mCgxVp7NBA2xhw62XMymQzPP72ZzhkdOG7ugtnQ1NpES2czj9/1ZNnn/fWn148nLwC6e7bS2pkh2uyDDW6w+5lgCEcqEQdriY3Ex4uW+RlLOj15V9+uGQmS8fKb1Pq5aVCL40SbM6QSJjsEJfeWnCB5kXvfBf/O5Q62ba08FWUqmaKpNcmmZyuvk0mlaW2fmJDJZCp9JYLx29afeCI+MhDB9SzFdShrHzhQvhBo9TizFiTp6E7jekG7vJAtuVawDJQkGcolL4LXyb9WIl6ckJm1IBi6krtotJYJyYvC9qYSKTKZ4kTFeNwy1zK+b/n+f/8MgKt+UDy0Z/zzt8H30Fo7PmvJtt6gW3ukKcz0ed2kkmlSiRSJWBI/k2HGgunjwx9y/v6Lm0jEEhinFdPxPxDaP7/Q7ca0vg8TOa7Me2ssm9b8mdFhp+gbkuupX5x8mrpvn/2FinWF0inLbVdNLOw7mUfuWsMDt7Yy2Ofle1lZiI04PHhHK+9Z/sm62pqKV94n3fvX1XXFvPJ7levDDPdPnsAt56PHnM6mDWWSDdbw3DNR7rm+tqFsAJf9z/cmxitw5uL/qDkmwBU/6ebyb89gYGv++/Pkg018++NzufKbv6wr5oZHt1ZcNjpUfShIJR876YIqSy1bt2yuOeaPP3Upvl9+/+/7Dt8952c1x8xkMvzt0ufIZJyin5VvDYNbPb7zwfqLBYuIiMi/o4snXWNX9sA4AshdDVwHvAioWuZ+y/o+BjYP0t7dRtu0VjKpDI7j4GTHKK+552n2e/HSCc974r5niv52nRFc19I1I4O1Ft/32JLtjZtJW1KJGImxBMYxhCIeLR0tjA3HyCSrn8D7JRf/tuRC2k9niJUMteicnqZvUxgvFIxpNxj6NxVfcPp+cZymVhgbqtwO60P3jBiDWyrf2spkfKz1x4s0WgsYH1vhBDYn0hIhMTpxzH4mHTzP9cALh0iMVT4xN44pmwjJX5BYppIAMcYwPOASbfGJNvtYXHqfLd6+7V0phgcqJ3IqvU4mXfy4yeY3XC9IkGR8U5RkKo3j+xbSE99jIlZ5+/auG2XDE2sYGSiuNWALZzTM+OM1PiAYQ58r4trUGqWpNcrJ7zyBZCLF9RWKtMZG4qx/fCN7HrgY487FdJyP9fvBxsCZjal2a7+BjA81MnbCp9i/xePPP/4pr3j3O2uKecsfH666/Fdf/DVHvrq2IQ8Xvu27gGGo32Nom4vnBVMO5+50j9ZRV2NnyVS9nq6vB4bjjgGh7P6w+JNKJhwu+/w3OOSEX9cU89cX/qPq8k3rJs6kMpmHbr+LoT6Xu65r5+7r2+ienSIZdxjqDw6Tbt1Hyx1fBSY+Wr0eyffe/yM++5vP1BTzmh/lDsfl98PX/uJG3v+td9QU8+FbH8/W+TD4vovBUvhrfeiWxyo/+QWq1l4MIlKbk5w38nf/t7u6GSIvWH7vadsdY1cmMDqBp7L/HgQmDFg3xpwFnAWwYMECoi2RwmV44eLmN7WWv2Bvao0SGylIPhRcoLkueCXXt47r4rj5dTzPJRzxiFeuk5h9XvUTfOMYjC0+KSzsXWGyw0VKpxssPP81JhgKMhk/U332CGOKu6AbA9ZWP9F23OBOf7kERsFahKORogRGacLCcR18mylJ8BS1rmo7ClfzQpZMJpi+NhQC17WkC55fPBymBiWlQIqu6Q0lvU3KP7/cOsax2ApDYjzP0NrZkf0sKoQt7igS9BspeaEZC6aTLDMVZqHS34pxait22QhyvW7KL4MFy8sPKasm2uwClXs2dPZMfWaYnDl7zGRDrg6MNePDkF4oKn3fc1o6ax/u0NzmEh+t/DkZp/Zky6wl83Dc3GxQhq0biw8MXri+BM5OUWX6dIAl+y+sOWRTa1PJtNrFn1uu+HQtumYVf7alqcZoi8aPNDIlU0REZIcLHQCp+7crxK681ToA5AaUt2f/LmKt/ZG1dqW1dmVPTw+dPR3ss3KPssEcx3Doy8tP8XbYycVTSI6OzBi/uHRcB9eFpmwttUizhxuK0NKeL1jX0tFM97zqF3jGmGzhs8LHiv7CcR26ZhSfdG/ZGCYSLR4JNGdRcYLALQjruAZsuOrJtOs5bFzXSihceYSRF/JIxL2S5IghFKn8nLauZmYu7Km4HGD6vG6mzS04aS1z7eKF3PyUo9l1crVIKqqwuKkl314v5DJ9dvH2HRvxCEcnr2ZbKtpU/JyRgfxQBQN4RbmziY3zPBfjTny8taNy9umg4xfR2TOThfsWXyQ7Ba/lem52Kt1sO0suAKLNEQ44bjkHnbA/oXD5/OS8vWczd8/ai1E2mmhL+WmSIfgN7X/kETXH/NglH6q6/DN11Ku44OpPVV1+3OlH1RxzMqXDhqaqZ17lopKT/kYr2PPQo3EqJBTaOtN84eof1Bzz8o3Ve2y89bO1FwadPnMOc5ZUTs7OWljfNq2e7axvmx5wTPnjYC7mmef9Z80xv3rj56ou//L1tRe+m7/3HNq7KxeUfeOHX1lzTBGR7aHeFyK7ltP96UnWqNZzPhtjxzSlLrcDJ2T/fSIwpYHQ//HJ1wUzLRQwxnDqh17J9LndZZ/z0rcdV3RHKjbWwuzFzbiuM97TomsGRJsN3bODgodNbU20drbQOaMDL+LheR7t06vcfTUQG/XomlGuD7bBCwcX7MMDrSVJAkNzaxrXtTjZE921j0fpnpUaP/F1HHCyNSa8UPChRlud8ufFBT1T5u2ZKnsn0nENxnFIJcJ0zyq+4Pc8v2xc13OYsXAWAK1d5S9yvJBLe3cb4XCY9uwsCKW9A3J1RdyQO/46peuUY4wpuoDKPeeJB5tp68xk4xqa28JBbZMC4Wi6QsHDyq+bShkKRmowtC1Ez5xk0OMh2w43VD6GMQY35OK6Lm52eFOuvSODHqHIxM+ka0aYj/8yKMp37k//m+a2wh5AQQ8Qx3VwXHc8vhdymbkon1ByPZczz3sD0eYIrZ0tnP6pU8enqsxpaW/izPPeWPF9704++str2GN5bMLj7V0ZFqxYWVfM5UceyqJl5b/fL3rFPCLR+u4Y73Vw+aJhxjF8+tLqSZNKzvnBuyou++3WH9cV85fPXlTx9/ixS86qK+YHvv0pWtom7lciUZ9Ia/WEaDUdM8ofvlzP54zPnl1XzGNPO5aW9olJxsXLY3zuqj/WFfPidZUrbZ/32/fVFfP//eNLFT+no167f9nHJzNr0Sxm7zGz7LIZC6azaN/6CsN+8tJzihPWWcuO2JsTzzy2rphSv0Wf+POU/xPZHSlBIbI7WFJxiTProUmfbUpnlPhXMsZ8EzgYeMBa+4Fq665cudKuWrUKgLHhGHdcvYp1j2ygbVorL3rlIczbu/pUhJl0hnuvf5CHb30ML+Rx8EkraGsf4I6rrma4f4SFyxZxwEmv5+Fbn+Xp1eto6WjmRa9aSWwkzqq/3U8qkWLp4XuRTqX5wYcvYXDLEK1dLZx5/ht57NY13PP31RjH8OJTD2ePFe385itXMTKQYO6eXbzlf97ObVc9zGN3PEFzRzOveu9J/P2iX3HnNevxMzB7UZT/+sE7ue7nf2bDE1vpnt3G6859A4/efC13/OVxHMfhyFcvZcnKU7ny29fSv2kbC5fO5yVnHMZPPvpj1q/pJxR2OfHMQ1l2xOH89ec3EBuOs9/RS1l50kwuPu9ietfGaZvmcsanXkXf5k5u++PdWGs58rWH0jPHcNkFf2CoL8bMBe28+ZNv4JdfvIK1j2zF8xxOOP0QTnnPadz553uJDcfY86DFJOJJvvOBnzI2HMMLeZzy7uM57rSjuO+Gh/AzPvsfs4xELMEln/8NA5sG6Zk/nbf972k8eNNj3H/DQ4Sbwpz0lmMYHRzjyu/+ldhInCX7L+TQVxzETz52GYmxBI7rcNSphzFr4Qz++fs7yKQzHHTC/hz12sO49Au/Y1vvANPnTuMdF7yR5MhqHrl9NeFIiINfeizX/3oNf7voDtKpDF0zWjjrwjfylx//lfWP99PSFub1//1ybrnyae6/4UF837Jg6Vxe/YGX8qv/vYKhvhHaprXy1s+fynWXXsPjdz+H4zgcdso+HH/64fzp+39iuD/GkhWzOeoNr+bHH/sdG5/qJRwN8+qzX86sxTO47hc3kUqkOOj4/Vj24n245LzfsGVDH50zOzjjM6dy51V/497r1mS/Myt411fOxSvo1jGwZRO/vuAHPHbXOprbwrziPScTj0/j2otuJBlLsv+xSzn5XSfywA0PsfGpXqbN7uKo1x5Gz7ziJN6mdVu47cq72bZpgHl7z+GIV6+krav2YRA7gzHmHmttzZmGwn0BwIWnn0x8NEUmbWhuy3DEaR/k6FNfs11tu+gz3+D337yVZNzS1Gp495dO5RXvqa8wZM5lX/w9F30m33NgyQEL+eF9/2+7Yj5y1xrOPfqzZFLBRXdzZzNX9k9e/Ggy79j3LDY8sQ1robUrxIXXf449D9xnu2J+8qXvIDGyEWMMmYzloJe9iree//7tivmlt3yG6y99lNyYij0PbOf79/58u2Lef8PN/OaLF9DX6+GFLDMXhfnsFfUlL3KGBgd508x3kU76BL3xDD9c/RUWLd2+avjvXnE2ax/aAgRD/M79ybt42Vtftl0xf/TxX/D7r16N71scx/C6D57Ce7/6tu2KuenZLXzrfT/m6QfX0dQa5TXvfzmvef/JRevsqP1Bji7A/71oxpIXjh21LzjJmXizRskNkcbh910AqcJzVg9n1iNF61TaH+zSBEYtKp2kiMjuaUdfsIjI7ksJDNmVak2Q1PL9aqTky86e6nZHbBedG4hITqX9we4x3YCIiIiIiIiIvKDtNj0wjDFbgHW7uh0issMstNbWXARB+wKRf0vaH4gIaF8gInll9we7TQJDRERERERERF64NIRERERERERERBqeEhgiIiIiIiIi0vCUwBARERERERGRhqcEhoiIiIiIiIg0PCUwRERERERERKThKYEhIiIiIiIiIg1PCQwRERERERERaXhKYIiIiIiIiIhIw1MCQ0REREREREQanhIYIiIiIiIiItLwlMAQERERERERkYanBIaIiIiIiIiINDxvVzdgqqZPn24XLVq0q5shIjvIPffcs9Va21Pr87QvEPn3o/2BiID2BSKSV2l/sNskMBYtWsSqVat2eFzfHwW/H5y5OE79HVKsPwI4GKc5+NsmwcbAtGFM+bh+phcyQzjhvbNtSYP/PDjdONk4fuJhMC5OeN/scwYgsxHcPXHccPBY8nFwp+G4wefrp9dBeitO9JDsc5KQeRLcuThuR/BYah0ATmhh9rXHwO8DZzaO42XjbASnGcfpzK4zBP4wjjc3uzwO6dXgLcbxsq+duBv8OE7T0dl1BiC9BrwVOF60IG4Ux5mWjTsC/rbxzyDf3jk4bmf57W3TYEey29fNxu0FJ5yPm+mHTC+4++C4Ln4mA5nHi+JaGwObwTitFT9bP7Ea3FYcb0m2vcXfGd/3wX8OnC6cCnGszWTb24Ixue3bCyaE43ZXfG3rD4GJYky4Qlwf7DCYZowJVYwzGWvjYJMYp73uGOXjJsAmysY1xqyrJ2a5fYE/8hdIPYDT9cn6GlqGH3scUv+A0Gk4TdN2TMwyv5kdEjf5FLhNOO6cHRczcQdk+nGaT9lxMeOPQuouCL0GJ1r+t117zAFIXQmhF+NE99ghMQH8bReAtydO25t2XMyS/eMOiZnuhfSz4B2M4+2YQ7qfiWX3wYtx3Mr7xppiptOQvhe8BTjerAnLd9T+wO/rg9Q5EDoJp/ttwWPxVZB6HKftjODvVApSt0BoH5xQ8JvxE6sg9SxO66nB32WOQ8GxtR8nelDldfz+4DP2snHHj+vTcJyWbJynIT04Hie/H28a39f7mY2QieGE9yhZJ7+v99NPQ2YEJ7Ki8jp+P/jJsts8x/pjgF/9OJjpA5vG8WZmX2viMc36o4AdjxOsMwymNX/cS60D6+OEF5eskz+W746sPwLGxZimXd2U3ULuHNk4HROW7chzAxHZvVXaH+w2CYwdzU9vgqGPQWo12Aw4rfjR1+C0f7ymODb1CHbsF5B6PPg7tAxMSzZuAtzpEH0tpil/EeDH/wlDHw8SBoBPGNz54G8FGwcTwjdd4G8E0tl1HDDtYIcAC7j4Zlr27wRg8GnN/juRfQ5AJzACZLLrdIFJgB3NrtMM7hzwe8GmwDThuwvA3xwkFYzBd+aDMZBZD9bHdzqCbWb7sm0BH2+8rQD+IEAISGUfMfh0gNMcXPwbg2/mggmB/2z2M2jHN82QeS4by+C786Dzp+OJFmvTMPZrbOJa8EfAacea6ZC6N9h+xuAzC8xIdvtaIIRvOsEOZNtj8J0eCL0I/KfBWqy3B6b5TEz4gPx7GPgMxH+f3XbZ9+jOzX5O2e+MMy9IXvjDYFx8byl0XDie5LHWh9jvsPFrgo3itGKdWUF7M5uDtnjzoO1TOJGj8t+rxM3Ysd8G28J42PARmJa3Y5xc4sVC7Aps/E/gD4BphugJ0HxGxWRH2e9vpg879jNI3gnWx3rzMU1vxkSOmHKMsnH9fuzoRZC8A2wa687DNJ+Gibx4u+KW8vv+C1J/y//d+3PAw5n1SP0xY6th8A0Fj3wNf9BAx+3blcjwt74O0o8w/psxHdD1i/HkZF0xh78OoxcDsWzMTmg/H6fp5fXH3PY5SPwq//fQh8BZgjPjr/XHjN8EA2eRe+9wAT4RnFkPbkfMOAwcQn4fAz4GOn+LE11Rf9zepeR+8yTAHz0P3Bfh9FxSf8xtH4fEFfm/BwkSLt0/qz9m/B4YeDfB/h2CfcnBONN/Ve1p1WNmYrDtzOz31AccfG9f6Poljlv/hZnfdyakVpE7Kvm0QucPcKKH1R2z7Ov07p3/I3U3fu8Xi5ePng8Y8t9D8HEZ/7wBf+QTQCS7TvZ4YTqzx8xE/jmmC+wg48cqZ0ZwjPbXg7X4zjRw5wUJDn8MjIfvzIbMWnLfWR8XvBXgtGWPi1GsMxeSN2ePV+DTBJHDg2NObh3TAcl/AsmCOAeB05Q9bjdh3T0hdU9w3MYG7Wt7P07T68bfq02vw45dBMkHgr9D+wTHwdDy/PZI/BOGvgSZDUEcdxaEDgd/Q/6YFjoIMv2QfjiI4+0L7gxI3RccG51WrOmCxD+B3LlHK4SPBAazx/JWiL4cmt60WyUybPIB7NilkH4KjMGGDsI0vw3jzdvVTWtI1h/Inhvclj03mINpfiMmcuyubpqI7EaMtXbyteoNbszhwNcJzg5WWWv/u2DZHOBSIAp81lp7XbVYK1eutDsqs+r7adj68uAivVTTG3E6zptSHJt+Cjv4qeDCPyfzHJAMTlzI3xE3Lf+JaXodfvIZ6H8lhRf7+ZMpL/tfisITqmKm5Dm5xyb7HEvXKY0TAtxsu9IE5VHCBCecyew60ez/45O81mSiJXEj2fYkCtpj8qubZpi+KuhFMfxNSNyUX+YPBAkEXIJtl6EwaVL8HnOP5f52wNuX8VIwxsW0n48JLcMf/DLEflqh/eHsc3Kfk0vhZ40zDab/FceJYkd/go39paC9w8EJLg5F+UMThq6LccLLsYmbsMPfnPiy7lxM51cxJowdvQQb+2OZph2G0/6JCu0uZm0MO3AuZDZNWGbaPo6JHD6lOBPjJrED/w2Z58vEPXc8iWGMucdau7LW+Ll9gT/yaxj5bIW16k9iFF0ElXBmrakv5pZTgguZCTyYvrquO+f+yA9h5Gtllrgw7TKc8EF1xPwNjHym/EJ3GU5Pme/cVOJW3KYuzqxH64y5L7kL4lJ1f069yyjeNxdo/hhO+7tqjzn0PRj7RvmF4VfiTCv3GU4SMz0AW4+g7HHCOxBn+m9qjgngbzkxe8Fbwp2D0/OP+mJuPR3S5Y7dDky/FccLeqBt9/6gyu+2fuWOIZOtkzue5Y4PuWNr4TGvVCd488COQebp8nGdGcF//jD4FW5Qm2nBDQkbz8YxFB2bjAPtX8Fpeik2swU7+OEgeVAUI4TpuADj7YmffBi2vaX4HId08F7c+WDagveZWQ+0gBv0zsDfEvSqcOcG28MOFnyvSt/XXHC68i8fPR7T+oEK26mx2NSj2KHPBsmlQk47pvNrGGfH9Nz7d2FtEjv4UUhP3MeYtnMwkeOCf2/nvkBE/n1U2h/s7CKe64DjrbVHAzOMMfsXLPsE8Bngpdn//+vEfl0+eQEQvxrfn9oFuo39ofjAbseCYSM2k+uCULDuFUGXueHPUT55QcHjlZIXpetvr3Kvnfu/n11e2JYUFU/ua+KXxE1n/67w3uwYjH4Nm3m+OHkBwYkSFMQrjGsrx8y1I9NbsHoGG/t98O/YpVWelyx5DyWfl98PYxdh/W3Y+N9Kaf2kugAAIABJREFUlm0mv10L2maTMPptrLXYsQoXH5nnIHEr1h/Gxv9coWl3YdPPVGl7gfg/yiYvAGysvgsgABI3l01eAJXfWz1GPldlYX3fU3/Lm6svH7649pjpLRWSFwBpGJ5awmmC0UoJtgwM/W99MUf+p/KyTJ0Joc0vrbI0gx9fXXvM0auolLwA8Lf+R80xA1W+N2NfqS/k2HcqL0tW+B1PZvDDVDxOpB8IhirVyE/cXj55AZDZGPQcrDVmOg7peyothcFza45ZNlJffQmrqZnKMbf0WFp4fMh9p1JUFvS2qLTfBIKef5BN2FdqxrbsOrnjTO44nlvuw+j3gn/H/zQxeQFgU8G5DcDoN0uSFzB+7Mpkz6H8gSCuHQmOYzYN/lDwmJ97XwXH2dLtWXIuZhP/wGYqnJ81GBv73cTkBQTvP37Nv75BjS55W9nkBQTnBjvzhqqI/HvZqQkMa22vtTZ3JpW7Ss1ZAdxurR0Bho0xbTuzLUWSd1ZeZuOQun9qcVIlJ/S24KTRxoqX+SOQeRbST0wtdlWlO/l6dvrlYpReEJSc/OCXWace6TJxJ3kPydsg9ViZBYUnV1OIUyo7lCYfLveZJmqLU7pdkquCz3rCyU3hHbiStqbWZO9UVTmJTT8C6SfLnFQWxpnaHW2brrJe+pmgPkg9qsXNbAjqeuwQ1T9rf7iOLv+ZSYY0jP6w9pix31Vfnrq79piQHT5WQWZtfTEr3iHeDn6Fi+KckS9WX17O6LeqL0/XkRQZvmnylepSLZlW5wl7tqt+xZjJa2uPGbuy+vJ4Hb1vUrdQ9T1W21fU9Drv2DFxJqg1eQETj0O2wnrlFB53ysWFKX2fCs9FSo9NmQ3BKtWOE7njYKr0fKUwVnZfMX6csMHr2nhBO3LLqrW5ZJm1kC53rG9AVbZh1e37QlVtm2R68wk4EZFJ/EumUTXGrACmW2sLr/hdm0+3DgJdZZ53ljFmlTFm1ZYtW0oXb0eDJsmVOJWLKhbHKS14VbA5y43hNK3gRCc+vt3M5KvskLiGHfOVKRd3kvdg2iBbAK16rFqVfE4m9xq1xi1Z32kriFVlvaJFTWAi5b874+u0Vohb+NqTLM+FqhbHhCnqelyLqnG94D3+K5jlk68zwSTv2amj8KQ7v/py01x7TKD6b/FftI2nZJLx7M6C2kM6kxVAnXodmLy96njOLmImOY44k3znyj5nkuOemeJxsSjm7Eli7qjjYaN9dvUelwp/05ViTCV2lTi5GknVjhO5ffiEz8eU+Xfh79sJhqmMc0vWnaIJ51YNairbUPKqnhs45Icpi4hUt9MTGMaYacB3gHeWLCq8Nd3OeB/KPGvtj6y1K621K3t6dlzFflreFhSlLMedhxOa2slQbrxe/oFWxg/UpUmS0D4YdxZET60WcUqvu2MSFqUxnIL/Ch8rPDnJ1ZnYXrkxwYVxJ7nIafsghA6E0tksihIOJXUlppIYcYu/V+OfadUT79x2ysUufR0DzWeCtzQoZFb0Aq0Fzyn5+UVfFlQwD1epPRE5Fry9gnHO5ZgmCE+xMF7p97cwTOTF41Xja1YlLuEXYXZYAqP6BZXTekjtITsq1CrIaft9zSGd5ldSNTHS9rGaYwLgVRnzHzmpvpjOoioL6yys1/Kh6i85rfahGZMWquy4sPaYbZPN4FLnBYmpti+pM2bL+6osjIzPblFbzA9Q+TN2ofWcmkM6keVAleKfzW+vOWbZ15n18x0SZ6KpHGtL1wlRfHzI7eer/X6yy0y1WaBy+81q35nsPnt8Zocyx5nwkcGSqvv/7LLoy0qXkD/HyR7LnOy5jnGzydimIFFdtKxam0sSuE4nhOovwvuvNKVtKHlVzw0OG5/FT0RkMjs1gWGCK6BLgY9aa3tLFq82xhxhgtvA7dZW6w+9YzmhvaHp9IlJDNMEHRdMPVDTq4oPtMYL7gw6bcV3EJwuTMvZwT/b/gvcPQtftODfubuG1S4cy/VeKPd4tedM9tqm4G+X/AW7W/BYvQpj5GLnHstd5JW0N3wSTvgQjAljWj+Yv4MEYOZk25p7rjvx+fmVS/5sLU40hfaB5tcH/552KZU/h9zrh5hQJA2g6XXZ9jrZ9hacwJvZBCeiJbFDSyH7HTEt7wB34kWPaT4d4y3CGBPELZ3yzniY1nOmPI2bCe2Lyb3fQt58aP7PKcUoG9dbgmkuU0vCnYNpeVvdcUs5s26vvDByWn0xm44DJk7rFizcC6epzpkY2j5N2e+ldyBO9CX1xez6XvneG+48nM7z6wrpzLiWioeFzh/VF7PtXVS86AqdUFdMALxKCappOE0n1hczfHrFRc6s++qL2fVnKu6TptVXFNVpeRO45RLtBtr/r76YblM2MVLmGNPy7vqnU+24sExMwF2C07pjEhiBytOEbp9akhiFNwFyx9Lc8aFKEjOXOHRzx7PSng4OuAuz686n/DHY5OOYboLEQMlxxp0D7dnCx+GjIVJmOt/QftD0muDfLR+AUOksSSEgmk/ym7bgxoIzM7iLbkzwb6cdTHZf6s4vaEvhOYsLbkEPLBPBtH2o/uT5v1rT68tsHzDR4yH8ol3QoMZmvAWY5jMnLnBnYZpL73GKiFS2s2ch+Q/gW0BuwO4ngdOttf9ljJkHXEJwe+Zz1tqqg3Z3RnVhP7EKxi4Kpitzl0Lb+3BqrBptrR9MD5dcBTgQPgLrTMck/oH1+zHeEoi8BFPS1dAf+TnELgcSEDoEmt4F8Ush/Qy4syB8BsQvglT2Ii18LIRfDrHvBtOkuYug+RyIXwLpB4AmaD4jKKwV+w3BTCj7QuuHYPQ7kFkXzI7RdA5k7oTE37NxT4TIy4M4mU3gLYGmt0LyGkjeFVx8R08DE4LYr4JCXaGDIXQMDH8eMhuDhE3oLEhfDZlgOjbcPSHyboh/PSho6vZAy6fBfzioZ0EkOPg77TB2WTCe3zsAml4BQ1+HzBPBspazJ0wJaTN9kLgem+nFuHOx4SMhfnV2WrkINL02W6zsx8AIuPtA63tg5PtBXFqh5d2Y0B6QuDXYVqGDIXx40fRtfnokKJaXvif4bCMvDWIXfmea/wNiv4X0g8Hds+YzcSJHFrfXH4D4DdjMBow7Axt6CST/CokbABeip0D0VBwnf+FobSKYci79aJBoiRyH8RaXxB0KtkN6PcadAZHjg//XyKafDIqj2jHw9oPIUTVNxVo57tOQuDGoM+Itg8jRRXF3VKVxv/doIFeM1EDrz3Bajyr73Knyt30GEr+F7JTFNH8Qp/292xcz9QQMfCSYftA0Bd/tlsoXzFOKmYnB8BeztQZC0PRanNaztysmgL/1tGwdCQtmOnRejhPZvmkB/f6PQPJPBOPow9D2RZyWV29fzJHLs4VHUwS/0TfhdNWXvBmPOXwTjBZO+dqFM6tK3aSpxEwmYegt+W3q7gEdv8YJb19XeX/kpzD6cyAWHBM6v4bjLdzOtt4HQ18Gvze4EG3/GE64jt5MhTHT62Dgw5B5BohC81tw2t5TtM6O2B/4fRdAqrDQbnPQEy59L8Hn2QItX4DY17LFI8NBb7nUQ5C6LbtOF7R/DWI/B/9JguPF2ZB6EOJ/AFJBzOZzYez7RccqsME6Ng7hIyB8CsR+EdR0cKYHvT9jf4H4VUA6mEK17QuYzH3Y9DMYtxsiJwQzTMWuIDg2HQEt52DSd2HTa/PrjPwgWygyHSTz2s7HZO7Cptdh3B6IHI9N3gaxq4N1wsdC83/iOMX7dpt8AJK3AxkIrYTwoZiCYSC+7wfvKf5nwIfICRA5BZO8OXtMm4mNnIDJrA+mzcZC+FCsuwSTuBGbeQ7jzsKGjgu26fi5x/HQ/E5M6p/YzEaMOweix+92M3dYmwnqqqXuBcLB8TNUz/DFFw6bfiZ7bjASzAQXOaaoZ6ZmIRGRnEr7g52awNiRtGMS+feikxQRydH+QERA+wIRydtV06iKiIiIiIiIiGy33WSgoYiIiIiIyNQt+sSfa1p/7ZdesZNaIiI7inpgiIiIiIiIiEjDUwJDRERERERERBqeEhgiIiIiIiIi0vCUwBARERERERGRhqcEhoiIiIiIiIg0PCUwRERERERERKThKYEhIiIiIiIiIg1PCQwRERERERERaXhKYIiIiIiIiIhIw1MCQ0REREREREQanhIYIiIiIiIiItLwlMAQERERERERkYanBIaIiIiIiIiINDwlMERERERERESk4SmBISIiIiIiIiINTwkMEREREREREWl4SmCIiIiIiIiISMNTAkNEREREREREGp4SGCIiIiIiIiLS8JTAEBEREREREZGGpwSGiIiIiIiIiDQ8JTBEREREREREpOEpgSEiIiIiIiIiDU8JDBERERERERFpeEpgiIiIiIiIiEjDUwJDRERERERERBqeEhgiIiIiIiIi0vCUwBARERERERGRhqcEhoiIiIiIiIg0PCUwRERERERERKThKYEhIiIiIiIiIg1PCQwRERERERERaXhKYIiIiIiIiIhIw1MCQ0REREREREQanhIYIiIiIiIiItLwlMAQERERERERkYanBIaIiIiIiIiINLydmsAwxswxxtxrjIkbY7ySZccaY+40xtxhjHnvzmyHiIiIiIiIiOzednYPjH7gBOCOMss+DLwROBJ4+05uh4iIiIiIiIjsxnZqAsNaG7fWbquw+GGgA4gAo+VWMMacZYxZZYxZtWXLlp3VTBERERERERFpcLuyBsYfgT8BjwGXlVvBWvsja+1Ka+3Knp6ef2njRERERERERKRx7MoExoXAi4G9gLcYY5p3YVtEREREREREpIHtygRGBhiw1iYBHwjtwraIiIiIiIiISAPb2bOQhIwx1wEHAH8zxhxujPl2dvGXgeuMMbcD/7DWDu7MtoiIiIiIiIjI7subfJX6WWtTwIklD9+ZXfZX4K878/VFRERERERE5N/DrhxCIiIiIiIiIiIyJUpgiIiIiIiIiEjDUwJDRERERERERBqeEhgiIiIiIiIi0vCUwBARERERERGRhqcEhoiIiIiIiIg0PCUwRERERERERKThKYEhIiIiIiIiIg1PCQwRERERERERaXhKYIiIiIiIiIhIw1MCQ0REREREREQanhIYIiIiIiIiItLwlMAQERERERERkYanBIaIiIiIiIiINDwlMERERERERESk4SmBISIiIiIiIiINTwkMEREREREREWl4SmCIiIiIiIiISMNTAkNEREREREREGp4SGCIiIiIiIiLS8JTAEBEREREREZGGpwSGiIiIiIiIiDQ8JTBEREREREREpOEpgSEiIiIiIiIiDU8JDBERERERERFpeEpgiIiIiIiIiEjDUwJDRERERERERBqeEhgiIiIiIiIi0vCUwBARERERERGRhqcEhoiIiIiIiIg0vCknMIwxRxljWrL/PtMY8zVjzMKd1zQRERERERERkUAtPTC+D4wZYw4APgasAy7ZKa0SERERERERESlQSwIjba21wGuAb1prvwm07ZxmiYiIiIiIiIjkeTWsO2yM+SRwJnCMMcYFQjunWSIiIiIiIiIiebX0wHgTkADeaa3tBeYCF+6UVomIiIiIiIiIFJhyD4xs0uJrBX8/i2pgiIiIiIiIiMi/wJQTGMaYYcBm/wwTDB8ZsdZ27IyGiYiIiIiIiIjk1NIDo6hgpzHmtcBhO7xFIiIiIiIiIiIlaqmBUcRa+0fg+GrrGGPmGGPuNcbEjTFeybKoMeanxpgbjDHfrrcdIiIiIiIiIvLvr5YhJKcW/OkAK8kPKamkHzgBuKLMsnOAX1prr59qG0RERERERETkhamWaVRfVfDvNLAWeE21J1hr40DcGFNu8XHAXGPMecDXrLVX1dAWEREREREREXkBqSWB8RNr7a2FDxhjjgI21/naewDfAD4F3GiM+Yu1Nl0S/yzgLIAFCxbU+TIiIiIiIiIisrurpQZGuToV21O7YhC4yVo7CjwJzCxdwVr7I2vtSmvtyp6enu14KRERERERERHZnU3aA8MYcwRwJNBjjDm3YFE74G7Ha98GrDDG3AssArZsRywRERERERER+Tc2lR4YYaCVINnRVvDfEPCGak80xoSMMdcBBwB/M8YcXjDjyJeBC4BbCYanJOt7CyIiIiIiIiLy727SHhjW2puAm4wxF1lr19US3FqbAk4sefjO7LLngZfWEk9EREREREREXphqKeI5Zoy5EFgORHMPWmuP3+GtEhEREREREREpUEsRz8uAx4DFwPkE06jevRPaJCIiIiIiIiJSpJYERre19qdAylp7k7X2HcCLdlK7RERERERERETG1TKEJJX9//PGmFcAG4F5O75JIiIiIiL/v737DpOiyvo4/j2TCTNkJAmYFRRQwBwAE+prXGXFnNecdlldV13UXcO66prWrJhzXDGgrjkjKKJiBAOgkoYZYPKc94+qaXqG7pnpYXq6G3+f5+GZ7gq3TxfVVbdO3XtLRESkvkQSGH83s07AH4HrCR6jemZSohIRERERERERiZJIAmOJuy8FlgKjAcxsu6REJSIiIiIiIiISJZExMK5v5jQRERERERERkVbVZAsMM9sG2BboYWZnR80qArKTFZiIiIiIiIiISJ3mdCHJAzqGyxZGTS8BDkxGUCIiIiIiIiIi0ZpMYLj768DrZjbJ3b8HMLMsoKO7lyQ7QBERERERERGRRMbAuMzMisysA/A58KWZTUhSXCIiIiIiIiIiEYkkMAaFLS72A54D+gOHJyUqEREREREREZEoiSQwcs0slyCB8bS7VwGenLBERERERERERFZKJIFxCzAH6AC8YWYDCAbyFBERERERERFJqmYnMNz9Onfv6+57ursDPwCj6+ab2ZHJCFBEREREREREJJEWGPV4oDpq0hmtEI+IiIiIiIiIyCpanMCIwVqxLBERERERERGRiNZMYGhATxERERERERFJCrXAEBEREREREZG015oJjLdbsSwRERERERERkYicphYws7Mbm+/uV4d/T22toEREREREREREojWZwAAKkx6FiIiIiIiIiEgjmkxguPtFbRGIiIiIiIiIiEg8zWmBAYCZFQDHAoOBgrrp7n5MEuISEREREREREYlIZBDPe4FewO7A60A/oDQZQYmIiIiIiIiIREskgbG+u18ALHf3u4G9gM2SE5aIiIiIiIiIyEqJJDCqwr/FZrYp0AkY2OoRiYiIiIiIiIg00OwxMIBbzawLcAHwDNAxfC0iIiIiIiIiklSJJDDucvcagvEv1k1SPCIiIiIiIiIiq0ikC8lsM7vVzHY2M0taRCIiIiIiIiIiDSSSwNgIeBk4BZhjZjeY2fbJCUtEREREREREZKVmJzDcvczdH3H3A4BhQBFBdxIRERERERERkaRKpAUGZraTmf0HmAYUAOOSEpWIiIiIiIiISJRmD+JpZrOBj4FHgAnuvjxpUYmIiIiIiIiIREnkKSRD3b0kaZGIiIiIiIiIiMTRZALDzP7s7v8E/mFm3nC+u5+elMhERERERERERELNaYHxRfh3ajIDERERERERERGJp8kEhrv/N3w5w92nJzkeEREREREREZFVJPIUkqvNbJaZXWJmg5MWkYiIiIiIiIhIA81OYLj7aGAUsAC41cw+NbPzG1vHzPqY2TQzKzezVVp7WOATMzsu0cBFRERERERE5LcjkRYYuPvP7n4dcCLBI1UvbGKVxcDOwHtx5u8D/JpIDCIiIiIiIiLy29PsBIaZbWJmE81sJnAD8A7Qr7F13L3c3Zc0ssh44OHmxiAiIiIiIiIiv03NeQpJnbuAB4Hd3H3e6n6wme0OvA5Ux4vDzE4ATgDo37//6n6kiIiIiIiIiGSoZrXAMLNs4Ft3v7Y1kheh4wiSInG5+63uPsLdR/To0aOVPlZEREREREREMk2zWmC4e42ZdTOzPHevbKXP3gB4CuhLMJ7nW+4+q5XKFhEREREREZE1SCJdSL4H3jazZ4DldRPd/ep4K5hZLvA8MBR40czOAw5z99PcfVi4zFFAjpIXIiIiIiIiIhJPIgmMeeG/LKCwOSu4exWwS4PJ7zdYZlICMYiIiIiIiIjIb1CzExjuflEyAxERERERERERiafZCQwzexXwhtPdfUyrRiQiIiIiIiIi0kAiXUj+FPW6APgdwSNQRURERERERESSKpEuJB81mPS2mb3eyvGIiIiIiIiIiKwikS4kXaPeZgEjgF6tHpGIiIiIiIiISAOJdCH5iGAMDAOqgDnAsUmISURERERERESknqwElj0HGObu6wD3AsuBFUmJSkREREREREQkSiIJjPPdvcTMtgd2BSYBNyUlKhERERERERGRKIkkMGrCv3sBN7v700Be64ckIiIiIiIiIlJfIgmMuWZ2CzAOeM7M8hNcX0RERERERESkRRIZxHMcMBb4l7sXm1lvYEJywhIRERERERGRTDLw3MkJLT/n8r0SWr7ZCQx3XwE8EfV+PjA/oU8TEREREREREWkBdQERERERERERkbSnBIaIiIiIiIiIpD0lMEREREREREQk7SmBISIiIiIiIiJpTwkMEREREREREUl7SmCIiIiIiIiISNpTAkNERERERERE0p4SGCIiIiIiIiKS9pTAEBEREREREZG0pwSGiIiIiIiIiKQ9JTBEREREREREJO0pgSEiIiIiIiIiaU8JDBERERERERFJe0pgiIiIiIiIiEjaUwJDRERERERERNKeEhgiIiIiIiIikvaUwBARERERERGRtKcEhoiIiIiIiIikPSUwRERERERERCTtKYEhIiIiIiIiImlPCQwRERERERERSXtKYIiIiIiIiIhI2lMCQ0RERERERETSnhIYIiIiIiIiIpL2lMAQERERERERkbSnBIaIiIiIiIiIpD0lMEREREREREQk7SU1gWFmfcxsmpmVm1lOg3l/M7N3w387JzMOEREREREREclsyW6BsRjYGXgvxrx73H0bYA/gb0mOQ0REREREREQyWE7Ti7Scu5cD5WYWa97s8GUF4MmMQ0REREREREQyWzqMgTERuCXWDDM7wcymmtnUBQsWtG1UIiIiIiIiIpI2UprAMLP9gW7u/kCs+e5+q7uPcPcRPXr0aOPoRERERERERCRdJLULSWPMbAhwCrBXqmIQERERERERkcyQ7KeQ5JrZy8BQ4EUz28rMrg9nXwmsFU5/OplxiIiIiIiIiEhmS/YgnlXALg0mvx/O2z2Zny0iIiIiIiIia450GMRTRERERERERKRRSmCIiIiIiIiISNpTAkNERERERERE0p4SGCIiIiIiIiKS9pTAEBEREREREZG0pwSGiIiIiIiIiKQ9JTBEREREREREJO0pgSEiIiIiIiIiaU8JDBERERERERFJe0pgiIiIiIiIiEjaUwJDRERERERERNKeEhgiIiIiIiIikvaUwBARERERERGRtKcEhoiIiIiIiIikPSUwRERERERERCTtKYEhIiIiIiIiImlPCQwRERERERERSXtKYIiIiIiIiIhI2lMCQ0RERERERETSnhIYIiIiIiIiIpL2lMAQERERERERkbSnBIaIiIiIiIiIpD0lMEREREREREQk7SmBISIiIiIiIiJpTwkMEREREREREUl7SmCIiIiIiIiISNpTAkNERERERERE0p4SGCIiIiIiIiKS9pTAEBEREREREZG0pwSGiIiIiIiIiKQ9JTBEREREREREJO0pgSEiIiIiIiIiaU8JDBERERERERFJe0pgiIiIiIiIiEjaUwJDRERERERERNKeEhgiIiIiIiIikvaUwBARERERERGRtJeT6gBaomxZGf854y5mffgNRd0KOfrv46mqqOL521+hqqKabfYdyW5H7LTKei/f9zrvPD2VnNxsxh47hqULS7j5rLspW15O3w16c8kz5/Dqg28z861ZFHUr5IAz96J0cSnP3fYKFSsq2er/tqDfRr25+MCrWbZkGe0L2/Hne0/nxtPvYN43P2PAxltvwLgJ+3LDqXeworSMXuusxfmPncUz177AjDc+p31ROw6/8CDefPJ9XrzzVWpraum7QW/OvO14Ju77L5YvXUF+uzzOuv0k/nvj83z+3tcYMGyXTTn4nP254dQ7WLqwlLU36sNptx3HZQdey49fziMnN5tx5+7HekMG8uClT1C+vJwtdtmMbQ/YkvP3upzy0nKy87I59bpj+Hn2Al65/w3cYcwh29FnvV7cePqdVFfWUNAxnwkPnslVh13H8uIVYLDl2KGMPnQn7p34COXLyxm0zUbsfequ/HWPy6muqMbMOGjCPpQvr+B/97+Ju7PVXsMZMnow1598GzVVNeTm5/Dn+0/ljgn388uchViWseNBW7P5mE256/yHqCirZOCm/Tn5+qP5y25/Z/mSFWTlZHHkxeOY9f7XfDB5OrXubLD5uhx24e+47NDrKF9eQUHHAiY+8SdevvcNPpryCTm52ex/xl5YtnH3BY9QVVHJWgN7ctHkc7np5Dv4dsb3dOzUnhOuPIJXHniTtx5/j9paZ92hAzj79pO45KCrKP5lKZ16FHH+Q2dy+7n389m7X5GVZYw5ZHvGHjOGO/7yACWLStlwxHocfekh/PuEm/lq6ne0L2rHcZcdSlG3jpF9Zsu9tmCLXYdw05l3Mffr+fRcuzsnXnMU90x8hA+fn46ZsduRO3HoBQfywXPTKVlYSr+N+jB4hw256y8P1dtn2nUs4L83TaF8eQXDdxvKLofvwLSXP+Xn736la+/OjNxjc36Zs4BP3/wCM2Po6MF07dWZD5+fTvGvJfTdsDfDxgzmyw++5Zvps2nXsYCRY4fRvW+3Nv8Nt6a9Ox1GeWlF5P2V0y5g2LAhq1XmJQdfxRuPvBd5f+wV4zl4wgGrVeaTN03mP6dMirxfa90e3PfNf1arTIBdsw6KvM7Jy+b58odWu8xxfY5jyc9LAcjKzuLhBbfQuXPn1SrzuCEBNzybAAAgAElEQVRn8f3MnyLv/3DN4Rx4xj6rVeakiQ9y/8VPRN5vvM36XP/2ZatV5g8//MDx6/2J2hoHoO+GazFp1g2rVSbA7nnjqK32yPuXah9d7TIPXe8kfp29MPL+ylcnMmynwatV5v8eeosrjrie2upasnKyOPu2P7D7kWNWq8ySkhJOGnoOC39aTHZuFr+fsC9HXnTwapUZy0EbH0nxVyvqT8wCale+PebSQ7jzvAci74eOHszy4mV8M/17IPgN/e25CVyy91VUllWBwb6njGXM+O3rHX+33X84lx92A3Nm/kjHzh045dqjmfXBNzzx72eprqphk6035NQbj+Xi313Fz7N/oX1hO0694Vg6dS9i8i0vRc4POxy4FU9cM5lvP55D975dOfCPe4PDRy/NoKqiik223oD1hg1k2kszmPv1z3TuWcSWe25ByaJSPnppBtWV1QzadiPWGtCd60+9gx+++Imuvbtw0lVHkpOfy/RXPqWmqobB223EgEH9mDrlk3rnjA5F7ettroqyCj6aMoOfZ/9Ktz5dGLnH5iyat5jpr8yktraWzXbYhL4b9Gbqix/z6/cL6danC1vuuTntOrarV07xwhIe+efT/PTVPPqs34txf9qHrr26NPl/OHvmD8x4/fPgHDZqEAMGrd3kOmXLyvjgueksmreEngO6M2L3YRS0z6+3zIrSMj58Plim1zo9Gb7bEPLb5ccp8beneMFSPnz+Y0oXL2PA4H4MG70p2TnZqQ5LRCQmc/eml0oDI0aM8KlTpzLz7Vmcs+vFVFVWR+Z5rZOVnUVu/sp8TN8NenPt23+noH0B5SvKOWuHC/nxy7mR+RVllRDjq+fk5ZCdEzRMqQovznPygoN4RXllvYpQQgzMLBJvMllW23zOb1VT29eyjLyCXACqq2qoqaoJ/u+t8XXW33wdINg3f/pyXrBs8FGr7ON17/uu34usnGzcnaULSijoUEBBh6BSVrasnMqySjr1KAKgtqaWpQtLKezSgZy8oJysLOOAM/+PMeO3b4Utkxgz+8jdRyS6Xt2xAOpfvNdbZo8hXDb5ghbFtXvuuMjFa7QOndvz1OK7W1TmgWsdw9IFpTHntfRC9uGrnuT2CQ/EnLc6F8fxtulfHjyDMb9v2X4Sr8zC7h154te7WlTmnu3GU1VRHXNeS7//A5c/wV3nPdiqZUL873/FyxeyxZjNWrXMdYcN4JZp/2pRmccMOp0fZ81fZXrv9dfinq9alsSZfMfL/Pv4W1aZXtilA08smhR5v7rHg3jbozXlt88DoKa6lurK2PteU6LPD7W1Tk1VDdm5WWRlBfWO2ppaOvfsROeenQCoqqymrKSMwm4dI3WI0sXLyMnLoV3HAgBKFpfy6/cL63+QQ+e1iiIJ6qqKKspKyynqVhg5rxS0z+cP/zqCjUauD8Dcb+Zz/al3ULJo5bGqdNEyctvlRhICVeVVlC0vp7DrynjaF7bjxKuPZP1hwTns/eemccUR11NZXhkpJzcvl7NvP5EdDtg65napra3lnomP8MHz0+tN32bvERx2wYGRz2ro20/mcNNZk1hRWhaZVti1I6ddfyz9NuwDwNfTvuOmsydRvnxloruoWyGn3XAsfdfvHbPc35IPX5jOPRc9Sk11TWRar3V6csZ/jqdT96I2j6c16gbRBp47OaFy5ly+V6IfLSINtNbvLt7xIKldSMysj5lNM7NyM8uJMe9/ZvaOme3S3DInHnDlKskLCE760Rcdc7+ez3Un3w7ADafdWS95UVVVFTN5AUQqJdVVNdTW1FJTXYPXOpWVq5G8gMjntUVSQYmL5Gpq+3qtU1NdS01NkLwAaCpR6LXOD18E++jcr+dTW1MbWafePl4d7ITVVTVUlFXy85wFACwrXk7JolIWzl2Eu1NbU8vCuYtZurCE5UuDO5LFvy6ldHEpi+YtiXxuba3z+DXP8tPXq160pLtdc+JfsEx9fkaLynzh7v/FTF4AQYukFoqXvAA4drMzW1RmvOQFxL+4bcrueb+PO++y8de2qMwJu06MO6904bIWlQnETV4A3DyhZUmReMkLgH27HN6iMhv7vzhnl4tbVOa4vsfFnffdx9+3qMyFCxfGTF4AzP/mFxYuXBhzXlNiJS8ASpcs58Yz72xRmalSVVEF0OLkBYTnh5rwOF5RHdQzKoPzRG1NLdVVNSyauzhy7lg0bzHFC5ZSsjA4hpQvK2fJL8UsmruY2tqgnF+/X4jX+srzjAfnnCU/L6WyMoh54dywnMUrj0XlKyq4/S/3UxUuc+dfH6yXvFhRUsaSX4tZ9NPiyHlo4dzFFP+6lGVLlq9crrSMO/5yPzXVNdTW1nL18TfVS14AVFVWce1Jt0XiaejdZ6aukrwAePe/U3nv2Y9irlNTXcPt595XL3kBQYLnjvMewN2pqqzi9r/cXy95AVCyqJQ7/xr/9/5bUbxg6SrJC4CfZ//Kg5c9maKoREQal+wxMBYDOwPvxZh3LnA+sFv4t0lffvRNvZNrQ9VV9SsVU6d8AgTZ5Wi1VY1fTNZW11IbdTCvrqrBaxpZoZnasrWLkhjJ1dT2ra6qxuNcCMdTsaKCysqqSMU1+KCG5dZQG1VRLVtWDqy8uK6tqaWstJwVpWV4WLldXrwcd2d5SVDJqyirqFcBd3fe+++qdy3SXhMJxZcfej3hIq86+qZG5+/ZfnzCZR7U59hG5//w2dxG57elugRZa/r4lc8anX9Az6MTLnO33HGNzn/8qucSLvOHH35odP6KpeUJl5ksS+YvbXT++ftdkXCZJ212bqPz/7DJhITLnP7qp43Of/amKQmXGUtbtL4AqK0JktOrq7qyut5xvDYqSQ3BYX/Jr0upqqiisixIBCwrXl7vb21tLStKyli6oGTl+ajuT1RdY9FPS6gsr4wkX+rWr7N86QpmvP45cz77kfnf/VJvXt2yNTU1lC0rp2JFZSTZ0bCcpQtL+eydL3n1obcjSfOGypeXM+Wu12LOe/eZ+OegePM+e+dLli6MXSf85fsFfDfje2a+NYvSxbETpfO/+4U5n/0Y93N/Cz58/uNVkhd1Zr71BaVLWp5kFhFJlqQmMNy93N2XxJk9BHjX3ZcBpWZW2HABMzvBzKaa2dQFCxYw75ufm/jA+m8rVlSEfytjLBxfba3XL8o9bouNhCin8NvhLUtY1TRxZ8+Dgle+d8drPVLxBaitqal3IVpTUxvcnauNmtagwhJ9N21N8caj77Z6mVXlid95Lf61pNXjWJMsa0EFOdHkYHPMeuPbVi8zVWa9/1XC66woabyF0YpliSdwZr41q9H5NTWtnzBLtla5EeFAjCR49JSaqup626fuGN9wWlWcFg2Rcqpr6iVdYiUply1ZHvMiv7Zm5Xmipqam3vvaGP93y5YsZ8EPjbfUWTx/cczpjV0ox5vX1HmrdPGyZi3zW9bYTcHaWmdFSVnc+SIiqZLKp5Bk+8qawFJgldGd3P1Wdx/h7iN69OjBFrsOJSsrdj9IWDk2QZ3ufbvW+7tywSYCy80mK6q/pWVnYa0w3Gm8Ppyy5rEsIysnsZ+XmZHXLq/R/TMrK6vefp6dk12vTzVAXkEeee1Wvs8vyCMrOysy7oWZkZu/cj5A/0H9Eoo1E1z8eON3lGPJLWj8h77hyPUSLnO/03ZPeJ3fkl2PHJXwOusMbXpgv0TtdtjoVi8zVU6/OX4Xk3iGjRrU6PzNtt8o4TL3PaPxfb9j5w4JlxlTp9Yppjmysle/2mRZhkWXEx7So+sd7Tq2Izc/N1JvyCsIxt/ID//WTevYpWOMD1j5sn1hAXkFUeW0y1tl8f6D+rH2xn3IbvDd8gpWDnKZ3y4vjKF+PA3L2WK3oY3WdYaN2TTm9AGNnIPizRswOP46WVlG/036MmBw/GNFdnYW/TbqE3f+b8HATeNvn8IuHejae/UGbxYRSYZUJjCibwEXAcVNrdCpayGbbBO/EpWbt/Liw8w48I/BCPfjJuxT74SaldN4IsEMssOBOzHIyckmL2/Vk3XC2ih/YWarJHOkFdmqybKGcvNzyc6OGsG7Gf8dXXt3Jjs7m/aF7Vau02C9nLwczCxSie4cDtBZ2K0QM6OgQwF57fIif82Mwm5BBbeoW9DIqbBLx3qV8M49O7HN3sObDjDNDBmzek9biOW5FY33ib7x/csTLvOUaxq/mLxyWssGG21MYbeWXRhuuFX8BI218GxxzYy/NTp/wu2nJFzmrdOvbnR+azzho6HdjhnVovWik4mt5eTrj2p0/o77bpNwmf+Y3HhPzn++NDHhMouKiiIDTcYs8+XW2fdfWtL6/9+x5OTnBDdRVvP0mpuXg9nKZEjd0x7qkt45OdkUdS8kOzuLjl06AEan7sHxu2PXDmRlZZHfLp+CDvkUtM+PDO5cV8+xMEDLMrr27kJ2TjYdOrUHLHIeqLPxyPVZZ9P+dO7Ria32qn8eKAoH6izoUEBeQR7ZudHl1E+cbLb9JvTboDcbbrEu6w0dGPN7Dxi0NkN2jH3c3uXwncjJXfWpF7l5Oexy2I4x1+m7fm82236TmPNGjt2crr260H/jvgyOU2/caq/hdOnZhtmvNDRs9Kb0WqdnzHk7H7ojuXmtf/wSEVldqUxgzDCzbcysA1Dk7s1qZ33N6xczeLuNIy0xLMtoX9QuaGURVio6dGrP4RcexB7HBI9+2+3I0RwxcRwdOgWV+tzcXDr1KFzlIjSvXS6FXYOTcnZ2Nr3W7UnP/t1XltulHVnZiddcsnOyIhUUyzKyc1u22RvGG+siOjc/p8nKVYuSG2meD2mydUuMZEBL5OTnRD4rOydrlRYTObnZdO3dmbpwirp3pFOPwsgI81lZRk7+qnf5+2/Sjx79gtHiNxy+LgMGrV1vn2lXWED3ft0i5Xbs3J4tdhlC73XXAqBDUTt2Grctm26/caTMYaMHs+NB21DQIbiA6NGvG2MO2SFyx8XM2Hjk+px50/GrPAIvE1z18sR6LU+irc4F7EET9o45/R8vJN6io869866POb3PBr1a/MjXuN8xC55YMKlFZd747uW07xR7X5hS3bJtuummm7JxnMRIU8mNxlz2YuwL7q323rzFZcbbpt37dW1RogVg8vL4g622dD/d/5S96NE/9uOPV2ffv+bt2IOKXrEaiYZnSu4lv8Oqj6s87ILfsf6wdVtcbkPJSFpF69ilQ6SFQlG3Qjp0bl/v2B+rxV3Dc21Wdhbd+nSOTC/qVsjATdeOPDkqKyuLdYYMYNt9R0bWWX/zddj9qNF06RXcCc9vl8/o8dszfLeVx43huw2h97o9V36eQZdendl675UDt280cn12P2pUJOmdk5vNVntuwfFXrhycdvxf9mfM+O0jTxxpV1jALoftyLDRK5MOg7bZgN2PHBVJhOTm5bDtPiM55tKV4wNd+sJfGbLT4EgSPysri8HbbswVU+Inyfpv3JeTrjmaPuutFZnWd4PenPzvoyNPE4nlmEvHs92+W0ZuYOW3y2P077fj0PN/F1nm2MsPZZu9R0SWKWifz5jx23PwufvFLfe3Ijsnm9NvPI4hO2wSqVd37NyBfU8Zy24taB0nItIWkvoYVTPLBZ4HhgPTgPOAw9z9NDPrB9wDtAP+5u6NjubV8PFISxeX8vErn9J3g96RR3d9N2MO5csr2HirDSIXjNFqa2uZ9f7X5LXLi6zz7uSpfPPRd+x61Gh69e9B+Ypyvp42m669O9N3veDxWrNn/kBZaRkbjlyPnJwcPnv3S1576C223Xckm48JKhEXH3Ql+R0KOGfSaQB8+MI0Pn/vK3Y9fCf6rNc7eE7589PpNaAnG20ZPLLsvksf49fvF3LclYdQVFTEp299wfO3v8zIPYczety2ANx45p3kFuRxwuWHATDrw2/44r0v2WbvkfQa2JPFi0t5+JLH6D+oH3sdvysAM9+exaJ5i9lm3xHk5eXx6Vuf88Q1z7LV3iMZe1TQTPqle16jptYj71+Y9Crv//dD/u/ksQzfeQgLFy7kn+NvoOfAnvzptpMB+OCFacz/9hd2PnIUHTu2Y9r/ZnDfxEcZudcWjD9nfwCevPF5qlZUMG5CUDGYfMcrvPXoO+z5h93YYf+tKC4u5saT76LnwB4cf2nwnd566n1mz/iefU7enU7dO/HNJ7O5/+JH2XyXzdjnpD0AuPO8BygpXsZJ1xxJfn4+702eyhuPvsuuR45i89GbUV1dzYt3vkph1w7seGCw7V669zW+mTab/c/ak17912Lhz0t4+/H3GLhZf4aGd4Hu/ttDLFlQEin3yw+/5r3JH7H1XsPZaOQGVFZWcveFj1DYuQMHnxt8x28+ns3cr+czbOfN6NS1kOKFxbz+8LusvUlftgj3h4b7zC8/LODzd79koxHr0Sfcrx658ily2+ez/ynBd1xWvJzSJcvo1qcrefm5MfeZOZ/9yPLi5Wy01frk5ORQUVbB4p+Lgwp1UXsAFvy0CMsyuvcJuk4tL1lByaJSuvXuQl5BHjU1NSz4cRHtOhak5BFpdVrzUWkHr30CSxeUcNFT57Dl2JZfwEa77Mjr+eDZj9j3tN05amLig3fG8sAVj3Hv3x6j/yZ9uKWJVgTN9csvv3DsemdT0CGfx35pvac6nLzln1n6awn/eO1cBg4c2CplnrnT+Xz14XeMP28/Dj+/8YE4m+uak25myl2vs8k263P1q5e0Spk//PADfx11OWut05N/vTKxVcqEYMDS6soqnim+r9XK/MMWf2Lu1/M59cajGXtEsx/o1agX7/4fL97xKrsfO5rdjxzTKmUu+GkBd//tETYcsR77nDR2lfmtdTyIHtCzLqmxV8dDqFxRxR8nncjYI3bm9Sfe5daz72a9zdfh4ifPAeDC/a5g/rc/c9GLE+jTpw8z3/2CSec9yOAdB3H0RQcDqx5/F/68hDcffYcNtliXTbcLWgK8ePerLPxpMfuftQft27dn3rfzeene19l0u00YvutQIDg/rFi6IlLO8pIVfPvxbHoN7EnP/j0AWPzzEqorq+mxdnfMjMryShbNX1LvWN9wmQVzFzHzrS9Yd8hABmwSdK9YNH8JNVUrl4l1zmiofEUFS34pplP3okiLwIXzFlNbU0vPtbvXW6Zzj6K4CfDihSX88PmP9NuwD117rdJLOK4FPy3CjMgjYJujbFkZxQtK6LJW50gCpqEVpWUsXdj4Mr9lpUuWsXzpCrr16ZLSlhd6jKpI5kv2Y1STmsBoTfEOTCKSmVq7kiIimUvHAxEBJTBE1gRKYITMbAEQ6wH33YGWPaA+NRRvcine5GrNeAe4e49EV1pDjgWZEmumxAmZE2umxAltG+tv+XhQRzEnX6bFC7+9mHUsWEmxt71MjRvWzNhjHg8yJoERj5lNbUmmNlUUb3Ip3uRK53jTObaGMiXWTIkTMifWTIkTMivWhjIxdsWcfJkWLyjm1ZVOsSRKsbe9TI0bfluxp3IQTxERERERERGRZlECQ0RERERERETS3pqQwLg11QEkSPEml+JNrnSON51jayhTYs2UOCFzYs2UOCGzYm0oE2NXzMmXafGCYl5d6RRLohR728vUuOE3FHvGj4EhIiIiIiIiImu+NaEFhoiIiIiIiIis4ZTAEBEREREREZG0pwSGiIiIiIiIiKS9nFQHICIiIrI6zGw4sDXQBSgG3nP3qamNSkRERFpbRg3iaWbZwH40qKQAT7l7dSpji0XxJpfiTa50jtfMugKHAouAJ4AJQBHwH3efncrYGkrn7RjNzAzYE6gBprh7bTh9X3d/OqXBNYOZXezuF6Y6jmhmtrm7TzezdsCJwMbAbOBmdy9ObXT1ZdJvqiEzuwbIB14GlhLEvQtQ4+6npzK2eDLluBBNMSdfpsUL6RuzmXWoi8fdl6Uqjt8qM9vQ3b9KdRyJMLMsYC1gQbr+3uIxswIg191LUx1Lc63ObzTTEhj3AjOAV6hfSRnq7oelMrZYFG9yKd7kSud4zWwKMAnoTHBhOJHgwusidx+VssBiSOftGM3M7gPmANXAzsBx7v6lmf3P3cekNLgGzOwH4AegFrBw8mBgprvvmLLAGqjbdmZ2N/Au8D9gGHCUu++Z2ujqy6TfVENm9kas//d409NBphwXoinm5Mu0eCH9YjazMcAFQEn4rwgoBC5195fbOp7mMrPRBMfdWuAWd38onP6ku++fytiaYmbHNJwEnAH8293vTEFIzWZm17r7GWa2N8F+8w0wALjd3e9KbXTxmdkJwDHAMuB+4EiCG1CvuvvfUxlbU1rjN5ppXUgGuvvhDaZNN7M3UxJN0xRvcine5ErnePPc/QEAMzvV3Z8IX6djRjadt2O0fnWVTTO7FZhkZjemOKZ4zgJ+B0wB7nP3ajN73t33SHFcDXnYsqUXQYXUga/M7JQUxxVLJv2mGppqZjcTtMCoqwztDExLaVSNy5TjQjTFnHyZFi+kX8wXA7u5+4q6CeGd3ikEx4h09XdgD6ASmBhe5J1CkFROd6cDvwAPE9wEqbuxkAmtGAaHf88GRrn7irBV0ZtA2iYwCG6EbB228Pwc2CCsC71NsC+ls9X+jWZaAuMZM3sWeI2VlZSdgP+mMqhGNIy3E7AjmRNvum/fpxVvUqXz/vCjmd0OOPCRmV0PLAYWpjasmDLl/z3LzIrcvcTd55nZ/wG3AsNTHVhD7v448LiZ7QncZ2bvALkpDiuWy4BHCJpUv2ZmbwGbAE+mNKrYMuk3VY+7n21mmwPbABsSbO9b3X16aiNrVLzjwjOpDKoJ8c4JmRhzuh1/62RavRHSbxtXAJsB70dN2wwoT004zWbuXhK+PtfM9gOeA7qmMKZmcfdhZrYPMJ5gP5gE/N7d70llXM00J2z98gmwjZm9AQwF0r0rRpaZ9SXogmFATzNbCmSnNqxmWe3faEZ1IQEwsxEEFZRsgqYyWe5+X2qjis/MugNbEpyEioER7n5JaqOKLyre4QTNqL5x9w9TG1VsZtYbqCKItzMwkKBZ+UPp2HctPLh/RHBg7EywP3zo7gtSGlgcZpYHHEzQlO4bggvEgcB1qe6/H97VHgbMJeivuBmwHvCPurEb0kWm7Kdmti3wGUEFdF+CC+3ZwDx3fyOVsTVkZr3dfX7YX7Uu1izg8jTbpr2BJcC2BK0wBgI/Ag+mU5wAZtaH4Lc0F9ieYLyO74DH0i3WNUXU+TZyPiC4m52W51wAM9ue4HhbTNBd4ENgXXd/v9EVU0j1xuRLp7pjeNw9FxhCcF6oIejicqW7z01FTM1hZhcCd7v791HThgD/dPexqYssMWa2G3AcsL67b5HqeJpiZvkELUhGEdQji4F3CLb7zykMrVFmtiPwR2Am8DRwJUGLl3+6+4upjK0prfEbzagEhpndEb6sBHoA8wgq2z3d/YSUBRZH2HyubgPXNacaBHyWjv1yzewFdx9rZmcS9F98FtgOmOvu56Y2ulVF9S+/DlgOvEpwUTvC3celNrpVmdk84HuCZnZPAs+4+5LURhWfmT0JfECQ3R0OTCboE3+Iu++e4tii99Wdw9i2A35y97+kMraGMmU/jYrzWqCMleM1pFWcsEqsK0j/bZrW//eQOdt0TREm32J50d13bdNgmsnMrgJ6ElQ2uwHHuPuCdBwnp47qjcmXaXVHEcl8mdaFZH133wnAzD519wPD16+mNqy4niTILk1y99cA0rSfdp288O/+wOjwTvbNYbPndFR3p32Qu+8Svp6SxvvDl+4+2szWAQ4AnjSzCuBpd/9PimOLpbO7XwaR39vV4eujUhpVIJP21UzZT+viHJzmcULmxJop//eQOdt0TbGM4EkN0YygzpCuRkTVwYYAj5rZhBTH1BTVG5MvI87HFg7WmOo4EpWpcYNiT4VMjRsSiz3TEhjR8Z4X9doaLpgO3P3qsBn+cWZ2IvBAqmNqwiAzu4egCVU+wV1YgILUhdSou8M+2z9a8ASF1wlO/FNTG1bjPHgk4VXAVWa2FkET+HS03MzOJ9gXfjazPxL0ia9IbVhAZu2rmbKfZkqckDmxZkqckFmxrgm+APZ396XRE83spRTF0xw5Zpbn7pXuPsPM9gfuY+UgeOlI9cbky5Tz8f2pDqCFMjVuUOypkBFxm9lggkedz4qa3OzjXaZ1IRkMzHL3mqhpecBYd0/nQaQwsxzgcGCjdG1SZ2YDot7Oc/cqM+sI7ODuz6cqrsaE/bZ3J+i7vRR4x90/SW1UsZnZ7uneLy2aBSMbjwW+Bb4meESTAQ80rHSnILaM2lczZT/NlDghc2LNlDghs2LNdGEf4EXuXtlgek66jjliZlsCc9z916hp2cBBHj7yMd2o3ph86XY+jtM9y4AX0rV7FmRu3KDYUyFT44ZId8S1CMbsaFF3xIxKYIiIiIiIiMRiZisIumcZ9ccTGeLu3VIWWBMyNW5Q7KmQqXEDmNnrDbojXgdMAK5obgIj07qQiIiIiIiIxJKJ3bMgc+MGxZ4KmRo3tEJ3RLXAEBERERGRjJeJ3bMgc+MGxZ4KmRo3tE53RCUwRERERERERCTtxXsOuYiISEYIM/ci0kJmdqKZHRG+Pioc0LVu3u1mNiiFsT1nZp3D18vCvwPNbGaqYmqKmY0ys2dTHYeIyJpICQxpc2b2lJl9ZGafmdkJ4bRjzewrM3vNzG4zsxvC6T3M7HEz+zD8t11qoxeR1WFml5jZGVHv/2Fmp5vZhPA3PsPMLoqav8rxIpy+zMwuNrP3gW3a+GuIrFHc/WZ3vyd8exTQJ2rece7+eUoCCz5/T3cvXp0ylOQUEVlzKIEhqXCMuw8HRgCnm1lf4AJga2BXYOOoZa8FrnH3kcDvgNvbOlgRaVV3EDySt+4xYAcDvwAbAFsCw4DhZrZjuHzD40Xd6NodgJnuvpW7v9WWX0AknYStEWaZ2d1hAvAxM2tvZjub2XQz+9TM7jSz/HD5y83s83DZf4XTJprZn8zsQILf2u4kGdsAAAi1SURBVP1m9rGZtQtvLIwws5PM7J9Rn3uUmV0fvj7MzD4I17mlsYSBmd1kZlPDpORF4bROZvalmW0Uvn/QzI4PX88xs+5NfP83zWxa+G/bcPooM3vVzB4APo2XPI1T5qjwez8Wbtv7zczCefG269hw2beAA6LK6hAu92G43r7h9MFR22yGmW0Q/39ZRETqKIEhqXC6mX1C8PiftQmec/66uy929yrg0ahldwFuMLOPgWeAIjMrbPOIRaRVuPscYJGZbQ7sBkwHRka9nkaQxKyrzDc8XtRNrwEeb7vIRdLaRsCt7j4EKAHOBiYBv3f3zQieOneSmXUF9gcGh8v+PboQd38MmAoc6u7D3L0savZjRF2YA78HHjazTcLX27n7MILf5qGNxPpXdx8BDAF2MrMh4Uj6pwKTzOxgoIu739bM7/4rsKu7bxHGcV3UvC3DzxtE7OTp/Y2UuzlwJjAIWBfYzswKiL1dC4DbgL2BHYBe0d8X+F94I2Y0cKWZdQBOBK4Nt9kI4Kdmfl+RjGZm+5jZuS1cd6KZ/SnBdUbVJTZbYzlJPT1GVdqUmY0iSEps4+4rzOw14EtgkzirZIXLlsWZLyKZ53aCZuq9gDuBnYHL3P2W6IXiHC8Kwtnl7l7TVgGLpLkf3f3t8PV9BK0aZ7v7V+G0u4FTgBuAcuB2M5sMNHucBndfYGbfmdnWwNcESZO3w3KHAx+GjRTaESQV4hkXdgfLAXoTJAhmuPtLZnYQcCMwtLlxAbkENzrqkicbRs37wN1nh/HPMbO65OlawHR3X9RIuR+4+08A4U2UgUApsbfra+H0r8Pl7wPqurztBuwTddFVAPQH3gX+amb9gCfq1hVZE1gjT8Nw92cIbkq2lVHAMuCdVlpOUkwtMKStdQKWhBcjGxN0G2lPcBemi5nlEHQVqTOF4K4MAGEFRUQy25PAWIKWFy+G/44xs44AZtbXzHoS+3ghIqtq1iPlwguKLQlaL+0HvJDg5zwMjCM4Tz/pwaPsDLg7bLExzN03cveJsVY2s3WAPwE7hy1AJhMmJcNWEZsAZUDXBGI6i6Ab2lCClgx5UfOWN1i2Lnl6NEHytDEVUa9rCBIu1sjy8f4PDPhd1Pbp7+5fuPsDwD4E3/dFMxvTRDwibS7sAjXZzD4xs5lm9vvobl1h97LXwtcTzexWM5sC3GNm75vZ4KiyXjOz4WH3sxvCrmNzwt8+FnR9+9HMcs3s+LDb1ScWjIXXvpnxnm4ru8g9ZGYDCVo7nRV219rBzPYOY5tuZi+b2VpxlptkQbe6urLrBhHubWZvhMvNNLMdVn9LSyKUwJC29gKQY2YzgEsImoXPBS4F3gdeBj4HlobLnw6MCA9EnxMcXEQkg4XPLX8VeMTda9x9CvAA8K6ZfUrQVL2Q2McLEVlVfzOrG8x2PMG5dKCZrR9OOxx4PUwSdnL35wi6R8S6KVBK8PuL5QmCxMd4gmQGwCvAgWHSETPramYD4qxfRJBUWGpmawF7RM07C/giLPtOM8tt7AtH6QTMd/dagu/Z2ICdDZOniZpFjO0aTl/HzNYLp4+PWudF4DSzyBgam4d/1wW+c/frCO5GD2lBPCLJNhaY5+5D3X1Tmk56Dgf2dfdDgIcIEp6YWW+gj7t/VLdg2HXsE2CncNLewIthd/In3H2kuw8lOC4c28x4zwU2DxOkJ4bdVm8mGE9vmLu/CbwFbO3um4cx/jnOcvEcEsY5jCBx+nEzY5NWoi4k0qbcvYL6FRYAzGyqu98atsB4kqDlBe6+kKBPq4isIcK7LVsDB9VNc/drCQbtbWiV40W4fMfkRCeSkb4AjjSzWwi6d5xBkPB7NDyvfkhQOe8KPB2O2WAESYOGJgE3m1kZDZ7w4+5LwpsJg9z9g3Da52Z2PjAl/G1XEXSr+L5hwe7+iZlNBz4DviPogoKZbQgcB2zp7qVm9gZwPvC3Znz3/wCPh91PXmXVVhfRn19pZq8CxS3pgubu5WZ2NA22q7tXhN1iJpvZQoILpE3D1S4B/g3MCJMYc4D/I6jbHGZmVcDPwMWJxiPSBj4F/mVmVwDPuvubYS4unmeiun0/ArxE8DseR/0x7uo8TPBbeJVgXJr/hNM3NbO/A52BjjQ/4TiDYBDip4Cn4izTj2D8nt4ELbZmN7PsOh+yMsn6lLsrgdHGLGj9J5JaFoyEvgtBU9IpwBmunVNkjWNmgwj63T/p7n9MdTwimS5s+vxseHdUGhEmWKYBB2nMCZHmsWDw3z0JWkFPAY4AtnX3X81se+Dv7j7KzCYCy9z9X1HrvkmQ0LwZ+IO7f2pmRwEj3P3UsFXYZwSD5n4MrOPuNWY2G9gvTHoeBYxy96NifUaDWLOBHQm6Z+0JDCZIhkbWCbu8XO3uz1gw1tbEWPGb2e3AFHd/JEw+Vrh7XjivD7AXQUvxK6MeQy1tQC0wJC24e0IjCotIZnL3zwlG9BcRaTMNkqdKXog0Q3ihvtjd7wvHgDiKoBXRcOB56o9bF8tDwJ8Juq592nCmuy8zsw8IWmA+G9UyqhCYH7ZyOJSgu3lTsWYBa7v7qxY8zvgQgtYbpQTd1+p0iirvyKjpDZer+56PAPsSDBhM2EVurrvfZsEThbYAlMBoQ0pgiIiIiGSosO922rW+MLP3gfwGkw+PdRHTFmIlT81sM+DeBotWuPtWbRaYSHrbjODRv7UE3cNOInjS0B1mdh7B+HWNeYwgOXFJI8s8TNC9ZFTUtAvCsr8n6MYSb1yeaNnAfWbWiaCL3DXuXmxm/wUeM7N9gdOAiQTdwOYSdLVbJ1y/4XK3EXS5+4BgrJ+67mmjgAlh969lBC1SpA2pC4mIiIiIiIiIpD09hURERERERERE0p66kIiIiIiIiEhGMLMbge0aTL7W3e9KRTzSttSFRERERERERETSnrqQiIiIiIiIiEjaUwJDRERERERERNKeEhgiIiIiIiIikvaUwBARERERERGRtPf/ibDogG7YmU0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1080x1080 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.plotting.scatter_matrix(df, c=y, figsize=(15, 15), marker='o',\n",
    "                        hist_kwds={'bins': 20}, s=60, alpha=.8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_folder + 'scatter_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(int(time.time()))\n",
    "k_values = np.arange(5, 200, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in k_values:\n",
    "    accs = []\n",
    "    for i in range(10):\n",
    "        seed = np.random.randint(999999999)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n",
    "        knn = KNeighborsClassifier(n_neighbors = k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        accs.append(knn.score(X_test, y_test))\n",
    "        \n",
    "    scores.append(accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(save_folder + 'scores.txt', scores)\n",
    "\n",
    "medians = []\n",
    "for i in range(len(scores)):\n",
    "    medians.append(np.median(scores[i]))\n",
    "\n",
    "_medians = list(np.copy(medians))\n",
    "max_list = sorted(_medians)\n",
    "max1 = _medians.index(max_list[-1])\n",
    "del(_medians[max1])\n",
    "max2 = _medians.index(max_list[-2]) + 1\n",
    "\n",
    "np.savetxt(save_folder + 'medians.txt', medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(k_values, medians)\n",
    "plt.plot(k_values[max1], medians[max1], 'o', label='max1', markersize=8)\n",
    "plt.plot(k_values[max2], medians[max2], 'o', label='max2', markersize=8)\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(min(medians) - 0.1, max(medians) + 0.1)\n",
    "plt.annotate('K = %i'%(k_values[max1]), (k_values[max1] - 4,  medians[max1] + 0.015))\n",
    "plt.annotate('K = %i'%(k_values[max2]), (k_values[max2] - 4,  medians[max2] + 0.015))\n",
    "plt.title('Mediana das Acuracias')\n",
    "plt.ylabel('Acuracia')\n",
    "plt.xlabel('N Vizinhos')\n",
    "plt.legend(['Teste'])\n",
    "plt.savefig(save_folder + 'P1_outs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0             0  1\n",
      "survival_status       \n",
      "0                45  3\n",
      "1                12  2\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = k_values[max1])\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "conf_mat = pd.crosstab(y_test, y_pred)\n",
    "print(conf_mat)\n",
    "\n",
    "conf_mat.to_excel(save_folder + 'conf_matrix.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0             1\n",
      "count  3.060000e+02  3.060000e+02\n",
      "mean   1.857628e-16 -2.539726e-17\n",
      "std    1.055942e+00  9.998604e-01\n",
      "min   -2.503637e+00 -1.336894e+00\n",
      "25%   -6.631953e-01 -7.743523e-01\n",
      "50%    9.907102e-03 -1.348652e-01\n",
      "75%    6.548567e-01  4.159343e-01\n",
      "max    4.268923e+00  5.517063e+00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "save_folder = './com_pca/'\n",
    "\n",
    "n_comps = 2\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components = n_comps)\n",
    "\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "df_X = pd.DataFrame(X)\n",
    "print(df_X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_X.hist(sharey=True)\n",
    "plt.tight_layout()\n",
    "plt.xlim(0, 100)\n",
    "plt.savefig(save_folder + 'X_hist.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in k_values:\n",
    "    accs = []\n",
    "    for i in range(10):\n",
    "        seed = np.random.randint(999999999)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n",
    "        knn = KNeighborsClassifier(n_neighbors = k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        accs.append(knn.score(X_test, y_test))\n",
    "        \n",
    "    scores.append(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(save_folder + 'scores.txt', scores)\n",
    "\n",
    "medians = []\n",
    "for i in range(len(scores)):\n",
    "    medians.append(np.median(scores[i]))\n",
    "\n",
    "_medians = list(np.copy(medians))\n",
    "max_list = sorted(_medians)\n",
    "max1 = _medians.index(max_list[-1])\n",
    "del(_medians[max1])\n",
    "max2 = _medians.index(max_list[-2]) + 1\n",
    "\n",
    "np.savetxt(save_folder + 'medians.txt', medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(k_values, medians)\n",
    "plt.plot(k_values[max1], medians[max1], 'o', label='max1', markersize=8)\n",
    "plt.plot(k_values[max2], medians[max2], 'o', label='max2', markersize=8)\n",
    "plt.xlim(0, 200)\n",
    "plt.ylim(min(medians) - 0.1, max(medians) + 0.1)\n",
    "plt.annotate('K = %i'%(k_values[max1]), (k_values[max1] - 4,  medians[max1] + 0.015))\n",
    "plt.annotate('K = %i'%(k_values[max2]), (k_values[max2] - 4,  medians[max2] + 0.015))\n",
    "plt.title('Mediana das Acuracias')\n",
    "plt.ylabel('Acuracia')\n",
    "plt.xlabel('N Vizinhos')\n",
    "plt.legend(['Teste'])\n",
    "plt.savefig(save_folder + 'P1_outs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_0             0  1\n",
      "survival_status       \n",
      "0                41  4\n",
      "1                15  2\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = k_values[max1])\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "conf_mat = pd.crosstab(y_test, y_pred)\n",
    "print(conf_mat)\n",
    "\n",
    "conf_mat.to_excel(save_folder + 'conf_matrix.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiLayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "layer_sizes = [5, 10, 50, 100, 200, 400, 500, 1000, 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7.47198377\n",
      "Iteration 2, loss = 7.31885207\n",
      "Iteration 3, loss = 7.16385816\n",
      "Iteration 4, loss = 7.00636692\n",
      "Iteration 5, loss = 6.85128999\n",
      "Iteration 6, loss = 6.68975363\n",
      "Iteration 7, loss = 6.53585405\n",
      "Iteration 8, loss = 6.37080195\n",
      "Iteration 9, loss = 6.21220730\n",
      "Iteration 10, loss = 6.04742925\n",
      "Iteration 11, loss = 5.88398451\n",
      "Iteration 12, loss = 5.71520430\n",
      "Iteration 13, loss = 5.54169279\n",
      "Iteration 14, loss = 5.37183852\n",
      "Iteration 15, loss = 5.19505328\n",
      "Iteration 16, loss = 5.02739571\n",
      "Iteration 17, loss = 4.85812964\n",
      "Iteration 18, loss = 4.67570931\n",
      "Iteration 19, loss = 4.50742728\n",
      "Iteration 20, loss = 4.33023417\n",
      "Iteration 21, loss = 4.16439671\n",
      "Iteration 22, loss = 3.97799234\n",
      "Iteration 23, loss = 3.81109954\n",
      "Iteration 24, loss = 3.63238024\n",
      "Iteration 25, loss = 3.45441420\n",
      "Iteration 26, loss = 3.28090991\n",
      "Iteration 27, loss = 3.10154371\n",
      "Iteration 28, loss = 2.92331051\n",
      "Iteration 29, loss = 2.74515440\n",
      "Iteration 30, loss = 2.56739693\n",
      "Iteration 31, loss = 2.37392625\n",
      "Iteration 32, loss = 2.20031402\n",
      "Iteration 33, loss = 2.01631062\n",
      "Iteration 34, loss = 1.82487958\n",
      "Iteration 35, loss = 1.64495097\n",
      "Iteration 36, loss = 1.46181131\n",
      "Iteration 37, loss = 1.27747993\n",
      "Iteration 38, loss = 1.12091897\n",
      "Iteration 39, loss = 0.98032146\n",
      "Iteration 40, loss = 0.86403928\n",
      "Iteration 41, loss = 0.80167178\n",
      "Iteration 42, loss = 0.78790645\n",
      "Iteration 43, loss = 0.79438997\n",
      "Iteration 44, loss = 0.81775806\n",
      "Iteration 45, loss = 0.82661528\n",
      "Iteration 46, loss = 0.81288979\n",
      "Iteration 47, loss = 0.78206770\n",
      "Iteration 48, loss = 0.74638628\n",
      "Iteration 49, loss = 0.71422923\n",
      "Iteration 50, loss = 0.69351712\n",
      "Iteration 51, loss = 0.68112897\n",
      "Iteration 52, loss = 0.67147324\n",
      "Iteration 53, loss = 0.66217612\n",
      "Iteration 54, loss = 0.65285188\n",
      "Iteration 55, loss = 0.64315740\n",
      "Iteration 56, loss = 0.63543782\n",
      "Iteration 57, loss = 0.63001282\n",
      "Iteration 58, loss = 0.62495659\n",
      "Iteration 59, loss = 0.62310515\n",
      "Iteration 60, loss = 0.61956108\n",
      "Iteration 61, loss = 0.61721270\n",
      "Iteration 62, loss = 0.61495379\n",
      "Iteration 63, loss = 0.61272214\n",
      "Iteration 64, loss = 0.61115199\n",
      "Iteration 65, loss = 0.60923162\n",
      "Iteration 66, loss = 0.60807244\n",
      "Iteration 67, loss = 0.60635363\n",
      "Iteration 68, loss = 0.60483754\n",
      "Iteration 69, loss = 0.60352392\n",
      "Iteration 70, loss = 0.60223069\n",
      "Iteration 71, loss = 0.60074648\n",
      "Iteration 72, loss = 0.59854583\n",
      "Iteration 73, loss = 0.59734264\n",
      "Iteration 74, loss = 0.59583772\n",
      "Iteration 75, loss = 0.59476798\n",
      "Iteration 76, loss = 0.59325896\n",
      "Iteration 77, loss = 0.59114050\n",
      "Iteration 78, loss = 0.58959768\n",
      "Iteration 79, loss = 0.58680035\n",
      "Iteration 80, loss = 0.58465902\n",
      "Iteration 81, loss = 0.58144541\n",
      "Iteration 82, loss = 0.57854052\n",
      "Iteration 83, loss = 0.57665667\n",
      "Iteration 84, loss = 0.57492692\n",
      "Iteration 85, loss = 0.57318706\n",
      "Iteration 86, loss = 0.57075257\n",
      "Iteration 87, loss = 0.56809909\n",
      "Iteration 88, loss = 0.56464459\n",
      "Iteration 89, loss = 0.55989875\n",
      "Iteration 90, loss = 0.55571785\n",
      "Iteration 91, loss = 0.55520211\n",
      "Iteration 92, loss = 0.55250016\n",
      "Iteration 93, loss = 0.55101703\n",
      "Iteration 94, loss = 0.54888996\n",
      "Iteration 95, loss = 0.54757766\n",
      "Iteration 96, loss = 0.54660522\n",
      "Iteration 97, loss = 0.54609709\n",
      "Iteration 98, loss = 0.54539599\n",
      "Iteration 99, loss = 0.54477988\n",
      "Iteration 100, loss = 0.54450787\n",
      "Iteration 101, loss = 0.54364844\n",
      "Iteration 102, loss = 0.54349792\n",
      "Iteration 103, loss = 0.54268970\n",
      "Iteration 104, loss = 0.54170929\n",
      "Iteration 105, loss = 0.54148134\n",
      "Iteration 106, loss = 0.54122848\n",
      "Iteration 107, loss = 0.54093574\n",
      "Iteration 108, loss = 0.54041431\n",
      "Iteration 109, loss = 0.53961550\n",
      "Iteration 110, loss = 0.53890455\n",
      "Iteration 111, loss = 0.53843215\n",
      "Iteration 112, loss = 0.53744051\n",
      "Iteration 113, loss = 0.53675377\n",
      "Iteration 114, loss = 0.53581015\n",
      "Iteration 115, loss = 0.53706444\n",
      "Iteration 116, loss = 0.53719719\n",
      "Iteration 117, loss = 0.53677179\n",
      "Iteration 118, loss = 0.53552308\n",
      "Iteration 119, loss = 0.53431500\n",
      "Iteration 120, loss = 0.53325053\n",
      "Iteration 121, loss = 0.53337721\n",
      "Iteration 122, loss = 0.53288303\n",
      "Iteration 123, loss = 0.53245908\n",
      "Iteration 124, loss = 0.53216601\n",
      "Iteration 125, loss = 0.53191189\n",
      "Iteration 126, loss = 0.53159154\n",
      "Iteration 127, loss = 0.53133045\n",
      "Iteration 128, loss = 0.53108553\n",
      "Iteration 129, loss = 0.53061294\n",
      "Iteration 130, loss = 0.53015323\n",
      "Iteration 131, loss = 0.52986012\n",
      "Iteration 132, loss = 0.53012880\n",
      "Iteration 133, loss = 0.53007423\n",
      "Iteration 134, loss = 0.52978654\n",
      "Iteration 135, loss = 0.52925279\n",
      "Iteration 136, loss = 0.52896372\n",
      "Iteration 137, loss = 0.52943365\n",
      "Iteration 138, loss = 0.52844856\n",
      "Iteration 139, loss = 0.52797724\n",
      "Iteration 140, loss = 0.52757530\n",
      "Iteration 141, loss = 0.52748430\n",
      "Iteration 142, loss = 0.52800160\n",
      "Iteration 143, loss = 0.52806972\n",
      "Iteration 144, loss = 0.52733868\n",
      "Iteration 145, loss = 0.52758986\n",
      "Iteration 146, loss = 0.52706134\n",
      "Iteration 147, loss = 0.52659109\n",
      "Iteration 148, loss = 0.52595419\n",
      "Iteration 149, loss = 0.52573474\n",
      "Iteration 150, loss = 0.52566678\n",
      "Iteration 151, loss = 0.52555990\n",
      "Iteration 152, loss = 0.52556677\n",
      "Iteration 153, loss = 0.52513219\n",
      "Iteration 154, loss = 0.52494062\n",
      "Iteration 155, loss = 0.52426439\n",
      "Iteration 156, loss = 0.52434582\n",
      "Iteration 157, loss = 0.52429874\n",
      "Iteration 158, loss = 0.52466981\n",
      "Iteration 159, loss = 0.52401951\n",
      "Iteration 160, loss = 0.52377902\n",
      "Iteration 161, loss = 0.52358181\n",
      "Iteration 162, loss = 0.52355935\n",
      "Iteration 163, loss = 0.52393588\n",
      "Iteration 164, loss = 0.52368432\n",
      "Iteration 165, loss = 0.52413466\n",
      "Iteration 166, loss = 0.52366018\n",
      "Iteration 167, loss = 0.52283503\n",
      "Iteration 168, loss = 0.52311402\n",
      "Iteration 169, loss = 0.52229119\n",
      "Iteration 170, loss = 0.52204724\n",
      "Iteration 171, loss = 0.52248192\n",
      "Iteration 172, loss = 0.52381042\n",
      "Iteration 173, loss = 0.52592236\n",
      "Iteration 174, loss = 0.52572140\n",
      "Iteration 175, loss = 0.52367633\n",
      "Iteration 176, loss = 0.52269411\n",
      "Iteration 177, loss = 0.52215073\n",
      "Iteration 178, loss = 0.52169899\n",
      "Iteration 179, loss = 0.52081087\n",
      "Iteration 180, loss = 0.52219045\n",
      "Iteration 181, loss = 0.52316659\n",
      "Iteration 182, loss = 0.52237818\n",
      "Iteration 183, loss = 0.52102686\n",
      "Iteration 184, loss = 0.52051717\n",
      "Iteration 185, loss = 0.52078990\n",
      "Iteration 186, loss = 0.52082407\n",
      "Iteration 187, loss = 0.52084998\n",
      "Iteration 188, loss = 0.52038369\n",
      "Iteration 189, loss = 0.52037798\n",
      "Iteration 190, loss = 0.52054072\n",
      "Iteration 191, loss = 0.51974665\n",
      "Iteration 192, loss = 0.51992233\n",
      "Iteration 193, loss = 0.52123437\n",
      "Iteration 194, loss = 0.52011023\n",
      "Iteration 195, loss = 0.51906295\n",
      "Iteration 196, loss = 0.51996423\n",
      "Iteration 197, loss = 0.52033432\n",
      "Iteration 198, loss = 0.51982092\n",
      "Iteration 199, loss = 0.51859906\n",
      "Iteration 200, loss = 0.52000848\n",
      "Iteration 201, loss = 0.52159845\n",
      "Iteration 202, loss = 0.52287370\n",
      "Iteration 203, loss = 0.52289733\n",
      "Iteration 204, loss = 0.52190489\n",
      "Iteration 205, loss = 0.51991750\n",
      "Iteration 206, loss = 0.51804568\n",
      "Iteration 207, loss = 0.51989321\n",
      "Iteration 208, loss = 0.52201192\n",
      "Iteration 209, loss = 0.52408623\n",
      "Iteration 210, loss = 0.52458104\n",
      "Iteration 211, loss = 0.52370201\n",
      "Iteration 212, loss = 0.52146443\n",
      "Iteration 213, loss = 0.51900790\n",
      "Iteration 214, loss = 0.51859121\n",
      "Iteration 215, loss = 0.51999140\n",
      "Iteration 216, loss = 0.51868790\n",
      "Iteration 217, loss = 0.51850501\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 9.01091606\n",
      "Iteration 2, loss = 9.01091606\n",
      "Iteration 3, loss = 9.01091605\n",
      "Iteration 4, loss = 9.01091604\n",
      "Iteration 5, loss = 9.01091603\n",
      "Iteration 6, loss = 9.01091602\n",
      "Iteration 7, loss = 9.01091602\n",
      "Iteration 8, loss = 9.00600117\n",
      "Iteration 9, loss = 9.00031000\n",
      "Iteration 10, loss = 8.99175434\n",
      "Iteration 11, loss = 8.98078690\n",
      "Iteration 12, loss = 8.96802329\n",
      "Iteration 13, loss = 8.94746905\n",
      "Iteration 14, loss = 8.92206460\n",
      "Iteration 15, loss = 8.88225922\n",
      "Iteration 16, loss = 8.83591573\n",
      "Iteration 17, loss = 8.76126490\n",
      "Iteration 18, loss = 8.67421586\n",
      "Iteration 19, loss = 8.57268461\n",
      "Iteration 20, loss = 8.45877973\n",
      "Iteration 21, loss = 8.31648920\n",
      "Iteration 22, loss = 8.15919103\n",
      "Iteration 23, loss = 7.99879758\n",
      "Iteration 24, loss = 7.82546099\n",
      "Iteration 25, loss = 7.65004181\n",
      "Iteration 26, loss = 7.44292032\n",
      "Iteration 27, loss = 7.24563754\n",
      "Iteration 28, loss = 7.03886502\n",
      "Iteration 29, loss = 6.84234851\n",
      "Iteration 30, loss = 6.63517894\n",
      "Iteration 31, loss = 6.43107270\n",
      "Iteration 32, loss = 6.22261601\n",
      "Iteration 33, loss = 6.02034510\n",
      "Iteration 34, loss = 5.81774804\n",
      "Iteration 35, loss = 5.61071157\n",
      "Iteration 36, loss = 5.40829357\n",
      "Iteration 37, loss = 5.21656239\n",
      "Iteration 38, loss = 5.00823105\n",
      "Iteration 39, loss = 4.79918865\n",
      "Iteration 40, loss = 4.60222543\n",
      "Iteration 41, loss = 4.40826255\n",
      "Iteration 42, loss = 4.19653869\n",
      "Iteration 43, loss = 3.99820692\n",
      "Iteration 44, loss = 3.79208909\n",
      "Iteration 45, loss = 3.59838089\n",
      "Iteration 46, loss = 3.39578067\n",
      "Iteration 47, loss = 3.19981813\n",
      "Iteration 48, loss = 3.00332306\n",
      "Iteration 49, loss = 2.81038864\n",
      "Iteration 50, loss = 2.61980134\n",
      "Iteration 51, loss = 2.44546559\n",
      "Iteration 52, loss = 2.27332232\n",
      "Iteration 53, loss = 2.12276262\n",
      "Iteration 54, loss = 1.97999267\n",
      "Iteration 55, loss = 1.87571072\n",
      "Iteration 56, loss = 1.77811613\n",
      "Iteration 57, loss = 1.71247942\n",
      "Iteration 58, loss = 1.67318895\n",
      "Iteration 59, loss = 1.63829142\n",
      "Iteration 60, loss = 1.62133121\n",
      "Iteration 61, loss = 1.61922869\n",
      "Iteration 62, loss = 1.61241614\n",
      "Iteration 63, loss = 1.61496756\n",
      "Iteration 64, loss = 1.61935602\n",
      "Iteration 65, loss = 1.61502607\n",
      "Iteration 66, loss = 1.60848055\n",
      "Iteration 67, loss = 1.60082578\n",
      "Iteration 68, loss = 1.59090664\n",
      "Iteration 69, loss = 1.58227803\n",
      "Iteration 70, loss = 1.57316853\n",
      "Iteration 71, loss = 1.56500508\n",
      "Iteration 72, loss = 1.55651479\n",
      "Iteration 73, loss = 1.54829964\n",
      "Iteration 74, loss = 1.54086048\n",
      "Iteration 75, loss = 1.53670111\n",
      "Iteration 76, loss = 1.52934756\n",
      "Iteration 77, loss = 1.52396348\n",
      "Iteration 78, loss = 1.51843892\n",
      "Iteration 79, loss = 1.51324501\n",
      "Iteration 80, loss = 1.50725231\n",
      "Iteration 81, loss = 1.50180157\n",
      "Iteration 82, loss = 1.49678709\n",
      "Iteration 83, loss = 1.49114314\n",
      "Iteration 84, loss = 1.48525258\n",
      "Iteration 85, loss = 1.47859172\n",
      "Iteration 86, loss = 1.47208120\n",
      "Iteration 87, loss = 1.46518628\n",
      "Iteration 88, loss = 1.45814817\n",
      "Iteration 89, loss = 1.45065736\n",
      "Iteration 90, loss = 1.44515859\n",
      "Iteration 91, loss = 1.43985265\n",
      "Iteration 92, loss = 1.43307145\n",
      "Iteration 93, loss = 1.42710414\n",
      "Iteration 94, loss = 1.42181077\n",
      "Iteration 95, loss = 1.41638851\n",
      "Iteration 96, loss = 1.41048705\n",
      "Iteration 97, loss = 1.40505460\n",
      "Iteration 98, loss = 1.39890527\n",
      "Iteration 99, loss = 1.39298451\n",
      "Iteration 100, loss = 1.38692789\n",
      "Iteration 101, loss = 1.38068437\n",
      "Iteration 102, loss = 1.37468145\n",
      "Iteration 103, loss = 1.36828781\n",
      "Iteration 104, loss = 1.36308599\n",
      "Iteration 105, loss = 1.35796725\n",
      "Iteration 106, loss = 1.35334549\n",
      "Iteration 107, loss = 1.34946188\n",
      "Iteration 108, loss = 1.34482825\n",
      "Iteration 109, loss = 1.33994056\n",
      "Iteration 110, loss = 1.33416404\n",
      "Iteration 111, loss = 1.32844972\n",
      "Iteration 112, loss = 1.32205092\n",
      "Iteration 113, loss = 1.31684268\n",
      "Iteration 114, loss = 1.31090601\n",
      "Iteration 115, loss = 1.30603315\n",
      "Iteration 116, loss = 1.30056753\n",
      "Iteration 117, loss = 1.29569234\n",
      "Iteration 118, loss = 1.29091795\n",
      "Iteration 119, loss = 1.28585987\n",
      "Iteration 120, loss = 1.28081573\n",
      "Iteration 121, loss = 1.27569717\n",
      "Iteration 122, loss = 1.27081227\n",
      "Iteration 123, loss = 1.26518768\n",
      "Iteration 124, loss = 1.26062445\n",
      "Iteration 125, loss = 1.25615485\n",
      "Iteration 126, loss = 1.25076319\n",
      "Iteration 127, loss = 1.24591148\n",
      "Iteration 128, loss = 1.24056346\n",
      "Iteration 129, loss = 1.23593112\n",
      "Iteration 130, loss = 1.23092000\n",
      "Iteration 131, loss = 1.22737861\n",
      "Iteration 132, loss = 1.22267174\n",
      "Iteration 133, loss = 1.21838933\n",
      "Iteration 134, loss = 1.21341249\n",
      "Iteration 135, loss = 1.20907431\n",
      "Iteration 136, loss = 1.20445753\n",
      "Iteration 137, loss = 1.20040700\n",
      "Iteration 138, loss = 1.19601787\n",
      "Iteration 139, loss = 1.19201130\n",
      "Iteration 140, loss = 1.18768308\n",
      "Iteration 141, loss = 1.18366068\n",
      "Iteration 142, loss = 1.18014284\n",
      "Iteration 143, loss = 1.17670135\n",
      "Iteration 144, loss = 1.17313893\n",
      "Iteration 145, loss = 1.16967604\n",
      "Iteration 146, loss = 1.16666808\n",
      "Iteration 147, loss = 1.16367028\n",
      "Iteration 148, loss = 1.16112617\n",
      "Iteration 149, loss = 1.15816615\n",
      "Iteration 150, loss = 1.15422233\n",
      "Iteration 151, loss = 1.14952837\n",
      "Iteration 152, loss = 1.14499472\n",
      "Iteration 153, loss = 1.14038767\n",
      "Iteration 154, loss = 1.13692916\n",
      "Iteration 155, loss = 1.13384380\n",
      "Iteration 156, loss = 1.12981582\n",
      "Iteration 157, loss = 1.12596512\n",
      "Iteration 158, loss = 1.12308260\n",
      "Iteration 159, loss = 1.11960363\n",
      "Iteration 160, loss = 1.11613535\n",
      "Iteration 161, loss = 1.11282584\n",
      "Iteration 162, loss = 1.10899632\n",
      "Iteration 163, loss = 1.10578437\n",
      "Iteration 164, loss = 1.10214714\n",
      "Iteration 165, loss = 1.10069041\n",
      "Iteration 166, loss = 1.09823531\n",
      "Iteration 167, loss = 1.09459958\n",
      "Iteration 168, loss = 1.09108328\n",
      "Iteration 169, loss = 1.08789201\n",
      "Iteration 170, loss = 1.08457594\n",
      "Iteration 171, loss = 1.08169210\n",
      "Iteration 172, loss = 1.07823714\n",
      "Iteration 173, loss = 1.07522749\n",
      "Iteration 174, loss = 1.07222311\n",
      "Iteration 175, loss = 1.06924104\n",
      "Iteration 176, loss = 1.06623499\n",
      "Iteration 177, loss = 1.06333437\n",
      "Iteration 178, loss = 1.05967093\n",
      "Iteration 179, loss = 1.05635348\n",
      "Iteration 180, loss = 1.05426376\n",
      "Iteration 181, loss = 1.05159759\n",
      "Iteration 182, loss = 1.04914906\n",
      "Iteration 183, loss = 1.04650344\n",
      "Iteration 184, loss = 1.04367279\n",
      "Iteration 185, loss = 1.04067161\n",
      "Iteration 186, loss = 1.03736251\n",
      "Iteration 187, loss = 1.03508506\n",
      "Iteration 188, loss = 1.03167042\n",
      "Iteration 189, loss = 1.02907031\n",
      "Iteration 190, loss = 1.02597695\n",
      "Iteration 191, loss = 1.02395027\n",
      "Iteration 192, loss = 1.02127148\n",
      "Iteration 193, loss = 1.01835301\n",
      "Iteration 194, loss = 1.01605180\n",
      "Iteration 195, loss = 1.01423217\n",
      "Iteration 196, loss = 1.01100105\n",
      "Iteration 197, loss = 1.00812115\n",
      "Iteration 198, loss = 1.00390183\n",
      "Iteration 199, loss = 1.00106484\n",
      "Iteration 200, loss = 0.99778012\n",
      "Iteration 201, loss = 0.99557177\n",
      "Iteration 202, loss = 0.99387961\n",
      "Iteration 203, loss = 0.99212031\n",
      "Iteration 204, loss = 0.98929372\n",
      "Iteration 205, loss = 0.98581933\n",
      "Iteration 206, loss = 0.98222776\n",
      "Iteration 207, loss = 0.98073219\n",
      "Iteration 208, loss = 0.97863108\n",
      "Iteration 209, loss = 0.97773996\n",
      "Iteration 210, loss = 0.97425385\n",
      "Iteration 211, loss = 0.97026283\n",
      "Iteration 212, loss = 0.96667095\n",
      "Iteration 213, loss = 0.96598411\n",
      "Iteration 214, loss = 0.96421358\n",
      "Iteration 215, loss = 0.96162874\n",
      "Iteration 216, loss = 0.95854907\n",
      "Iteration 217, loss = 0.95507297\n",
      "Iteration 218, loss = 0.95298530\n",
      "Iteration 219, loss = 0.95040256\n",
      "Iteration 220, loss = 0.94806433\n",
      "Iteration 221, loss = 0.94586940\n",
      "Iteration 222, loss = 0.94359838\n",
      "Iteration 223, loss = 0.94088640\n",
      "Iteration 224, loss = 0.93845785\n",
      "Iteration 225, loss = 0.93606620\n",
      "Iteration 226, loss = 0.93388843\n",
      "Iteration 227, loss = 0.93167955\n",
      "Iteration 228, loss = 0.92972519\n",
      "Iteration 229, loss = 0.92856809\n",
      "Iteration 230, loss = 0.92543193\n",
      "Iteration 231, loss = 0.92178066\n",
      "Iteration 232, loss = 0.91855707\n",
      "Iteration 233, loss = 0.91910875\n",
      "Iteration 234, loss = 0.91605376\n",
      "Iteration 235, loss = 0.91452970\n",
      "Iteration 236, loss = 0.91273008\n",
      "Iteration 237, loss = 0.90979048\n",
      "Iteration 238, loss = 0.90552580\n",
      "Iteration 239, loss = 0.90346414\n",
      "Iteration 240, loss = 0.89970748\n",
      "Iteration 241, loss = 0.89869712\n",
      "Iteration 242, loss = 0.89686155\n",
      "Iteration 243, loss = 0.89619145\n",
      "Iteration 244, loss = 0.89471624\n",
      "Iteration 245, loss = 0.89231634\n",
      "Iteration 246, loss = 0.88885475\n",
      "Iteration 247, loss = 0.88474850\n",
      "Iteration 248, loss = 0.88047206\n",
      "Iteration 249, loss = 0.87942407\n",
      "Iteration 250, loss = 0.87605947\n",
      "Iteration 251, loss = 0.87402873\n",
      "Iteration 252, loss = 0.87204424\n",
      "Iteration 253, loss = 0.86983705\n",
      "Iteration 254, loss = 0.86768779\n",
      "Iteration 255, loss = 0.86505057\n",
      "Iteration 256, loss = 0.86331972\n",
      "Iteration 257, loss = 0.86148449\n",
      "Iteration 258, loss = 0.85959637\n",
      "Iteration 259, loss = 0.85754533\n",
      "Iteration 260, loss = 0.85410917\n",
      "Iteration 261, loss = 0.85187235\n",
      "Iteration 262, loss = 0.84999329\n",
      "Iteration 263, loss = 0.84745869\n",
      "Iteration 264, loss = 0.84561724\n",
      "Iteration 265, loss = 0.84383667\n",
      "Iteration 266, loss = 0.84229728\n",
      "Iteration 267, loss = 0.84091714\n",
      "Iteration 268, loss = 0.83882045\n",
      "Iteration 269, loss = 0.83585737\n",
      "Iteration 270, loss = 0.83295831\n",
      "Iteration 271, loss = 0.83076367\n",
      "Iteration 272, loss = 0.82875880\n",
      "Iteration 273, loss = 0.82630161\n",
      "Iteration 274, loss = 0.82392594\n",
      "Iteration 275, loss = 0.82186840\n",
      "Iteration 276, loss = 0.82007756\n",
      "Iteration 277, loss = 0.81824596\n",
      "Iteration 278, loss = 0.81602601\n",
      "Iteration 279, loss = 0.81388081\n",
      "Iteration 280, loss = 0.81171913\n",
      "Iteration 281, loss = 0.80949434\n",
      "Iteration 282, loss = 0.80798530\n",
      "Iteration 283, loss = 0.80555650\n",
      "Iteration 284, loss = 0.80326519\n",
      "Iteration 285, loss = 0.80114840\n",
      "Iteration 286, loss = 0.79959767\n",
      "Iteration 287, loss = 0.79763211\n",
      "Iteration 288, loss = 0.79596670\n",
      "Iteration 289, loss = 0.79431538\n",
      "Iteration 290, loss = 0.79249545\n",
      "Iteration 291, loss = 0.79074250\n",
      "Iteration 292, loss = 0.78912066\n",
      "Iteration 293, loss = 0.78885112\n",
      "Iteration 294, loss = 0.78829229\n",
      "Iteration 295, loss = 0.78645439\n",
      "Iteration 296, loss = 0.78348480\n",
      "Iteration 297, loss = 0.77870577\n",
      "Iteration 298, loss = 0.77658554\n",
      "Iteration 299, loss = 0.77386542\n",
      "Iteration 300, loss = 0.77306357\n",
      "Iteration 301, loss = 0.77142171\n",
      "Iteration 302, loss = 0.76946034\n",
      "Iteration 303, loss = 0.76852464\n",
      "Iteration 304, loss = 0.76681368\n",
      "Iteration 305, loss = 0.76453410\n",
      "Iteration 306, loss = 0.76235956\n",
      "Iteration 307, loss = 0.76033977\n",
      "Iteration 308, loss = 0.75831622\n",
      "Iteration 309, loss = 0.75621980\n",
      "Iteration 310, loss = 0.75502176\n",
      "Iteration 311, loss = 0.75266247\n",
      "Iteration 312, loss = 0.75084458\n",
      "Iteration 313, loss = 0.74935925\n",
      "Iteration 314, loss = 0.74775627\n",
      "Iteration 315, loss = 0.74600290\n",
      "Iteration 316, loss = 0.74442131\n",
      "Iteration 317, loss = 0.74255219\n",
      "Iteration 318, loss = 0.74114522\n",
      "Iteration 319, loss = 0.74042233\n",
      "Iteration 320, loss = 0.73959561\n",
      "Iteration 321, loss = 0.73801654\n",
      "Iteration 322, loss = 0.73636312\n",
      "Iteration 323, loss = 0.73395710\n",
      "Iteration 324, loss = 0.73178456\n",
      "Iteration 325, loss = 0.73012294\n",
      "Iteration 326, loss = 0.72774752\n",
      "Iteration 327, loss = 0.72576941\n",
      "Iteration 328, loss = 0.72423714\n",
      "Iteration 329, loss = 0.72328531\n",
      "Iteration 330, loss = 0.72195577\n",
      "Iteration 331, loss = 0.72035523\n",
      "Iteration 332, loss = 0.71876295\n",
      "Iteration 333, loss = 0.71723692\n",
      "Iteration 334, loss = 0.71448824\n",
      "Iteration 335, loss = 0.71243365\n",
      "Iteration 336, loss = 0.71149552\n",
      "Iteration 337, loss = 0.70928694\n",
      "Iteration 338, loss = 0.70781200\n",
      "Iteration 339, loss = 0.70601475\n",
      "Iteration 340, loss = 0.70457669\n",
      "Iteration 341, loss = 0.70301471\n",
      "Iteration 342, loss = 0.70088367\n",
      "Iteration 343, loss = 0.69915705\n",
      "Iteration 344, loss = 0.69752243\n",
      "Iteration 345, loss = 0.69609919\n",
      "Iteration 346, loss = 0.69560021\n",
      "Iteration 347, loss = 0.69562520\n",
      "Iteration 348, loss = 0.69322646\n",
      "Iteration 349, loss = 0.69012976\n",
      "Iteration 350, loss = 0.68759450\n",
      "Iteration 351, loss = 0.68663511\n",
      "Iteration 352, loss = 0.68612885\n",
      "Iteration 353, loss = 0.68625018\n",
      "Iteration 354, loss = 0.68618568\n",
      "Iteration 355, loss = 0.68610931\n",
      "Iteration 356, loss = 0.68364256\n",
      "Iteration 357, loss = 0.67961328\n",
      "Iteration 358, loss = 0.67600234\n",
      "Iteration 359, loss = 0.67281192\n",
      "Iteration 360, loss = 0.67455079\n",
      "Iteration 361, loss = 0.67424549\n",
      "Iteration 362, loss = 0.67295543\n",
      "Iteration 363, loss = 0.66991326\n",
      "Iteration 364, loss = 0.66743775\n",
      "Iteration 365, loss = 0.66495027\n",
      "Iteration 366, loss = 0.66322129\n",
      "Iteration 367, loss = 0.66181433\n",
      "Iteration 368, loss = 0.66052267\n",
      "Iteration 369, loss = 0.65880914\n",
      "Iteration 370, loss = 0.65749990\n",
      "Iteration 371, loss = 0.65659097\n",
      "Iteration 372, loss = 0.65501783\n",
      "Iteration 373, loss = 0.65347946\n",
      "Iteration 374, loss = 0.65213057\n",
      "Iteration 375, loss = 0.65136029\n",
      "Iteration 376, loss = 0.65013368\n",
      "Iteration 377, loss = 0.64864964\n",
      "Iteration 378, loss = 0.64694488\n",
      "Iteration 379, loss = 0.64555509\n",
      "Iteration 380, loss = 0.64426879\n",
      "Iteration 381, loss = 0.64386652\n",
      "Iteration 382, loss = 0.64385477\n",
      "Iteration 383, loss = 0.64324892\n",
      "Iteration 384, loss = 0.64111765\n",
      "Iteration 385, loss = 0.63835954\n",
      "Iteration 386, loss = 0.63695140\n",
      "Iteration 387, loss = 0.63494104\n",
      "Iteration 388, loss = 0.63374840\n",
      "Iteration 389, loss = 0.63247842\n",
      "Iteration 390, loss = 0.63127759\n",
      "Iteration 391, loss = 0.63022200\n",
      "Iteration 392, loss = 0.62979421\n",
      "Iteration 393, loss = 0.63092782\n",
      "Iteration 394, loss = 0.63115099\n",
      "Iteration 395, loss = 0.62972090\n",
      "Iteration 396, loss = 0.62690048\n",
      "Iteration 397, loss = 0.62327456\n",
      "Iteration 398, loss = 0.62083748\n",
      "Iteration 399, loss = 0.62009688\n",
      "Iteration 400, loss = 0.61906965\n",
      "Iteration 401, loss = 0.61758585\n",
      "Iteration 402, loss = 0.61617770\n",
      "Iteration 403, loss = 0.61502753\n",
      "Iteration 404, loss = 0.61366867\n",
      "Iteration 405, loss = 0.61249401\n",
      "Iteration 406, loss = 0.61143594\n",
      "Iteration 407, loss = 0.61081226\n",
      "Iteration 408, loss = 0.60915926\n",
      "Iteration 409, loss = 0.60787886\n",
      "Iteration 410, loss = 0.60661615\n",
      "Iteration 411, loss = 0.60645533\n",
      "Iteration 412, loss = 0.60556313\n",
      "Iteration 413, loss = 0.60466581\n",
      "Iteration 414, loss = 0.60363837\n",
      "Iteration 415, loss = 0.60314478\n",
      "Iteration 416, loss = 0.60213571\n",
      "Iteration 417, loss = 0.60064456\n",
      "Iteration 418, loss = 0.59865593\n",
      "Iteration 419, loss = 0.59702632\n",
      "Iteration 420, loss = 0.59530220\n",
      "Iteration 421, loss = 0.59591719\n",
      "Iteration 422, loss = 0.59528186\n",
      "Iteration 423, loss = 0.59375792\n",
      "Iteration 424, loss = 0.59107583\n",
      "Iteration 425, loss = 0.59038450\n",
      "Iteration 426, loss = 0.59057533\n",
      "Iteration 427, loss = 0.58945808\n",
      "Iteration 428, loss = 0.58819592\n",
      "Iteration 429, loss = 0.58612620\n",
      "Iteration 430, loss = 0.58458707\n",
      "Iteration 431, loss = 0.58554191\n",
      "Iteration 432, loss = 0.58863927\n",
      "Iteration 433, loss = 0.58709848\n",
      "Iteration 434, loss = 0.58368749\n",
      "Iteration 435, loss = 0.58065699\n",
      "Iteration 436, loss = 0.57907883\n",
      "Iteration 437, loss = 0.57901392\n",
      "Iteration 438, loss = 0.57903359\n",
      "Iteration 439, loss = 0.57933580\n",
      "Iteration 440, loss = 0.57903075\n",
      "Iteration 441, loss = 0.57726310\n",
      "Iteration 442, loss = 0.57588643\n",
      "Iteration 443, loss = 0.57361414\n",
      "Iteration 444, loss = 0.57253723\n",
      "Iteration 445, loss = 0.57158942\n",
      "Iteration 446, loss = 0.57215557\n",
      "Iteration 447, loss = 0.57162596\n",
      "Iteration 448, loss = 0.57106481\n",
      "Iteration 449, loss = 0.56893312\n",
      "Iteration 450, loss = 0.56817644\n",
      "Iteration 451, loss = 0.56747348\n",
      "Iteration 452, loss = 0.56644936\n",
      "Iteration 453, loss = 0.56529830\n",
      "Iteration 454, loss = 0.56493891\n",
      "Iteration 455, loss = 0.56405734\n",
      "Iteration 456, loss = 0.56334974\n",
      "Iteration 457, loss = 0.56268972\n",
      "Iteration 458, loss = 0.56196983\n",
      "Iteration 459, loss = 0.56090115\n",
      "Iteration 460, loss = 0.56009304\n",
      "Iteration 461, loss = 0.55925325\n",
      "Iteration 462, loss = 0.55851793\n",
      "Iteration 463, loss = 0.55843259\n",
      "Iteration 464, loss = 0.55918885\n",
      "Iteration 465, loss = 0.55969821\n",
      "Iteration 466, loss = 0.55983209\n",
      "Iteration 467, loss = 0.55879148\n",
      "Iteration 468, loss = 0.55615446\n",
      "Iteration 469, loss = 0.55465628\n",
      "Iteration 470, loss = 0.55381482\n",
      "Iteration 471, loss = 0.55341817\n",
      "Iteration 472, loss = 0.55269551\n",
      "Iteration 473, loss = 0.55254395\n",
      "Iteration 474, loss = 0.55168425\n",
      "Iteration 475, loss = 0.55202594\n",
      "Iteration 476, loss = 0.55017844\n",
      "Iteration 477, loss = 0.54901776\n",
      "Iteration 478, loss = 0.55050724\n",
      "Iteration 479, loss = 0.55355631\n",
      "Iteration 480, loss = 0.55491687\n",
      "Iteration 481, loss = 0.55368169\n",
      "Iteration 482, loss = 0.55074757\n",
      "Iteration 483, loss = 0.54885451\n",
      "Iteration 484, loss = 0.54660811\n",
      "Iteration 485, loss = 0.54614179\n",
      "Iteration 486, loss = 0.54505342\n",
      "Iteration 487, loss = 0.54471371\n",
      "Iteration 488, loss = 0.54497435\n",
      "Iteration 489, loss = 0.54399167\n",
      "Iteration 490, loss = 0.54391879\n",
      "Iteration 491, loss = 0.54268356\n",
      "Iteration 492, loss = 0.54231971\n",
      "Iteration 493, loss = 0.54184115\n",
      "Iteration 494, loss = 0.54127669\n",
      "Iteration 495, loss = 0.54096890\n",
      "Iteration 496, loss = 0.54034373\n",
      "Iteration 497, loss = 0.53989818\n",
      "Iteration 498, loss = 0.53933473\n",
      "Iteration 499, loss = 0.53900579\n",
      "Iteration 500, loss = 0.53852930\n",
      "Iteration 501, loss = 0.53810179\n",
      "Iteration 502, loss = 0.53754010\n",
      "Iteration 503, loss = 0.53712826\n",
      "Iteration 504, loss = 0.53643276\n",
      "Iteration 505, loss = 0.53593345\n",
      "Iteration 506, loss = 0.53537281\n",
      "Iteration 507, loss = 0.53602664\n",
      "Iteration 508, loss = 0.53656550\n",
      "Iteration 509, loss = 0.53609980\n",
      "Iteration 510, loss = 0.53468632\n",
      "Iteration 511, loss = 0.53427859\n",
      "Iteration 512, loss = 0.53396466\n",
      "Iteration 513, loss = 0.53339161\n",
      "Iteration 514, loss = 0.53158884\n",
      "Iteration 515, loss = 0.53271802\n",
      "Iteration 516, loss = 0.53618139\n",
      "Iteration 517, loss = 0.53819343\n",
      "Iteration 518, loss = 0.53611055\n",
      "Iteration 519, loss = 0.53290402\n",
      "Iteration 520, loss = 0.53027414\n",
      "Iteration 521, loss = 0.53088200\n",
      "Iteration 522, loss = 0.53043984\n",
      "Iteration 523, loss = 0.52991653\n",
      "Iteration 524, loss = 0.52983609\n",
      "Iteration 525, loss = 0.52885129\n",
      "Iteration 526, loss = 0.52866112\n",
      "Iteration 527, loss = 0.52845784\n",
      "Iteration 528, loss = 0.52847286\n",
      "Iteration 529, loss = 0.52833726\n",
      "Iteration 530, loss = 0.52746334\n",
      "Iteration 531, loss = 0.52727791\n",
      "Iteration 532, loss = 0.52711208\n",
      "Iteration 533, loss = 0.52674198\n",
      "Iteration 534, loss = 0.52651420\n",
      "Iteration 535, loss = 0.52642600\n",
      "Iteration 536, loss = 0.52646227\n",
      "Iteration 537, loss = 0.52642156\n",
      "Iteration 538, loss = 0.52646763\n",
      "Iteration 539, loss = 0.52628098\n",
      "Iteration 540, loss = 0.52539068\n",
      "Iteration 541, loss = 0.52500717\n",
      "Iteration 542, loss = 0.52478864\n",
      "Iteration 543, loss = 0.52443750\n",
      "Iteration 544, loss = 0.52511961\n",
      "Iteration 545, loss = 0.52497066\n",
      "Iteration 546, loss = 0.52431991\n",
      "Iteration 547, loss = 0.52384623\n",
      "Iteration 548, loss = 0.52279905\n",
      "Iteration 549, loss = 0.52291636\n",
      "Iteration 550, loss = 0.52378159\n",
      "Iteration 551, loss = 0.52505921\n",
      "Iteration 552, loss = 0.52400903\n",
      "Iteration 553, loss = 0.52284474\n",
      "Iteration 554, loss = 0.52183478\n",
      "Iteration 555, loss = 0.52304491\n",
      "Iteration 556, loss = 0.52481153\n",
      "Iteration 557, loss = 0.52430789\n",
      "Iteration 558, loss = 0.52281347\n",
      "Iteration 559, loss = 0.52092237\n",
      "Iteration 560, loss = 0.52065505\n",
      "Iteration 561, loss = 0.52305092\n",
      "Iteration 562, loss = 0.52454386\n",
      "Iteration 563, loss = 0.52399073\n",
      "Iteration 564, loss = 0.52201432\n",
      "Iteration 565, loss = 0.52022025\n",
      "Iteration 566, loss = 0.52179313\n",
      "Iteration 567, loss = 0.52059158\n",
      "Iteration 568, loss = 0.52029113\n",
      "Iteration 569, loss = 0.51993733\n",
      "Iteration 570, loss = 0.51999253\n",
      "Iteration 571, loss = 0.51922682\n",
      "Iteration 572, loss = 0.51925305\n",
      "Iteration 573, loss = 0.52102046\n",
      "Iteration 574, loss = 0.52143047\n",
      "Iteration 575, loss = 0.52029699\n",
      "Iteration 576, loss = 0.51928691\n",
      "Iteration 577, loss = 0.51850649\n",
      "Iteration 578, loss = 0.51949689\n",
      "Iteration 579, loss = 0.52006846\n",
      "Iteration 580, loss = 0.51918285\n",
      "Iteration 581, loss = 0.51805647\n",
      "Iteration 582, loss = 0.51936683\n",
      "Iteration 583, loss = 0.52068133\n",
      "Iteration 584, loss = 0.52139749\n",
      "Iteration 585, loss = 0.52073524\n",
      "Iteration 586, loss = 0.51837494\n",
      "Iteration 587, loss = 0.51694831\n",
      "Iteration 588, loss = 0.51707179\n",
      "Iteration 589, loss = 0.52098446\n",
      "Iteration 590, loss = 0.52506363\n",
      "Iteration 591, loss = 0.52292530\n",
      "Iteration 592, loss = 0.51892578\n",
      "Iteration 593, loss = 0.51707535\n",
      "Iteration 594, loss = 0.51759683\n",
      "Iteration 595, loss = 0.51733717\n",
      "Iteration 596, loss = 0.51669867\n",
      "Iteration 597, loss = 0.51618271\n",
      "Iteration 598, loss = 0.51635857\n",
      "Iteration 599, loss = 0.51887040\n",
      "Iteration 600, loss = 0.51908403\n",
      "Iteration 601, loss = 0.51605589\n",
      "Iteration 602, loss = 0.51582248\n",
      "Iteration 603, loss = 0.52109049\n",
      "Iteration 604, loss = 0.52208274\n",
      "Iteration 605, loss = 0.52022522\n",
      "Iteration 606, loss = 0.51862243\n",
      "Iteration 607, loss = 0.51646444\n",
      "Iteration 608, loss = 0.51605761\n",
      "Iteration 609, loss = 0.51611841\n",
      "Iteration 610, loss = 0.51740705\n",
      "Iteration 611, loss = 0.51705259\n",
      "Iteration 612, loss = 0.51581537\n",
      "Iteration 613, loss = 0.51498758\n",
      "Iteration 614, loss = 0.51558785\n",
      "Iteration 615, loss = 0.51565960\n",
      "Iteration 616, loss = 0.51562807\n",
      "Iteration 617, loss = 0.51551350\n",
      "Iteration 618, loss = 0.51528298\n",
      "Iteration 619, loss = 0.51523781\n",
      "Iteration 620, loss = 0.51535827\n",
      "Iteration 621, loss = 0.51521907\n",
      "Iteration 622, loss = 0.51461915\n",
      "Iteration 623, loss = 0.51427875\n",
      "Iteration 624, loss = 0.51430354\n",
      "Iteration 625, loss = 0.51545912\n",
      "Iteration 626, loss = 0.51680315\n",
      "Iteration 627, loss = 0.51705666\n",
      "Iteration 628, loss = 0.51644923\n",
      "Iteration 629, loss = 0.51467103\n",
      "Iteration 630, loss = 0.51349817\n",
      "Iteration 631, loss = 0.51460743\n",
      "Iteration 632, loss = 0.51677818\n",
      "Iteration 633, loss = 0.51896579\n",
      "Iteration 634, loss = 0.51764268\n",
      "Iteration 635, loss = 0.51425761\n",
      "Iteration 636, loss = 0.51340744\n",
      "Iteration 637, loss = 0.51390199\n",
      "Iteration 638, loss = 0.51474847\n",
      "Iteration 639, loss = 0.51394080\n",
      "Iteration 640, loss = 0.51320423\n",
      "Iteration 641, loss = 0.51368548\n",
      "Iteration 642, loss = 0.51407991\n",
      "Iteration 643, loss = 0.51420154\n",
      "Iteration 644, loss = 0.51394750\n",
      "Iteration 645, loss = 0.51319833\n",
      "Iteration 646, loss = 0.51310626\n",
      "Iteration 647, loss = 0.51305791\n",
      "Iteration 648, loss = 0.51394217\n",
      "Iteration 649, loss = 0.51419901\n",
      "Iteration 650, loss = 0.51324150\n",
      "Iteration 651, loss = 0.51318613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.83734980\n",
      "Iteration 2, loss = 0.82614945\n",
      "Iteration 3, loss = 0.81677280\n",
      "Iteration 4, loss = 0.80786002\n",
      "Iteration 5, loss = 0.79762755\n",
      "Iteration 6, loss = 0.79051130\n",
      "Iteration 7, loss = 0.78084603\n",
      "Iteration 8, loss = 0.77241512\n",
      "Iteration 9, loss = 0.76306664\n",
      "Iteration 10, loss = 0.75478629\n",
      "Iteration 11, loss = 0.74726934\n",
      "Iteration 12, loss = 0.74001430\n",
      "Iteration 13, loss = 0.73054520\n",
      "Iteration 14, loss = 0.72089177\n",
      "Iteration 15, loss = 0.71070232\n",
      "Iteration 16, loss = 0.70213881\n",
      "Iteration 17, loss = 0.69299577\n",
      "Iteration 18, loss = 0.68480378\n",
      "Iteration 19, loss = 0.67626340\n",
      "Iteration 20, loss = 0.66918657\n",
      "Iteration 21, loss = 0.66140669\n",
      "Iteration 22, loss = 0.65620983\n",
      "Iteration 23, loss = 0.65119296\n",
      "Iteration 24, loss = 0.64776220\n",
      "Iteration 25, loss = 0.64496181\n",
      "Iteration 26, loss = 0.64267516\n",
      "Iteration 27, loss = 0.64129823\n",
      "Iteration 28, loss = 0.63976611\n",
      "Iteration 29, loss = 0.63879963\n",
      "Iteration 30, loss = 0.63742858\n",
      "Iteration 31, loss = 0.63612662\n",
      "Iteration 32, loss = 0.63461276\n",
      "Iteration 33, loss = 0.63312911\n",
      "Iteration 34, loss = 0.63172283\n",
      "Iteration 35, loss = 0.63024136\n",
      "Iteration 36, loss = 0.62880597\n",
      "Iteration 37, loss = 0.62746833\n",
      "Iteration 38, loss = 0.62623851\n",
      "Iteration 39, loss = 0.62500015\n",
      "Iteration 40, loss = 0.62379008\n",
      "Iteration 41, loss = 0.62235126\n",
      "Iteration 42, loss = 0.62113299\n",
      "Iteration 43, loss = 0.61976806\n",
      "Iteration 44, loss = 0.61863663\n",
      "Iteration 45, loss = 0.61718021\n",
      "Iteration 46, loss = 0.61581332\n",
      "Iteration 47, loss = 0.61459012\n",
      "Iteration 48, loss = 0.61328224\n",
      "Iteration 49, loss = 0.61172942\n",
      "Iteration 50, loss = 0.61046296\n",
      "Iteration 51, loss = 0.60896412\n",
      "Iteration 52, loss = 0.60753462\n",
      "Iteration 53, loss = 0.60616596\n",
      "Iteration 54, loss = 0.60511136\n",
      "Iteration 55, loss = 0.60398450\n",
      "Iteration 56, loss = 0.60319337\n",
      "Iteration 57, loss = 0.60227894\n",
      "Iteration 58, loss = 0.60156415\n",
      "Iteration 59, loss = 0.60028578\n",
      "Iteration 60, loss = 0.59864497\n",
      "Iteration 61, loss = 0.59705458\n",
      "Iteration 62, loss = 0.59509295\n",
      "Iteration 63, loss = 0.59462264\n",
      "Iteration 64, loss = 0.59306957\n",
      "Iteration 65, loss = 0.59163593\n",
      "Iteration 66, loss = 0.59066782\n",
      "Iteration 67, loss = 0.58953621\n",
      "Iteration 68, loss = 0.58853481\n",
      "Iteration 69, loss = 0.58746738\n",
      "Iteration 70, loss = 0.58644257\n",
      "Iteration 71, loss = 0.58560043\n",
      "Iteration 72, loss = 0.58484803\n",
      "Iteration 73, loss = 0.58393604\n",
      "Iteration 74, loss = 0.58329235\n",
      "Iteration 75, loss = 0.58243168\n",
      "Iteration 76, loss = 0.58146232\n",
      "Iteration 77, loss = 0.58041804\n",
      "Iteration 78, loss = 0.57966626\n",
      "Iteration 79, loss = 0.57881504\n",
      "Iteration 80, loss = 0.57812462\n",
      "Iteration 81, loss = 0.57727308\n",
      "Iteration 82, loss = 0.57651671\n",
      "Iteration 83, loss = 0.57574458\n",
      "Iteration 84, loss = 0.57504436\n",
      "Iteration 85, loss = 0.57422347\n",
      "Iteration 86, loss = 0.57350421\n",
      "Iteration 87, loss = 0.57284375\n",
      "Iteration 88, loss = 0.57212310\n",
      "Iteration 89, loss = 0.57154482\n",
      "Iteration 90, loss = 0.57081603\n",
      "Iteration 91, loss = 0.57011927\n",
      "Iteration 92, loss = 0.56946538\n",
      "Iteration 93, loss = 0.56889184\n",
      "Iteration 94, loss = 0.56836332\n",
      "Iteration 95, loss = 0.56776185\n",
      "Iteration 96, loss = 0.56711398\n",
      "Iteration 97, loss = 0.56651677\n",
      "Iteration 98, loss = 0.56589902\n",
      "Iteration 99, loss = 0.56522275\n",
      "Iteration 100, loss = 0.56447381\n",
      "Iteration 101, loss = 0.56372958\n",
      "Iteration 102, loss = 0.56271951\n",
      "Iteration 103, loss = 0.56206894\n",
      "Iteration 104, loss = 0.56148146\n",
      "Iteration 105, loss = 0.56066463\n",
      "Iteration 106, loss = 0.56001755\n",
      "Iteration 107, loss = 0.55940602\n",
      "Iteration 108, loss = 0.55868966\n",
      "Iteration 109, loss = 0.55808191\n",
      "Iteration 110, loss = 0.55743223\n",
      "Iteration 111, loss = 0.55685134\n",
      "Iteration 112, loss = 0.55618865\n",
      "Iteration 113, loss = 0.55564476\n",
      "Iteration 114, loss = 0.55505440\n",
      "Iteration 115, loss = 0.55444643\n",
      "Iteration 116, loss = 0.55388774\n",
      "Iteration 117, loss = 0.55330729\n",
      "Iteration 118, loss = 0.55275593\n",
      "Iteration 119, loss = 0.55223791\n",
      "Iteration 120, loss = 0.55167696\n",
      "Iteration 121, loss = 0.55106222\n",
      "Iteration 122, loss = 0.55063596\n",
      "Iteration 123, loss = 0.55034759\n",
      "Iteration 124, loss = 0.55008162\n",
      "Iteration 125, loss = 0.54979031\n",
      "Iteration 126, loss = 0.54946103\n",
      "Iteration 127, loss = 0.54906047\n",
      "Iteration 128, loss = 0.54863195\n",
      "Iteration 129, loss = 0.54813003\n",
      "Iteration 130, loss = 0.54760673\n",
      "Iteration 131, loss = 0.54704076\n",
      "Iteration 132, loss = 0.54646701\n",
      "Iteration 133, loss = 0.54586581\n",
      "Iteration 134, loss = 0.54512720\n",
      "Iteration 135, loss = 0.54465291\n",
      "Iteration 136, loss = 0.54432393\n",
      "Iteration 137, loss = 0.54360373\n",
      "Iteration 138, loss = 0.54315000\n",
      "Iteration 139, loss = 0.54293104\n",
      "Iteration 140, loss = 0.54245969\n",
      "Iteration 141, loss = 0.54209885\n",
      "Iteration 142, loss = 0.54168609\n",
      "Iteration 143, loss = 0.54130415\n",
      "Iteration 144, loss = 0.54080762\n",
      "Iteration 145, loss = 0.54026467\n",
      "Iteration 146, loss = 0.53968511\n",
      "Iteration 147, loss = 0.53926059\n",
      "Iteration 148, loss = 0.53920771\n",
      "Iteration 149, loss = 0.53869732\n",
      "Iteration 150, loss = 0.53843073\n",
      "Iteration 151, loss = 0.53844689\n",
      "Iteration 152, loss = 0.53822745\n",
      "Iteration 153, loss = 0.53802175\n",
      "Iteration 154, loss = 0.53818379\n",
      "Iteration 155, loss = 0.53780740\n",
      "Iteration 156, loss = 0.53734413\n",
      "Iteration 157, loss = 0.53686650\n",
      "Iteration 158, loss = 0.53640508\n",
      "Iteration 159, loss = 0.53586488\n",
      "Iteration 160, loss = 0.53528368\n",
      "Iteration 161, loss = 0.53483163\n",
      "Iteration 162, loss = 0.53450233\n",
      "Iteration 163, loss = 0.53418665\n",
      "Iteration 164, loss = 0.53395163\n",
      "Iteration 165, loss = 0.53373311\n",
      "Iteration 166, loss = 0.53359778\n",
      "Iteration 167, loss = 0.53334537\n",
      "Iteration 168, loss = 0.53305820\n",
      "Iteration 169, loss = 0.53275598\n",
      "Iteration 170, loss = 0.53240193\n",
      "Iteration 171, loss = 0.53212219\n",
      "Iteration 172, loss = 0.53174758\n",
      "Iteration 173, loss = 0.53150664\n",
      "Iteration 174, loss = 0.53123016\n",
      "Iteration 175, loss = 0.53103328\n",
      "Iteration 176, loss = 0.53074322\n",
      "Iteration 177, loss = 0.53051704\n",
      "Iteration 178, loss = 0.53033430\n",
      "Iteration 179, loss = 0.53031446\n",
      "Iteration 180, loss = 0.52994061\n",
      "Iteration 181, loss = 0.52961390\n",
      "Iteration 182, loss = 0.52938382\n",
      "Iteration 183, loss = 0.52923907\n",
      "Iteration 184, loss = 0.52920072\n",
      "Iteration 185, loss = 0.52889395\n",
      "Iteration 186, loss = 0.52868984\n",
      "Iteration 187, loss = 0.52854126\n",
      "Iteration 188, loss = 0.52831725\n",
      "Iteration 189, loss = 0.52814937\n",
      "Iteration 190, loss = 0.52801382\n",
      "Iteration 191, loss = 0.52786662\n",
      "Iteration 192, loss = 0.52770470\n",
      "Iteration 193, loss = 0.52759765\n",
      "Iteration 194, loss = 0.52752155\n",
      "Iteration 195, loss = 0.52742011\n",
      "Iteration 196, loss = 0.52734626\n",
      "Iteration 197, loss = 0.52722873\n",
      "Iteration 198, loss = 0.52704307\n",
      "Iteration 199, loss = 0.52697254\n",
      "Iteration 200, loss = 0.52669200\n",
      "Iteration 201, loss = 0.52652900\n",
      "Iteration 202, loss = 0.52640163\n",
      "Iteration 203, loss = 0.52630235\n",
      "Iteration 204, loss = 0.52620400\n",
      "Iteration 205, loss = 0.52622197\n",
      "Iteration 206, loss = 0.52615487\n",
      "Iteration 207, loss = 0.52604023\n",
      "Iteration 208, loss = 0.52587636\n",
      "Iteration 209, loss = 0.52574109\n",
      "Iteration 210, loss = 0.52563083\n",
      "Iteration 211, loss = 0.52547723\n",
      "Iteration 212, loss = 0.52527356\n",
      "Iteration 213, loss = 0.52512809\n",
      "Iteration 214, loss = 0.52496422\n",
      "Iteration 215, loss = 0.52483313\n",
      "Iteration 216, loss = 0.52460969\n",
      "Iteration 217, loss = 0.52449312\n",
      "Iteration 218, loss = 0.52429561\n",
      "Iteration 219, loss = 0.52420960\n",
      "Iteration 220, loss = 0.52410113\n",
      "Iteration 221, loss = 0.52395429\n",
      "Iteration 222, loss = 0.52389162\n",
      "Iteration 223, loss = 0.52376601\n",
      "Iteration 224, loss = 0.52369504\n",
      "Iteration 225, loss = 0.52365398\n",
      "Iteration 226, loss = 0.52363731\n",
      "Iteration 227, loss = 0.52360972\n",
      "Iteration 228, loss = 0.52362645\n",
      "Iteration 229, loss = 0.52334533\n",
      "Iteration 230, loss = 0.52328939\n",
      "Iteration 231, loss = 0.52315261\n",
      "Iteration 232, loss = 0.52312948\n",
      "Iteration 233, loss = 0.52317411\n",
      "Iteration 234, loss = 0.52325801\n",
      "Iteration 235, loss = 0.52337314\n",
      "Iteration 236, loss = 0.52353325\n",
      "Iteration 237, loss = 0.52355431\n",
      "Iteration 238, loss = 0.52349828\n",
      "Iteration 239, loss = 0.52346443\n",
      "Iteration 240, loss = 0.52329838\n",
      "Iteration 241, loss = 0.52315047\n",
      "Iteration 242, loss = 0.52284581\n",
      "Iteration 243, loss = 0.52266983\n",
      "Iteration 244, loss = 0.52233058\n",
      "Iteration 245, loss = 0.52223931\n",
      "Iteration 246, loss = 0.52205231\n",
      "Iteration 247, loss = 0.52187061\n",
      "Iteration 248, loss = 0.52184269\n",
      "Iteration 249, loss = 0.52174891\n",
      "Iteration 250, loss = 0.52174853\n",
      "Iteration 251, loss = 0.52173913\n",
      "Iteration 252, loss = 0.52139181\n",
      "Iteration 253, loss = 0.52096919\n",
      "Iteration 254, loss = 0.52080598\n",
      "Iteration 255, loss = 0.52020497\n",
      "Iteration 256, loss = 0.52014058\n",
      "Iteration 257, loss = 0.52034644\n",
      "Iteration 258, loss = 0.52061259\n",
      "Iteration 259, loss = 0.52007799\n",
      "Iteration 260, loss = 0.51987666\n",
      "Iteration 261, loss = 0.52002638\n",
      "Iteration 262, loss = 0.51998331\n",
      "Iteration 263, loss = 0.51998755\n",
      "Iteration 264, loss = 0.51996953\n",
      "Iteration 265, loss = 0.51993039\n",
      "Iteration 266, loss = 0.51990188\n",
      "Iteration 267, loss = 0.51986086\n",
      "Iteration 268, loss = 0.51980166\n",
      "Iteration 269, loss = 0.51973756\n",
      "Iteration 270, loss = 0.51966634\n",
      "Iteration 271, loss = 0.51956186\n",
      "Iteration 272, loss = 0.51938835\n",
      "Iteration 273, loss = 0.51932254\n",
      "Iteration 274, loss = 0.51922817\n",
      "Iteration 275, loss = 0.51913376\n",
      "Iteration 276, loss = 0.51903705\n",
      "Iteration 277, loss = 0.51897313\n",
      "Iteration 278, loss = 0.51888755\n",
      "Iteration 279, loss = 0.51885569\n",
      "Iteration 280, loss = 0.51876111\n",
      "Iteration 281, loss = 0.51865056\n",
      "Iteration 282, loss = 0.51857171\n",
      "Iteration 283, loss = 0.51852690\n",
      "Iteration 284, loss = 0.51835913\n",
      "Iteration 285, loss = 0.51831534\n",
      "Iteration 286, loss = 0.51828969\n",
      "Iteration 287, loss = 0.51821958\n",
      "Iteration 288, loss = 0.51822848\n",
      "Iteration 289, loss = 0.51811515\n",
      "Iteration 290, loss = 0.51808172\n",
      "Iteration 291, loss = 0.51812028\n",
      "Iteration 292, loss = 0.51808506\n",
      "Iteration 293, loss = 0.51822162\n",
      "Iteration 294, loss = 0.51825153\n",
      "Iteration 295, loss = 0.51849346\n",
      "Iteration 296, loss = 0.51857446\n",
      "Iteration 297, loss = 0.51858802\n",
      "Iteration 298, loss = 0.51855741\n",
      "Iteration 299, loss = 0.51852300\n",
      "Iteration 300, loss = 0.51839933\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 18.67929419\n",
      "Iteration 2, loss = 18.15093006\n",
      "Iteration 3, loss = 17.60352170\n",
      "Iteration 4, loss = 17.03192445\n",
      "Iteration 5, loss = 16.46715838\n",
      "Iteration 6, loss = 15.88255920\n",
      "Iteration 7, loss = 15.27040595\n",
      "Iteration 8, loss = 14.68134463\n",
      "Iteration 9, loss = 14.06469354\n",
      "Iteration 10, loss = 13.45302070\n",
      "Iteration 11, loss = 12.86194496\n",
      "Iteration 12, loss = 12.24089437\n",
      "Iteration 13, loss = 11.64931365\n",
      "Iteration 14, loss = 11.05057140\n",
      "Iteration 15, loss = 10.48201423\n",
      "Iteration 16, loss = 9.91178170\n",
      "Iteration 17, loss = 9.36432883\n",
      "Iteration 18, loss = 8.82536256\n",
      "Iteration 19, loss = 8.29941529\n",
      "Iteration 20, loss = 7.78894679\n",
      "Iteration 21, loss = 7.31214437\n",
      "Iteration 22, loss = 6.85703061\n",
      "Iteration 23, loss = 6.41820645\n",
      "Iteration 24, loss = 6.03343482\n",
      "Iteration 25, loss = 5.67042304\n",
      "Iteration 26, loss = 5.34439567\n",
      "Iteration 27, loss = 5.05460222\n",
      "Iteration 28, loss = 4.77961943\n",
      "Iteration 29, loss = 4.54664627\n",
      "Iteration 30, loss = 4.34098298\n",
      "Iteration 31, loss = 4.15671211\n",
      "Iteration 32, loss = 3.99131969\n",
      "Iteration 33, loss = 3.85412270\n",
      "Iteration 34, loss = 3.73792411\n",
      "Iteration 35, loss = 3.62726290\n",
      "Iteration 36, loss = 3.53911633\n",
      "Iteration 37, loss = 3.46771580\n",
      "Iteration 38, loss = 3.41013309\n",
      "Iteration 39, loss = 3.35504493\n",
      "Iteration 40, loss = 3.31918961\n",
      "Iteration 41, loss = 3.28073570\n",
      "Iteration 42, loss = 3.25231429\n",
      "Iteration 43, loss = 3.22682179\n",
      "Iteration 44, loss = 3.20550115\n",
      "Iteration 45, loss = 3.18654468\n",
      "Iteration 46, loss = 3.16970873\n",
      "Iteration 47, loss = 3.15208863\n",
      "Iteration 48, loss = 3.13795820\n",
      "Iteration 49, loss = 3.12553667\n",
      "Iteration 50, loss = 3.11086159\n",
      "Iteration 51, loss = 3.09742269\n",
      "Iteration 52, loss = 3.08439026\n",
      "Iteration 53, loss = 3.07229767\n",
      "Iteration 54, loss = 3.05939848\n",
      "Iteration 55, loss = 3.04647465\n",
      "Iteration 56, loss = 3.03472709\n",
      "Iteration 57, loss = 3.02254156\n",
      "Iteration 58, loss = 3.01077864\n",
      "Iteration 59, loss = 2.99906704\n",
      "Iteration 60, loss = 2.98714844\n",
      "Iteration 61, loss = 2.97612908\n",
      "Iteration 62, loss = 2.96432596\n",
      "Iteration 63, loss = 2.95244121\n",
      "Iteration 64, loss = 2.94152117\n",
      "Iteration 65, loss = 2.92943370\n",
      "Iteration 66, loss = 2.91800615\n",
      "Iteration 67, loss = 2.90554126\n",
      "Iteration 68, loss = 2.89527544\n",
      "Iteration 69, loss = 2.88237783\n",
      "Iteration 70, loss = 2.87026385\n",
      "Iteration 71, loss = 2.85756408\n",
      "Iteration 72, loss = 2.84656258\n",
      "Iteration 73, loss = 2.83395897\n",
      "Iteration 74, loss = 2.82224192\n",
      "Iteration 75, loss = 2.81077006\n",
      "Iteration 76, loss = 2.79771906\n",
      "Iteration 77, loss = 2.78716921\n",
      "Iteration 78, loss = 2.77461450\n",
      "Iteration 79, loss = 2.76352611\n",
      "Iteration 80, loss = 2.75223919\n",
      "Iteration 81, loss = 2.74083240\n",
      "Iteration 82, loss = 2.72905937\n",
      "Iteration 83, loss = 2.71697322\n",
      "Iteration 84, loss = 2.70461276\n",
      "Iteration 85, loss = 2.69203563\n",
      "Iteration 86, loss = 2.68017152\n",
      "Iteration 87, loss = 2.66848398\n",
      "Iteration 88, loss = 2.65704199\n",
      "Iteration 89, loss = 2.64527237\n",
      "Iteration 90, loss = 2.63321738\n",
      "Iteration 91, loss = 2.62166838\n",
      "Iteration 92, loss = 2.61085635\n",
      "Iteration 93, loss = 2.59862150\n",
      "Iteration 94, loss = 2.58573957\n",
      "Iteration 95, loss = 2.57456762\n",
      "Iteration 96, loss = 2.56180603\n",
      "Iteration 97, loss = 2.54938684\n",
      "Iteration 98, loss = 2.53787902\n",
      "Iteration 99, loss = 2.52521824\n",
      "Iteration 100, loss = 2.51346657\n",
      "Iteration 101, loss = 2.50182142\n",
      "Iteration 102, loss = 2.49036185\n",
      "Iteration 103, loss = 2.47845735\n",
      "Iteration 104, loss = 2.46598976\n",
      "Iteration 105, loss = 2.45536847\n",
      "Iteration 106, loss = 2.44282155\n",
      "Iteration 107, loss = 2.43148396\n",
      "Iteration 108, loss = 2.41967855\n",
      "Iteration 109, loss = 2.40840108\n",
      "Iteration 110, loss = 2.39705456\n",
      "Iteration 111, loss = 2.38529234\n",
      "Iteration 112, loss = 2.37449151\n",
      "Iteration 113, loss = 2.36327682\n",
      "Iteration 114, loss = 2.35133801\n",
      "Iteration 115, loss = 2.34027415\n",
      "Iteration 116, loss = 2.32859839\n",
      "Iteration 117, loss = 2.31830590\n",
      "Iteration 118, loss = 2.30728265\n",
      "Iteration 119, loss = 2.29604733\n",
      "Iteration 120, loss = 2.28460895\n",
      "Iteration 121, loss = 2.27442076\n",
      "Iteration 122, loss = 2.26220331\n",
      "Iteration 123, loss = 2.25194508\n",
      "Iteration 124, loss = 2.24058121\n",
      "Iteration 125, loss = 2.23043325\n",
      "Iteration 126, loss = 2.21961621\n",
      "Iteration 127, loss = 2.20944679\n",
      "Iteration 128, loss = 2.19871119\n",
      "Iteration 129, loss = 2.18862537\n",
      "Iteration 130, loss = 2.17786692\n",
      "Iteration 131, loss = 2.16746077\n",
      "Iteration 132, loss = 2.15699905\n",
      "Iteration 133, loss = 2.14690445\n",
      "Iteration 134, loss = 2.13664122\n",
      "Iteration 135, loss = 2.12667859\n",
      "Iteration 136, loss = 2.11694607\n",
      "Iteration 137, loss = 2.10661095\n",
      "Iteration 138, loss = 2.09652951\n",
      "Iteration 139, loss = 2.08686515\n",
      "Iteration 140, loss = 2.07660352\n",
      "Iteration 141, loss = 2.06660943\n",
      "Iteration 142, loss = 2.05626549\n",
      "Iteration 143, loss = 2.04684274\n",
      "Iteration 144, loss = 2.03649905\n",
      "Iteration 145, loss = 2.02676601\n",
      "Iteration 146, loss = 2.01775368\n",
      "Iteration 147, loss = 2.00746431\n",
      "Iteration 148, loss = 1.99650595\n",
      "Iteration 149, loss = 1.98788139\n",
      "Iteration 150, loss = 1.97775418\n",
      "Iteration 151, loss = 1.96784635\n",
      "Iteration 152, loss = 1.95809728\n",
      "Iteration 153, loss = 1.94865781\n",
      "Iteration 154, loss = 1.93868567\n",
      "Iteration 155, loss = 1.92999736\n",
      "Iteration 156, loss = 1.92019414\n",
      "Iteration 157, loss = 1.91016082\n",
      "Iteration 158, loss = 1.90079075\n",
      "Iteration 159, loss = 1.89049472\n",
      "Iteration 160, loss = 1.88102545\n",
      "Iteration 161, loss = 1.87190800\n",
      "Iteration 162, loss = 1.86241001\n",
      "Iteration 163, loss = 1.85337223\n",
      "Iteration 164, loss = 1.84391975\n",
      "Iteration 165, loss = 1.83555787\n",
      "Iteration 166, loss = 1.82544256\n",
      "Iteration 167, loss = 1.81687375\n",
      "Iteration 168, loss = 1.80885097\n",
      "Iteration 169, loss = 1.79923355\n",
      "Iteration 170, loss = 1.79064756\n",
      "Iteration 171, loss = 1.78183644\n",
      "Iteration 172, loss = 1.77350964\n",
      "Iteration 173, loss = 1.76509836\n",
      "Iteration 174, loss = 1.75651961\n",
      "Iteration 175, loss = 1.74876143\n",
      "Iteration 176, loss = 1.74040716\n",
      "Iteration 177, loss = 1.73247422\n",
      "Iteration 178, loss = 1.72470576\n",
      "Iteration 179, loss = 1.71699694\n",
      "Iteration 180, loss = 1.70923907\n",
      "Iteration 181, loss = 1.70204097\n",
      "Iteration 182, loss = 1.69466267\n",
      "Iteration 183, loss = 1.68729165\n",
      "Iteration 184, loss = 1.68036127\n",
      "Iteration 185, loss = 1.67332058\n",
      "Iteration 186, loss = 1.66585206\n",
      "Iteration 187, loss = 1.65846604\n",
      "Iteration 188, loss = 1.65175855\n",
      "Iteration 189, loss = 1.64462448\n",
      "Iteration 190, loss = 1.63730274\n",
      "Iteration 191, loss = 1.63077503\n",
      "Iteration 192, loss = 1.62401580\n",
      "Iteration 193, loss = 1.61679596\n",
      "Iteration 194, loss = 1.61015193\n",
      "Iteration 195, loss = 1.60279169\n",
      "Iteration 196, loss = 1.59623919\n",
      "Iteration 197, loss = 1.58993713\n",
      "Iteration 198, loss = 1.58324394\n",
      "Iteration 199, loss = 1.57691486\n",
      "Iteration 200, loss = 1.57088421\n",
      "Iteration 201, loss = 1.56385080\n",
      "Iteration 202, loss = 1.55834053\n",
      "Iteration 203, loss = 1.55148756\n",
      "Iteration 204, loss = 1.54605688\n",
      "Iteration 205, loss = 1.53939962\n",
      "Iteration 206, loss = 1.53375170\n",
      "Iteration 207, loss = 1.52788972\n",
      "Iteration 208, loss = 1.52320268\n",
      "Iteration 209, loss = 1.51832120\n",
      "Iteration 210, loss = 1.51262335\n",
      "Iteration 211, loss = 1.50703679\n",
      "Iteration 212, loss = 1.50187745\n",
      "Iteration 213, loss = 1.49577046\n",
      "Iteration 214, loss = 1.48948365\n",
      "Iteration 215, loss = 1.48452659\n",
      "Iteration 216, loss = 1.47823812\n",
      "Iteration 217, loss = 1.47369627\n",
      "Iteration 218, loss = 1.46878103\n",
      "Iteration 219, loss = 1.46493445\n",
      "Iteration 220, loss = 1.46028031\n",
      "Iteration 221, loss = 1.45586060\n",
      "Iteration 222, loss = 1.45145919\n",
      "Iteration 223, loss = 1.44670280\n",
      "Iteration 224, loss = 1.44272167\n",
      "Iteration 225, loss = 1.43956784\n",
      "Iteration 226, loss = 1.43627583\n",
      "Iteration 227, loss = 1.43257429\n",
      "Iteration 228, loss = 1.42792010\n",
      "Iteration 229, loss = 1.42570245\n",
      "Iteration 230, loss = 1.41869125\n",
      "Iteration 231, loss = 1.41475997\n",
      "Iteration 232, loss = 1.41063200\n",
      "Iteration 233, loss = 1.40668168\n",
      "Iteration 234, loss = 1.40275230\n",
      "Iteration 235, loss = 1.39880412\n",
      "Iteration 236, loss = 1.39467809\n",
      "Iteration 237, loss = 1.39111924\n",
      "Iteration 238, loss = 1.38880502\n",
      "Iteration 239, loss = 1.38510754\n",
      "Iteration 240, loss = 1.38207387\n",
      "Iteration 241, loss = 1.37981260\n",
      "Iteration 242, loss = 1.37657959\n",
      "Iteration 243, loss = 1.37363845\n",
      "Iteration 244, loss = 1.36964327\n",
      "Iteration 245, loss = 1.36672034\n",
      "Iteration 246, loss = 1.36381255\n",
      "Iteration 247, loss = 1.36099939\n",
      "Iteration 248, loss = 1.35817597\n",
      "Iteration 249, loss = 1.35617196\n",
      "Iteration 250, loss = 1.35323679\n",
      "Iteration 251, loss = 1.34912352\n",
      "Iteration 252, loss = 1.34573433\n",
      "Iteration 253, loss = 1.34228151\n",
      "Iteration 254, loss = 1.33747461\n",
      "Iteration 255, loss = 1.33402981\n",
      "Iteration 256, loss = 1.32984983\n",
      "Iteration 257, loss = 1.32830396\n",
      "Iteration 258, loss = 1.32534277\n",
      "Iteration 259, loss = 1.32297342\n",
      "Iteration 260, loss = 1.32066397\n",
      "Iteration 261, loss = 1.31849570\n",
      "Iteration 262, loss = 1.31514562\n",
      "Iteration 263, loss = 1.31112814\n",
      "Iteration 264, loss = 1.30770640\n",
      "Iteration 265, loss = 1.30442544\n",
      "Iteration 266, loss = 1.30095120\n",
      "Iteration 267, loss = 1.29758099\n",
      "Iteration 268, loss = 1.29356296\n",
      "Iteration 269, loss = 1.29013444\n",
      "Iteration 270, loss = 1.28658717\n",
      "Iteration 271, loss = 1.28338230\n",
      "Iteration 272, loss = 1.28015385\n",
      "Iteration 273, loss = 1.27755064\n",
      "Iteration 274, loss = 1.27517258\n",
      "Iteration 275, loss = 1.27243823\n",
      "Iteration 276, loss = 1.26951082\n",
      "Iteration 277, loss = 1.26635657\n",
      "Iteration 278, loss = 1.26324972\n",
      "Iteration 279, loss = 1.25963140\n",
      "Iteration 280, loss = 1.25646497\n",
      "Iteration 281, loss = 1.25362584\n",
      "Iteration 282, loss = 1.25041544\n",
      "Iteration 283, loss = 1.24755713\n",
      "Iteration 284, loss = 1.24547902\n",
      "Iteration 285, loss = 1.24289824\n",
      "Iteration 286, loss = 1.23975619\n",
      "Iteration 287, loss = 1.23660876\n",
      "Iteration 288, loss = 1.23328124\n",
      "Iteration 289, loss = 1.23112587\n",
      "Iteration 290, loss = 1.22772172\n",
      "Iteration 291, loss = 1.22473944\n",
      "Iteration 292, loss = 1.22190450\n",
      "Iteration 293, loss = 1.21930032\n",
      "Iteration 294, loss = 1.21623025\n",
      "Iteration 295, loss = 1.21390297\n",
      "Iteration 296, loss = 1.21096501\n",
      "Iteration 297, loss = 1.20824707\n",
      "Iteration 298, loss = 1.20514047\n",
      "Iteration 299, loss = 1.20232932\n",
      "Iteration 300, loss = 1.19953028\n",
      "Iteration 301, loss = 1.19661734\n",
      "Iteration 302, loss = 1.19359498\n",
      "Iteration 303, loss = 1.19051699\n",
      "Iteration 304, loss = 1.18751046\n",
      "Iteration 305, loss = 1.18434534\n",
      "Iteration 306, loss = 1.18136059\n",
      "Iteration 307, loss = 1.17813001\n",
      "Iteration 308, loss = 1.17500937\n",
      "Iteration 309, loss = 1.17189809\n",
      "Iteration 310, loss = 1.16900827\n",
      "Iteration 311, loss = 1.16602582\n",
      "Iteration 312, loss = 1.16343278\n",
      "Iteration 313, loss = 1.16095094\n",
      "Iteration 314, loss = 1.15903302\n",
      "Iteration 315, loss = 1.15705294\n",
      "Iteration 316, loss = 1.15535373\n",
      "Iteration 317, loss = 1.15267961\n",
      "Iteration 318, loss = 1.14877791\n",
      "Iteration 319, loss = 1.14573345\n",
      "Iteration 320, loss = 1.14276852\n",
      "Iteration 321, loss = 1.14008944\n",
      "Iteration 322, loss = 1.13697325\n",
      "Iteration 323, loss = 1.13345780\n",
      "Iteration 324, loss = 1.12943916\n",
      "Iteration 325, loss = 1.12711945\n",
      "Iteration 326, loss = 1.12352419\n",
      "Iteration 327, loss = 1.12161417\n",
      "Iteration 328, loss = 1.11937030\n",
      "Iteration 329, loss = 1.11810052\n",
      "Iteration 330, loss = 1.11536110\n",
      "Iteration 331, loss = 1.11232498\n",
      "Iteration 332, loss = 1.10902962\n",
      "Iteration 333, loss = 1.10553285\n",
      "Iteration 334, loss = 1.10194359\n",
      "Iteration 335, loss = 1.10088666\n",
      "Iteration 336, loss = 1.09747895\n",
      "Iteration 337, loss = 1.09522376\n",
      "Iteration 338, loss = 1.09227448\n",
      "Iteration 339, loss = 1.08915738\n",
      "Iteration 340, loss = 1.08597187\n",
      "Iteration 341, loss = 1.08315346\n",
      "Iteration 342, loss = 1.08011825\n",
      "Iteration 343, loss = 1.07704977\n",
      "Iteration 344, loss = 1.07540535\n",
      "Iteration 345, loss = 1.07628318\n",
      "Iteration 346, loss = 1.07364123\n",
      "Iteration 347, loss = 1.07085667\n",
      "Iteration 348, loss = 1.06744819\n",
      "Iteration 349, loss = 1.06357907\n",
      "Iteration 350, loss = 1.06076751\n",
      "Iteration 351, loss = 1.05620612\n",
      "Iteration 352, loss = 1.05328983\n",
      "Iteration 353, loss = 1.04989512\n",
      "Iteration 354, loss = 1.04711760\n",
      "Iteration 355, loss = 1.04477056\n",
      "Iteration 356, loss = 1.04159983\n",
      "Iteration 357, loss = 1.03891798\n",
      "Iteration 358, loss = 1.03625318\n",
      "Iteration 359, loss = 1.03392305\n",
      "Iteration 360, loss = 1.03060419\n",
      "Iteration 361, loss = 1.02793606\n",
      "Iteration 362, loss = 1.02539322\n",
      "Iteration 363, loss = 1.02283805\n",
      "Iteration 364, loss = 1.02015279\n",
      "Iteration 365, loss = 1.01731560\n",
      "Iteration 366, loss = 1.01433083\n",
      "Iteration 367, loss = 1.01120679\n",
      "Iteration 368, loss = 1.00829399\n",
      "Iteration 369, loss = 1.00546857\n",
      "Iteration 370, loss = 1.00258757\n",
      "Iteration 371, loss = 0.99996411\n",
      "Iteration 372, loss = 0.99734406\n",
      "Iteration 373, loss = 0.99604268\n",
      "Iteration 374, loss = 0.99401649\n",
      "Iteration 375, loss = 0.99156658\n",
      "Iteration 376, loss = 0.98796993\n",
      "Iteration 377, loss = 0.98508177\n",
      "Iteration 378, loss = 0.98135865\n",
      "Iteration 379, loss = 0.97848111\n",
      "Iteration 380, loss = 0.97595137\n",
      "Iteration 381, loss = 0.97345439\n",
      "Iteration 382, loss = 0.97114325\n",
      "Iteration 383, loss = 0.96938266\n",
      "Iteration 384, loss = 0.96686939\n",
      "Iteration 385, loss = 0.96262362\n",
      "Iteration 386, loss = 0.95868581\n",
      "Iteration 387, loss = 0.95606786\n",
      "Iteration 388, loss = 0.95688518\n",
      "Iteration 389, loss = 0.95430627\n",
      "Iteration 390, loss = 0.95128461\n",
      "Iteration 391, loss = 0.94695354\n",
      "Iteration 392, loss = 0.94367797\n",
      "Iteration 393, loss = 0.94091110\n",
      "Iteration 394, loss = 0.93745374\n",
      "Iteration 395, loss = 0.93505385\n",
      "Iteration 396, loss = 0.93302655\n",
      "Iteration 397, loss = 0.93134245\n",
      "Iteration 398, loss = 0.92933965\n",
      "Iteration 399, loss = 0.92629093\n",
      "Iteration 400, loss = 0.92236228\n",
      "Iteration 401, loss = 0.92016670\n",
      "Iteration 402, loss = 0.91671856\n",
      "Iteration 403, loss = 0.91409449\n",
      "Iteration 404, loss = 0.91174466\n",
      "Iteration 405, loss = 0.90908485\n",
      "Iteration 406, loss = 0.90619981\n",
      "Iteration 407, loss = 0.90426267\n",
      "Iteration 408, loss = 0.90251125\n",
      "Iteration 409, loss = 0.89995434\n",
      "Iteration 410, loss = 0.89740583\n",
      "Iteration 411, loss = 0.89443914\n",
      "Iteration 412, loss = 0.89188391\n",
      "Iteration 413, loss = 0.88958651\n",
      "Iteration 414, loss = 0.88873411\n",
      "Iteration 415, loss = 0.88567379\n",
      "Iteration 416, loss = 0.88286816\n",
      "Iteration 417, loss = 0.87944034\n",
      "Iteration 418, loss = 0.87684886\n",
      "Iteration 419, loss = 0.87368175\n",
      "Iteration 420, loss = 0.87144409\n",
      "Iteration 421, loss = 0.87000397\n",
      "Iteration 422, loss = 0.86835257\n",
      "Iteration 423, loss = 0.86684055\n",
      "Iteration 424, loss = 0.86459309\n",
      "Iteration 425, loss = 0.86154984\n",
      "Iteration 426, loss = 0.85868094\n",
      "Iteration 427, loss = 0.85548622\n",
      "Iteration 428, loss = 0.85227099\n",
      "Iteration 429, loss = 0.84970707\n",
      "Iteration 430, loss = 0.84726380\n",
      "Iteration 431, loss = 0.84542230\n",
      "Iteration 432, loss = 0.84465448\n",
      "Iteration 433, loss = 0.84180750\n",
      "Iteration 434, loss = 0.83826521\n",
      "Iteration 435, loss = 0.83592222\n",
      "Iteration 436, loss = 0.83292203\n",
      "Iteration 437, loss = 0.83059948\n",
      "Iteration 438, loss = 0.82866439\n",
      "Iteration 439, loss = 0.82576370\n",
      "Iteration 440, loss = 0.82349269\n",
      "Iteration 441, loss = 0.82067154\n",
      "Iteration 442, loss = 0.81812099\n",
      "Iteration 443, loss = 0.81574284\n",
      "Iteration 444, loss = 0.81420265\n",
      "Iteration 445, loss = 0.81248039\n",
      "Iteration 446, loss = 0.81037182\n",
      "Iteration 447, loss = 0.80800054\n",
      "Iteration 448, loss = 0.80554850\n",
      "Iteration 449, loss = 0.80291313\n",
      "Iteration 450, loss = 0.79988963\n",
      "Iteration 451, loss = 0.79853623\n",
      "Iteration 452, loss = 0.79527247\n",
      "Iteration 453, loss = 0.79310452\n",
      "Iteration 454, loss = 0.79163466\n",
      "Iteration 455, loss = 0.78921050\n",
      "Iteration 456, loss = 0.78657746\n",
      "Iteration 457, loss = 0.78366550\n",
      "Iteration 458, loss = 0.78077016\n",
      "Iteration 459, loss = 0.77862976\n",
      "Iteration 460, loss = 0.77855862\n",
      "Iteration 461, loss = 0.77683554\n",
      "Iteration 462, loss = 0.77534002\n",
      "Iteration 463, loss = 0.77589366\n",
      "Iteration 464, loss = 0.77399002\n",
      "Iteration 465, loss = 0.77086273\n",
      "Iteration 466, loss = 0.76727039\n",
      "Iteration 467, loss = 0.76399634\n",
      "Iteration 468, loss = 0.76209775\n",
      "Iteration 469, loss = 0.75934726\n",
      "Iteration 470, loss = 0.75757318\n",
      "Iteration 471, loss = 0.75251278\n",
      "Iteration 472, loss = 0.74990403\n",
      "Iteration 473, loss = 0.74744066\n",
      "Iteration 474, loss = 0.74529946\n",
      "Iteration 475, loss = 0.74308857\n",
      "Iteration 476, loss = 0.74130800\n",
      "Iteration 477, loss = 0.73912030\n",
      "Iteration 478, loss = 0.73681537\n",
      "Iteration 479, loss = 0.73473174\n",
      "Iteration 480, loss = 0.73332362\n",
      "Iteration 481, loss = 0.73049645\n",
      "Iteration 482, loss = 0.72856760\n",
      "Iteration 483, loss = 0.72668685\n",
      "Iteration 484, loss = 0.72504437\n",
      "Iteration 485, loss = 0.72329952\n",
      "Iteration 486, loss = 0.72238269\n",
      "Iteration 487, loss = 0.72036576\n",
      "Iteration 488, loss = 0.71769812\n",
      "Iteration 489, loss = 0.71489270\n",
      "Iteration 490, loss = 0.71322143\n",
      "Iteration 491, loss = 0.71289903\n",
      "Iteration 492, loss = 0.71248004\n",
      "Iteration 493, loss = 0.71134905\n",
      "Iteration 494, loss = 0.70833676\n",
      "Iteration 495, loss = 0.70475916\n",
      "Iteration 496, loss = 0.70160688\n",
      "Iteration 497, loss = 0.70143262\n",
      "Iteration 498, loss = 0.70211005\n",
      "Iteration 499, loss = 0.70243493\n",
      "Iteration 500, loss = 0.70079972\n",
      "Iteration 501, loss = 0.69724326\n",
      "Iteration 502, loss = 0.69270109\n",
      "Iteration 503, loss = 0.69020047\n",
      "Iteration 504, loss = 0.68790723\n",
      "Iteration 505, loss = 0.68929640\n",
      "Iteration 506, loss = 0.68954038\n",
      "Iteration 507, loss = 0.69022239\n",
      "Iteration 508, loss = 0.68698496\n",
      "Iteration 509, loss = 0.68133963\n",
      "Iteration 510, loss = 0.67703145\n",
      "Iteration 511, loss = 0.67637747\n",
      "Iteration 512, loss = 0.67798027\n",
      "Iteration 513, loss = 0.67851671\n",
      "Iteration 514, loss = 0.67741649\n",
      "Iteration 515, loss = 0.67480177\n",
      "Iteration 516, loss = 0.67215092\n",
      "Iteration 517, loss = 0.66820627\n",
      "Iteration 518, loss = 0.66484535\n",
      "Iteration 519, loss = 0.66308626\n",
      "Iteration 520, loss = 0.66234660\n",
      "Iteration 521, loss = 0.66189142\n",
      "Iteration 522, loss = 0.65996366\n",
      "Iteration 523, loss = 0.65729256\n",
      "Iteration 524, loss = 0.65567250\n",
      "Iteration 525, loss = 0.65356242\n",
      "Iteration 526, loss = 0.65256012\n",
      "Iteration 527, loss = 0.65109001\n",
      "Iteration 528, loss = 0.64922089\n",
      "Iteration 529, loss = 0.64801179\n",
      "Iteration 530, loss = 0.64649108\n",
      "Iteration 531, loss = 0.64582088\n",
      "Iteration 532, loss = 0.64347170\n",
      "Iteration 533, loss = 0.64210097\n",
      "Iteration 534, loss = 0.64068624\n",
      "Iteration 535, loss = 0.63930474\n",
      "Iteration 536, loss = 0.63805914\n",
      "Iteration 537, loss = 0.63659155\n",
      "Iteration 538, loss = 0.63561381\n",
      "Iteration 539, loss = 0.63490281\n",
      "Iteration 540, loss = 0.63417696\n",
      "Iteration 541, loss = 0.63337949\n",
      "Iteration 542, loss = 0.63142985\n",
      "Iteration 543, loss = 0.62988453\n",
      "Iteration 544, loss = 0.62834457\n",
      "Iteration 545, loss = 0.62746799\n",
      "Iteration 546, loss = 0.62568750\n",
      "Iteration 547, loss = 0.62442145\n",
      "Iteration 548, loss = 0.62341719\n",
      "Iteration 549, loss = 0.62214427\n",
      "Iteration 550, loss = 0.62141496\n",
      "Iteration 551, loss = 0.62072211\n",
      "Iteration 552, loss = 0.61958642\n",
      "Iteration 553, loss = 0.61774111\n",
      "Iteration 554, loss = 0.61698099\n",
      "Iteration 555, loss = 0.61530495\n",
      "Iteration 556, loss = 0.61409253\n",
      "Iteration 557, loss = 0.61525001\n",
      "Iteration 558, loss = 0.61585418\n",
      "Iteration 559, loss = 0.61492540\n",
      "Iteration 560, loss = 0.61231116\n",
      "Iteration 561, loss = 0.61037635\n",
      "Iteration 562, loss = 0.60791650\n",
      "Iteration 563, loss = 0.60689652\n",
      "Iteration 564, loss = 0.60628709\n",
      "Iteration 565, loss = 0.60496118\n",
      "Iteration 566, loss = 0.60360693\n",
      "Iteration 567, loss = 0.60282337\n",
      "Iteration 568, loss = 0.60269676\n",
      "Iteration 569, loss = 0.60284496\n",
      "Iteration 570, loss = 0.60206082\n",
      "Iteration 571, loss = 0.59912567\n",
      "Iteration 572, loss = 0.59812719\n",
      "Iteration 573, loss = 0.59707620\n",
      "Iteration 574, loss = 0.59625430\n",
      "Iteration 575, loss = 0.59588455\n",
      "Iteration 576, loss = 0.59535990\n",
      "Iteration 577, loss = 0.59447989\n",
      "Iteration 578, loss = 0.59338061\n",
      "Iteration 579, loss = 0.59220239\n",
      "Iteration 580, loss = 0.59129349\n",
      "Iteration 581, loss = 0.59154884\n",
      "Iteration 582, loss = 0.59018378\n",
      "Iteration 583, loss = 0.58900007\n",
      "Iteration 584, loss = 0.58816215\n",
      "Iteration 585, loss = 0.58981419\n",
      "Iteration 586, loss = 0.59110864\n",
      "Iteration 587, loss = 0.59196904\n",
      "Iteration 588, loss = 0.59204451\n",
      "Iteration 589, loss = 0.59019146\n",
      "Iteration 590, loss = 0.58802985\n",
      "Iteration 591, loss = 0.58570211\n",
      "Iteration 592, loss = 0.58368278\n",
      "Iteration 593, loss = 0.58300420\n",
      "Iteration 594, loss = 0.58202033\n",
      "Iteration 595, loss = 0.58189299\n",
      "Iteration 596, loss = 0.58163217\n",
      "Iteration 597, loss = 0.58096014\n",
      "Iteration 598, loss = 0.58021555\n",
      "Iteration 599, loss = 0.57941271\n",
      "Iteration 600, loss = 0.57921844\n",
      "Iteration 601, loss = 0.57849252\n",
      "Iteration 602, loss = 0.57795675\n",
      "Iteration 603, loss = 0.57769618\n",
      "Iteration 604, loss = 0.57722198\n",
      "Iteration 605, loss = 0.57685718\n",
      "Iteration 606, loss = 0.57653205\n",
      "Iteration 607, loss = 0.57630378\n",
      "Iteration 608, loss = 0.57559345\n",
      "Iteration 609, loss = 0.57538687\n",
      "Iteration 610, loss = 0.57805052\n",
      "Iteration 611, loss = 0.57797269\n",
      "Iteration 612, loss = 0.57568139\n",
      "Iteration 613, loss = 0.57313823\n",
      "Iteration 614, loss = 0.57357738\n",
      "Iteration 615, loss = 0.57361373\n",
      "Iteration 616, loss = 0.57503207\n",
      "Iteration 617, loss = 0.57416745\n",
      "Iteration 618, loss = 0.57268348\n",
      "Iteration 619, loss = 0.57158064\n",
      "Iteration 620, loss = 0.57035005\n",
      "Iteration 621, loss = 0.57014130\n",
      "Iteration 622, loss = 0.56939439\n",
      "Iteration 623, loss = 0.56905308\n",
      "Iteration 624, loss = 0.56859032\n",
      "Iteration 625, loss = 0.56841025\n",
      "Iteration 626, loss = 0.56774686\n",
      "Iteration 627, loss = 0.56760697\n",
      "Iteration 628, loss = 0.56805496\n",
      "Iteration 629, loss = 0.56856133\n",
      "Iteration 630, loss = 0.56870503\n",
      "Iteration 631, loss = 0.56832761\n",
      "Iteration 632, loss = 0.56820141\n",
      "Iteration 633, loss = 0.56805945\n",
      "Iteration 634, loss = 0.56711056\n",
      "Iteration 635, loss = 0.56571252\n",
      "Iteration 636, loss = 0.56521253\n",
      "Iteration 637, loss = 0.56436637\n",
      "Iteration 638, loss = 0.56423267\n",
      "Iteration 639, loss = 0.56535981\n",
      "Iteration 640, loss = 0.56587312\n",
      "Iteration 641, loss = 0.56488254\n",
      "Iteration 642, loss = 0.56406872\n",
      "Iteration 643, loss = 0.56270230\n",
      "Iteration 644, loss = 0.56284822\n",
      "Iteration 645, loss = 0.56296033\n",
      "Iteration 646, loss = 0.56322377\n",
      "Iteration 647, loss = 0.56327258\n",
      "Iteration 648, loss = 0.56243184\n",
      "Iteration 649, loss = 0.56135988\n",
      "Iteration 650, loss = 0.56077244\n",
      "Iteration 651, loss = 0.56167405\n",
      "Iteration 652, loss = 0.56185315\n",
      "Iteration 653, loss = 0.56164943\n",
      "Iteration 654, loss = 0.56105089\n",
      "Iteration 655, loss = 0.56016978\n",
      "Iteration 656, loss = 0.55979815\n",
      "Iteration 657, loss = 0.56088609\n",
      "Iteration 658, loss = 0.56015136\n",
      "Iteration 659, loss = 0.55945696\n",
      "Iteration 660, loss = 0.55914684\n",
      "Iteration 661, loss = 0.55914810\n",
      "Iteration 662, loss = 0.55921865\n",
      "Iteration 663, loss = 0.55847085\n",
      "Iteration 664, loss = 0.55853613\n",
      "Iteration 665, loss = 0.55930012\n",
      "Iteration 666, loss = 0.55960461\n",
      "Iteration 667, loss = 0.55923760\n",
      "Iteration 668, loss = 0.55790421\n",
      "Iteration 669, loss = 0.55703278\n",
      "Iteration 670, loss = 0.55900544\n",
      "Iteration 671, loss = 0.56211346\n",
      "Iteration 672, loss = 0.56379060\n",
      "Iteration 673, loss = 0.56267935\n",
      "Iteration 674, loss = 0.55893346\n",
      "Iteration 675, loss = 0.55640860\n",
      "Iteration 676, loss = 0.55753658\n",
      "Iteration 677, loss = 0.56022895\n",
      "Iteration 678, loss = 0.55948238\n",
      "Iteration 679, loss = 0.55666414\n",
      "Iteration 680, loss = 0.55500624\n",
      "Iteration 681, loss = 0.55860715\n",
      "Iteration 682, loss = 0.55997818\n",
      "Iteration 683, loss = 0.56004247\n",
      "Iteration 684, loss = 0.55803975\n",
      "Iteration 685, loss = 0.55487733\n",
      "Iteration 686, loss = 0.55697541\n",
      "Iteration 687, loss = 0.55921123\n",
      "Iteration 688, loss = 0.55958060\n",
      "Iteration 689, loss = 0.55817254\n",
      "Iteration 690, loss = 0.55525789\n",
      "Iteration 691, loss = 0.55604458\n",
      "Iteration 692, loss = 0.55492299\n",
      "Iteration 693, loss = 0.55424463\n",
      "Iteration 694, loss = 0.55498017\n",
      "Iteration 695, loss = 0.55603399\n",
      "Iteration 696, loss = 0.55588443\n",
      "Iteration 697, loss = 0.55494504\n",
      "Iteration 698, loss = 0.55458508\n",
      "Iteration 699, loss = 0.55431962\n",
      "Iteration 700, loss = 0.55441221\n",
      "Iteration 701, loss = 0.55449383\n",
      "Iteration 702, loss = 0.55410695\n",
      "Iteration 703, loss = 0.55402395\n",
      "Iteration 704, loss = 0.55566704\n",
      "Iteration 705, loss = 0.55566406\n",
      "Iteration 706, loss = 0.55372710\n",
      "Iteration 707, loss = 0.55424977\n",
      "Iteration 708, loss = 0.55649798\n",
      "Iteration 709, loss = 0.55659742\n",
      "Iteration 710, loss = 0.55509863\n",
      "Iteration 711, loss = 0.55350985\n",
      "Iteration 712, loss = 0.55355638\n",
      "Iteration 713, loss = 0.55437561\n",
      "Iteration 714, loss = 0.55572390\n",
      "Iteration 715, loss = 0.55695243\n",
      "Iteration 716, loss = 0.55725915\n",
      "Iteration 717, loss = 0.55783204\n",
      "Iteration 718, loss = 0.55738117\n",
      "Iteration 719, loss = 0.55635191\n",
      "Iteration 720, loss = 0.55313007\n",
      "Iteration 721, loss = 0.55335879\n",
      "Iteration 722, loss = 0.55482826\n",
      "Iteration 723, loss = 0.55380647\n",
      "Iteration 724, loss = 0.55254796\n",
      "Iteration 725, loss = 0.55390188\n",
      "Iteration 726, loss = 0.55471991\n",
      "Iteration 727, loss = 0.55303304\n",
      "Iteration 728, loss = 0.55267395\n",
      "Iteration 729, loss = 0.55489847\n",
      "Iteration 730, loss = 0.55505286\n",
      "Iteration 731, loss = 0.55347540\n",
      "Iteration 732, loss = 0.55282097\n",
      "Iteration 733, loss = 0.55313803\n",
      "Iteration 734, loss = 0.55391256\n",
      "Iteration 735, loss = 0.55329391\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 26.29414287\n",
      "Iteration 2, loss = 26.29414287\n",
      "Iteration 3, loss = 26.29414286\n",
      "Iteration 4, loss = 26.29414285\n",
      "Iteration 5, loss = 26.29414284\n",
      "Iteration 6, loss = 26.29414284\n",
      "Iteration 7, loss = 26.29414283\n",
      "Iteration 8, loss = 26.29414282\n",
      "Iteration 9, loss = 26.29414281\n",
      "Iteration 10, loss = 26.29414281\n",
      "Iteration 11, loss = 26.29414280\n",
      "Iteration 12, loss = 26.29414279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 6.74654637\n",
      "Iteration 2, loss = 6.62072720\n",
      "Iteration 3, loss = 6.49551534\n",
      "Iteration 4, loss = 6.36922475\n",
      "Iteration 5, loss = 6.24639594\n",
      "Iteration 6, loss = 6.12381258\n",
      "Iteration 7, loss = 6.00420726\n",
      "Iteration 8, loss = 5.88790047\n",
      "Iteration 9, loss = 5.76740380\n",
      "Iteration 10, loss = 5.65441390\n",
      "Iteration 11, loss = 5.53515222\n",
      "Iteration 12, loss = 5.42373012\n",
      "Iteration 13, loss = 5.31277375\n",
      "Iteration 14, loss = 5.19900757\n",
      "Iteration 15, loss = 5.08882061\n",
      "Iteration 16, loss = 4.98113401\n",
      "Iteration 17, loss = 4.87663799\n",
      "Iteration 18, loss = 4.77231768\n",
      "Iteration 19, loss = 4.67225363\n",
      "Iteration 20, loss = 4.56876296\n",
      "Iteration 21, loss = 4.47424184\n",
      "Iteration 22, loss = 4.37486793\n",
      "Iteration 23, loss = 4.28314229\n",
      "Iteration 24, loss = 4.18622589\n",
      "Iteration 25, loss = 4.10067113\n",
      "Iteration 26, loss = 4.00801357\n",
      "Iteration 27, loss = 3.92079178\n",
      "Iteration 28, loss = 3.83646656\n",
      "Iteration 29, loss = 3.75376201\n",
      "Iteration 30, loss = 3.66853823\n",
      "Iteration 31, loss = 3.58918793\n",
      "Iteration 32, loss = 3.50797188\n",
      "Iteration 33, loss = 3.43145940\n",
      "Iteration 34, loss = 3.35298681\n",
      "Iteration 35, loss = 3.27620859\n",
      "Iteration 36, loss = 3.20570113\n",
      "Iteration 37, loss = 3.13108688\n",
      "Iteration 38, loss = 3.06045828\n",
      "Iteration 39, loss = 2.99070755\n",
      "Iteration 40, loss = 2.92717786\n",
      "Iteration 41, loss = 2.86239768\n",
      "Iteration 42, loss = 2.80225692\n",
      "Iteration 43, loss = 2.74281737\n",
      "Iteration 44, loss = 2.68551966\n",
      "Iteration 45, loss = 2.63073074\n",
      "Iteration 46, loss = 2.57593581\n",
      "Iteration 47, loss = 2.52168088\n",
      "Iteration 48, loss = 2.46974360\n",
      "Iteration 49, loss = 2.41749400\n",
      "Iteration 50, loss = 2.36852122\n",
      "Iteration 51, loss = 2.31727323\n",
      "Iteration 52, loss = 2.26953809\n",
      "Iteration 53, loss = 2.21999996\n",
      "Iteration 54, loss = 2.17445235\n",
      "Iteration 55, loss = 2.12611366\n",
      "Iteration 56, loss = 2.07960962\n",
      "Iteration 57, loss = 2.03286549\n",
      "Iteration 58, loss = 1.98789892\n",
      "Iteration 59, loss = 1.94179730\n",
      "Iteration 60, loss = 1.89739428\n",
      "Iteration 61, loss = 1.85642779\n",
      "Iteration 62, loss = 1.81300911\n",
      "Iteration 63, loss = 1.77346729\n",
      "Iteration 64, loss = 1.73084381\n",
      "Iteration 65, loss = 1.69157532\n",
      "Iteration 66, loss = 1.65242406\n",
      "Iteration 67, loss = 1.61256802\n",
      "Iteration 68, loss = 1.57387194\n",
      "Iteration 69, loss = 1.53712042\n",
      "Iteration 70, loss = 1.49834112\n",
      "Iteration 71, loss = 1.46121993\n",
      "Iteration 72, loss = 1.42555977\n",
      "Iteration 73, loss = 1.38842368\n",
      "Iteration 74, loss = 1.35489243\n",
      "Iteration 75, loss = 1.32124555\n",
      "Iteration 76, loss = 1.28735586\n",
      "Iteration 77, loss = 1.25572885\n",
      "Iteration 78, loss = 1.22460571\n",
      "Iteration 79, loss = 1.19500731\n",
      "Iteration 80, loss = 1.16624572\n",
      "Iteration 81, loss = 1.13866802\n",
      "Iteration 82, loss = 1.11237775\n",
      "Iteration 83, loss = 1.08722576\n",
      "Iteration 84, loss = 1.06166993\n",
      "Iteration 85, loss = 1.03785689\n",
      "Iteration 86, loss = 1.01462706\n",
      "Iteration 87, loss = 0.99124780\n",
      "Iteration 88, loss = 0.96946803\n",
      "Iteration 89, loss = 0.94773583\n",
      "Iteration 90, loss = 0.92477249\n",
      "Iteration 91, loss = 0.90517083\n",
      "Iteration 92, loss = 0.88458149\n",
      "Iteration 93, loss = 0.86581746\n",
      "Iteration 94, loss = 0.84729643\n",
      "Iteration 95, loss = 0.82947084\n",
      "Iteration 96, loss = 0.81360268\n",
      "Iteration 97, loss = 0.79816421\n",
      "Iteration 98, loss = 0.78310003\n",
      "Iteration 99, loss = 0.76939378\n",
      "Iteration 100, loss = 0.75651909\n",
      "Iteration 101, loss = 0.74426381\n",
      "Iteration 102, loss = 0.73237627\n",
      "Iteration 103, loss = 0.72181310\n",
      "Iteration 104, loss = 0.71175012\n",
      "Iteration 105, loss = 0.70233994\n",
      "Iteration 106, loss = 0.69324336\n",
      "Iteration 107, loss = 0.68499900\n",
      "Iteration 108, loss = 0.67708181\n",
      "Iteration 109, loss = 0.66904233\n",
      "Iteration 110, loss = 0.66248790\n",
      "Iteration 111, loss = 0.65558862\n",
      "Iteration 112, loss = 0.64898928\n",
      "Iteration 113, loss = 0.64323830\n",
      "Iteration 114, loss = 0.63749594\n",
      "Iteration 115, loss = 0.63280599\n",
      "Iteration 116, loss = 0.62823476\n",
      "Iteration 117, loss = 0.62369467\n",
      "Iteration 118, loss = 0.61994933\n",
      "Iteration 119, loss = 0.61668076\n",
      "Iteration 120, loss = 0.61374433\n",
      "Iteration 121, loss = 0.61076896\n",
      "Iteration 122, loss = 0.60832640\n",
      "Iteration 123, loss = 0.60609473\n",
      "Iteration 124, loss = 0.60392312\n",
      "Iteration 125, loss = 0.60207781\n",
      "Iteration 126, loss = 0.60039791\n",
      "Iteration 127, loss = 0.59878138\n",
      "Iteration 128, loss = 0.59737762\n",
      "Iteration 129, loss = 0.59594377\n",
      "Iteration 130, loss = 0.59469499\n",
      "Iteration 131, loss = 0.59390319\n",
      "Iteration 132, loss = 0.59272982\n",
      "Iteration 133, loss = 0.59186280\n",
      "Iteration 134, loss = 0.59124737\n",
      "Iteration 135, loss = 0.59046194\n",
      "Iteration 136, loss = 0.58996845\n",
      "Iteration 137, loss = 0.58927988\n",
      "Iteration 138, loss = 0.58883851\n",
      "Iteration 139, loss = 0.58847594\n",
      "Iteration 140, loss = 0.58794323\n",
      "Iteration 141, loss = 0.58757527\n",
      "Iteration 142, loss = 0.58713082\n",
      "Iteration 143, loss = 0.58683152\n",
      "Iteration 144, loss = 0.58643752\n",
      "Iteration 145, loss = 0.58607470\n",
      "Iteration 146, loss = 0.58577692\n",
      "Iteration 147, loss = 0.58540978\n",
      "Iteration 148, loss = 0.58507480\n",
      "Iteration 149, loss = 0.58479920\n",
      "Iteration 150, loss = 0.58441124\n",
      "Iteration 151, loss = 0.58415606\n",
      "Iteration 152, loss = 0.58377570\n",
      "Iteration 153, loss = 0.58347763\n",
      "Iteration 154, loss = 0.58316900\n",
      "Iteration 155, loss = 0.58280433\n",
      "Iteration 156, loss = 0.58253385\n",
      "Iteration 157, loss = 0.58219004\n",
      "Iteration 158, loss = 0.58183062\n",
      "Iteration 159, loss = 0.58149151\n",
      "Iteration 160, loss = 0.58116308\n",
      "Iteration 161, loss = 0.58077026\n",
      "Iteration 162, loss = 0.58037114\n",
      "Iteration 163, loss = 0.58000409\n",
      "Iteration 164, loss = 0.57963232\n",
      "Iteration 165, loss = 0.57922229\n",
      "Iteration 166, loss = 0.57886895\n",
      "Iteration 167, loss = 0.57846563\n",
      "Iteration 168, loss = 0.57816651\n",
      "Iteration 169, loss = 0.57781364\n",
      "Iteration 170, loss = 0.57748551\n",
      "Iteration 171, loss = 0.57726139\n",
      "Iteration 172, loss = 0.57688666\n",
      "Iteration 173, loss = 0.57657549\n",
      "Iteration 174, loss = 0.57627513\n",
      "Iteration 175, loss = 0.57593814\n",
      "Iteration 176, loss = 0.57564431\n",
      "Iteration 177, loss = 0.57535421\n",
      "Iteration 178, loss = 0.57503199\n",
      "Iteration 179, loss = 0.57472673\n",
      "Iteration 180, loss = 0.57442185\n",
      "Iteration 181, loss = 0.57413450\n",
      "Iteration 182, loss = 0.57382836\n",
      "Iteration 183, loss = 0.57358814\n",
      "Iteration 184, loss = 0.57327102\n",
      "Iteration 185, loss = 0.57299692\n",
      "Iteration 186, loss = 0.57271606\n",
      "Iteration 187, loss = 0.57257686\n",
      "Iteration 188, loss = 0.57233964\n",
      "Iteration 189, loss = 0.57203625\n",
      "Iteration 190, loss = 0.57179333\n",
      "Iteration 191, loss = 0.57155907\n",
      "Iteration 192, loss = 0.57124415\n",
      "Iteration 193, loss = 0.57095855\n",
      "Iteration 194, loss = 0.57064938\n",
      "Iteration 195, loss = 0.57042804\n",
      "Iteration 196, loss = 0.57009292\n",
      "Iteration 197, loss = 0.56988625\n",
      "Iteration 198, loss = 0.56959737\n",
      "Iteration 199, loss = 0.56934758\n",
      "Iteration 200, loss = 0.56909419\n",
      "Iteration 201, loss = 0.56888401\n",
      "Iteration 202, loss = 0.56861124\n",
      "Iteration 203, loss = 0.56841451\n",
      "Iteration 204, loss = 0.56821128\n",
      "Iteration 205, loss = 0.56798023\n",
      "Iteration 206, loss = 0.56779208\n",
      "Iteration 207, loss = 0.56762433\n",
      "Iteration 208, loss = 0.56735977\n",
      "Iteration 209, loss = 0.56717909\n",
      "Iteration 210, loss = 0.56694739\n",
      "Iteration 211, loss = 0.56674400\n",
      "Iteration 212, loss = 0.56656051\n",
      "Iteration 213, loss = 0.56635962\n",
      "Iteration 214, loss = 0.56615473\n",
      "Iteration 215, loss = 0.56598183\n",
      "Iteration 216, loss = 0.56577234\n",
      "Iteration 217, loss = 0.56557491\n",
      "Iteration 218, loss = 0.56537664\n",
      "Iteration 219, loss = 0.56514564\n",
      "Iteration 220, loss = 0.56496791\n",
      "Iteration 221, loss = 0.56477860\n",
      "Iteration 222, loss = 0.56449688\n",
      "Iteration 223, loss = 0.56428816\n",
      "Iteration 224, loss = 0.56404631\n",
      "Iteration 225, loss = 0.56382853\n",
      "Iteration 226, loss = 0.56368388\n",
      "Iteration 227, loss = 0.56343168\n",
      "Iteration 228, loss = 0.56322351\n",
      "Iteration 229, loss = 0.56299912\n",
      "Iteration 230, loss = 0.56279433\n",
      "Iteration 231, loss = 0.56259415\n",
      "Iteration 232, loss = 0.56239416\n",
      "Iteration 233, loss = 0.56226548\n",
      "Iteration 234, loss = 0.56205160\n",
      "Iteration 235, loss = 0.56188998\n",
      "Iteration 236, loss = 0.56171549\n",
      "Iteration 237, loss = 0.56154432\n",
      "Iteration 238, loss = 0.56136008\n",
      "Iteration 239, loss = 0.56118733\n",
      "Iteration 240, loss = 0.56100011\n",
      "Iteration 241, loss = 0.56082918\n",
      "Iteration 242, loss = 0.56069557\n",
      "Iteration 243, loss = 0.56051428\n",
      "Iteration 244, loss = 0.56039278\n",
      "Iteration 245, loss = 0.56027302\n",
      "Iteration 246, loss = 0.56012648\n",
      "Iteration 247, loss = 0.56001211\n",
      "Iteration 248, loss = 0.55988172\n",
      "Iteration 249, loss = 0.55977828\n",
      "Iteration 250, loss = 0.55965324\n",
      "Iteration 251, loss = 0.55949164\n",
      "Iteration 252, loss = 0.55939990\n",
      "Iteration 253, loss = 0.55925275\n",
      "Iteration 254, loss = 0.55911722\n",
      "Iteration 255, loss = 0.55899689\n",
      "Iteration 256, loss = 0.55886878\n",
      "Iteration 257, loss = 0.55873538\n",
      "Iteration 258, loss = 0.55857572\n",
      "Iteration 259, loss = 0.55842412\n",
      "Iteration 260, loss = 0.55828047\n",
      "Iteration 261, loss = 0.55812808\n",
      "Iteration 262, loss = 0.55796818\n",
      "Iteration 263, loss = 0.55784340\n",
      "Iteration 264, loss = 0.55767573\n",
      "Iteration 265, loss = 0.55753567\n",
      "Iteration 266, loss = 0.55741457\n",
      "Iteration 267, loss = 0.55730178\n",
      "Iteration 268, loss = 0.55717005\n",
      "Iteration 269, loss = 0.55704371\n",
      "Iteration 270, loss = 0.55693217\n",
      "Iteration 271, loss = 0.55682325\n",
      "Iteration 272, loss = 0.55670893\n",
      "Iteration 273, loss = 0.55657683\n",
      "Iteration 274, loss = 0.55646007\n",
      "Iteration 275, loss = 0.55635327\n",
      "Iteration 276, loss = 0.55622676\n",
      "Iteration 277, loss = 0.55611192\n",
      "Iteration 278, loss = 0.55598470\n",
      "Iteration 279, loss = 0.55587124\n",
      "Iteration 280, loss = 0.55572213\n",
      "Iteration 281, loss = 0.55563424\n",
      "Iteration 282, loss = 0.55549343\n",
      "Iteration 283, loss = 0.55537598\n",
      "Iteration 284, loss = 0.55529481\n",
      "Iteration 285, loss = 0.55519686\n",
      "Iteration 286, loss = 0.55510575\n",
      "Iteration 287, loss = 0.55508746\n",
      "Iteration 288, loss = 0.55494793\n",
      "Iteration 289, loss = 0.55485559\n",
      "Iteration 290, loss = 0.55470897\n",
      "Iteration 291, loss = 0.55458969\n",
      "Iteration 292, loss = 0.55449933\n",
      "Iteration 293, loss = 0.55433222\n",
      "Iteration 294, loss = 0.55425246\n",
      "Iteration 295, loss = 0.55411380\n",
      "Iteration 296, loss = 0.55399308\n",
      "Iteration 297, loss = 0.55387141\n",
      "Iteration 298, loss = 0.55378937\n",
      "Iteration 299, loss = 0.55365452\n",
      "Iteration 300, loss = 0.55355352\n",
      "Iteration 301, loss = 0.55343917\n",
      "Iteration 302, loss = 0.55335015\n",
      "Iteration 303, loss = 0.55325319\n",
      "Iteration 304, loss = 0.55312474\n",
      "Iteration 305, loss = 0.55302008\n",
      "Iteration 306, loss = 0.55292058\n",
      "Iteration 307, loss = 0.55278636\n",
      "Iteration 308, loss = 0.55266646\n",
      "Iteration 309, loss = 0.55256317\n",
      "Iteration 310, loss = 0.55245022\n",
      "Iteration 311, loss = 0.55235198\n",
      "Iteration 312, loss = 0.55223866\n",
      "Iteration 313, loss = 0.55211946\n",
      "Iteration 314, loss = 0.55205620\n",
      "Iteration 315, loss = 0.55197956\n",
      "Iteration 316, loss = 0.55187356\n",
      "Iteration 317, loss = 0.55180087\n",
      "Iteration 318, loss = 0.55173770\n",
      "Iteration 319, loss = 0.55164410\n",
      "Iteration 320, loss = 0.55155867\n",
      "Iteration 321, loss = 0.55147478\n",
      "Iteration 322, loss = 0.55136187\n",
      "Iteration 323, loss = 0.55127993\n",
      "Iteration 324, loss = 0.55123207\n",
      "Iteration 325, loss = 0.55111378\n",
      "Iteration 326, loss = 0.55099826\n",
      "Iteration 327, loss = 0.55091663\n",
      "Iteration 328, loss = 0.55080110\n",
      "Iteration 329, loss = 0.55075288\n",
      "Iteration 330, loss = 0.55069987\n",
      "Iteration 331, loss = 0.55066629\n",
      "Iteration 332, loss = 0.55057547\n",
      "Iteration 333, loss = 0.55046986\n",
      "Iteration 334, loss = 0.55036738\n",
      "Iteration 335, loss = 0.55025461\n",
      "Iteration 336, loss = 0.55011874\n",
      "Iteration 337, loss = 0.55008745\n",
      "Iteration 338, loss = 0.54992367\n",
      "Iteration 339, loss = 0.54982052\n",
      "Iteration 340, loss = 0.54980083\n",
      "Iteration 341, loss = 0.54964296\n",
      "Iteration 342, loss = 0.54960415\n",
      "Iteration 343, loss = 0.54952579\n",
      "Iteration 344, loss = 0.54948063\n",
      "Iteration 345, loss = 0.54942560\n",
      "Iteration 346, loss = 0.54933311\n",
      "Iteration 347, loss = 0.54927216\n",
      "Iteration 348, loss = 0.54921364\n",
      "Iteration 349, loss = 0.54910256\n",
      "Iteration 350, loss = 0.54900456\n",
      "Iteration 351, loss = 0.54893850\n",
      "Iteration 352, loss = 0.54881061\n",
      "Iteration 353, loss = 0.54869269\n",
      "Iteration 354, loss = 0.54861076\n",
      "Iteration 355, loss = 0.54849152\n",
      "Iteration 356, loss = 0.54844934\n",
      "Iteration 357, loss = 0.54827131\n",
      "Iteration 358, loss = 0.54821053\n",
      "Iteration 359, loss = 0.54807538\n",
      "Iteration 360, loss = 0.54796821\n",
      "Iteration 361, loss = 0.54789833\n",
      "Iteration 362, loss = 0.54780155\n",
      "Iteration 363, loss = 0.54770827\n",
      "Iteration 364, loss = 0.54761678\n",
      "Iteration 365, loss = 0.54755636\n",
      "Iteration 366, loss = 0.54748572\n",
      "Iteration 367, loss = 0.54738723\n",
      "Iteration 368, loss = 0.54735498\n",
      "Iteration 369, loss = 0.54724453\n",
      "Iteration 370, loss = 0.54723246\n",
      "Iteration 371, loss = 0.54712350\n",
      "Iteration 372, loss = 0.54703930\n",
      "Iteration 373, loss = 0.54695217\n",
      "Iteration 374, loss = 0.54684209\n",
      "Iteration 375, loss = 0.54679734\n",
      "Iteration 376, loss = 0.54662304\n",
      "Iteration 377, loss = 0.54652383\n",
      "Iteration 378, loss = 0.54645787\n",
      "Iteration 379, loss = 0.54634241\n",
      "Iteration 380, loss = 0.54623250\n",
      "Iteration 381, loss = 0.54617427\n",
      "Iteration 382, loss = 0.54608938\n",
      "Iteration 383, loss = 0.54600997\n",
      "Iteration 384, loss = 0.54592762\n",
      "Iteration 385, loss = 0.54595803\n",
      "Iteration 386, loss = 0.54594396\n",
      "Iteration 387, loss = 0.54589525\n",
      "Iteration 388, loss = 0.54582704\n",
      "Iteration 389, loss = 0.54576917\n",
      "Iteration 390, loss = 0.54569285\n",
      "Iteration 391, loss = 0.54561673\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69863929\n",
      "Iteration 2, loss = 0.69168552\n",
      "Iteration 3, loss = 0.68633415\n",
      "Iteration 4, loss = 0.68274671\n",
      "Iteration 5, loss = 0.68069146\n",
      "Iteration 6, loss = 0.67807088\n",
      "Iteration 7, loss = 0.67497587\n",
      "Iteration 8, loss = 0.67179437\n",
      "Iteration 9, loss = 0.66837671\n",
      "Iteration 10, loss = 0.66546767\n",
      "Iteration 11, loss = 0.66240449\n",
      "Iteration 12, loss = 0.65935444\n",
      "Iteration 13, loss = 0.65737211\n",
      "Iteration 14, loss = 0.65398399\n",
      "Iteration 15, loss = 0.65155568\n",
      "Iteration 16, loss = 0.64893583\n",
      "Iteration 17, loss = 0.64583037\n",
      "Iteration 18, loss = 0.64250774\n",
      "Iteration 19, loss = 0.63962262\n",
      "Iteration 20, loss = 0.63703163\n",
      "Iteration 21, loss = 0.63475923\n",
      "Iteration 22, loss = 0.63261502\n",
      "Iteration 23, loss = 0.63166769\n",
      "Iteration 24, loss = 0.63056373\n",
      "Iteration 25, loss = 0.62828191\n",
      "Iteration 26, loss = 0.62511990\n",
      "Iteration 27, loss = 0.62092003\n",
      "Iteration 28, loss = 0.61765797\n",
      "Iteration 29, loss = 0.61459717\n",
      "Iteration 30, loss = 0.61158397\n",
      "Iteration 31, loss = 0.60911216\n",
      "Iteration 32, loss = 0.60682545\n",
      "Iteration 33, loss = 0.60430692\n",
      "Iteration 34, loss = 0.60201085\n",
      "Iteration 35, loss = 0.59966656\n",
      "Iteration 36, loss = 0.59747618\n",
      "Iteration 37, loss = 0.59581010\n",
      "Iteration 38, loss = 0.59345883\n",
      "Iteration 39, loss = 0.59103164\n",
      "Iteration 40, loss = 0.58868403\n",
      "Iteration 41, loss = 0.58810674\n",
      "Iteration 42, loss = 0.58600868\n",
      "Iteration 43, loss = 0.58467057\n",
      "Iteration 44, loss = 0.58265696\n",
      "Iteration 45, loss = 0.58008807\n",
      "Iteration 46, loss = 0.57831322\n",
      "Iteration 47, loss = 0.57680831\n",
      "Iteration 48, loss = 0.57470585\n",
      "Iteration 49, loss = 0.57288922\n",
      "Iteration 50, loss = 0.57160109\n",
      "Iteration 51, loss = 0.57024286\n",
      "Iteration 52, loss = 0.56914882\n",
      "Iteration 53, loss = 0.56726758\n",
      "Iteration 54, loss = 0.56524645\n",
      "Iteration 55, loss = 0.56435023\n",
      "Iteration 56, loss = 0.56323241\n",
      "Iteration 57, loss = 0.56254588\n",
      "Iteration 58, loss = 0.56246161\n",
      "Iteration 59, loss = 0.56166347\n",
      "Iteration 60, loss = 0.55958022\n",
      "Iteration 61, loss = 0.55687669\n",
      "Iteration 62, loss = 0.55507447\n",
      "Iteration 63, loss = 0.55396157\n",
      "Iteration 64, loss = 0.55301992\n",
      "Iteration 65, loss = 0.55286851\n",
      "Iteration 66, loss = 0.55319425\n",
      "Iteration 67, loss = 0.55187913\n",
      "Iteration 68, loss = 0.54964545\n",
      "Iteration 69, loss = 0.54736490\n",
      "Iteration 70, loss = 0.54797092\n",
      "Iteration 71, loss = 0.54663499\n",
      "Iteration 72, loss = 0.54582162\n",
      "Iteration 73, loss = 0.54503725\n",
      "Iteration 74, loss = 0.54383690\n",
      "Iteration 75, loss = 0.54286872\n",
      "Iteration 76, loss = 0.54188583\n",
      "Iteration 77, loss = 0.54124392\n",
      "Iteration 78, loss = 0.54186888\n",
      "Iteration 79, loss = 0.54146821\n",
      "Iteration 80, loss = 0.54016830\n",
      "Iteration 81, loss = 0.53879407\n",
      "Iteration 82, loss = 0.53822114\n",
      "Iteration 83, loss = 0.53756657\n",
      "Iteration 84, loss = 0.53694030\n",
      "Iteration 85, loss = 0.53634730\n",
      "Iteration 86, loss = 0.53587875\n",
      "Iteration 87, loss = 0.53541343\n",
      "Iteration 88, loss = 0.53486413\n",
      "Iteration 89, loss = 0.53456191\n",
      "Iteration 90, loss = 0.53400917\n",
      "Iteration 91, loss = 0.53361011\n",
      "Iteration 92, loss = 0.53314997\n",
      "Iteration 93, loss = 0.53289148\n",
      "Iteration 94, loss = 0.53258734\n",
      "Iteration 95, loss = 0.53264499\n",
      "Iteration 96, loss = 0.53228919\n",
      "Iteration 97, loss = 0.53180866\n",
      "Iteration 98, loss = 0.53159700\n",
      "Iteration 99, loss = 0.53068461\n",
      "Iteration 100, loss = 0.53069189\n",
      "Iteration 101, loss = 0.53053746\n",
      "Iteration 102, loss = 0.53008876\n",
      "Iteration 103, loss = 0.52955617\n",
      "Iteration 104, loss = 0.52894689\n",
      "Iteration 105, loss = 0.52884709\n",
      "Iteration 106, loss = 0.52848147\n",
      "Iteration 107, loss = 0.52814775\n",
      "Iteration 108, loss = 0.52780881\n",
      "Iteration 109, loss = 0.52757134\n",
      "Iteration 110, loss = 0.52787077\n",
      "Iteration 111, loss = 0.52741839\n",
      "Iteration 112, loss = 0.52729881\n",
      "Iteration 113, loss = 0.52668784\n",
      "Iteration 114, loss = 0.52639214\n",
      "Iteration 115, loss = 0.52605939\n",
      "Iteration 116, loss = 0.52585678\n",
      "Iteration 117, loss = 0.52563753\n",
      "Iteration 118, loss = 0.52561217\n",
      "Iteration 119, loss = 0.52711371\n",
      "Iteration 120, loss = 0.52694750\n",
      "Iteration 121, loss = 0.52562186\n",
      "Iteration 122, loss = 0.52463649\n",
      "Iteration 123, loss = 0.52453228\n",
      "Iteration 124, loss = 0.52550333\n",
      "Iteration 125, loss = 0.52618532\n",
      "Iteration 126, loss = 0.52604534\n",
      "Iteration 127, loss = 0.52499831\n",
      "Iteration 128, loss = 0.52387012\n",
      "Iteration 129, loss = 0.52326678\n",
      "Iteration 130, loss = 0.52374487\n",
      "Iteration 131, loss = 0.52455833\n",
      "Iteration 132, loss = 0.52532806\n",
      "Iteration 133, loss = 0.52557969\n",
      "Iteration 134, loss = 0.52517084\n",
      "Iteration 135, loss = 0.52396493\n",
      "Iteration 136, loss = 0.52242771\n",
      "Iteration 137, loss = 0.52366288\n",
      "Iteration 138, loss = 0.52299060\n",
      "Iteration 139, loss = 0.52366306\n",
      "Iteration 140, loss = 0.52276126\n",
      "Iteration 141, loss = 0.52202610\n",
      "Iteration 142, loss = 0.52171311\n",
      "Iteration 143, loss = 0.52266871\n",
      "Iteration 144, loss = 0.52314098\n",
      "Iteration 145, loss = 0.52323909\n",
      "Iteration 146, loss = 0.52289675\n",
      "Iteration 147, loss = 0.52253619\n",
      "Iteration 148, loss = 0.52198186\n",
      "Iteration 149, loss = 0.52181360\n",
      "Iteration 150, loss = 0.52133651\n",
      "Iteration 151, loss = 0.52103379\n",
      "Iteration 152, loss = 0.52155870\n",
      "Iteration 153, loss = 0.52078794\n",
      "Iteration 154, loss = 0.52068840\n",
      "Iteration 155, loss = 0.52059892\n",
      "Iteration 156, loss = 0.52054200\n",
      "Iteration 157, loss = 0.52047723\n",
      "Iteration 158, loss = 0.52036530\n",
      "Iteration 159, loss = 0.52034451\n",
      "Iteration 160, loss = 0.52027547\n",
      "Iteration 161, loss = 0.52019292\n",
      "Iteration 162, loss = 0.52026870\n",
      "Iteration 163, loss = 0.51997662\n",
      "Iteration 164, loss = 0.52010904\n",
      "Iteration 165, loss = 0.52015508\n",
      "Iteration 166, loss = 0.52015506\n",
      "Iteration 167, loss = 0.52004802\n",
      "Iteration 168, loss = 0.52006147\n",
      "Iteration 169, loss = 0.52009266\n",
      "Iteration 170, loss = 0.52020136\n",
      "Iteration 171, loss = 0.52007527\n",
      "Iteration 172, loss = 0.51953926\n",
      "Iteration 173, loss = 0.51933433\n",
      "Iteration 174, loss = 0.52062232\n",
      "Iteration 175, loss = 0.52066256\n",
      "Iteration 176, loss = 0.52044235\n",
      "Iteration 177, loss = 0.52026503\n",
      "Iteration 178, loss = 0.51979631\n",
      "Iteration 179, loss = 0.51964513\n",
      "Iteration 180, loss = 0.52020540\n",
      "Iteration 181, loss = 0.52068218\n",
      "Iteration 182, loss = 0.52085675\n",
      "Iteration 183, loss = 0.52043536\n",
      "Iteration 184, loss = 0.51959671\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.59415968\n",
      "Iteration 2, loss = 1.55322272\n",
      "Iteration 3, loss = 1.52234202\n",
      "Iteration 4, loss = 1.49263835\n",
      "Iteration 5, loss = 1.46617340\n",
      "Iteration 6, loss = 1.43824989\n",
      "Iteration 7, loss = 1.41301629\n",
      "Iteration 8, loss = 1.38955607\n",
      "Iteration 9, loss = 1.36439075\n",
      "Iteration 10, loss = 1.34004899\n",
      "Iteration 11, loss = 1.31538784\n",
      "Iteration 12, loss = 1.29188981\n",
      "Iteration 13, loss = 1.26827680\n",
      "Iteration 14, loss = 1.24519197\n",
      "Iteration 15, loss = 1.22252495\n",
      "Iteration 16, loss = 1.20201554\n",
      "Iteration 17, loss = 1.18211255\n",
      "Iteration 18, loss = 1.16295660\n",
      "Iteration 19, loss = 1.14406603\n",
      "Iteration 20, loss = 1.12596931\n",
      "Iteration 21, loss = 1.11048304\n",
      "Iteration 22, loss = 1.09502654\n",
      "Iteration 23, loss = 1.07771686\n",
      "Iteration 24, loss = 1.06205420\n",
      "Iteration 25, loss = 1.04704853\n",
      "Iteration 26, loss = 1.03297370\n",
      "Iteration 27, loss = 1.01938313\n",
      "Iteration 28, loss = 1.00673875\n",
      "Iteration 29, loss = 0.99588669\n",
      "Iteration 30, loss = 0.98398951\n",
      "Iteration 31, loss = 0.97303636\n",
      "Iteration 32, loss = 0.96305249\n",
      "Iteration 33, loss = 0.95249525\n",
      "Iteration 34, loss = 0.94297638\n",
      "Iteration 35, loss = 0.93332891\n",
      "Iteration 36, loss = 0.92395239\n",
      "Iteration 37, loss = 0.91577046\n",
      "Iteration 38, loss = 0.90745909\n",
      "Iteration 39, loss = 0.90004253\n",
      "Iteration 40, loss = 0.89241554\n",
      "Iteration 41, loss = 0.88634949\n",
      "Iteration 42, loss = 0.88057871\n",
      "Iteration 43, loss = 0.87393017\n",
      "Iteration 44, loss = 0.86953433\n",
      "Iteration 45, loss = 0.86501091\n",
      "Iteration 46, loss = 0.86080354\n",
      "Iteration 47, loss = 0.85609621\n",
      "Iteration 48, loss = 0.85272901\n",
      "Iteration 49, loss = 0.84867339\n",
      "Iteration 50, loss = 0.84500416\n",
      "Iteration 51, loss = 0.84193127\n",
      "Iteration 52, loss = 0.83842072\n",
      "Iteration 53, loss = 0.83534368\n",
      "Iteration 54, loss = 0.83200762\n",
      "Iteration 55, loss = 0.82894994\n",
      "Iteration 56, loss = 0.82603376\n",
      "Iteration 57, loss = 0.82291702\n",
      "Iteration 58, loss = 0.82007887\n",
      "Iteration 59, loss = 0.81753081\n",
      "Iteration 60, loss = 0.81480240\n",
      "Iteration 61, loss = 0.81245123\n",
      "Iteration 62, loss = 0.80960692\n",
      "Iteration 63, loss = 0.80716742\n",
      "Iteration 64, loss = 0.80468691\n",
      "Iteration 65, loss = 0.80215565\n",
      "Iteration 66, loss = 0.79989714\n",
      "Iteration 67, loss = 0.79722183\n",
      "Iteration 68, loss = 0.79528610\n",
      "Iteration 69, loss = 0.79281115\n",
      "Iteration 70, loss = 0.79067411\n",
      "Iteration 71, loss = 0.78875533\n",
      "Iteration 72, loss = 0.78649296\n",
      "Iteration 73, loss = 0.78453478\n",
      "Iteration 74, loss = 0.78241925\n",
      "Iteration 75, loss = 0.78050123\n",
      "Iteration 76, loss = 0.77843956\n",
      "Iteration 77, loss = 0.77645777\n",
      "Iteration 78, loss = 0.77504398\n",
      "Iteration 79, loss = 0.77292875\n",
      "Iteration 80, loss = 0.77100410\n",
      "Iteration 81, loss = 0.76921944\n",
      "Iteration 82, loss = 0.76741535\n",
      "Iteration 83, loss = 0.76564589\n",
      "Iteration 84, loss = 0.76386324\n",
      "Iteration 85, loss = 0.76223697\n",
      "Iteration 86, loss = 0.76054163\n",
      "Iteration 87, loss = 0.75909900\n",
      "Iteration 88, loss = 0.75708181\n",
      "Iteration 89, loss = 0.75546740\n",
      "Iteration 90, loss = 0.75360571\n",
      "Iteration 91, loss = 0.75186001\n",
      "Iteration 92, loss = 0.75003734\n",
      "Iteration 93, loss = 0.74845500\n",
      "Iteration 94, loss = 0.74683221\n",
      "Iteration 95, loss = 0.74528176\n",
      "Iteration 96, loss = 0.74378116\n",
      "Iteration 97, loss = 0.74230296\n",
      "Iteration 98, loss = 0.74052668\n",
      "Iteration 99, loss = 0.73880482\n",
      "Iteration 100, loss = 0.73711295\n",
      "Iteration 101, loss = 0.73537970\n",
      "Iteration 102, loss = 0.73436112\n",
      "Iteration 103, loss = 0.73241451\n",
      "Iteration 104, loss = 0.73071607\n",
      "Iteration 105, loss = 0.72946417\n",
      "Iteration 106, loss = 0.72805640\n",
      "Iteration 107, loss = 0.72650937\n",
      "Iteration 108, loss = 0.72495777\n",
      "Iteration 109, loss = 0.72337316\n",
      "Iteration 110, loss = 0.72189772\n",
      "Iteration 111, loss = 0.72051347\n",
      "Iteration 112, loss = 0.71856381\n",
      "Iteration 113, loss = 0.71694959\n",
      "Iteration 114, loss = 0.71543646\n",
      "Iteration 115, loss = 0.71361507\n",
      "Iteration 116, loss = 0.71216650\n",
      "Iteration 117, loss = 0.71046312\n",
      "Iteration 118, loss = 0.70898648\n",
      "Iteration 119, loss = 0.70745625\n",
      "Iteration 120, loss = 0.70621582\n",
      "Iteration 121, loss = 0.70457960\n",
      "Iteration 122, loss = 0.70293831\n",
      "Iteration 123, loss = 0.70174490\n",
      "Iteration 124, loss = 0.70012351\n",
      "Iteration 125, loss = 0.69873291\n",
      "Iteration 126, loss = 0.69734158\n",
      "Iteration 127, loss = 0.69643462\n",
      "Iteration 128, loss = 0.69529825\n",
      "Iteration 129, loss = 0.69409605\n",
      "Iteration 130, loss = 0.69287429\n",
      "Iteration 131, loss = 0.69153097\n",
      "Iteration 132, loss = 0.69009976\n",
      "Iteration 133, loss = 0.68867079\n",
      "Iteration 134, loss = 0.68726528\n",
      "Iteration 135, loss = 0.68591540\n",
      "Iteration 136, loss = 0.68454919\n",
      "Iteration 137, loss = 0.68313732\n",
      "Iteration 138, loss = 0.68196769\n",
      "Iteration 139, loss = 0.68070599\n",
      "Iteration 140, loss = 0.67937102\n",
      "Iteration 141, loss = 0.67812559\n",
      "Iteration 142, loss = 0.67731023\n",
      "Iteration 143, loss = 0.67576999\n",
      "Iteration 144, loss = 0.67453774\n",
      "Iteration 145, loss = 0.67331693\n",
      "Iteration 146, loss = 0.67210584\n",
      "Iteration 147, loss = 0.67100586\n",
      "Iteration 148, loss = 0.66979480\n",
      "Iteration 149, loss = 0.66868377\n",
      "Iteration 150, loss = 0.66787560\n",
      "Iteration 151, loss = 0.66670609\n",
      "Iteration 152, loss = 0.66556102\n",
      "Iteration 153, loss = 0.66445521\n",
      "Iteration 154, loss = 0.66338283\n",
      "Iteration 155, loss = 0.66210620\n",
      "Iteration 156, loss = 0.66089760\n",
      "Iteration 157, loss = 0.65969713\n",
      "Iteration 158, loss = 0.65843876\n",
      "Iteration 159, loss = 0.65715654\n",
      "Iteration 160, loss = 0.65600094\n",
      "Iteration 161, loss = 0.65467702\n",
      "Iteration 162, loss = 0.65387648\n",
      "Iteration 163, loss = 0.65234881\n",
      "Iteration 164, loss = 0.65106061\n",
      "Iteration 165, loss = 0.64992769\n",
      "Iteration 166, loss = 0.64872169\n",
      "Iteration 167, loss = 0.64778244\n",
      "Iteration 168, loss = 0.64656129\n",
      "Iteration 169, loss = 0.64545973\n",
      "Iteration 170, loss = 0.64432591\n",
      "Iteration 171, loss = 0.64323039\n",
      "Iteration 172, loss = 0.64223811\n",
      "Iteration 173, loss = 0.64113804\n",
      "Iteration 174, loss = 0.64016971\n",
      "Iteration 175, loss = 0.63913760\n",
      "Iteration 176, loss = 0.63819779\n",
      "Iteration 177, loss = 0.63719916\n",
      "Iteration 178, loss = 0.63630663\n",
      "Iteration 179, loss = 0.63527140\n",
      "Iteration 180, loss = 0.63430973\n",
      "Iteration 181, loss = 0.63348935\n",
      "Iteration 182, loss = 0.63255520\n",
      "Iteration 183, loss = 0.63168012\n",
      "Iteration 184, loss = 0.63094717\n",
      "Iteration 185, loss = 0.63018283\n",
      "Iteration 186, loss = 0.62949383\n",
      "Iteration 187, loss = 0.62872441\n",
      "Iteration 188, loss = 0.62770264\n",
      "Iteration 189, loss = 0.62683940\n",
      "Iteration 190, loss = 0.62673553\n",
      "Iteration 191, loss = 0.62583528\n",
      "Iteration 192, loss = 0.62528713\n",
      "Iteration 193, loss = 0.62480793\n",
      "Iteration 194, loss = 0.62427995\n",
      "Iteration 195, loss = 0.62354820\n",
      "Iteration 196, loss = 0.62295435\n",
      "Iteration 197, loss = 0.62212049\n",
      "Iteration 198, loss = 0.62109937\n",
      "Iteration 199, loss = 0.62015437\n",
      "Iteration 200, loss = 0.61910021\n",
      "Iteration 201, loss = 0.61831739\n",
      "Iteration 202, loss = 0.61774862\n",
      "Iteration 203, loss = 0.61700018\n",
      "Iteration 204, loss = 0.61654212\n",
      "Iteration 205, loss = 0.61578487\n",
      "Iteration 206, loss = 0.61511462\n",
      "Iteration 207, loss = 0.61441744\n",
      "Iteration 208, loss = 0.61369388\n",
      "Iteration 209, loss = 0.61292889\n",
      "Iteration 210, loss = 0.61211904\n",
      "Iteration 211, loss = 0.61153590\n",
      "Iteration 212, loss = 0.61083079\n",
      "Iteration 213, loss = 0.61012486\n",
      "Iteration 214, loss = 0.60941136\n",
      "Iteration 215, loss = 0.60866555\n",
      "Iteration 216, loss = 0.60800665\n",
      "Iteration 217, loss = 0.60724202\n",
      "Iteration 218, loss = 0.60624271\n",
      "Iteration 219, loss = 0.60565670\n",
      "Iteration 220, loss = 0.60493971\n",
      "Iteration 221, loss = 0.60450559\n",
      "Iteration 222, loss = 0.60384345\n",
      "Iteration 223, loss = 0.60333836\n",
      "Iteration 224, loss = 0.60279412\n",
      "Iteration 225, loss = 0.60242282\n",
      "Iteration 226, loss = 0.60174368\n",
      "Iteration 227, loss = 0.60104657\n",
      "Iteration 228, loss = 0.60040477\n",
      "Iteration 229, loss = 0.59983830\n",
      "Iteration 230, loss = 0.59941403\n",
      "Iteration 231, loss = 0.59896510\n",
      "Iteration 232, loss = 0.59842255\n",
      "Iteration 233, loss = 0.59790724\n",
      "Iteration 234, loss = 0.59744179\n",
      "Iteration 235, loss = 0.59682578\n",
      "Iteration 236, loss = 0.59630617\n",
      "Iteration 237, loss = 0.59623657\n",
      "Iteration 238, loss = 0.59579959\n",
      "Iteration 239, loss = 0.59552051\n",
      "Iteration 240, loss = 0.59520088\n",
      "Iteration 241, loss = 0.59504825\n",
      "Iteration 242, loss = 0.59479564\n",
      "Iteration 243, loss = 0.59442119\n",
      "Iteration 244, loss = 0.59404661\n",
      "Iteration 245, loss = 0.59349138\n",
      "Iteration 246, loss = 0.59273445\n",
      "Iteration 247, loss = 0.59199363\n",
      "Iteration 248, loss = 0.59128446\n",
      "Iteration 249, loss = 0.59070864\n",
      "Iteration 250, loss = 0.59015742\n",
      "Iteration 251, loss = 0.58958041\n",
      "Iteration 252, loss = 0.58919396\n",
      "Iteration 253, loss = 0.58862998\n",
      "Iteration 254, loss = 0.58813009\n",
      "Iteration 255, loss = 0.58749901\n",
      "Iteration 256, loss = 0.58682780\n",
      "Iteration 257, loss = 0.58640736\n",
      "Iteration 258, loss = 0.58615455\n",
      "Iteration 259, loss = 0.58604104\n",
      "Iteration 260, loss = 0.58610575\n",
      "Iteration 261, loss = 0.58579220\n",
      "Iteration 262, loss = 0.58544936\n",
      "Iteration 263, loss = 0.58508351\n",
      "Iteration 264, loss = 0.58467074\n",
      "Iteration 265, loss = 0.58416231\n",
      "Iteration 266, loss = 0.58368517\n",
      "Iteration 267, loss = 0.58319484\n",
      "Iteration 268, loss = 0.58263793\n",
      "Iteration 269, loss = 0.58206555\n",
      "Iteration 270, loss = 0.58152771\n",
      "Iteration 271, loss = 0.58084788\n",
      "Iteration 272, loss = 0.58035087\n",
      "Iteration 273, loss = 0.57991392\n",
      "Iteration 274, loss = 0.57941340\n",
      "Iteration 275, loss = 0.57900286\n",
      "Iteration 276, loss = 0.57850464\n",
      "Iteration 277, loss = 0.57810151\n",
      "Iteration 278, loss = 0.57770062\n",
      "Iteration 279, loss = 0.57725093\n",
      "Iteration 280, loss = 0.57684014\n",
      "Iteration 281, loss = 0.57654787\n",
      "Iteration 282, loss = 0.57609582\n",
      "Iteration 283, loss = 0.57571369\n",
      "Iteration 284, loss = 0.57537253\n",
      "Iteration 285, loss = 0.57501951\n",
      "Iteration 286, loss = 0.57475322\n",
      "Iteration 287, loss = 0.57462400\n",
      "Iteration 288, loss = 0.57427090\n",
      "Iteration 289, loss = 0.57391054\n",
      "Iteration 290, loss = 0.57352898\n",
      "Iteration 291, loss = 0.57342707\n",
      "Iteration 292, loss = 0.57269555\n",
      "Iteration 293, loss = 0.57234767\n",
      "Iteration 294, loss = 0.57201304\n",
      "Iteration 295, loss = 0.57166027\n",
      "Iteration 296, loss = 0.57146401\n",
      "Iteration 297, loss = 0.57098477\n",
      "Iteration 298, loss = 0.57066259\n",
      "Iteration 299, loss = 0.57041891\n",
      "Iteration 300, loss = 0.57007920\n",
      "Iteration 301, loss = 0.56976414\n",
      "Iteration 302, loss = 0.56949435\n",
      "Iteration 303, loss = 0.56925096\n",
      "Iteration 304, loss = 0.56894957\n",
      "Iteration 305, loss = 0.56868929\n",
      "Iteration 306, loss = 0.56853772\n",
      "Iteration 307, loss = 0.56823569\n",
      "Iteration 308, loss = 0.56792933\n",
      "Iteration 309, loss = 0.56767306\n",
      "Iteration 310, loss = 0.56749047\n",
      "Iteration 311, loss = 0.56727928\n",
      "Iteration 312, loss = 0.56707697\n",
      "Iteration 313, loss = 0.56690446\n",
      "Iteration 314, loss = 0.56668101\n",
      "Iteration 315, loss = 0.56642905\n",
      "Iteration 316, loss = 0.56616309\n",
      "Iteration 317, loss = 0.56588611\n",
      "Iteration 318, loss = 0.56565263\n",
      "Iteration 319, loss = 0.56538119\n",
      "Iteration 320, loss = 0.56510673\n",
      "Iteration 321, loss = 0.56483472\n",
      "Iteration 322, loss = 0.56467807\n",
      "Iteration 323, loss = 0.56467646\n",
      "Iteration 324, loss = 0.56443121\n",
      "Iteration 325, loss = 0.56436270\n",
      "Iteration 326, loss = 0.56426274\n",
      "Iteration 327, loss = 0.56407633\n",
      "Iteration 328, loss = 0.56382945\n",
      "Iteration 329, loss = 0.56370031\n",
      "Iteration 330, loss = 0.56345550\n",
      "Iteration 331, loss = 0.56325983\n",
      "Iteration 332, loss = 0.56308702\n",
      "Iteration 333, loss = 0.56298475\n",
      "Iteration 334, loss = 0.56272163\n",
      "Iteration 335, loss = 0.56265506\n",
      "Iteration 336, loss = 0.56259622\n",
      "Iteration 337, loss = 0.56249461\n",
      "Iteration 338, loss = 0.56237119\n",
      "Iteration 339, loss = 0.56227204\n",
      "Iteration 340, loss = 0.56213130\n",
      "Iteration 341, loss = 0.56204931\n",
      "Iteration 342, loss = 0.56184846\n",
      "Iteration 343, loss = 0.56179960\n",
      "Iteration 344, loss = 0.56168179\n",
      "Iteration 345, loss = 0.56148242\n",
      "Iteration 346, loss = 0.56136624\n",
      "Iteration 347, loss = 0.56121810\n",
      "Iteration 348, loss = 0.56112785\n",
      "Iteration 349, loss = 0.56094798\n",
      "Iteration 350, loss = 0.56097402\n",
      "Iteration 351, loss = 0.56083550\n",
      "Iteration 352, loss = 0.56067647\n",
      "Iteration 353, loss = 0.56053785\n",
      "Iteration 354, loss = 0.56032043\n",
      "Iteration 355, loss = 0.56014908\n",
      "Iteration 356, loss = 0.55992195\n",
      "Iteration 357, loss = 0.55965893\n",
      "Iteration 358, loss = 0.55951389\n",
      "Iteration 359, loss = 0.55923421\n",
      "Iteration 360, loss = 0.55930570\n",
      "Iteration 361, loss = 0.55915386\n",
      "Iteration 362, loss = 0.55897349\n",
      "Iteration 363, loss = 0.55883728\n",
      "Iteration 364, loss = 0.55872823\n",
      "Iteration 365, loss = 0.55863341\n",
      "Iteration 366, loss = 0.55850413\n",
      "Iteration 367, loss = 0.55834331\n",
      "Iteration 368, loss = 0.55819964\n",
      "Iteration 369, loss = 0.55809708\n",
      "Iteration 370, loss = 0.55796176\n",
      "Iteration 371, loss = 0.55786938\n",
      "Iteration 372, loss = 0.55774087\n",
      "Iteration 373, loss = 0.55758367\n",
      "Iteration 374, loss = 0.55742690\n",
      "Iteration 375, loss = 0.55728089\n",
      "Iteration 376, loss = 0.55724939\n",
      "Iteration 377, loss = 0.55701092\n",
      "Iteration 378, loss = 0.55689670\n",
      "Iteration 379, loss = 0.55679361\n",
      "Iteration 380, loss = 0.55668791\n",
      "Iteration 381, loss = 0.55654467\n",
      "Iteration 382, loss = 0.55655250\n",
      "Iteration 383, loss = 0.55657553\n",
      "Iteration 384, loss = 0.55648717\n",
      "Iteration 385, loss = 0.55637840\n",
      "Iteration 386, loss = 0.55621876\n",
      "Iteration 387, loss = 0.55599267\n",
      "Iteration 388, loss = 0.55582887\n",
      "Iteration 389, loss = 0.55565256\n",
      "Iteration 390, loss = 0.55553704\n",
      "Iteration 391, loss = 0.55548892\n",
      "Iteration 392, loss = 0.55536803\n",
      "Iteration 393, loss = 0.55529855\n",
      "Iteration 394, loss = 0.55520340\n",
      "Iteration 395, loss = 0.55515269\n",
      "Iteration 396, loss = 0.55500165\n",
      "Iteration 397, loss = 0.55483829\n",
      "Iteration 398, loss = 0.55478661\n",
      "Iteration 399, loss = 0.55461623\n",
      "Iteration 400, loss = 0.55450075\n",
      "Iteration 401, loss = 0.55441567\n",
      "Iteration 402, loss = 0.55427501\n",
      "Iteration 403, loss = 0.55418666\n",
      "Iteration 404, loss = 0.55417269\n",
      "Iteration 405, loss = 0.55415049\n",
      "Iteration 406, loss = 0.55406288\n",
      "Iteration 407, loss = 0.55399626\n",
      "Iteration 408, loss = 0.55388021\n",
      "Iteration 409, loss = 0.55375647\n",
      "Iteration 410, loss = 0.55368124\n",
      "Iteration 411, loss = 0.55352151\n",
      "Iteration 412, loss = 0.55350143\n",
      "Iteration 413, loss = 0.55340336\n",
      "Iteration 414, loss = 0.55329937\n",
      "Iteration 415, loss = 0.55321493\n",
      "Iteration 416, loss = 0.55309061\n",
      "Iteration 417, loss = 0.55299765\n",
      "Iteration 418, loss = 0.55291745\n",
      "Iteration 419, loss = 0.55285181\n",
      "Iteration 420, loss = 0.55269786\n",
      "Iteration 421, loss = 0.55262327\n",
      "Iteration 422, loss = 0.55250065\n",
      "Iteration 423, loss = 0.55236686\n",
      "Iteration 424, loss = 0.55224434\n",
      "Iteration 425, loss = 0.55210841\n",
      "Iteration 426, loss = 0.55194077\n",
      "Iteration 427, loss = 0.55216518\n",
      "Iteration 428, loss = 0.55188365\n",
      "Iteration 429, loss = 0.55179818\n",
      "Iteration 430, loss = 0.55173318\n",
      "Iteration 431, loss = 0.55162175\n",
      "Iteration 432, loss = 0.55166329\n",
      "Iteration 433, loss = 0.55148426\n",
      "Iteration 434, loss = 0.55134467\n",
      "Iteration 435, loss = 0.55129741\n",
      "Iteration 436, loss = 0.55130375\n",
      "Iteration 437, loss = 0.55116667\n",
      "Iteration 438, loss = 0.55108714\n",
      "Iteration 439, loss = 0.55105998\n",
      "Iteration 440, loss = 0.55110145\n",
      "Iteration 441, loss = 0.55102378\n",
      "Iteration 442, loss = 0.55093369\n",
      "Iteration 443, loss = 0.55090850\n",
      "Iteration 444, loss = 0.55081746\n",
      "Iteration 445, loss = 0.55069843\n",
      "Iteration 446, loss = 0.55056396\n",
      "Iteration 447, loss = 0.55039393\n",
      "Iteration 448, loss = 0.55049499\n",
      "Iteration 449, loss = 0.55024233\n",
      "Iteration 450, loss = 0.55015523\n",
      "Iteration 451, loss = 0.55005661\n",
      "Iteration 452, loss = 0.54995766\n",
      "Iteration 453, loss = 0.54987707\n",
      "Iteration 454, loss = 0.54987236\n",
      "Iteration 455, loss = 0.54975603\n",
      "Iteration 456, loss = 0.54972829\n",
      "Iteration 457, loss = 0.54963656\n",
      "Iteration 458, loss = 0.54956768\n",
      "Iteration 459, loss = 0.54940717\n",
      "Iteration 460, loss = 0.54926349\n",
      "Iteration 461, loss = 0.54921840\n",
      "Iteration 462, loss = 0.54913196\n",
      "Iteration 463, loss = 0.54908809\n",
      "Iteration 464, loss = 0.54900299\n",
      "Iteration 465, loss = 0.54918326\n",
      "Iteration 466, loss = 0.54898419\n",
      "Iteration 467, loss = 0.54890862\n",
      "Iteration 468, loss = 0.54886209\n",
      "Iteration 469, loss = 0.54872115\n",
      "Iteration 470, loss = 0.54854988\n",
      "Iteration 471, loss = 0.54846350\n",
      "Iteration 472, loss = 0.54824027\n",
      "Iteration 473, loss = 0.54821397\n",
      "Iteration 474, loss = 0.54818423\n",
      "Iteration 475, loss = 0.54815347\n",
      "Iteration 476, loss = 0.54806929\n",
      "Iteration 477, loss = 0.54796705\n",
      "Iteration 478, loss = 0.54783142\n",
      "Iteration 479, loss = 0.54769445\n",
      "Iteration 480, loss = 0.54754512\n",
      "Iteration 481, loss = 0.54768831\n",
      "Iteration 482, loss = 0.54782531\n",
      "Iteration 483, loss = 0.54766277\n",
      "Iteration 484, loss = 0.54758272\n",
      "Iteration 485, loss = 0.54746732\n",
      "Iteration 486, loss = 0.54735685\n",
      "Iteration 487, loss = 0.54727287\n",
      "Iteration 488, loss = 0.54718544\n",
      "Iteration 489, loss = 0.54706834\n",
      "Iteration 490, loss = 0.54707840\n",
      "Iteration 491, loss = 0.54695388\n",
      "Iteration 492, loss = 0.54695707\n",
      "Iteration 493, loss = 0.54685172\n",
      "Iteration 494, loss = 0.54676452\n",
      "Iteration 495, loss = 0.54672400\n",
      "Iteration 496, loss = 0.54661355\n",
      "Iteration 497, loss = 0.54664963\n",
      "Iteration 498, loss = 0.54648474\n",
      "Iteration 499, loss = 0.54637664\n",
      "Iteration 500, loss = 0.54632328\n",
      "Iteration 501, loss = 0.54620536\n",
      "Iteration 502, loss = 0.54614209\n",
      "Iteration 503, loss = 0.54605755\n",
      "Iteration 504, loss = 0.54608233\n",
      "Iteration 505, loss = 0.54594710\n",
      "Iteration 506, loss = 0.54590617\n",
      "Iteration 507, loss = 0.54589233\n",
      "Iteration 508, loss = 0.54572874\n",
      "Iteration 509, loss = 0.54566290\n",
      "Iteration 510, loss = 0.54560067\n",
      "Iteration 511, loss = 0.54556039\n",
      "Iteration 512, loss = 0.54548150\n",
      "Iteration 513, loss = 0.54545049\n",
      "Iteration 514, loss = 0.54534671\n",
      "Iteration 515, loss = 0.54529701\n",
      "Iteration 516, loss = 0.54524124\n",
      "Iteration 517, loss = 0.54519202\n",
      "Iteration 518, loss = 0.54517803\n",
      "Iteration 519, loss = 0.54502941\n",
      "Iteration 520, loss = 0.54503623\n",
      "Iteration 521, loss = 0.54504919\n",
      "Iteration 522, loss = 0.54528978\n",
      "Iteration 523, loss = 0.54514880\n",
      "Iteration 524, loss = 0.54511596\n",
      "Iteration 525, loss = 0.54496190\n",
      "Iteration 526, loss = 0.54476647\n",
      "Iteration 527, loss = 0.54453523\n",
      "Iteration 528, loss = 0.54438715\n",
      "Iteration 529, loss = 0.54468450\n",
      "Iteration 530, loss = 0.54435689\n",
      "Iteration 531, loss = 0.54430382\n",
      "Iteration 532, loss = 0.54429043\n",
      "Iteration 533, loss = 0.54428508\n",
      "Iteration 534, loss = 0.54421008\n",
      "Iteration 535, loss = 0.54411299\n",
      "Iteration 536, loss = 0.54405210\n",
      "Iteration 537, loss = 0.54391299\n",
      "Iteration 538, loss = 0.54396954\n",
      "Iteration 539, loss = 0.54379705\n",
      "Iteration 540, loss = 0.54371827\n",
      "Iteration 541, loss = 0.54363085\n",
      "Iteration 542, loss = 0.54366989\n",
      "Iteration 543, loss = 0.54360826\n",
      "Iteration 544, loss = 0.54368367\n",
      "Iteration 545, loss = 0.54371775\n",
      "Iteration 546, loss = 0.54368936\n",
      "Iteration 547, loss = 0.54369606\n",
      "Iteration 548, loss = 0.54358453\n",
      "Iteration 549, loss = 0.54342697\n",
      "Iteration 550, loss = 0.54332351\n",
      "Iteration 551, loss = 0.54309131\n",
      "Iteration 552, loss = 0.54289677\n",
      "Iteration 553, loss = 0.54276234\n",
      "Iteration 554, loss = 0.54271076\n",
      "Iteration 555, loss = 0.54265281\n",
      "Iteration 556, loss = 0.54251653\n",
      "Iteration 557, loss = 0.54251472\n",
      "Iteration 558, loss = 0.54241502\n",
      "Iteration 559, loss = 0.54234458\n",
      "Iteration 560, loss = 0.54218705\n",
      "Iteration 561, loss = 0.54213876\n",
      "Iteration 562, loss = 0.54201748\n",
      "Iteration 563, loss = 0.54190439\n",
      "Iteration 564, loss = 0.54180474\n",
      "Iteration 565, loss = 0.54168964\n",
      "Iteration 566, loss = 0.54171693\n",
      "Iteration 567, loss = 0.54178799\n",
      "Iteration 568, loss = 0.54181577\n",
      "Iteration 569, loss = 0.54186998\n",
      "Iteration 570, loss = 0.54187822\n",
      "Iteration 571, loss = 0.54184322\n",
      "Iteration 572, loss = 0.54176632\n",
      "Iteration 573, loss = 0.54172756\n",
      "Iteration 574, loss = 0.54146967\n",
      "Iteration 575, loss = 0.54136338\n",
      "Iteration 576, loss = 0.54105978\n",
      "Iteration 577, loss = 0.54092356\n",
      "Iteration 578, loss = 0.54091914\n",
      "Iteration 579, loss = 0.54098153\n",
      "Iteration 580, loss = 0.54100105\n",
      "Iteration 581, loss = 0.54113177\n",
      "Iteration 582, loss = 0.54101771\n",
      "Iteration 583, loss = 0.54092260\n",
      "Iteration 584, loss = 0.54080976\n",
      "Iteration 585, loss = 0.54076301\n",
      "Iteration 586, loss = 0.54068197\n",
      "Iteration 587, loss = 0.54052492\n",
      "Iteration 588, loss = 0.54042971\n",
      "Iteration 589, loss = 0.54045021\n",
      "Iteration 590, loss = 0.54020555\n",
      "Iteration 591, loss = 0.54016257\n",
      "Iteration 592, loss = 0.54011056\n",
      "Iteration 593, loss = 0.54011714\n",
      "Iteration 594, loss = 0.54010314\n",
      "Iteration 595, loss = 0.54005320\n",
      "Iteration 596, loss = 0.54001286\n",
      "Iteration 597, loss = 0.53994117\n",
      "Iteration 598, loss = 0.54000613\n",
      "Iteration 599, loss = 0.53990617\n",
      "Iteration 600, loss = 0.53976796\n",
      "Iteration 601, loss = 0.53962320\n",
      "Iteration 602, loss = 0.53949000\n",
      "Iteration 603, loss = 0.53926916\n",
      "Iteration 604, loss = 0.53917348\n",
      "Iteration 605, loss = 0.53910631\n",
      "Iteration 606, loss = 0.53888591\n",
      "Iteration 607, loss = 0.53892609\n",
      "Iteration 608, loss = 0.53879213\n",
      "Iteration 609, loss = 0.53881594\n",
      "Iteration 610, loss = 0.53862352\n",
      "Iteration 611, loss = 0.53856908\n",
      "Iteration 612, loss = 0.53851893\n",
      "Iteration 613, loss = 0.53836408\n",
      "Iteration 614, loss = 0.53830586\n",
      "Iteration 615, loss = 0.53823438\n",
      "Iteration 616, loss = 0.53816349\n",
      "Iteration 617, loss = 0.53815013\n",
      "Iteration 618, loss = 0.53806533\n",
      "Iteration 619, loss = 0.53801762\n",
      "Iteration 620, loss = 0.53796578\n",
      "Iteration 621, loss = 0.53784824\n",
      "Iteration 622, loss = 0.53776024\n",
      "Iteration 623, loss = 0.53766571\n",
      "Iteration 624, loss = 0.53755223\n",
      "Iteration 625, loss = 0.53749379\n",
      "Iteration 626, loss = 0.53735933\n",
      "Iteration 627, loss = 0.53725825\n",
      "Iteration 628, loss = 0.53722485\n",
      "Iteration 629, loss = 0.53705180\n",
      "Iteration 630, loss = 0.53693690\n",
      "Iteration 631, loss = 0.53701372\n",
      "Iteration 632, loss = 0.53705096\n",
      "Iteration 633, loss = 0.53700472\n",
      "Iteration 634, loss = 0.53702017\n",
      "Iteration 635, loss = 0.53695320\n",
      "Iteration 636, loss = 0.53685650\n",
      "Iteration 637, loss = 0.53673749\n",
      "Iteration 638, loss = 0.53661660\n",
      "Iteration 639, loss = 0.53638419\n",
      "Iteration 640, loss = 0.53621348\n",
      "Iteration 641, loss = 0.53611172\n",
      "Iteration 642, loss = 0.53611824\n",
      "Iteration 643, loss = 0.53592350\n",
      "Iteration 644, loss = 0.53585272\n",
      "Iteration 645, loss = 0.53577237\n",
      "Iteration 646, loss = 0.53574266\n",
      "Iteration 647, loss = 0.53565672\n",
      "Iteration 648, loss = 0.53558643\n",
      "Iteration 649, loss = 0.53551737\n",
      "Iteration 650, loss = 0.53544985\n",
      "Iteration 651, loss = 0.53537628\n",
      "Iteration 652, loss = 0.53541344\n",
      "Iteration 653, loss = 0.53534989\n",
      "Iteration 654, loss = 0.53527334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30856346\n",
      "Iteration 2, loss = 2.20169403\n",
      "Iteration 3, loss = 2.09788709\n",
      "Iteration 4, loss = 2.00109847\n",
      "Iteration 5, loss = 1.91223050\n",
      "Iteration 6, loss = 1.82677529\n",
      "Iteration 7, loss = 1.75022556\n",
      "Iteration 8, loss = 1.67891664\n",
      "Iteration 9, loss = 1.61795842\n",
      "Iteration 10, loss = 1.55722031\n",
      "Iteration 11, loss = 1.50492905\n",
      "Iteration 12, loss = 1.45523245\n",
      "Iteration 13, loss = 1.41138485\n",
      "Iteration 14, loss = 1.36920056\n",
      "Iteration 15, loss = 1.33024289\n",
      "Iteration 16, loss = 1.29761535\n",
      "Iteration 17, loss = 1.26239094\n",
      "Iteration 18, loss = 1.23213030\n",
      "Iteration 19, loss = 1.20419727\n",
      "Iteration 20, loss = 1.17732571\n",
      "Iteration 21, loss = 1.14861468\n",
      "Iteration 22, loss = 1.12034429\n",
      "Iteration 23, loss = 1.09295328\n",
      "Iteration 24, loss = 1.06449715\n",
      "Iteration 25, loss = 1.03763887\n",
      "Iteration 26, loss = 1.01181028\n",
      "Iteration 27, loss = 0.98912068\n",
      "Iteration 28, loss = 0.96815158\n",
      "Iteration 29, loss = 0.94952920\n",
      "Iteration 30, loss = 0.93197174\n",
      "Iteration 31, loss = 0.91684763\n",
      "Iteration 32, loss = 0.90428690\n",
      "Iteration 33, loss = 0.89215042\n",
      "Iteration 34, loss = 0.88045474\n",
      "Iteration 35, loss = 0.87092457\n",
      "Iteration 36, loss = 0.86058306\n",
      "Iteration 37, loss = 0.85142596\n",
      "Iteration 38, loss = 0.84227635\n",
      "Iteration 39, loss = 0.83372811\n",
      "Iteration 40, loss = 0.82536406\n",
      "Iteration 41, loss = 0.81666860\n",
      "Iteration 42, loss = 0.80876491\n",
      "Iteration 43, loss = 0.80138173\n",
      "Iteration 44, loss = 0.79327497\n",
      "Iteration 45, loss = 0.78616422\n",
      "Iteration 46, loss = 0.77962128\n",
      "Iteration 47, loss = 0.77212011\n",
      "Iteration 48, loss = 0.76494191\n",
      "Iteration 49, loss = 0.75751381\n",
      "Iteration 50, loss = 0.75062760\n",
      "Iteration 51, loss = 0.74369797\n",
      "Iteration 52, loss = 0.73753756\n",
      "Iteration 53, loss = 0.73055106\n",
      "Iteration 54, loss = 0.72421421\n",
      "Iteration 55, loss = 0.71816625\n",
      "Iteration 56, loss = 0.71201424\n",
      "Iteration 57, loss = 0.70587285\n",
      "Iteration 58, loss = 0.70021753\n",
      "Iteration 59, loss = 0.69454873\n",
      "Iteration 60, loss = 0.68903934\n",
      "Iteration 61, loss = 0.68401552\n",
      "Iteration 62, loss = 0.67879032\n",
      "Iteration 63, loss = 0.67400447\n",
      "Iteration 64, loss = 0.66946745\n",
      "Iteration 65, loss = 0.66446485\n",
      "Iteration 66, loss = 0.66023593\n",
      "Iteration 67, loss = 0.65600056\n",
      "Iteration 68, loss = 0.65163174\n",
      "Iteration 69, loss = 0.64752328\n",
      "Iteration 70, loss = 0.64335036\n",
      "Iteration 71, loss = 0.63983674\n",
      "Iteration 72, loss = 0.63542311\n",
      "Iteration 73, loss = 0.63217779\n",
      "Iteration 74, loss = 0.62819257\n",
      "Iteration 75, loss = 0.62465419\n",
      "Iteration 76, loss = 0.62115366\n",
      "Iteration 77, loss = 0.61751946\n",
      "Iteration 78, loss = 0.61468305\n",
      "Iteration 79, loss = 0.61091596\n",
      "Iteration 80, loss = 0.60764486\n",
      "Iteration 81, loss = 0.60431331\n",
      "Iteration 82, loss = 0.60091890\n",
      "Iteration 83, loss = 0.59828017\n",
      "Iteration 84, loss = 0.59536494\n",
      "Iteration 85, loss = 0.59294441\n",
      "Iteration 86, loss = 0.59073876\n",
      "Iteration 87, loss = 0.58799466\n",
      "Iteration 88, loss = 0.58527986\n",
      "Iteration 89, loss = 0.58300171\n",
      "Iteration 90, loss = 0.58061206\n",
      "Iteration 91, loss = 0.57850843\n",
      "Iteration 92, loss = 0.57690762\n",
      "Iteration 93, loss = 0.57462608\n",
      "Iteration 94, loss = 0.57274928\n",
      "Iteration 95, loss = 0.57077608\n",
      "Iteration 96, loss = 0.56899002\n",
      "Iteration 97, loss = 0.56849393\n",
      "Iteration 98, loss = 0.56687994\n",
      "Iteration 99, loss = 0.56541900\n",
      "Iteration 100, loss = 0.56368717\n",
      "Iteration 101, loss = 0.56236001\n",
      "Iteration 102, loss = 0.56068080\n",
      "Iteration 103, loss = 0.55935117\n",
      "Iteration 104, loss = 0.55884253\n",
      "Iteration 105, loss = 0.55810879\n",
      "Iteration 106, loss = 0.55772126\n",
      "Iteration 107, loss = 0.55687847\n",
      "Iteration 108, loss = 0.55565599\n",
      "Iteration 109, loss = 0.55431875\n",
      "Iteration 110, loss = 0.55268055\n",
      "Iteration 111, loss = 0.55193525\n",
      "Iteration 112, loss = 0.55240458\n",
      "Iteration 113, loss = 0.55382243\n",
      "Iteration 114, loss = 0.55316945\n",
      "Iteration 115, loss = 0.55121090\n",
      "Iteration 116, loss = 0.54935843\n",
      "Iteration 117, loss = 0.54809290\n",
      "Iteration 118, loss = 0.54795593\n",
      "Iteration 119, loss = 0.54913734\n",
      "Iteration 120, loss = 0.54960379\n",
      "Iteration 121, loss = 0.54879396\n",
      "Iteration 122, loss = 0.54693537\n",
      "Iteration 123, loss = 0.54606562\n",
      "Iteration 124, loss = 0.54553061\n",
      "Iteration 125, loss = 0.54614495\n",
      "Iteration 126, loss = 0.54553734\n",
      "Iteration 127, loss = 0.54506338\n",
      "Iteration 128, loss = 0.54441844\n",
      "Iteration 129, loss = 0.54494544\n",
      "Iteration 130, loss = 0.54394560\n",
      "Iteration 131, loss = 0.54355428\n",
      "Iteration 132, loss = 0.54314945\n",
      "Iteration 133, loss = 0.54379620\n",
      "Iteration 134, loss = 0.54451682\n",
      "Iteration 135, loss = 0.54501668\n",
      "Iteration 136, loss = 0.54497575\n",
      "Iteration 137, loss = 0.54454993\n",
      "Iteration 138, loss = 0.54347509\n",
      "Iteration 139, loss = 0.54267922\n",
      "Iteration 140, loss = 0.54208723\n",
      "Iteration 141, loss = 0.54154761\n",
      "Iteration 142, loss = 0.54147318\n",
      "Iteration 143, loss = 0.54414457\n",
      "Iteration 144, loss = 0.54436857\n",
      "Iteration 145, loss = 0.54313934\n",
      "Iteration 146, loss = 0.54166510\n",
      "Iteration 147, loss = 0.54077283\n",
      "Iteration 148, loss = 0.54114349\n",
      "Iteration 149, loss = 0.54181807\n",
      "Iteration 150, loss = 0.54244135\n",
      "Iteration 151, loss = 0.54256476\n",
      "Iteration 152, loss = 0.54235030\n",
      "Iteration 153, loss = 0.54115555\n",
      "Iteration 154, loss = 0.54116755\n",
      "Iteration 155, loss = 0.54064775\n",
      "Iteration 156, loss = 0.54059564\n",
      "Iteration 157, loss = 0.54056358\n",
      "Iteration 158, loss = 0.54059084\n",
      "Iteration 159, loss = 0.54025066\n",
      "Iteration 160, loss = 0.54034043\n",
      "Iteration 161, loss = 0.54013668\n",
      "Iteration 162, loss = 0.54021945\n",
      "Iteration 163, loss = 0.54030806\n",
      "Iteration 164, loss = 0.54041745\n",
      "Iteration 165, loss = 0.54262005\n",
      "Iteration 166, loss = 0.54261597\n",
      "Iteration 167, loss = 0.54143417\n",
      "Iteration 168, loss = 0.54074680\n",
      "Iteration 169, loss = 0.54012308\n",
      "Iteration 170, loss = 0.54047075\n",
      "Iteration 171, loss = 0.54105655\n",
      "Iteration 172, loss = 0.54211395\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 25.96532083\n",
      "Iteration 2, loss = 25.87954926\n",
      "Iteration 3, loss = 25.77513129\n",
      "Iteration 4, loss = 25.65271729\n",
      "Iteration 5, loss = 25.51348214\n",
      "Iteration 6, loss = 25.36802042\n",
      "Iteration 7, loss = 25.19145808\n",
      "Iteration 8, loss = 24.99904784\n",
      "Iteration 9, loss = 24.79080922\n",
      "Iteration 10, loss = 24.56426380\n",
      "Iteration 11, loss = 24.32619983\n",
      "Iteration 12, loss = 24.06651485\n",
      "Iteration 13, loss = 23.77865622\n",
      "Iteration 14, loss = 23.48982453\n",
      "Iteration 15, loss = 23.17846477\n",
      "Iteration 16, loss = 22.85189873\n",
      "Iteration 17, loss = 22.52060896\n",
      "Iteration 18, loss = 22.17319686\n",
      "Iteration 19, loss = 21.78778618\n",
      "Iteration 20, loss = 21.40961627\n",
      "Iteration 21, loss = 21.02937305\n",
      "Iteration 22, loss = 20.63580754\n",
      "Iteration 23, loss = 20.23450929\n",
      "Iteration 24, loss = 19.83615711\n",
      "Iteration 25, loss = 19.42573271\n",
      "Iteration 26, loss = 19.02610325\n",
      "Iteration 27, loss = 18.61153411\n",
      "Iteration 28, loss = 18.20904235\n",
      "Iteration 29, loss = 17.79457516\n",
      "Iteration 30, loss = 17.37763159\n",
      "Iteration 31, loss = 16.95683412\n",
      "Iteration 32, loss = 16.53210881\n",
      "Iteration 33, loss = 16.10158986\n",
      "Iteration 34, loss = 15.67107127\n",
      "Iteration 35, loss = 15.24233919\n",
      "Iteration 36, loss = 14.80293210\n",
      "Iteration 37, loss = 14.37104738\n",
      "Iteration 38, loss = 13.93159513\n",
      "Iteration 39, loss = 13.48940850\n",
      "Iteration 40, loss = 13.03934619\n",
      "Iteration 41, loss = 12.58919845\n",
      "Iteration 42, loss = 12.13502564\n",
      "Iteration 43, loss = 11.68825685\n",
      "Iteration 44, loss = 11.21915551\n",
      "Iteration 45, loss = 10.76100563\n",
      "Iteration 46, loss = 10.30238859\n",
      "Iteration 47, loss = 9.83549339\n",
      "Iteration 48, loss = 9.37608069\n",
      "Iteration 49, loss = 8.91294106\n",
      "Iteration 50, loss = 8.44803475\n",
      "Iteration 51, loss = 7.98302331\n",
      "Iteration 52, loss = 7.51996191\n",
      "Iteration 53, loss = 7.05334910\n",
      "Iteration 54, loss = 6.58701963\n",
      "Iteration 55, loss = 6.11905010\n",
      "Iteration 56, loss = 5.65028029\n",
      "Iteration 57, loss = 5.17754727\n",
      "Iteration 58, loss = 4.70361285\n",
      "Iteration 59, loss = 4.23736048\n",
      "Iteration 60, loss = 3.77378953\n",
      "Iteration 61, loss = 3.32103848\n",
      "Iteration 62, loss = 2.88992899\n",
      "Iteration 63, loss = 2.47743402\n",
      "Iteration 64, loss = 2.10474321\n",
      "Iteration 65, loss = 1.78207751\n",
      "Iteration 66, loss = 1.49610165\n",
      "Iteration 67, loss = 1.27025549\n",
      "Iteration 68, loss = 1.09623929\n",
      "Iteration 69, loss = 0.96331447\n",
      "Iteration 70, loss = 0.88215742\n",
      "Iteration 71, loss = 0.82624025\n",
      "Iteration 72, loss = 0.79791388\n",
      "Iteration 73, loss = 0.78379239\n",
      "Iteration 74, loss = 0.78437863\n",
      "Iteration 75, loss = 0.78151100\n",
      "Iteration 76, loss = 0.78890679\n",
      "Iteration 77, loss = 0.79413733\n",
      "Iteration 78, loss = 0.79801863\n",
      "Iteration 79, loss = 0.80123092\n",
      "Iteration 80, loss = 0.80161100\n",
      "Iteration 81, loss = 0.80102819\n",
      "Iteration 82, loss = 0.79910329\n",
      "Iteration 83, loss = 0.79625157\n",
      "Iteration 84, loss = 0.79251870\n",
      "Iteration 85, loss = 0.78913658\n",
      "Iteration 86, loss = 0.78538348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.16630458\n",
      "Iteration 2, loss = 1.10457422\n",
      "Iteration 3, loss = 1.04411094\n",
      "Iteration 4, loss = 0.98441028\n",
      "Iteration 5, loss = 0.93055913\n",
      "Iteration 6, loss = 0.87752784\n",
      "Iteration 7, loss = 0.82960095\n",
      "Iteration 8, loss = 0.78653642\n",
      "Iteration 9, loss = 0.74742793\n",
      "Iteration 10, loss = 0.71468791\n",
      "Iteration 11, loss = 0.68830842\n",
      "Iteration 12, loss = 0.66578631\n",
      "Iteration 13, loss = 0.65323665\n",
      "Iteration 14, loss = 0.64465072\n",
      "Iteration 15, loss = 0.64249011\n",
      "Iteration 16, loss = 0.64221870\n",
      "Iteration 17, loss = 0.64440241\n",
      "Iteration 18, loss = 0.64468499\n",
      "Iteration 19, loss = 0.64554057\n",
      "Iteration 20, loss = 0.64671076\n",
      "Iteration 21, loss = 0.64592152\n",
      "Iteration 22, loss = 0.64415313\n",
      "Iteration 23, loss = 0.64181009\n",
      "Iteration 24, loss = 0.63824416\n",
      "Iteration 25, loss = 0.63498187\n",
      "Iteration 26, loss = 0.63212359\n",
      "Iteration 27, loss = 0.62997660\n",
      "Iteration 28, loss = 0.62869277\n",
      "Iteration 29, loss = 0.62879973\n",
      "Iteration 30, loss = 0.62865241\n",
      "Iteration 31, loss = 0.62830592\n",
      "Iteration 32, loss = 0.62744575\n",
      "Iteration 33, loss = 0.62629997\n",
      "Iteration 34, loss = 0.62527205\n",
      "Iteration 35, loss = 0.62427951\n",
      "Iteration 36, loss = 0.62343012\n",
      "Iteration 37, loss = 0.62268834\n",
      "Iteration 38, loss = 0.62187916\n",
      "Iteration 39, loss = 0.62120738\n",
      "Iteration 40, loss = 0.62051175\n",
      "Iteration 41, loss = 0.61965579\n",
      "Iteration 42, loss = 0.61896716\n",
      "Iteration 43, loss = 0.61832716\n",
      "Iteration 44, loss = 0.61808341\n",
      "Iteration 45, loss = 0.61760029\n",
      "Iteration 46, loss = 0.61687692\n",
      "Iteration 47, loss = 0.61631726\n",
      "Iteration 48, loss = 0.61551969\n",
      "Iteration 49, loss = 0.61496514\n",
      "Iteration 50, loss = 0.61429761\n",
      "Iteration 51, loss = 0.61372577\n",
      "Iteration 52, loss = 0.61331024\n",
      "Iteration 53, loss = 0.61260416\n",
      "Iteration 54, loss = 0.61207585\n",
      "Iteration 55, loss = 0.61155026\n",
      "Iteration 56, loss = 0.61094568\n",
      "Iteration 57, loss = 0.61063359\n",
      "Iteration 58, loss = 0.61046553\n",
      "Iteration 59, loss = 0.61037860\n",
      "Iteration 60, loss = 0.61013190\n",
      "Iteration 61, loss = 0.60961618\n",
      "Iteration 62, loss = 0.60875120\n",
      "Iteration 63, loss = 0.60776812\n",
      "Iteration 64, loss = 0.60682927\n",
      "Iteration 65, loss = 0.60621760\n",
      "Iteration 66, loss = 0.60593654\n",
      "Iteration 67, loss = 0.60564165\n",
      "Iteration 68, loss = 0.60483866\n",
      "Iteration 69, loss = 0.60441322\n",
      "Iteration 70, loss = 0.60392980\n",
      "Iteration 71, loss = 0.60335010\n",
      "Iteration 72, loss = 0.60277594\n",
      "Iteration 73, loss = 0.60238586\n",
      "Iteration 74, loss = 0.60184247\n",
      "Iteration 75, loss = 0.60155563\n",
      "Iteration 76, loss = 0.60110824\n",
      "Iteration 77, loss = 0.60061752\n",
      "Iteration 78, loss = 0.60019351\n",
      "Iteration 79, loss = 0.59982847\n",
      "Iteration 80, loss = 0.59935229\n",
      "Iteration 81, loss = 0.59896516\n",
      "Iteration 82, loss = 0.59844191\n",
      "Iteration 83, loss = 0.59774371\n",
      "Iteration 84, loss = 0.59717922\n",
      "Iteration 85, loss = 0.59673370\n",
      "Iteration 86, loss = 0.59623598\n",
      "Iteration 87, loss = 0.59581644\n",
      "Iteration 88, loss = 0.59550458\n",
      "Iteration 89, loss = 0.59503299\n",
      "Iteration 90, loss = 0.59465097\n",
      "Iteration 91, loss = 0.59414821\n",
      "Iteration 92, loss = 0.59378256\n",
      "Iteration 93, loss = 0.59340342\n",
      "Iteration 94, loss = 0.59322976\n",
      "Iteration 95, loss = 0.59312395\n",
      "Iteration 96, loss = 0.59275775\n",
      "Iteration 97, loss = 0.59215215\n",
      "Iteration 98, loss = 0.59151055\n",
      "Iteration 99, loss = 0.59100606\n",
      "Iteration 100, loss = 0.59044560\n",
      "Iteration 101, loss = 0.59003458\n",
      "Iteration 102, loss = 0.58986703\n",
      "Iteration 103, loss = 0.58945117\n",
      "Iteration 104, loss = 0.58919337\n",
      "Iteration 105, loss = 0.58925622\n",
      "Iteration 106, loss = 0.58861073\n",
      "Iteration 107, loss = 0.58779575\n",
      "Iteration 108, loss = 0.58709341\n",
      "Iteration 109, loss = 0.58693863\n",
      "Iteration 110, loss = 0.58725388\n",
      "Iteration 111, loss = 0.58730568\n",
      "Iteration 112, loss = 0.58703597\n",
      "Iteration 113, loss = 0.58657150\n",
      "Iteration 114, loss = 0.58611533\n",
      "Iteration 115, loss = 0.58559692\n",
      "Iteration 116, loss = 0.58513984\n",
      "Iteration 117, loss = 0.58445869\n",
      "Iteration 118, loss = 0.58373136\n",
      "Iteration 119, loss = 0.58308492\n",
      "Iteration 120, loss = 0.58313067\n",
      "Iteration 121, loss = 0.58358205\n",
      "Iteration 122, loss = 0.58396266\n",
      "Iteration 123, loss = 0.58392846\n",
      "Iteration 124, loss = 0.58348069\n",
      "Iteration 125, loss = 0.58295144\n",
      "Iteration 126, loss = 0.58140064\n",
      "Iteration 127, loss = 0.58107462\n",
      "Iteration 128, loss = 0.58054246\n",
      "Iteration 129, loss = 0.58020004\n",
      "Iteration 130, loss = 0.57979863\n",
      "Iteration 131, loss = 0.57940240\n",
      "Iteration 132, loss = 0.57907645\n",
      "Iteration 133, loss = 0.57876194\n",
      "Iteration 134, loss = 0.57844193\n",
      "Iteration 135, loss = 0.57828910\n",
      "Iteration 136, loss = 0.57810794\n",
      "Iteration 137, loss = 0.57729904\n",
      "Iteration 138, loss = 0.57736836\n",
      "Iteration 139, loss = 0.57745199\n",
      "Iteration 140, loss = 0.57758896\n",
      "Iteration 141, loss = 0.57752385\n",
      "Iteration 142, loss = 0.57742211\n",
      "Iteration 143, loss = 0.57750756\n",
      "Iteration 144, loss = 0.57758823\n",
      "Iteration 145, loss = 0.57718243\n",
      "Iteration 146, loss = 0.57606821\n",
      "Iteration 147, loss = 0.57518318\n",
      "Iteration 148, loss = 0.57415227\n",
      "Iteration 149, loss = 0.57353094\n",
      "Iteration 150, loss = 0.57352084\n",
      "Iteration 151, loss = 0.57277204\n",
      "Iteration 152, loss = 0.57252670\n",
      "Iteration 153, loss = 0.57220180\n",
      "Iteration 154, loss = 0.57177869\n",
      "Iteration 155, loss = 0.57191715\n",
      "Iteration 156, loss = 0.57134255\n",
      "Iteration 157, loss = 0.57108539\n",
      "Iteration 158, loss = 0.57074926\n",
      "Iteration 159, loss = 0.57067613\n",
      "Iteration 160, loss = 0.57046398\n",
      "Iteration 161, loss = 0.57015656\n",
      "Iteration 162, loss = 0.56978386\n",
      "Iteration 163, loss = 0.56955841\n",
      "Iteration 164, loss = 0.56913399\n",
      "Iteration 165, loss = 0.56893681\n",
      "Iteration 166, loss = 0.56860730\n",
      "Iteration 167, loss = 0.56836861\n",
      "Iteration 168, loss = 0.56814338\n",
      "Iteration 169, loss = 0.56794050\n",
      "Iteration 170, loss = 0.56772937\n",
      "Iteration 171, loss = 0.56784103\n",
      "Iteration 172, loss = 0.56780276\n",
      "Iteration 173, loss = 0.56767678\n",
      "Iteration 174, loss = 0.56780955\n",
      "Iteration 175, loss = 0.56742582\n",
      "Iteration 176, loss = 0.56668671\n",
      "Iteration 177, loss = 0.56593091\n",
      "Iteration 178, loss = 0.56554863\n",
      "Iteration 179, loss = 0.56588449\n",
      "Iteration 180, loss = 0.56650636\n",
      "Iteration 181, loss = 0.56655924\n",
      "Iteration 182, loss = 0.56641815\n",
      "Iteration 183, loss = 0.56593225\n",
      "Iteration 184, loss = 0.56562287\n",
      "Iteration 185, loss = 0.56532592\n",
      "Iteration 186, loss = 0.56478152\n",
      "Iteration 187, loss = 0.56445087\n",
      "Iteration 188, loss = 0.56378075\n",
      "Iteration 189, loss = 0.56371540\n",
      "Iteration 190, loss = 0.56335311\n",
      "Iteration 191, loss = 0.56323569\n",
      "Iteration 192, loss = 0.56309287\n",
      "Iteration 193, loss = 0.56280076\n",
      "Iteration 194, loss = 0.56245800\n",
      "Iteration 195, loss = 0.56234431\n",
      "Iteration 196, loss = 0.56218527\n",
      "Iteration 197, loss = 0.56235185\n",
      "Iteration 198, loss = 0.56239490\n",
      "Iteration 199, loss = 0.56224809\n",
      "Iteration 200, loss = 0.56183120\n",
      "Iteration 201, loss = 0.56149487\n",
      "Iteration 202, loss = 0.56124331\n",
      "Iteration 203, loss = 0.56081180\n",
      "Iteration 204, loss = 0.56062691\n",
      "Iteration 205, loss = 0.56039870\n",
      "Iteration 206, loss = 0.56017658\n",
      "Iteration 207, loss = 0.55994808\n",
      "Iteration 208, loss = 0.55988371\n",
      "Iteration 209, loss = 0.55949832\n",
      "Iteration 210, loss = 0.55952581\n",
      "Iteration 211, loss = 0.55913215\n",
      "Iteration 212, loss = 0.55883531\n",
      "Iteration 213, loss = 0.55859916\n",
      "Iteration 214, loss = 0.55844430\n",
      "Iteration 215, loss = 0.55907783\n",
      "Iteration 216, loss = 0.55924788\n",
      "Iteration 217, loss = 0.55915831\n",
      "Iteration 218, loss = 0.55870389\n",
      "Iteration 219, loss = 0.55827917\n",
      "Iteration 220, loss = 0.55772120\n",
      "Iteration 221, loss = 0.55726179\n",
      "Iteration 222, loss = 0.55728959\n",
      "Iteration 223, loss = 0.55714060\n",
      "Iteration 224, loss = 0.55704080\n",
      "Iteration 225, loss = 0.55689104\n",
      "Iteration 226, loss = 0.55672580\n",
      "Iteration 227, loss = 0.55650949\n",
      "Iteration 228, loss = 0.55605128\n",
      "Iteration 229, loss = 0.55585007\n",
      "Iteration 230, loss = 0.55567017\n",
      "Iteration 231, loss = 0.55563970\n",
      "Iteration 232, loss = 0.55579500\n",
      "Iteration 233, loss = 0.55562390\n",
      "Iteration 234, loss = 0.55507174\n",
      "Iteration 235, loss = 0.55529560\n",
      "Iteration 236, loss = 0.55443376\n",
      "Iteration 237, loss = 0.55412266\n",
      "Iteration 238, loss = 0.55454911\n",
      "Iteration 239, loss = 0.55411438\n",
      "Iteration 240, loss = 0.55357218\n",
      "Iteration 241, loss = 0.55333802\n",
      "Iteration 242, loss = 0.55313713\n",
      "Iteration 243, loss = 0.55283375\n",
      "Iteration 244, loss = 0.55279959\n",
      "Iteration 245, loss = 0.55243199\n",
      "Iteration 246, loss = 0.55211865\n",
      "Iteration 247, loss = 0.55216805\n",
      "Iteration 248, loss = 0.55183668\n",
      "Iteration 249, loss = 0.55160753\n",
      "Iteration 250, loss = 0.55131951\n",
      "Iteration 251, loss = 0.55104639\n",
      "Iteration 252, loss = 0.55087965\n",
      "Iteration 253, loss = 0.55115104\n",
      "Iteration 254, loss = 0.55055311\n",
      "Iteration 255, loss = 0.55013871\n",
      "Iteration 256, loss = 0.55015769\n",
      "Iteration 257, loss = 0.55040446\n",
      "Iteration 258, loss = 0.55073403\n",
      "Iteration 259, loss = 0.55151149\n",
      "Iteration 260, loss = 0.55207660\n",
      "Iteration 261, loss = 0.55167582\n",
      "Iteration 262, loss = 0.55075440\n",
      "Iteration 263, loss = 0.54935597\n",
      "Iteration 264, loss = 0.54858255\n",
      "Iteration 265, loss = 0.54824875\n",
      "Iteration 266, loss = 0.54844584\n",
      "Iteration 267, loss = 0.54851120\n",
      "Iteration 268, loss = 0.54800503\n",
      "Iteration 269, loss = 0.54750549\n",
      "Iteration 270, loss = 0.54743937\n",
      "Iteration 271, loss = 0.54719378\n",
      "Iteration 272, loss = 0.54699415\n",
      "Iteration 273, loss = 0.54682673\n",
      "Iteration 274, loss = 0.54671399\n",
      "Iteration 275, loss = 0.54626736\n",
      "Iteration 276, loss = 0.54603154\n",
      "Iteration 277, loss = 0.54583652\n",
      "Iteration 278, loss = 0.54561867\n",
      "Iteration 279, loss = 0.54544662\n",
      "Iteration 280, loss = 0.54548234\n",
      "Iteration 281, loss = 0.54581879\n",
      "Iteration 282, loss = 0.54611816\n",
      "Iteration 283, loss = 0.54618827\n",
      "Iteration 284, loss = 0.54498778\n",
      "Iteration 285, loss = 0.54421001\n",
      "Iteration 286, loss = 0.54460496\n",
      "Iteration 287, loss = 0.54483600\n",
      "Iteration 288, loss = 0.54556661\n",
      "Iteration 289, loss = 0.54591689\n",
      "Iteration 290, loss = 0.54601414\n",
      "Iteration 291, loss = 0.54608977\n",
      "Iteration 292, loss = 0.54680899\n",
      "Iteration 293, loss = 0.54739807\n",
      "Iteration 294, loss = 0.54714073\n",
      "Iteration 295, loss = 0.54592566\n",
      "Iteration 296, loss = 0.54466862\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 17.58571657\n",
      "Iteration 2, loss = 17.05924607\n",
      "Iteration 3, loss = 16.51324111\n",
      "Iteration 4, loss = 15.98691681\n",
      "Iteration 5, loss = 15.45409107\n",
      "Iteration 6, loss = 14.91795783\n",
      "Iteration 7, loss = 14.38771018\n",
      "Iteration 8, loss = 13.86343590\n",
      "Iteration 9, loss = 13.32372412\n",
      "Iteration 10, loss = 12.80148510\n",
      "Iteration 11, loss = 12.27350089\n",
      "Iteration 12, loss = 11.76770077\n",
      "Iteration 13, loss = 11.24220822\n",
      "Iteration 14, loss = 10.74506038\n",
      "Iteration 15, loss = 10.26114738\n",
      "Iteration 16, loss = 9.79015296\n",
      "Iteration 17, loss = 9.32139937\n",
      "Iteration 18, loss = 8.88583106\n",
      "Iteration 19, loss = 8.47954619\n",
      "Iteration 20, loss = 8.07140597\n",
      "Iteration 21, loss = 7.68691426\n",
      "Iteration 22, loss = 7.34325706\n",
      "Iteration 23, loss = 7.00097395\n",
      "Iteration 24, loss = 6.68982655\n",
      "Iteration 25, loss = 6.40118260\n",
      "Iteration 26, loss = 6.14206066\n",
      "Iteration 27, loss = 5.89873070\n",
      "Iteration 28, loss = 5.68455790\n",
      "Iteration 29, loss = 5.47975669\n",
      "Iteration 30, loss = 5.30527316\n",
      "Iteration 31, loss = 5.13712088\n",
      "Iteration 32, loss = 4.99936971\n",
      "Iteration 33, loss = 4.86009804\n",
      "Iteration 34, loss = 4.75172194\n",
      "Iteration 35, loss = 4.64066467\n",
      "Iteration 36, loss = 4.55123532\n",
      "Iteration 37, loss = 4.46541364\n",
      "Iteration 38, loss = 4.39230989\n",
      "Iteration 39, loss = 4.31911570\n",
      "Iteration 40, loss = 4.25784163\n",
      "Iteration 41, loss = 4.19237202\n",
      "Iteration 42, loss = 4.13456489\n",
      "Iteration 43, loss = 4.08439919\n",
      "Iteration 44, loss = 4.03863451\n",
      "Iteration 45, loss = 3.98937525\n",
      "Iteration 46, loss = 3.94651450\n",
      "Iteration 47, loss = 3.90238047\n",
      "Iteration 48, loss = 3.86133818\n",
      "Iteration 49, loss = 3.82205497\n",
      "Iteration 50, loss = 3.78809219\n",
      "Iteration 51, loss = 3.75015896\n",
      "Iteration 52, loss = 3.71539100\n",
      "Iteration 53, loss = 3.68275414\n",
      "Iteration 54, loss = 3.65275995\n",
      "Iteration 55, loss = 3.62071205\n",
      "Iteration 56, loss = 3.59015419\n",
      "Iteration 57, loss = 3.56057031\n",
      "Iteration 58, loss = 3.53028431\n",
      "Iteration 59, loss = 3.50055489\n",
      "Iteration 60, loss = 3.47045341\n",
      "Iteration 61, loss = 3.43988809\n",
      "Iteration 62, loss = 3.41097914\n",
      "Iteration 63, loss = 3.38174829\n",
      "Iteration 64, loss = 3.35247378\n",
      "Iteration 65, loss = 3.32090893\n",
      "Iteration 66, loss = 3.29078027\n",
      "Iteration 67, loss = 3.25954446\n",
      "Iteration 68, loss = 3.22985843\n",
      "Iteration 69, loss = 3.19785501\n",
      "Iteration 70, loss = 3.16743501\n",
      "Iteration 71, loss = 3.13645356\n",
      "Iteration 72, loss = 3.10590774\n",
      "Iteration 73, loss = 3.07540711\n",
      "Iteration 74, loss = 3.04362790\n",
      "Iteration 75, loss = 3.01359891\n",
      "Iteration 76, loss = 2.98257406\n",
      "Iteration 77, loss = 2.95153531\n",
      "Iteration 78, loss = 2.92101855\n",
      "Iteration 79, loss = 2.88903892\n",
      "Iteration 80, loss = 2.85708367\n",
      "Iteration 81, loss = 2.82723680\n",
      "Iteration 82, loss = 2.79554948\n",
      "Iteration 83, loss = 2.76398257\n",
      "Iteration 84, loss = 2.73202664\n",
      "Iteration 85, loss = 2.69958814\n",
      "Iteration 86, loss = 2.66650192\n",
      "Iteration 87, loss = 2.63529002\n",
      "Iteration 88, loss = 2.60375492\n",
      "Iteration 89, loss = 2.57164216\n",
      "Iteration 90, loss = 2.54008097\n",
      "Iteration 91, loss = 2.50988001\n",
      "Iteration 92, loss = 2.47982499\n",
      "Iteration 93, loss = 2.44997279\n",
      "Iteration 94, loss = 2.41973530\n",
      "Iteration 95, loss = 2.38895971\n",
      "Iteration 96, loss = 2.35669310\n",
      "Iteration 97, loss = 2.32809772\n",
      "Iteration 98, loss = 2.29909124\n",
      "Iteration 99, loss = 2.27030085\n",
      "Iteration 100, loss = 2.24273970\n",
      "Iteration 101, loss = 2.21672347\n",
      "Iteration 102, loss = 2.19051472\n",
      "Iteration 103, loss = 2.16546764\n",
      "Iteration 104, loss = 2.14033882\n",
      "Iteration 105, loss = 2.11729028\n",
      "Iteration 106, loss = 2.09344786\n",
      "Iteration 107, loss = 2.07097058\n",
      "Iteration 108, loss = 2.04745622\n",
      "Iteration 109, loss = 2.02483969\n",
      "Iteration 110, loss = 2.00140122\n",
      "Iteration 111, loss = 1.97965470\n",
      "Iteration 112, loss = 1.95895385\n",
      "Iteration 113, loss = 1.93756212\n",
      "Iteration 114, loss = 1.91752364\n",
      "Iteration 115, loss = 1.89915400\n",
      "Iteration 116, loss = 1.88021588\n",
      "Iteration 117, loss = 1.86103336\n",
      "Iteration 118, loss = 1.84377150\n",
      "Iteration 119, loss = 1.82771530\n",
      "Iteration 120, loss = 1.81165705\n",
      "Iteration 121, loss = 1.79659575\n",
      "Iteration 122, loss = 1.78480731\n",
      "Iteration 123, loss = 1.77059149\n",
      "Iteration 124, loss = 1.75588988\n",
      "Iteration 125, loss = 1.74032593\n",
      "Iteration 126, loss = 1.72417886\n",
      "Iteration 127, loss = 1.70795326\n",
      "Iteration 128, loss = 1.69413244\n",
      "Iteration 129, loss = 1.68093585\n",
      "Iteration 130, loss = 1.66909065\n",
      "Iteration 131, loss = 1.65793653\n",
      "Iteration 132, loss = 1.64530588\n",
      "Iteration 133, loss = 1.63263805\n",
      "Iteration 134, loss = 1.61933733\n",
      "Iteration 135, loss = 1.60527567\n",
      "Iteration 136, loss = 1.59285653\n",
      "Iteration 137, loss = 1.57978167\n",
      "Iteration 138, loss = 1.56778260\n",
      "Iteration 139, loss = 1.55581574\n",
      "Iteration 140, loss = 1.54588433\n",
      "Iteration 141, loss = 1.53457567\n",
      "Iteration 142, loss = 1.52264614\n",
      "Iteration 143, loss = 1.51153071\n",
      "Iteration 144, loss = 1.50055338\n",
      "Iteration 145, loss = 1.48841435\n",
      "Iteration 146, loss = 1.47788290\n",
      "Iteration 147, loss = 1.46644590\n",
      "Iteration 148, loss = 1.45632559\n",
      "Iteration 149, loss = 1.44558710\n",
      "Iteration 150, loss = 1.43587803\n",
      "Iteration 151, loss = 1.42543522\n",
      "Iteration 152, loss = 1.41594987\n",
      "Iteration 153, loss = 1.40603842\n",
      "Iteration 154, loss = 1.39503817\n",
      "Iteration 155, loss = 1.38693161\n",
      "Iteration 156, loss = 1.37972673\n",
      "Iteration 157, loss = 1.37359140\n",
      "Iteration 158, loss = 1.36522865\n",
      "Iteration 159, loss = 1.35491858\n",
      "Iteration 160, loss = 1.34406358\n",
      "Iteration 161, loss = 1.33144962\n",
      "Iteration 162, loss = 1.32064034\n",
      "Iteration 163, loss = 1.31008841\n",
      "Iteration 164, loss = 1.30025411\n",
      "Iteration 165, loss = 1.29105002\n",
      "Iteration 166, loss = 1.28244093\n",
      "Iteration 167, loss = 1.27359121\n",
      "Iteration 168, loss = 1.26477892\n",
      "Iteration 169, loss = 1.25631242\n",
      "Iteration 170, loss = 1.24799117\n",
      "Iteration 171, loss = 1.24011262\n",
      "Iteration 172, loss = 1.23208221\n",
      "Iteration 173, loss = 1.22302151\n",
      "Iteration 174, loss = 1.21349824\n",
      "Iteration 175, loss = 1.20565320\n",
      "Iteration 176, loss = 1.19689930\n",
      "Iteration 177, loss = 1.18837397\n",
      "Iteration 178, loss = 1.18041309\n",
      "Iteration 179, loss = 1.17295966\n",
      "Iteration 180, loss = 1.16526609\n",
      "Iteration 181, loss = 1.15989404\n",
      "Iteration 182, loss = 1.15101965\n",
      "Iteration 183, loss = 1.14076742\n",
      "Iteration 184, loss = 1.13027742\n",
      "Iteration 185, loss = 1.12366424\n",
      "Iteration 186, loss = 1.11757883\n",
      "Iteration 187, loss = 1.11356890\n",
      "Iteration 188, loss = 1.10861585\n",
      "Iteration 189, loss = 1.10392932\n",
      "Iteration 190, loss = 1.09591451\n",
      "Iteration 191, loss = 1.08497521\n",
      "Iteration 192, loss = 1.07389154\n",
      "Iteration 193, loss = 1.06557964\n",
      "Iteration 194, loss = 1.05614898\n",
      "Iteration 195, loss = 1.05127725\n",
      "Iteration 196, loss = 1.04461659\n",
      "Iteration 197, loss = 1.03700927\n",
      "Iteration 198, loss = 1.02987334\n",
      "Iteration 199, loss = 1.02170169\n",
      "Iteration 200, loss = 1.01483780\n",
      "Iteration 201, loss = 1.00708706\n",
      "Iteration 202, loss = 1.00013695\n",
      "Iteration 203, loss = 0.98981776\n",
      "Iteration 204, loss = 0.98183300\n",
      "Iteration 205, loss = 0.97470983\n",
      "Iteration 206, loss = 0.96744295\n",
      "Iteration 207, loss = 0.96079344\n",
      "Iteration 208, loss = 0.95402488\n",
      "Iteration 209, loss = 0.94752748\n",
      "Iteration 210, loss = 0.94049365\n",
      "Iteration 211, loss = 0.93361328\n",
      "Iteration 212, loss = 0.92660535\n",
      "Iteration 213, loss = 0.92166421\n",
      "Iteration 214, loss = 0.91353067\n",
      "Iteration 215, loss = 0.90932195\n",
      "Iteration 216, loss = 0.90486938\n",
      "Iteration 217, loss = 0.89984951\n",
      "Iteration 218, loss = 0.89372160\n",
      "Iteration 219, loss = 0.88668862\n",
      "Iteration 220, loss = 0.87940465\n",
      "Iteration 221, loss = 0.87136522\n",
      "Iteration 222, loss = 0.86509543\n",
      "Iteration 223, loss = 0.86211157\n",
      "Iteration 224, loss = 0.85656983\n",
      "Iteration 225, loss = 0.85132221\n",
      "Iteration 226, loss = 0.84393771\n",
      "Iteration 227, loss = 0.83639616\n",
      "Iteration 228, loss = 0.83049794\n",
      "Iteration 229, loss = 0.82836631\n",
      "Iteration 230, loss = 0.82574386\n",
      "Iteration 231, loss = 0.82056931\n",
      "Iteration 232, loss = 0.81396189\n",
      "Iteration 233, loss = 0.80491094\n",
      "Iteration 234, loss = 0.80053935\n",
      "Iteration 235, loss = 0.79404267\n",
      "Iteration 236, loss = 0.78941431\n",
      "Iteration 237, loss = 0.78435942\n",
      "Iteration 238, loss = 0.77923000\n",
      "Iteration 239, loss = 0.77400295\n",
      "Iteration 240, loss = 0.77013056\n",
      "Iteration 241, loss = 0.76413659\n",
      "Iteration 242, loss = 0.75905184\n",
      "Iteration 243, loss = 0.75458883\n",
      "Iteration 244, loss = 0.75061868\n",
      "Iteration 245, loss = 0.74604536\n",
      "Iteration 246, loss = 0.74115132\n",
      "Iteration 247, loss = 0.73546359\n",
      "Iteration 248, loss = 0.73077860\n",
      "Iteration 249, loss = 0.72781174\n",
      "Iteration 250, loss = 0.72335097\n",
      "Iteration 251, loss = 0.71892113\n",
      "Iteration 252, loss = 0.71512191\n",
      "Iteration 253, loss = 0.70993863\n",
      "Iteration 254, loss = 0.70662343\n",
      "Iteration 255, loss = 0.70276253\n",
      "Iteration 256, loss = 0.69946528\n",
      "Iteration 257, loss = 0.69555723\n",
      "Iteration 258, loss = 0.69133431\n",
      "Iteration 259, loss = 0.68869811\n",
      "Iteration 260, loss = 0.68568279\n",
      "Iteration 261, loss = 0.68300112\n",
      "Iteration 262, loss = 0.67869186\n",
      "Iteration 263, loss = 0.67482009\n",
      "Iteration 264, loss = 0.67123375\n",
      "Iteration 265, loss = 0.66827217\n",
      "Iteration 266, loss = 0.66591750\n",
      "Iteration 267, loss = 0.66276489\n",
      "Iteration 268, loss = 0.65944006\n",
      "Iteration 269, loss = 0.65634213\n",
      "Iteration 270, loss = 0.65373525\n",
      "Iteration 271, loss = 0.65164207\n",
      "Iteration 272, loss = 0.64886526\n",
      "Iteration 273, loss = 0.64614732\n",
      "Iteration 274, loss = 0.64343683\n",
      "Iteration 275, loss = 0.63999780\n",
      "Iteration 276, loss = 0.63774180\n",
      "Iteration 277, loss = 0.63540809\n",
      "Iteration 278, loss = 0.63383353\n",
      "Iteration 279, loss = 0.63103555\n",
      "Iteration 280, loss = 0.62967644\n",
      "Iteration 281, loss = 0.62799050\n",
      "Iteration 282, loss = 0.62645849\n",
      "Iteration 283, loss = 0.62405876\n",
      "Iteration 284, loss = 0.62135325\n",
      "Iteration 285, loss = 0.61961077\n",
      "Iteration 286, loss = 0.61883477\n",
      "Iteration 287, loss = 0.61851585\n",
      "Iteration 288, loss = 0.61899455\n",
      "Iteration 289, loss = 0.61901162\n",
      "Iteration 290, loss = 0.61461764\n",
      "Iteration 291, loss = 0.60938443\n",
      "Iteration 292, loss = 0.60713551\n",
      "Iteration 293, loss = 0.60829322\n",
      "Iteration 294, loss = 0.60771328\n",
      "Iteration 295, loss = 0.60403035\n",
      "Iteration 296, loss = 0.60081640\n",
      "Iteration 297, loss = 0.59983273\n",
      "Iteration 298, loss = 0.60040997\n",
      "Iteration 299, loss = 0.59901147\n",
      "Iteration 300, loss = 0.59620926\n",
      "Iteration 301, loss = 0.59386470\n",
      "Iteration 302, loss = 0.59332590\n",
      "Iteration 303, loss = 0.59608038\n",
      "Iteration 304, loss = 0.59601287\n",
      "Iteration 305, loss = 0.59373118\n",
      "Iteration 306, loss = 0.59035842\n",
      "Iteration 307, loss = 0.58847366\n",
      "Iteration 308, loss = 0.58759860\n",
      "Iteration 309, loss = 0.58666683\n",
      "Iteration 310, loss = 0.58565111\n",
      "Iteration 311, loss = 0.58454720\n",
      "Iteration 312, loss = 0.58429853\n",
      "Iteration 313, loss = 0.58766316\n",
      "Iteration 314, loss = 0.58552695\n",
      "Iteration 315, loss = 0.58355901\n",
      "Iteration 316, loss = 0.58182213\n",
      "Iteration 317, loss = 0.58185495\n",
      "Iteration 318, loss = 0.58160494\n",
      "Iteration 319, loss = 0.57983556\n",
      "Iteration 320, loss = 0.58040665\n",
      "Iteration 321, loss = 0.58019053\n",
      "Iteration 322, loss = 0.57938682\n",
      "Iteration 323, loss = 0.57746835\n",
      "Iteration 324, loss = 0.57688441\n",
      "Iteration 325, loss = 0.57956949\n",
      "Iteration 326, loss = 0.57941213\n",
      "Iteration 327, loss = 0.57720077\n",
      "Iteration 328, loss = 0.57570900\n",
      "Iteration 329, loss = 0.57521419\n",
      "Iteration 330, loss = 0.57468349\n",
      "Iteration 331, loss = 0.57418041\n",
      "Iteration 332, loss = 0.57361632\n",
      "Iteration 333, loss = 0.57302020\n",
      "Iteration 334, loss = 0.57401639\n",
      "Iteration 335, loss = 0.57278810\n",
      "Iteration 336, loss = 0.57173171\n",
      "Iteration 337, loss = 0.57157202\n",
      "Iteration 338, loss = 0.57403826\n",
      "Iteration 339, loss = 0.57635132\n",
      "Iteration 340, loss = 0.57583939\n",
      "Iteration 341, loss = 0.57291690\n",
      "Iteration 342, loss = 0.57097956\n",
      "Iteration 343, loss = 0.57045226\n",
      "Iteration 344, loss = 0.56974611\n",
      "Iteration 345, loss = 0.56932266\n",
      "Iteration 346, loss = 0.56945765\n",
      "Iteration 347, loss = 0.56992909\n",
      "Iteration 348, loss = 0.57087905\n",
      "Iteration 349, loss = 0.57014065\n",
      "Iteration 350, loss = 0.56930707\n",
      "Iteration 351, loss = 0.56786592\n",
      "Iteration 352, loss = 0.56792919\n",
      "Iteration 353, loss = 0.56725440\n",
      "Iteration 354, loss = 0.56686229\n",
      "Iteration 355, loss = 0.56853207\n",
      "Iteration 356, loss = 0.56851740\n",
      "Iteration 357, loss = 0.56608715\n",
      "Iteration 358, loss = 0.56549418\n",
      "Iteration 359, loss = 0.57309973\n",
      "Iteration 360, loss = 0.57564936\n",
      "Iteration 361, loss = 0.57244364\n",
      "Iteration 362, loss = 0.56742807\n",
      "Iteration 363, loss = 0.56516471\n",
      "Iteration 364, loss = 0.56674496\n",
      "Iteration 365, loss = 0.56808505\n",
      "Iteration 366, loss = 0.56844922\n",
      "Iteration 367, loss = 0.56821105\n",
      "Iteration 368, loss = 0.56653490\n",
      "Iteration 369, loss = 0.56441946\n",
      "Iteration 370, loss = 0.56457158\n",
      "Iteration 371, loss = 0.56631791\n",
      "Iteration 372, loss = 0.56666375\n",
      "Iteration 373, loss = 0.56569340\n",
      "Iteration 374, loss = 0.56428907\n",
      "Iteration 375, loss = 0.56413803\n",
      "Iteration 376, loss = 0.56365749\n",
      "Iteration 377, loss = 0.56336249\n",
      "Iteration 378, loss = 0.56323029\n",
      "Iteration 379, loss = 0.56319403\n",
      "Iteration 380, loss = 0.56327150\n",
      "Iteration 381, loss = 0.56344855\n",
      "Iteration 382, loss = 0.56375678\n",
      "Iteration 383, loss = 0.56359106\n",
      "Iteration 384, loss = 0.56284681\n",
      "Iteration 385, loss = 0.56245969\n",
      "Iteration 386, loss = 0.56246466\n",
      "Iteration 387, loss = 0.56234739\n",
      "Iteration 388, loss = 0.56234196\n",
      "Iteration 389, loss = 0.56224508\n",
      "Iteration 390, loss = 0.56206307\n",
      "Iteration 391, loss = 0.56200463\n",
      "Iteration 392, loss = 0.56342270\n",
      "Iteration 393, loss = 0.56288307\n",
      "Iteration 394, loss = 0.56265336\n",
      "Iteration 395, loss = 0.56183281\n",
      "Iteration 396, loss = 0.56312386\n",
      "Iteration 397, loss = 0.56241595\n",
      "Iteration 398, loss = 0.56208318\n",
      "Iteration 399, loss = 0.56192066\n",
      "Iteration 400, loss = 0.56146886\n",
      "Iteration 401, loss = 0.56204597\n",
      "Iteration 402, loss = 0.56275056\n",
      "Iteration 403, loss = 0.56383212\n",
      "Iteration 404, loss = 0.56461568\n",
      "Iteration 405, loss = 0.56332481\n",
      "Iteration 406, loss = 0.56176027\n",
      "Iteration 407, loss = 0.56104169\n",
      "Iteration 408, loss = 0.56114168\n",
      "Iteration 409, loss = 0.56125731\n",
      "Iteration 410, loss = 0.56126844\n",
      "Iteration 411, loss = 0.56115267\n",
      "Iteration 412, loss = 0.56085655\n",
      "Iteration 413, loss = 0.56038753\n",
      "Iteration 414, loss = 0.56052565\n",
      "Iteration 415, loss = 0.56190283\n",
      "Iteration 416, loss = 0.56280113\n",
      "Iteration 417, loss = 0.56273632\n",
      "Iteration 418, loss = 0.56281817\n",
      "Iteration 419, loss = 0.56250798\n",
      "Iteration 420, loss = 0.56143897\n",
      "Iteration 421, loss = 0.56017051\n",
      "Iteration 422, loss = 0.56072733\n",
      "Iteration 423, loss = 0.56150144\n",
      "Iteration 424, loss = 0.56282443\n",
      "Iteration 425, loss = 0.56254488\n",
      "Iteration 426, loss = 0.56030705\n",
      "Iteration 427, loss = 0.55966262\n",
      "Iteration 428, loss = 0.56219105\n",
      "Iteration 429, loss = 0.56116774\n",
      "Iteration 430, loss = 0.55970996\n",
      "Iteration 431, loss = 0.56034947\n",
      "Iteration 432, loss = 0.56223946\n",
      "Iteration 433, loss = 0.56440915\n",
      "Iteration 434, loss = 0.56733582\n",
      "Iteration 435, loss = 0.56671562\n",
      "Iteration 436, loss = 0.56278229\n",
      "Iteration 437, loss = 0.55955398\n",
      "Iteration 438, loss = 0.55981833\n",
      "Iteration 439, loss = 0.56298467\n",
      "Iteration 440, loss = 0.56341429\n",
      "Iteration 441, loss = 0.56097814\n",
      "Iteration 442, loss = 0.56016322\n",
      "Iteration 443, loss = 0.56025498\n",
      "Iteration 444, loss = 0.55996964\n",
      "Iteration 445, loss = 0.55975622\n",
      "Iteration 446, loss = 0.55904919\n",
      "Iteration 447, loss = 0.55906167\n",
      "Iteration 448, loss = 0.55891257\n",
      "Iteration 449, loss = 0.55933649\n",
      "Iteration 450, loss = 0.55942898\n",
      "Iteration 451, loss = 0.55891508\n",
      "Iteration 452, loss = 0.55914104\n",
      "Iteration 453, loss = 0.55970764\n",
      "Iteration 454, loss = 0.55914281\n",
      "Iteration 455, loss = 0.56038475\n",
      "Iteration 456, loss = 0.55990163\n",
      "Iteration 457, loss = 0.55882043\n",
      "Iteration 458, loss = 0.55850054\n",
      "Iteration 459, loss = 0.55863124\n",
      "Iteration 460, loss = 0.55913936\n",
      "Iteration 461, loss = 0.56074337\n",
      "Iteration 462, loss = 0.55926740\n",
      "Iteration 463, loss = 0.55865410\n",
      "Iteration 464, loss = 0.55960465\n",
      "Iteration 465, loss = 0.56012824\n",
      "Iteration 466, loss = 0.55868910\n",
      "Iteration 467, loss = 0.55849403\n",
      "Iteration 468, loss = 0.55799189\n",
      "Iteration 469, loss = 0.55835754\n",
      "Iteration 470, loss = 0.55885371\n",
      "Iteration 471, loss = 0.55888286\n",
      "Iteration 472, loss = 0.55812136\n",
      "Iteration 473, loss = 0.55794887\n",
      "Iteration 474, loss = 0.55817401\n",
      "Iteration 475, loss = 0.55827971\n",
      "Iteration 476, loss = 0.55799853\n",
      "Iteration 477, loss = 0.55801248\n",
      "Iteration 478, loss = 0.55825497\n",
      "Iteration 479, loss = 0.55842023\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86195625\n",
      "Iteration 2, loss = 0.83998856\n",
      "Iteration 3, loss = 0.82013064\n",
      "Iteration 4, loss = 0.80093172\n",
      "Iteration 5, loss = 0.78270350\n",
      "Iteration 6, loss = 0.76618188\n",
      "Iteration 7, loss = 0.74982190\n",
      "Iteration 8, loss = 0.73410941\n",
      "Iteration 9, loss = 0.72051226\n",
      "Iteration 10, loss = 0.70730830\n",
      "Iteration 11, loss = 0.69446356\n",
      "Iteration 12, loss = 0.68337958\n",
      "Iteration 13, loss = 0.67228977\n",
      "Iteration 14, loss = 0.66217140\n",
      "Iteration 15, loss = 0.65359259\n",
      "Iteration 16, loss = 0.64560395\n",
      "Iteration 17, loss = 0.63777929\n",
      "Iteration 18, loss = 0.63126655\n",
      "Iteration 19, loss = 0.62486862\n",
      "Iteration 20, loss = 0.61974281\n",
      "Iteration 21, loss = 0.61506690\n",
      "Iteration 22, loss = 0.61052338\n",
      "Iteration 23, loss = 0.60755241\n",
      "Iteration 24, loss = 0.60379567\n",
      "Iteration 25, loss = 0.60190794\n",
      "Iteration 26, loss = 0.59989824\n",
      "Iteration 27, loss = 0.59789158\n",
      "Iteration 28, loss = 0.59649950\n",
      "Iteration 29, loss = 0.59537170\n",
      "Iteration 30, loss = 0.59446732\n",
      "Iteration 31, loss = 0.59369919\n",
      "Iteration 32, loss = 0.59323103\n",
      "Iteration 33, loss = 0.59277691\n",
      "Iteration 34, loss = 0.59267072\n",
      "Iteration 35, loss = 0.59211653\n",
      "Iteration 36, loss = 0.59179232\n",
      "Iteration 37, loss = 0.59143547\n",
      "Iteration 38, loss = 0.59100894\n",
      "Iteration 39, loss = 0.59056499\n",
      "Iteration 40, loss = 0.59015263\n",
      "Iteration 41, loss = 0.58973732\n",
      "Iteration 42, loss = 0.58933760\n",
      "Iteration 43, loss = 0.58875939\n",
      "Iteration 44, loss = 0.58855060\n",
      "Iteration 45, loss = 0.58808980\n",
      "Iteration 46, loss = 0.58773503\n",
      "Iteration 47, loss = 0.58735668\n",
      "Iteration 48, loss = 0.58707329\n",
      "Iteration 49, loss = 0.58683354\n",
      "Iteration 50, loss = 0.58652942\n",
      "Iteration 51, loss = 0.58623058\n",
      "Iteration 52, loss = 0.58594899\n",
      "Iteration 53, loss = 0.58570120\n",
      "Iteration 54, loss = 0.58541813\n",
      "Iteration 55, loss = 0.58511207\n",
      "Iteration 56, loss = 0.58489904\n",
      "Iteration 57, loss = 0.58461410\n",
      "Iteration 58, loss = 0.58436881\n",
      "Iteration 59, loss = 0.58412966\n",
      "Iteration 60, loss = 0.58396370\n",
      "Iteration 61, loss = 0.58376851\n",
      "Iteration 62, loss = 0.58362302\n",
      "Iteration 63, loss = 0.58333265\n",
      "Iteration 64, loss = 0.58299148\n",
      "Iteration 65, loss = 0.58278377\n",
      "Iteration 66, loss = 0.58252413\n",
      "Iteration 67, loss = 0.58228083\n",
      "Iteration 68, loss = 0.58207767\n",
      "Iteration 69, loss = 0.58187479\n",
      "Iteration 70, loss = 0.58172284\n",
      "Iteration 71, loss = 0.58154250\n",
      "Iteration 72, loss = 0.58141356\n",
      "Iteration 73, loss = 0.58128819\n",
      "Iteration 74, loss = 0.58110797\n",
      "Iteration 75, loss = 0.58097609\n",
      "Iteration 76, loss = 0.58084988\n",
      "Iteration 77, loss = 0.58068072\n",
      "Iteration 78, loss = 0.58054913\n",
      "Iteration 79, loss = 0.58025232\n",
      "Iteration 80, loss = 0.58015058\n",
      "Iteration 81, loss = 0.58016408\n",
      "Iteration 82, loss = 0.57992987\n",
      "Iteration 83, loss = 0.57983006\n",
      "Iteration 84, loss = 0.57983956\n",
      "Iteration 85, loss = 0.57975701\n",
      "Iteration 86, loss = 0.57973834\n",
      "Iteration 87, loss = 0.57965120\n",
      "Iteration 88, loss = 0.57954940\n",
      "Iteration 89, loss = 0.57938490\n",
      "Iteration 90, loss = 0.57927083\n",
      "Iteration 91, loss = 0.57920191\n",
      "Iteration 92, loss = 0.57910287\n",
      "Iteration 93, loss = 0.57898611\n",
      "Iteration 94, loss = 0.57883753\n",
      "Iteration 95, loss = 0.57874080\n",
      "Iteration 96, loss = 0.57861180\n",
      "Iteration 97, loss = 0.57849359\n",
      "Iteration 98, loss = 0.57841079\n",
      "Iteration 99, loss = 0.57825553\n",
      "Iteration 100, loss = 0.57816686\n",
      "Iteration 101, loss = 0.57799371\n",
      "Iteration 102, loss = 0.57795255\n",
      "Iteration 103, loss = 0.57779444\n",
      "Iteration 104, loss = 0.57768807\n",
      "Iteration 105, loss = 0.57760300\n",
      "Iteration 106, loss = 0.57754064\n",
      "Iteration 107, loss = 0.57743422\n",
      "Iteration 108, loss = 0.57748899\n",
      "Iteration 109, loss = 0.57760467\n",
      "Iteration 110, loss = 0.57772083\n",
      "Iteration 111, loss = 0.57806140\n",
      "Iteration 112, loss = 0.57795342\n",
      "Iteration 113, loss = 0.57784709\n",
      "Iteration 114, loss = 0.57769408\n",
      "Iteration 115, loss = 0.57757933\n",
      "Iteration 116, loss = 0.57745885\n",
      "Iteration 117, loss = 0.57729100\n",
      "Iteration 118, loss = 0.57711266\n",
      "Iteration 119, loss = 0.57685915\n",
      "Iteration 120, loss = 0.57674264\n",
      "Iteration 121, loss = 0.57651676\n",
      "Iteration 122, loss = 0.57634144\n",
      "Iteration 123, loss = 0.57628875\n",
      "Iteration 124, loss = 0.57619984\n",
      "Iteration 125, loss = 0.57602659\n",
      "Iteration 126, loss = 0.57594113\n",
      "Iteration 127, loss = 0.57585704\n",
      "Iteration 128, loss = 0.57583924\n",
      "Iteration 129, loss = 0.57573813\n",
      "Iteration 130, loss = 0.57577688\n",
      "Iteration 131, loss = 0.57577031\n",
      "Iteration 132, loss = 0.57583092\n",
      "Iteration 133, loss = 0.57575904\n",
      "Iteration 134, loss = 0.57558655\n",
      "Iteration 135, loss = 0.57552324\n",
      "Iteration 136, loss = 0.57529520\n",
      "Iteration 137, loss = 0.57514077\n",
      "Iteration 138, loss = 0.57508008\n",
      "Iteration 139, loss = 0.57485168\n",
      "Iteration 140, loss = 0.57478921\n",
      "Iteration 141, loss = 0.57477771\n",
      "Iteration 142, loss = 0.57488923\n",
      "Iteration 143, loss = 0.57503929\n",
      "Iteration 144, loss = 0.57522150\n",
      "Iteration 145, loss = 0.57523747\n",
      "Iteration 146, loss = 0.57517044\n",
      "Iteration 147, loss = 0.57499243\n",
      "Iteration 148, loss = 0.57473853\n",
      "Iteration 149, loss = 0.57451178\n",
      "Iteration 150, loss = 0.57448456\n",
      "Iteration 151, loss = 0.57426434\n",
      "Iteration 152, loss = 0.57411114\n",
      "Iteration 153, loss = 0.57402640\n",
      "Iteration 154, loss = 0.57399893\n",
      "Iteration 155, loss = 0.57388655\n",
      "Iteration 156, loss = 0.57385440\n",
      "Iteration 157, loss = 0.57377760\n",
      "Iteration 158, loss = 0.57364712\n",
      "Iteration 159, loss = 0.57358493\n",
      "Iteration 160, loss = 0.57346925\n",
      "Iteration 161, loss = 0.57345104\n",
      "Iteration 162, loss = 0.57336007\n",
      "Iteration 163, loss = 0.57329984\n",
      "Iteration 164, loss = 0.57323404\n",
      "Iteration 165, loss = 0.57317767\n",
      "Iteration 166, loss = 0.57314394\n",
      "Iteration 167, loss = 0.57311108\n",
      "Iteration 168, loss = 0.57316783\n",
      "Iteration 169, loss = 0.57347010\n",
      "Iteration 170, loss = 0.57352028\n",
      "Iteration 171, loss = 0.57366393\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 24.61957208\n",
      "Iteration 2, loss = 24.40153165\n",
      "Iteration 3, loss = 24.15920580\n",
      "Iteration 4, loss = 23.91544989\n",
      "Iteration 5, loss = 23.64910915\n",
      "Iteration 6, loss = 23.38755163\n",
      "Iteration 7, loss = 23.10175330\n",
      "Iteration 8, loss = 22.80709466\n",
      "Iteration 9, loss = 22.50324728\n",
      "Iteration 10, loss = 22.17445365\n",
      "Iteration 11, loss = 21.81923397\n",
      "Iteration 12, loss = 21.46867159\n",
      "Iteration 13, loss = 21.10951602\n",
      "Iteration 14, loss = 20.73728924\n",
      "Iteration 15, loss = 20.36789482\n",
      "Iteration 16, loss = 19.99757356\n",
      "Iteration 17, loss = 19.61703984\n",
      "Iteration 18, loss = 19.24065767\n",
      "Iteration 19, loss = 18.85059289\n",
      "Iteration 20, loss = 18.47288394\n",
      "Iteration 21, loss = 18.07725900\n",
      "Iteration 22, loss = 17.68258593\n",
      "Iteration 23, loss = 17.28598934\n",
      "Iteration 24, loss = 16.88469898\n",
      "Iteration 25, loss = 16.48170029\n",
      "Iteration 26, loss = 16.06652919\n",
      "Iteration 27, loss = 15.64881612\n",
      "Iteration 28, loss = 15.23686733\n",
      "Iteration 29, loss = 14.80518899\n",
      "Iteration 30, loss = 14.37565042\n",
      "Iteration 31, loss = 13.94268684\n",
      "Iteration 32, loss = 13.50339393\n",
      "Iteration 33, loss = 13.05821058\n",
      "Iteration 34, loss = 12.61086768\n",
      "Iteration 35, loss = 12.15540000\n",
      "Iteration 36, loss = 11.70360060\n",
      "Iteration 37, loss = 11.23823519\n",
      "Iteration 38, loss = 10.77811121\n",
      "Iteration 39, loss = 10.30735320\n",
      "Iteration 40, loss = 9.84139494\n",
      "Iteration 41, loss = 9.36926995\n",
      "Iteration 42, loss = 8.89268572\n",
      "Iteration 43, loss = 8.40840871\n",
      "Iteration 44, loss = 7.93535192\n",
      "Iteration 45, loss = 7.44650975\n",
      "Iteration 46, loss = 6.96861521\n",
      "Iteration 47, loss = 6.48047916\n",
      "Iteration 48, loss = 6.00814517\n",
      "Iteration 49, loss = 5.53323111\n",
      "Iteration 50, loss = 5.06934037\n",
      "Iteration 51, loss = 4.60992554\n",
      "Iteration 52, loss = 4.16306398\n",
      "Iteration 53, loss = 3.73660571\n",
      "Iteration 54, loss = 3.31291360\n",
      "Iteration 55, loss = 2.92440805\n",
      "Iteration 56, loss = 2.54708203\n",
      "Iteration 57, loss = 2.22049206\n",
      "Iteration 58, loss = 1.93023599\n",
      "Iteration 59, loss = 1.68951297\n",
      "Iteration 60, loss = 1.52178559\n",
      "Iteration 61, loss = 1.40343382\n",
      "Iteration 62, loss = 1.34518754\n",
      "Iteration 63, loss = 1.32146797\n",
      "Iteration 64, loss = 1.32207267\n",
      "Iteration 65, loss = 1.33250794\n",
      "Iteration 66, loss = 1.34321238\n",
      "Iteration 67, loss = 1.35297250\n",
      "Iteration 68, loss = 1.35512094\n",
      "Iteration 69, loss = 1.35210555\n",
      "Iteration 70, loss = 1.34556673\n",
      "Iteration 71, loss = 1.33473609\n",
      "Iteration 72, loss = 1.32136837\n",
      "Iteration 73, loss = 1.30603917\n",
      "Iteration 74, loss = 1.29185694\n",
      "Iteration 75, loss = 1.27588195\n",
      "Iteration 76, loss = 1.26318101\n",
      "Iteration 77, loss = 1.25053024\n",
      "Iteration 78, loss = 1.23850650\n",
      "Iteration 79, loss = 1.22871595\n",
      "Iteration 80, loss = 1.22171750\n",
      "Iteration 81, loss = 1.21437466\n",
      "Iteration 82, loss = 1.20957469\n",
      "Iteration 83, loss = 1.20281751\n",
      "Iteration 84, loss = 1.19794720\n",
      "Iteration 85, loss = 1.19330888\n",
      "Iteration 86, loss = 1.18738571\n",
      "Iteration 87, loss = 1.18210677\n",
      "Iteration 88, loss = 1.17652516\n",
      "Iteration 89, loss = 1.17134002\n",
      "Iteration 90, loss = 1.16622528\n",
      "Iteration 91, loss = 1.16119224\n",
      "Iteration 92, loss = 1.15644919\n",
      "Iteration 93, loss = 1.15041402\n",
      "Iteration 94, loss = 1.14435175\n",
      "Iteration 95, loss = 1.13809545\n",
      "Iteration 96, loss = 1.13197354\n",
      "Iteration 97, loss = 1.12594428\n",
      "Iteration 98, loss = 1.11958382\n",
      "Iteration 99, loss = 1.11471478\n",
      "Iteration 100, loss = 1.10918587\n",
      "Iteration 101, loss = 1.10367725\n",
      "Iteration 102, loss = 1.09901128\n",
      "Iteration 103, loss = 1.09391466\n",
      "Iteration 104, loss = 1.08961643\n",
      "Iteration 105, loss = 1.08440268\n",
      "Iteration 106, loss = 1.07964881\n",
      "Iteration 107, loss = 1.07467884\n",
      "Iteration 108, loss = 1.06931308\n",
      "Iteration 109, loss = 1.06376442\n",
      "Iteration 110, loss = 1.05816225\n",
      "Iteration 111, loss = 1.05315463\n",
      "Iteration 112, loss = 1.04742985\n",
      "Iteration 113, loss = 1.04275411\n",
      "Iteration 114, loss = 1.03755086\n",
      "Iteration 115, loss = 1.03197977\n",
      "Iteration 116, loss = 1.02746086\n",
      "Iteration 117, loss = 1.02221165\n",
      "Iteration 118, loss = 1.01727296\n",
      "Iteration 119, loss = 1.01208506\n",
      "Iteration 120, loss = 1.00756856\n",
      "Iteration 121, loss = 1.00210843\n",
      "Iteration 122, loss = 0.99796456\n",
      "Iteration 123, loss = 0.99288523\n",
      "Iteration 124, loss = 0.98807187\n",
      "Iteration 125, loss = 0.98348737\n",
      "Iteration 126, loss = 0.97872329\n",
      "Iteration 127, loss = 0.97437830\n",
      "Iteration 128, loss = 0.96970037\n",
      "Iteration 129, loss = 0.96471949\n",
      "Iteration 130, loss = 0.95998944\n",
      "Iteration 131, loss = 0.95502008\n",
      "Iteration 132, loss = 0.94992075\n",
      "Iteration 133, loss = 0.94485800\n",
      "Iteration 134, loss = 0.94006742\n",
      "Iteration 135, loss = 0.93511102\n",
      "Iteration 136, loss = 0.93029907\n",
      "Iteration 137, loss = 0.92565056\n",
      "Iteration 138, loss = 0.92089734\n",
      "Iteration 139, loss = 0.91626242\n",
      "Iteration 140, loss = 0.91180082\n",
      "Iteration 141, loss = 0.90705022\n",
      "Iteration 142, loss = 0.90260781\n",
      "Iteration 143, loss = 0.89819545\n",
      "Iteration 144, loss = 0.89349990\n",
      "Iteration 145, loss = 0.88860470\n",
      "Iteration 146, loss = 0.88438397\n",
      "Iteration 147, loss = 0.87942536\n",
      "Iteration 148, loss = 0.87475593\n",
      "Iteration 149, loss = 0.87003791\n",
      "Iteration 150, loss = 0.86521935\n",
      "Iteration 151, loss = 0.86092654\n",
      "Iteration 152, loss = 0.85621166\n",
      "Iteration 153, loss = 0.85194619\n",
      "Iteration 154, loss = 0.84736772\n",
      "Iteration 155, loss = 0.84315783\n",
      "Iteration 156, loss = 0.83874431\n",
      "Iteration 157, loss = 0.83463090\n",
      "Iteration 158, loss = 0.83021513\n",
      "Iteration 159, loss = 0.82622616\n",
      "Iteration 160, loss = 0.82165382\n",
      "Iteration 161, loss = 0.81727609\n",
      "Iteration 162, loss = 0.81287791\n",
      "Iteration 163, loss = 0.80834155\n",
      "Iteration 164, loss = 0.80424332\n",
      "Iteration 165, loss = 0.79955806\n",
      "Iteration 166, loss = 0.79539803\n",
      "Iteration 167, loss = 0.79117730\n",
      "Iteration 168, loss = 0.78699564\n",
      "Iteration 169, loss = 0.78303873\n",
      "Iteration 170, loss = 0.77848766\n",
      "Iteration 171, loss = 0.77435095\n",
      "Iteration 172, loss = 0.77024495\n",
      "Iteration 173, loss = 0.76539063\n",
      "Iteration 174, loss = 0.76126778\n",
      "Iteration 175, loss = 0.75702543\n",
      "Iteration 176, loss = 0.75285587\n",
      "Iteration 177, loss = 0.74803758\n",
      "Iteration 178, loss = 0.74443828\n",
      "Iteration 179, loss = 0.73956180\n",
      "Iteration 180, loss = 0.73568114\n",
      "Iteration 181, loss = 0.73168841\n",
      "Iteration 182, loss = 0.72794623\n",
      "Iteration 183, loss = 0.72405811\n",
      "Iteration 184, loss = 0.72045528\n",
      "Iteration 185, loss = 0.71647523\n",
      "Iteration 186, loss = 0.71237358\n",
      "Iteration 187, loss = 0.70922488\n",
      "Iteration 188, loss = 0.70522180\n",
      "Iteration 189, loss = 0.70155333\n",
      "Iteration 190, loss = 0.69784435\n",
      "Iteration 191, loss = 0.69436543\n",
      "Iteration 192, loss = 0.69087624\n",
      "Iteration 193, loss = 0.68697508\n",
      "Iteration 194, loss = 0.68317893\n",
      "Iteration 195, loss = 0.67992111\n",
      "Iteration 196, loss = 0.67697301\n",
      "Iteration 197, loss = 0.67407288\n",
      "Iteration 198, loss = 0.67025238\n",
      "Iteration 199, loss = 0.66698469\n",
      "Iteration 200, loss = 0.66289966\n",
      "Iteration 201, loss = 0.65941999\n",
      "Iteration 202, loss = 0.65583034\n",
      "Iteration 203, loss = 0.65256269\n",
      "Iteration 204, loss = 0.64917115\n",
      "Iteration 205, loss = 0.64632567\n",
      "Iteration 206, loss = 0.64363829\n",
      "Iteration 207, loss = 0.64081113\n",
      "Iteration 208, loss = 0.63752263\n",
      "Iteration 209, loss = 0.63499071\n",
      "Iteration 210, loss = 0.63180589\n",
      "Iteration 211, loss = 0.62943237\n",
      "Iteration 212, loss = 0.62637257\n",
      "Iteration 213, loss = 0.62395245\n",
      "Iteration 214, loss = 0.62111307\n",
      "Iteration 215, loss = 0.61831114\n",
      "Iteration 216, loss = 0.61612000\n",
      "Iteration 217, loss = 0.61321024\n",
      "Iteration 218, loss = 0.61041797\n",
      "Iteration 219, loss = 0.60870879\n",
      "Iteration 220, loss = 0.60611347\n",
      "Iteration 221, loss = 0.60379652\n",
      "Iteration 222, loss = 0.60142980\n",
      "Iteration 223, loss = 0.59921026\n",
      "Iteration 224, loss = 0.59661513\n",
      "Iteration 225, loss = 0.59425361\n",
      "Iteration 226, loss = 0.59201925\n",
      "Iteration 227, loss = 0.59008328\n",
      "Iteration 228, loss = 0.58846993\n",
      "Iteration 229, loss = 0.58649048\n",
      "Iteration 230, loss = 0.58476568\n",
      "Iteration 231, loss = 0.58290086\n",
      "Iteration 232, loss = 0.58136931\n",
      "Iteration 233, loss = 0.57977230\n",
      "Iteration 234, loss = 0.57830358\n",
      "Iteration 235, loss = 0.57703810\n",
      "Iteration 236, loss = 0.57552635\n",
      "Iteration 237, loss = 0.57419067\n",
      "Iteration 238, loss = 0.57308981\n",
      "Iteration 239, loss = 0.57184747\n",
      "Iteration 240, loss = 0.57072015\n",
      "Iteration 241, loss = 0.56984077\n",
      "Iteration 242, loss = 0.56894074\n",
      "Iteration 243, loss = 0.56777350\n",
      "Iteration 244, loss = 0.56684422\n",
      "Iteration 245, loss = 0.56617071\n",
      "Iteration 246, loss = 0.56492478\n",
      "Iteration 247, loss = 0.56470366\n",
      "Iteration 248, loss = 0.56357411\n",
      "Iteration 249, loss = 0.56257610\n",
      "Iteration 250, loss = 0.56186617\n",
      "Iteration 251, loss = 0.56140688\n",
      "Iteration 252, loss = 0.56084381\n",
      "Iteration 253, loss = 0.56020016\n",
      "Iteration 254, loss = 0.55946357\n",
      "Iteration 255, loss = 0.55884396\n",
      "Iteration 256, loss = 0.55804256\n",
      "Iteration 257, loss = 0.55781461\n",
      "Iteration 258, loss = 0.55751715\n",
      "Iteration 259, loss = 0.55691701\n",
      "Iteration 260, loss = 0.55623202\n",
      "Iteration 261, loss = 0.55556594\n",
      "Iteration 262, loss = 0.55485228\n",
      "Iteration 263, loss = 0.55446748\n",
      "Iteration 264, loss = 0.55498290\n",
      "Iteration 265, loss = 0.55608087\n",
      "Iteration 266, loss = 0.55698814\n",
      "Iteration 267, loss = 0.55628430\n",
      "Iteration 268, loss = 0.55543936\n",
      "Iteration 269, loss = 0.55347134\n",
      "Iteration 270, loss = 0.55246681\n",
      "Iteration 271, loss = 0.55242700\n",
      "Iteration 272, loss = 0.55202995\n",
      "Iteration 273, loss = 0.55256617\n",
      "Iteration 274, loss = 0.55281559\n",
      "Iteration 275, loss = 0.55279460\n",
      "Iteration 276, loss = 0.55252141\n",
      "Iteration 277, loss = 0.55215248\n",
      "Iteration 278, loss = 0.55148826\n",
      "Iteration 279, loss = 0.55117533\n",
      "Iteration 280, loss = 0.55032580\n",
      "Iteration 281, loss = 0.55028455\n",
      "Iteration 282, loss = 0.54986147\n",
      "Iteration 283, loss = 0.54962601\n",
      "Iteration 284, loss = 0.54947761\n",
      "Iteration 285, loss = 0.54943203\n",
      "Iteration 286, loss = 0.54933391\n",
      "Iteration 287, loss = 0.54917799\n",
      "Iteration 288, loss = 0.54887812\n",
      "Iteration 289, loss = 0.54857567\n",
      "Iteration 290, loss = 0.54847483\n",
      "Iteration 291, loss = 0.54865181\n",
      "Iteration 292, loss = 0.54929172\n",
      "Iteration 293, loss = 0.54923453\n",
      "Iteration 294, loss = 0.54914023\n",
      "Iteration 295, loss = 0.54917699\n",
      "Iteration 296, loss = 0.54950019\n",
      "Iteration 297, loss = 0.54936391\n",
      "Iteration 298, loss = 0.54877112\n",
      "Iteration 299, loss = 0.54828536\n",
      "Iteration 300, loss = 0.54790316\n",
      "Iteration 301, loss = 0.54733503\n",
      "Iteration 302, loss = 0.54708832\n",
      "Iteration 303, loss = 0.54699634\n",
      "Iteration 304, loss = 0.54678155\n",
      "Iteration 305, loss = 0.54688580\n",
      "Iteration 306, loss = 0.54698798\n",
      "Iteration 307, loss = 0.54696495\n",
      "Iteration 308, loss = 0.54680323\n",
      "Iteration 309, loss = 0.54681415\n",
      "Iteration 310, loss = 0.54680560\n",
      "Iteration 311, loss = 0.54676465\n",
      "Iteration 312, loss = 0.54653889\n",
      "Iteration 313, loss = 0.54675198\n",
      "Iteration 314, loss = 0.54696682\n",
      "Iteration 315, loss = 0.54681769\n",
      "Iteration 316, loss = 0.54641112\n",
      "Iteration 317, loss = 0.54590152\n",
      "Iteration 318, loss = 0.54563080\n",
      "Iteration 319, loss = 0.54546051\n",
      "Iteration 320, loss = 0.54528676\n",
      "Iteration 321, loss = 0.54530966\n",
      "Iteration 322, loss = 0.54626693\n",
      "Iteration 323, loss = 0.54636032\n",
      "Iteration 324, loss = 0.54641269\n",
      "Iteration 325, loss = 0.54642703\n",
      "Iteration 326, loss = 0.54620743\n",
      "Iteration 327, loss = 0.54596727\n",
      "Iteration 328, loss = 0.54548857\n",
      "Iteration 329, loss = 0.54502858\n",
      "Iteration 330, loss = 0.54454425\n",
      "Iteration 331, loss = 0.54457248\n",
      "Iteration 332, loss = 0.54440524\n",
      "Iteration 333, loss = 0.54445845\n",
      "Iteration 334, loss = 0.54430304\n",
      "Iteration 335, loss = 0.54432814\n",
      "Iteration 336, loss = 0.54404064\n",
      "Iteration 337, loss = 0.54400814\n",
      "Iteration 338, loss = 0.54399942\n",
      "Iteration 339, loss = 0.54385220\n",
      "Iteration 340, loss = 0.54381609\n",
      "Iteration 341, loss = 0.54385702\n",
      "Iteration 342, loss = 0.54380048\n",
      "Iteration 343, loss = 0.54368500\n",
      "Iteration 344, loss = 0.54349094\n",
      "Iteration 345, loss = 0.54337958\n",
      "Iteration 346, loss = 0.54333534\n",
      "Iteration 347, loss = 0.54348055\n",
      "Iteration 348, loss = 0.54329783\n",
      "Iteration 349, loss = 0.54303537\n",
      "Iteration 350, loss = 0.54319658\n",
      "Iteration 351, loss = 0.54330173\n",
      "Iteration 352, loss = 0.54341324\n",
      "Iteration 353, loss = 0.54368366\n",
      "Iteration 354, loss = 0.54357398\n",
      "Iteration 355, loss = 0.54311325\n",
      "Iteration 356, loss = 0.54291755\n",
      "Iteration 357, loss = 0.54284337\n",
      "Iteration 358, loss = 0.54273160\n",
      "Iteration 359, loss = 0.54274018\n",
      "Iteration 360, loss = 0.54250463\n",
      "Iteration 361, loss = 0.54251529\n",
      "Iteration 362, loss = 0.54249801\n",
      "Iteration 363, loss = 0.54253490\n",
      "Iteration 364, loss = 0.54261268\n",
      "Iteration 365, loss = 0.54282533\n",
      "Iteration 366, loss = 0.54299295\n",
      "Iteration 367, loss = 0.54279842\n",
      "Iteration 368, loss = 0.54280279\n",
      "Iteration 369, loss = 0.54231181\n",
      "Iteration 370, loss = 0.54220299\n",
      "Iteration 371, loss = 0.54202785\n",
      "Iteration 372, loss = 0.54198943\n",
      "Iteration 373, loss = 0.54189572\n",
      "Iteration 374, loss = 0.54178585\n",
      "Iteration 375, loss = 0.54159729\n",
      "Iteration 376, loss = 0.54152203\n",
      "Iteration 377, loss = 0.54168365\n",
      "Iteration 378, loss = 0.54142320\n",
      "Iteration 379, loss = 0.54144469\n",
      "Iteration 380, loss = 0.54111011\n",
      "Iteration 381, loss = 0.54104631\n",
      "Iteration 382, loss = 0.54097335\n",
      "Iteration 383, loss = 0.54087796\n",
      "Iteration 384, loss = 0.54062572\n",
      "Iteration 385, loss = 0.54041102\n",
      "Iteration 386, loss = 0.54054162\n",
      "Iteration 387, loss = 0.53999972\n",
      "Iteration 388, loss = 0.54048251\n",
      "Iteration 389, loss = 0.54012047\n",
      "Iteration 390, loss = 0.53973828\n",
      "Iteration 391, loss = 0.53966879\n",
      "Iteration 392, loss = 0.53938085\n",
      "Iteration 393, loss = 0.53922184\n",
      "Iteration 394, loss = 0.53873099\n",
      "Iteration 395, loss = 0.53951667\n",
      "Iteration 396, loss = 0.53938724\n",
      "Iteration 397, loss = 0.53914231\n",
      "Iteration 398, loss = 0.53884719\n",
      "Iteration 399, loss = 0.53836885\n",
      "Iteration 400, loss = 0.53824888\n",
      "Iteration 401, loss = 0.53794138\n",
      "Iteration 402, loss = 0.53770317\n",
      "Iteration 403, loss = 0.53779894\n",
      "Iteration 404, loss = 0.53769178\n",
      "Iteration 405, loss = 0.53749230\n",
      "Iteration 406, loss = 0.53718654\n",
      "Iteration 407, loss = 0.53700218\n",
      "Iteration 408, loss = 0.53729210\n",
      "Iteration 409, loss = 0.53687684\n",
      "Iteration 410, loss = 0.53663738\n",
      "Iteration 411, loss = 0.53664834\n",
      "Iteration 412, loss = 0.53649733\n",
      "Iteration 413, loss = 0.53608827\n",
      "Iteration 414, loss = 0.53574273\n",
      "Iteration 415, loss = 0.53550769\n",
      "Iteration 416, loss = 0.53529432\n",
      "Iteration 417, loss = 0.53514211\n",
      "Iteration 418, loss = 0.53504856\n",
      "Iteration 419, loss = 0.53499005\n",
      "Iteration 420, loss = 0.53515854\n",
      "Iteration 421, loss = 0.53628994\n",
      "Iteration 422, loss = 0.53580943\n",
      "Iteration 423, loss = 0.53438763\n",
      "Iteration 424, loss = 0.53427593\n",
      "Iteration 425, loss = 0.53464256\n",
      "Iteration 426, loss = 0.53585600\n",
      "Iteration 427, loss = 0.53636251\n",
      "Iteration 428, loss = 0.53550368\n",
      "Iteration 429, loss = 0.53435775\n",
      "Iteration 430, loss = 0.53329789\n",
      "Iteration 431, loss = 0.53361211\n",
      "Iteration 432, loss = 0.53346656\n",
      "Iteration 433, loss = 0.53351561\n",
      "Iteration 434, loss = 0.53351321\n",
      "Iteration 435, loss = 0.53234702\n",
      "Iteration 436, loss = 0.53231784\n",
      "Iteration 437, loss = 0.53195251\n",
      "Iteration 438, loss = 0.53169808\n",
      "Iteration 439, loss = 0.53254578\n",
      "Iteration 440, loss = 0.53275764\n",
      "Iteration 441, loss = 0.53216455\n",
      "Iteration 442, loss = 0.53178555\n",
      "Iteration 443, loss = 0.53093772\n",
      "Iteration 444, loss = 0.53151720\n",
      "Iteration 445, loss = 0.53101752\n",
      "Iteration 446, loss = 0.53069343\n",
      "Iteration 447, loss = 0.53063680\n",
      "Iteration 448, loss = 0.53098691\n",
      "Iteration 449, loss = 0.53136493\n",
      "Iteration 450, loss = 0.53110933\n",
      "Iteration 451, loss = 0.52994166\n",
      "Iteration 452, loss = 0.52940122\n",
      "Iteration 453, loss = 0.53175563\n",
      "Iteration 454, loss = 0.53242089\n",
      "Iteration 455, loss = 0.53143126\n",
      "Iteration 456, loss = 0.52976677\n",
      "Iteration 457, loss = 0.52999313\n",
      "Iteration 458, loss = 0.52898026\n",
      "Iteration 459, loss = 0.52883522\n",
      "Iteration 460, loss = 0.52918471\n",
      "Iteration 461, loss = 0.52957668\n",
      "Iteration 462, loss = 0.52987785\n",
      "Iteration 463, loss = 0.52992736\n",
      "Iteration 464, loss = 0.52951935\n",
      "Iteration 465, loss = 0.52891275\n",
      "Iteration 466, loss = 0.52791412\n",
      "Iteration 467, loss = 0.52877232\n",
      "Iteration 468, loss = 0.52928315\n",
      "Iteration 469, loss = 0.52935981\n",
      "Iteration 470, loss = 0.52854135\n",
      "Iteration 471, loss = 0.52810437\n",
      "Iteration 472, loss = 0.52710624\n",
      "Iteration 473, loss = 0.52718637\n",
      "Iteration 474, loss = 0.52673589\n",
      "Iteration 475, loss = 0.52653704\n",
      "Iteration 476, loss = 0.52651329\n",
      "Iteration 477, loss = 0.52652883\n",
      "Iteration 478, loss = 0.52675356\n",
      "Iteration 479, loss = 0.52691212\n",
      "Iteration 480, loss = 0.52675163\n",
      "Iteration 481, loss = 0.52637306\n",
      "Iteration 482, loss = 0.52547786\n",
      "Iteration 483, loss = 0.52596951\n",
      "Iteration 484, loss = 0.52551094\n",
      "Iteration 485, loss = 0.52542782\n",
      "Iteration 486, loss = 0.52456802\n",
      "Iteration 487, loss = 0.52458532\n",
      "Iteration 488, loss = 0.52610789\n",
      "Iteration 489, loss = 0.52844912\n",
      "Iteration 490, loss = 0.52878860\n",
      "Iteration 491, loss = 0.52667374\n",
      "Iteration 492, loss = 0.52475694\n",
      "Iteration 493, loss = 0.52449046\n",
      "Iteration 494, loss = 0.52426683\n",
      "Iteration 495, loss = 0.52432365\n",
      "Iteration 496, loss = 0.52433095\n",
      "Iteration 497, loss = 0.52421072\n",
      "Iteration 498, loss = 0.52420161\n",
      "Iteration 499, loss = 0.52442469\n",
      "Iteration 500, loss = 0.52376557\n",
      "Iteration 501, loss = 0.52410466\n",
      "Iteration 502, loss = 0.52485953\n",
      "Iteration 503, loss = 0.52451656\n",
      "Iteration 504, loss = 0.52347648\n",
      "Iteration 505, loss = 0.52255651\n",
      "Iteration 506, loss = 0.52239714\n",
      "Iteration 507, loss = 0.52426473\n",
      "Iteration 508, loss = 0.52530841\n",
      "Iteration 509, loss = 0.52481105\n",
      "Iteration 510, loss = 0.52329180\n",
      "Iteration 511, loss = 0.52287241\n",
      "Iteration 512, loss = 0.52227664\n",
      "Iteration 513, loss = 0.52204864\n",
      "Iteration 514, loss = 0.52246302\n",
      "Iteration 515, loss = 0.52222370\n",
      "Iteration 516, loss = 0.52180025\n",
      "Iteration 517, loss = 0.52173993\n",
      "Iteration 518, loss = 0.52150508\n",
      "Iteration 519, loss = 0.52144527\n",
      "Iteration 520, loss = 0.52169919\n",
      "Iteration 521, loss = 0.52170800\n",
      "Iteration 522, loss = 0.52129500\n",
      "Iteration 523, loss = 0.52092014\n",
      "Iteration 524, loss = 0.52141013\n",
      "Iteration 525, loss = 0.52109226\n",
      "Iteration 526, loss = 0.52084206\n",
      "Iteration 527, loss = 0.52081356\n",
      "Iteration 528, loss = 0.52068524\n",
      "Iteration 529, loss = 0.52057311\n",
      "Iteration 530, loss = 0.52054507\n",
      "Iteration 531, loss = 0.52063262\n",
      "Iteration 532, loss = 0.52089712\n",
      "Iteration 533, loss = 0.52084351\n",
      "Iteration 534, loss = 0.52105525\n",
      "Iteration 535, loss = 0.52012356\n",
      "Iteration 536, loss = 0.52002802\n",
      "Iteration 537, loss = 0.51996887\n",
      "Iteration 538, loss = 0.52013517\n",
      "Iteration 539, loss = 0.51975185\n",
      "Iteration 540, loss = 0.51962404\n",
      "Iteration 541, loss = 0.52045801\n",
      "Iteration 542, loss = 0.52033953\n",
      "Iteration 543, loss = 0.52040670\n",
      "Iteration 544, loss = 0.52046365\n",
      "Iteration 545, loss = 0.52048322\n",
      "Iteration 546, loss = 0.52037108\n",
      "Iteration 547, loss = 0.52013595\n",
      "Iteration 548, loss = 0.51939315\n",
      "Iteration 549, loss = 0.51900129\n",
      "Iteration 550, loss = 0.52103856\n",
      "Iteration 551, loss = 0.52266122\n",
      "Iteration 552, loss = 0.52315395\n",
      "Iteration 553, loss = 0.52232798\n",
      "Iteration 554, loss = 0.51985569\n",
      "Iteration 555, loss = 0.51971403\n",
      "Iteration 556, loss = 0.52064559\n",
      "Iteration 557, loss = 0.52099191\n",
      "Iteration 558, loss = 0.52034629\n",
      "Iteration 559, loss = 0.51921802\n",
      "Iteration 560, loss = 0.52038561\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 26.17532943\n",
      "Iteration 2, loss = 26.13311805\n",
      "Iteration 3, loss = 26.09961442\n",
      "Iteration 4, loss = 26.04118577\n",
      "Iteration 5, loss = 25.96841581\n",
      "Iteration 6, loss = 25.85488483\n",
      "Iteration 7, loss = 25.67194481\n",
      "Iteration 8, loss = 25.47333483\n",
      "Iteration 9, loss = 25.19212308\n",
      "Iteration 10, loss = 24.86476412\n",
      "Iteration 11, loss = 24.48652521\n",
      "Iteration 12, loss = 24.06235812\n",
      "Iteration 13, loss = 23.60431140\n",
      "Iteration 14, loss = 23.10040865\n",
      "Iteration 15, loss = 22.57965992\n",
      "Iteration 16, loss = 22.04625351\n",
      "Iteration 17, loss = 21.52045653\n",
      "Iteration 18, loss = 20.99088453\n",
      "Iteration 19, loss = 20.47190335\n",
      "Iteration 20, loss = 19.94844681\n",
      "Iteration 21, loss = 19.42078240\n",
      "Iteration 22, loss = 18.90165382\n",
      "Iteration 23, loss = 18.38470528\n",
      "Iteration 24, loss = 17.86582507\n",
      "Iteration 25, loss = 17.34220165\n",
      "Iteration 26, loss = 16.83330335\n",
      "Iteration 27, loss = 16.31100765\n",
      "Iteration 28, loss = 15.78917411\n",
      "Iteration 29, loss = 15.27628266\n",
      "Iteration 30, loss = 14.75204903\n",
      "Iteration 31, loss = 14.24098420\n",
      "Iteration 32, loss = 13.71366806\n",
      "Iteration 33, loss = 13.20499711\n",
      "Iteration 34, loss = 12.68509320\n",
      "Iteration 35, loss = 12.17426310\n",
      "Iteration 36, loss = 11.65882648\n",
      "Iteration 37, loss = 11.13872522\n",
      "Iteration 38, loss = 10.63673495\n",
      "Iteration 39, loss = 10.12062583\n",
      "Iteration 40, loss = 9.60671893\n",
      "Iteration 41, loss = 9.09525567\n",
      "Iteration 42, loss = 8.59192599\n",
      "Iteration 43, loss = 8.08507886\n",
      "Iteration 44, loss = 7.57219221\n",
      "Iteration 45, loss = 7.07838676\n",
      "Iteration 46, loss = 6.57570400\n",
      "Iteration 47, loss = 6.07869487\n",
      "Iteration 48, loss = 5.58217455\n",
      "Iteration 49, loss = 5.10105020\n",
      "Iteration 50, loss = 4.62864478\n",
      "Iteration 51, loss = 4.16024449\n",
      "Iteration 52, loss = 3.72960253\n",
      "Iteration 53, loss = 3.29954410\n",
      "Iteration 54, loss = 2.91230635\n",
      "Iteration 55, loss = 2.56476532\n",
      "Iteration 56, loss = 2.25244885\n",
      "Iteration 57, loss = 1.99616106\n",
      "Iteration 58, loss = 1.79281783\n",
      "Iteration 59, loss = 1.64424755\n",
      "Iteration 60, loss = 1.53555269\n",
      "Iteration 61, loss = 1.47451344\n",
      "Iteration 62, loss = 1.44112358\n",
      "Iteration 63, loss = 1.42626431\n",
      "Iteration 64, loss = 1.41846945\n",
      "Iteration 65, loss = 1.42511215\n",
      "Iteration 66, loss = 1.43016494\n",
      "Iteration 67, loss = 1.43476107\n",
      "Iteration 68, loss = 1.44034720\n",
      "Iteration 69, loss = 1.44371193\n",
      "Iteration 70, loss = 1.44307456\n",
      "Iteration 71, loss = 1.43877847\n",
      "Iteration 72, loss = 1.43261928\n",
      "Iteration 73, loss = 1.42375302\n",
      "Iteration 74, loss = 1.41476774\n",
      "Iteration 75, loss = 1.40589511\n",
      "Iteration 76, loss = 1.39518729\n",
      "Iteration 77, loss = 1.38475878\n",
      "Iteration 78, loss = 1.37630177\n",
      "Iteration 79, loss = 1.36700779\n",
      "Iteration 80, loss = 1.35983219\n",
      "Iteration 81, loss = 1.35217982\n",
      "Iteration 82, loss = 1.34694372\n",
      "Iteration 83, loss = 1.34149803\n",
      "Iteration 84, loss = 1.33690462\n",
      "Iteration 85, loss = 1.33235278\n",
      "Iteration 86, loss = 1.32743525\n",
      "Iteration 87, loss = 1.32393525\n",
      "Iteration 88, loss = 1.31953148\n",
      "Iteration 89, loss = 1.31575253\n",
      "Iteration 90, loss = 1.31152953\n",
      "Iteration 91, loss = 1.30749406\n",
      "Iteration 92, loss = 1.30348892\n",
      "Iteration 93, loss = 1.29989406\n",
      "Iteration 94, loss = 1.29545287\n",
      "Iteration 95, loss = 1.29150597\n",
      "Iteration 96, loss = 1.28699421\n",
      "Iteration 97, loss = 1.28310986\n",
      "Iteration 98, loss = 1.27856750\n",
      "Iteration 99, loss = 1.27408372\n",
      "Iteration 100, loss = 1.26949942\n",
      "Iteration 101, loss = 1.26529985\n",
      "Iteration 102, loss = 1.26096983\n",
      "Iteration 103, loss = 1.25664476\n",
      "Iteration 104, loss = 1.25223449\n",
      "Iteration 105, loss = 1.24803648\n",
      "Iteration 106, loss = 1.24388971\n",
      "Iteration 107, loss = 1.23936307\n",
      "Iteration 108, loss = 1.23525626\n",
      "Iteration 109, loss = 1.23149468\n",
      "Iteration 110, loss = 1.22720487\n",
      "Iteration 111, loss = 1.22312573\n",
      "Iteration 112, loss = 1.21886505\n",
      "Iteration 113, loss = 1.21457730\n",
      "Iteration 114, loss = 1.21047730\n",
      "Iteration 115, loss = 1.20599035\n",
      "Iteration 116, loss = 1.20205039\n",
      "Iteration 117, loss = 1.19735433\n",
      "Iteration 118, loss = 1.19351785\n",
      "Iteration 119, loss = 1.18909015\n",
      "Iteration 120, loss = 1.18488265\n",
      "Iteration 121, loss = 1.18098580\n",
      "Iteration 122, loss = 1.17690226\n",
      "Iteration 123, loss = 1.17255912\n",
      "Iteration 124, loss = 1.16839147\n",
      "Iteration 125, loss = 1.16432752\n",
      "Iteration 126, loss = 1.16010390\n",
      "Iteration 127, loss = 1.15617225\n",
      "Iteration 128, loss = 1.15200231\n",
      "Iteration 129, loss = 1.14799413\n",
      "Iteration 130, loss = 1.14397170\n",
      "Iteration 131, loss = 1.14026102\n",
      "Iteration 132, loss = 1.13649465\n",
      "Iteration 133, loss = 1.13239482\n",
      "Iteration 134, loss = 1.12861368\n",
      "Iteration 135, loss = 1.12490741\n",
      "Iteration 136, loss = 1.12120958\n",
      "Iteration 137, loss = 1.11733128\n",
      "Iteration 138, loss = 1.11427848\n",
      "Iteration 139, loss = 1.11030745\n",
      "Iteration 140, loss = 1.10674831\n",
      "Iteration 141, loss = 1.10282014\n",
      "Iteration 142, loss = 1.09916128\n",
      "Iteration 143, loss = 1.09565666\n",
      "Iteration 144, loss = 1.09147135\n",
      "Iteration 145, loss = 1.08750912\n",
      "Iteration 146, loss = 1.08366906\n",
      "Iteration 147, loss = 1.08006527\n",
      "Iteration 148, loss = 1.07629409\n",
      "Iteration 149, loss = 1.07263349\n",
      "Iteration 150, loss = 1.06876393\n",
      "Iteration 151, loss = 1.06521307\n",
      "Iteration 152, loss = 1.06108548\n",
      "Iteration 153, loss = 1.05766731\n",
      "Iteration 154, loss = 1.05369566\n",
      "Iteration 155, loss = 1.05007841\n",
      "Iteration 156, loss = 1.04608198\n",
      "Iteration 157, loss = 1.04246687\n",
      "Iteration 158, loss = 1.03860607\n",
      "Iteration 159, loss = 1.03517312\n",
      "Iteration 160, loss = 1.03148131\n",
      "Iteration 161, loss = 1.02816255\n",
      "Iteration 162, loss = 1.02452513\n",
      "Iteration 163, loss = 1.02131781\n",
      "Iteration 164, loss = 1.01775448\n",
      "Iteration 165, loss = 1.01403443\n",
      "Iteration 166, loss = 1.01021090\n",
      "Iteration 167, loss = 1.00685316\n",
      "Iteration 168, loss = 1.00256550\n",
      "Iteration 169, loss = 0.99899121\n",
      "Iteration 170, loss = 0.99494484\n",
      "Iteration 171, loss = 0.99162964\n",
      "Iteration 172, loss = 0.98779377\n",
      "Iteration 173, loss = 0.98429021\n",
      "Iteration 174, loss = 0.98081518\n",
      "Iteration 175, loss = 0.97774160\n",
      "Iteration 176, loss = 0.97424403\n",
      "Iteration 177, loss = 0.97114779\n",
      "Iteration 178, loss = 0.96765424\n",
      "Iteration 179, loss = 0.96470692\n",
      "Iteration 180, loss = 0.96109858\n",
      "Iteration 181, loss = 0.95782239\n",
      "Iteration 182, loss = 0.95433984\n",
      "Iteration 183, loss = 0.95110732\n",
      "Iteration 184, loss = 0.94789740\n",
      "Iteration 185, loss = 0.94475499\n",
      "Iteration 186, loss = 0.94104152\n",
      "Iteration 187, loss = 0.93750468\n",
      "Iteration 188, loss = 0.93346058\n",
      "Iteration 189, loss = 0.93027361\n",
      "Iteration 190, loss = 0.92717893\n",
      "Iteration 191, loss = 0.92347434\n",
      "Iteration 192, loss = 0.92019594\n",
      "Iteration 193, loss = 0.91677108\n",
      "Iteration 194, loss = 0.91348431\n",
      "Iteration 195, loss = 0.91066549\n",
      "Iteration 196, loss = 0.90715024\n",
      "Iteration 197, loss = 0.90429339\n",
      "Iteration 198, loss = 0.90076235\n",
      "Iteration 199, loss = 0.89773005\n",
      "Iteration 200, loss = 0.89448435\n",
      "Iteration 201, loss = 0.89174221\n",
      "Iteration 202, loss = 0.88870447\n",
      "Iteration 203, loss = 0.88568697\n",
      "Iteration 204, loss = 0.88321824\n",
      "Iteration 205, loss = 0.88058385\n",
      "Iteration 206, loss = 0.87798065\n",
      "Iteration 207, loss = 0.87535707\n",
      "Iteration 208, loss = 0.87231706\n",
      "Iteration 209, loss = 0.86987095\n",
      "Iteration 210, loss = 0.86670561\n",
      "Iteration 211, loss = 0.86445308\n",
      "Iteration 212, loss = 0.86183425\n",
      "Iteration 213, loss = 0.85908052\n",
      "Iteration 214, loss = 0.85573383\n",
      "Iteration 215, loss = 0.85260806\n",
      "Iteration 216, loss = 0.84921470\n",
      "Iteration 217, loss = 0.84606549\n",
      "Iteration 218, loss = 0.84277770\n",
      "Iteration 219, loss = 0.83995005\n",
      "Iteration 220, loss = 0.83702296\n",
      "Iteration 221, loss = 0.83477129\n",
      "Iteration 222, loss = 0.83333926\n",
      "Iteration 223, loss = 0.83020477\n",
      "Iteration 224, loss = 0.82769372\n",
      "Iteration 225, loss = 0.82507078\n",
      "Iteration 226, loss = 0.82237781\n",
      "Iteration 227, loss = 0.81947541\n",
      "Iteration 228, loss = 0.81664580\n",
      "Iteration 229, loss = 0.81402246\n",
      "Iteration 230, loss = 0.81179284\n",
      "Iteration 231, loss = 0.80939818\n",
      "Iteration 232, loss = 0.80702636\n",
      "Iteration 233, loss = 0.80466113\n",
      "Iteration 234, loss = 0.80193438\n",
      "Iteration 235, loss = 0.79968072\n",
      "Iteration 236, loss = 0.79688984\n",
      "Iteration 237, loss = 0.79468615\n",
      "Iteration 238, loss = 0.79248396\n",
      "Iteration 239, loss = 0.79003855\n",
      "Iteration 240, loss = 0.78791451\n",
      "Iteration 241, loss = 0.78580560\n",
      "Iteration 242, loss = 0.78342938\n",
      "Iteration 243, loss = 0.78105136\n",
      "Iteration 244, loss = 0.77902055\n",
      "Iteration 245, loss = 0.77688368\n",
      "Iteration 246, loss = 0.77485568\n",
      "Iteration 247, loss = 0.77289260\n",
      "Iteration 248, loss = 0.77093460\n",
      "Iteration 249, loss = 0.76908582\n",
      "Iteration 250, loss = 0.76690428\n",
      "Iteration 251, loss = 0.76548971\n",
      "Iteration 252, loss = 0.76328395\n",
      "Iteration 253, loss = 0.76148399\n",
      "Iteration 254, loss = 0.75972238\n",
      "Iteration 255, loss = 0.75794882\n",
      "Iteration 256, loss = 0.75653749\n",
      "Iteration 257, loss = 0.75482607\n",
      "Iteration 258, loss = 0.75336394\n",
      "Iteration 259, loss = 0.75279356\n",
      "Iteration 260, loss = 0.75154839\n",
      "Iteration 261, loss = 0.75062605\n",
      "Iteration 262, loss = 0.74946053\n",
      "Iteration 263, loss = 0.74811116\n",
      "Iteration 264, loss = 0.74675169\n",
      "Iteration 265, loss = 0.74524648\n",
      "Iteration 266, loss = 0.74369579\n",
      "Iteration 267, loss = 0.74289297\n",
      "Iteration 268, loss = 0.74103505\n",
      "Iteration 269, loss = 0.73852335\n",
      "Iteration 270, loss = 0.73628022\n",
      "Iteration 271, loss = 0.73371780\n",
      "Iteration 272, loss = 0.73211344\n",
      "Iteration 273, loss = 0.73020953\n",
      "Iteration 274, loss = 0.72879934\n",
      "Iteration 275, loss = 0.72756399\n",
      "Iteration 276, loss = 0.72624586\n",
      "Iteration 277, loss = 0.72484708\n",
      "Iteration 278, loss = 0.72344063\n",
      "Iteration 279, loss = 0.72210975\n",
      "Iteration 280, loss = 0.72069980\n",
      "Iteration 281, loss = 0.71930165\n",
      "Iteration 282, loss = 0.71790049\n",
      "Iteration 283, loss = 0.71651388\n",
      "Iteration 284, loss = 0.71501230\n",
      "Iteration 285, loss = 0.71358043\n",
      "Iteration 286, loss = 0.71230364\n",
      "Iteration 287, loss = 0.71102654\n",
      "Iteration 288, loss = 0.70995215\n",
      "Iteration 289, loss = 0.70861361\n",
      "Iteration 290, loss = 0.70730338\n",
      "Iteration 291, loss = 0.70625839\n",
      "Iteration 292, loss = 0.70499842\n",
      "Iteration 293, loss = 0.70402007\n",
      "Iteration 294, loss = 0.70276518\n",
      "Iteration 295, loss = 0.70156545\n",
      "Iteration 296, loss = 0.70033116\n",
      "Iteration 297, loss = 0.69909256\n",
      "Iteration 298, loss = 0.69795782\n",
      "Iteration 299, loss = 0.69692594\n",
      "Iteration 300, loss = 0.69579023\n",
      "Iteration 301, loss = 0.69467875\n",
      "Iteration 302, loss = 0.69369499\n",
      "Iteration 303, loss = 0.69251479\n",
      "Iteration 304, loss = 0.69120985\n",
      "Iteration 305, loss = 0.69032525\n",
      "Iteration 306, loss = 0.68943655\n",
      "Iteration 307, loss = 0.68872600\n",
      "Iteration 308, loss = 0.68783647\n",
      "Iteration 309, loss = 0.68678026\n",
      "Iteration 310, loss = 0.68559874\n",
      "Iteration 311, loss = 0.68459268\n",
      "Iteration 312, loss = 0.68343941\n",
      "Iteration 313, loss = 0.68237085\n",
      "Iteration 314, loss = 0.68116080\n",
      "Iteration 315, loss = 0.67997060\n",
      "Iteration 316, loss = 0.67872430\n",
      "Iteration 317, loss = 0.67815878\n",
      "Iteration 318, loss = 0.67674813\n",
      "Iteration 319, loss = 0.67582938\n",
      "Iteration 320, loss = 0.67496144\n",
      "Iteration 321, loss = 0.67414921\n",
      "Iteration 322, loss = 0.67314869\n",
      "Iteration 323, loss = 0.67189330\n",
      "Iteration 324, loss = 0.67041706\n",
      "Iteration 325, loss = 0.66975150\n",
      "Iteration 326, loss = 0.66865712\n",
      "Iteration 327, loss = 0.66793947\n",
      "Iteration 328, loss = 0.66698408\n",
      "Iteration 329, loss = 0.66579095\n",
      "Iteration 330, loss = 0.66468726\n",
      "Iteration 331, loss = 0.66373380\n",
      "Iteration 332, loss = 0.66321645\n",
      "Iteration 333, loss = 0.66284761\n",
      "Iteration 334, loss = 0.66210879\n",
      "Iteration 335, loss = 0.66131229\n",
      "Iteration 336, loss = 0.66032674\n",
      "Iteration 337, loss = 0.65923097\n",
      "Iteration 338, loss = 0.65836952\n",
      "Iteration 339, loss = 0.65741340\n",
      "Iteration 340, loss = 0.65657872\n",
      "Iteration 341, loss = 0.65579308\n",
      "Iteration 342, loss = 0.65489574\n",
      "Iteration 343, loss = 0.65448714\n",
      "Iteration 344, loss = 0.65353883\n",
      "Iteration 345, loss = 0.65278520\n",
      "Iteration 346, loss = 0.65204778\n",
      "Iteration 347, loss = 0.65109041\n",
      "Iteration 348, loss = 0.65067613\n",
      "Iteration 349, loss = 0.65022883\n",
      "Iteration 350, loss = 0.64955257\n",
      "Iteration 351, loss = 0.64840634\n",
      "Iteration 352, loss = 0.64713100\n",
      "Iteration 353, loss = 0.64726268\n",
      "Iteration 354, loss = 0.64644016\n",
      "Iteration 355, loss = 0.64572500\n",
      "Iteration 356, loss = 0.64506323\n",
      "Iteration 357, loss = 0.64439176\n",
      "Iteration 358, loss = 0.64342245\n",
      "Iteration 359, loss = 0.64249307\n",
      "Iteration 360, loss = 0.64174944\n",
      "Iteration 361, loss = 0.64114922\n",
      "Iteration 362, loss = 0.64084963\n",
      "Iteration 363, loss = 0.64040080\n",
      "Iteration 364, loss = 0.64003640\n",
      "Iteration 365, loss = 0.63968986\n",
      "Iteration 366, loss = 0.63917487\n",
      "Iteration 367, loss = 0.63830632\n",
      "Iteration 368, loss = 0.63720366\n",
      "Iteration 369, loss = 0.63617609\n",
      "Iteration 370, loss = 0.63511560\n",
      "Iteration 371, loss = 0.63454803\n",
      "Iteration 372, loss = 0.63397622\n",
      "Iteration 373, loss = 0.63371038\n",
      "Iteration 374, loss = 0.63288598\n",
      "Iteration 375, loss = 0.63278498\n",
      "Iteration 376, loss = 0.63143254\n",
      "Iteration 377, loss = 0.63082496\n",
      "Iteration 378, loss = 0.63020307\n",
      "Iteration 379, loss = 0.62951140\n",
      "Iteration 380, loss = 0.62886950\n",
      "Iteration 381, loss = 0.62820248\n",
      "Iteration 382, loss = 0.62787237\n",
      "Iteration 383, loss = 0.62740611\n",
      "Iteration 384, loss = 0.62744216\n",
      "Iteration 385, loss = 0.62674801\n",
      "Iteration 386, loss = 0.62576575\n",
      "Iteration 387, loss = 0.62463650\n",
      "Iteration 388, loss = 0.62377994\n",
      "Iteration 389, loss = 0.62294463\n",
      "Iteration 390, loss = 0.62270052\n",
      "Iteration 391, loss = 0.62251529\n",
      "Iteration 392, loss = 0.62277532\n",
      "Iteration 393, loss = 0.62324281\n",
      "Iteration 394, loss = 0.62396165\n",
      "Iteration 395, loss = 0.62394111\n",
      "Iteration 396, loss = 0.62301745\n",
      "Iteration 397, loss = 0.62146280\n",
      "Iteration 398, loss = 0.61974142\n",
      "Iteration 399, loss = 0.61763390\n",
      "Iteration 400, loss = 0.61649570\n",
      "Iteration 401, loss = 0.61525527\n",
      "Iteration 402, loss = 0.61608211\n",
      "Iteration 403, loss = 0.61844440\n",
      "Iteration 404, loss = 0.61960601\n",
      "Iteration 405, loss = 0.62011993\n",
      "Iteration 406, loss = 0.62007411\n",
      "Iteration 407, loss = 0.61930210\n",
      "Iteration 408, loss = 0.61756656\n",
      "Iteration 409, loss = 0.61557199\n",
      "Iteration 410, loss = 0.61332815\n",
      "Iteration 411, loss = 0.61140735\n",
      "Iteration 412, loss = 0.61023079\n",
      "Iteration 413, loss = 0.60998696\n",
      "Iteration 414, loss = 0.60960017\n",
      "Iteration 415, loss = 0.60960650\n",
      "Iteration 416, loss = 0.60997772\n",
      "Iteration 417, loss = 0.61020111\n",
      "Iteration 418, loss = 0.60987021\n",
      "Iteration 419, loss = 0.60872085\n",
      "Iteration 420, loss = 0.60699906\n",
      "Iteration 421, loss = 0.60524763\n",
      "Iteration 422, loss = 0.60509318\n",
      "Iteration 423, loss = 0.60593108\n",
      "Iteration 424, loss = 0.60573300\n",
      "Iteration 425, loss = 0.60524834\n",
      "Iteration 426, loss = 0.60449111\n",
      "Iteration 427, loss = 0.60357715\n",
      "Iteration 428, loss = 0.60273324\n",
      "Iteration 429, loss = 0.60183874\n",
      "Iteration 430, loss = 0.60148692\n",
      "Iteration 431, loss = 0.60073108\n",
      "Iteration 432, loss = 0.60063335\n",
      "Iteration 433, loss = 0.60049807\n",
      "Iteration 434, loss = 0.60035897\n",
      "Iteration 435, loss = 0.59988074\n",
      "Iteration 436, loss = 0.59913597\n",
      "Iteration 437, loss = 0.59801763\n",
      "Iteration 438, loss = 0.59797223\n",
      "Iteration 439, loss = 0.59853555\n",
      "Iteration 440, loss = 0.59843591\n",
      "Iteration 441, loss = 0.59853156\n",
      "Iteration 442, loss = 0.59884770\n",
      "Iteration 443, loss = 0.59817666\n",
      "Iteration 444, loss = 0.59721626\n",
      "Iteration 445, loss = 0.59613410\n",
      "Iteration 446, loss = 0.59543094\n",
      "Iteration 447, loss = 0.59490444\n",
      "Iteration 448, loss = 0.59405434\n",
      "Iteration 449, loss = 0.59354722\n",
      "Iteration 450, loss = 0.59315143\n",
      "Iteration 451, loss = 0.59275568\n",
      "Iteration 452, loss = 0.59228785\n",
      "Iteration 453, loss = 0.59211974\n",
      "Iteration 454, loss = 0.59170082\n",
      "Iteration 455, loss = 0.59149793\n",
      "Iteration 456, loss = 0.59117348\n",
      "Iteration 457, loss = 0.59062984\n",
      "Iteration 458, loss = 0.59023603\n",
      "Iteration 459, loss = 0.58964241\n",
      "Iteration 460, loss = 0.58965729\n",
      "Iteration 461, loss = 0.58942219\n",
      "Iteration 462, loss = 0.58917826\n",
      "Iteration 463, loss = 0.58878870\n",
      "Iteration 464, loss = 0.58863419\n",
      "Iteration 465, loss = 0.58844030\n",
      "Iteration 466, loss = 0.58817107\n",
      "Iteration 467, loss = 0.58735906\n",
      "Iteration 468, loss = 0.58644632\n",
      "Iteration 469, loss = 0.58621508\n",
      "Iteration 470, loss = 0.58654089\n",
      "Iteration 471, loss = 0.58635545\n",
      "Iteration 472, loss = 0.58600244\n",
      "Iteration 473, loss = 0.58523113\n",
      "Iteration 474, loss = 0.58481568\n",
      "Iteration 475, loss = 0.58401292\n",
      "Iteration 476, loss = 0.58385771\n",
      "Iteration 477, loss = 0.58369030\n",
      "Iteration 478, loss = 0.58341844\n",
      "Iteration 479, loss = 0.58309758\n",
      "Iteration 480, loss = 0.58259408\n",
      "Iteration 481, loss = 0.58241645\n",
      "Iteration 482, loss = 0.58182169\n",
      "Iteration 483, loss = 0.58154837\n",
      "Iteration 484, loss = 0.58120088\n",
      "Iteration 485, loss = 0.58095699\n",
      "Iteration 486, loss = 0.58069456\n",
      "Iteration 487, loss = 0.58037115\n",
      "Iteration 488, loss = 0.58007104\n",
      "Iteration 489, loss = 0.58025822\n",
      "Iteration 490, loss = 0.57962189\n",
      "Iteration 491, loss = 0.57914881\n",
      "Iteration 492, loss = 0.57937042\n",
      "Iteration 493, loss = 0.57890696\n",
      "Iteration 494, loss = 0.57856948\n",
      "Iteration 495, loss = 0.57803250\n",
      "Iteration 496, loss = 0.57775707\n",
      "Iteration 497, loss = 0.57806366\n",
      "Iteration 498, loss = 0.57794320\n",
      "Iteration 499, loss = 0.57808408\n",
      "Iteration 500, loss = 0.57764064\n",
      "Iteration 501, loss = 0.57718141\n",
      "Iteration 502, loss = 0.57646340\n",
      "Iteration 503, loss = 0.57615465\n",
      "Iteration 504, loss = 0.57587847\n",
      "Iteration 505, loss = 0.57583699\n",
      "Iteration 506, loss = 0.57587555\n",
      "Iteration 507, loss = 0.57634493\n",
      "Iteration 508, loss = 0.57733039\n",
      "Iteration 509, loss = 0.57715477\n",
      "Iteration 510, loss = 0.57632413\n",
      "Iteration 511, loss = 0.57531243\n",
      "Iteration 512, loss = 0.57437450\n",
      "Iteration 513, loss = 0.57434401\n",
      "Iteration 514, loss = 0.57352217\n",
      "Iteration 515, loss = 0.57330252\n",
      "Iteration 516, loss = 0.57305151\n",
      "Iteration 517, loss = 0.57281329\n",
      "Iteration 518, loss = 0.57255379\n",
      "Iteration 519, loss = 0.57238679\n",
      "Iteration 520, loss = 0.57224575\n",
      "Iteration 521, loss = 0.57212089\n",
      "Iteration 522, loss = 0.57200796\n",
      "Iteration 523, loss = 0.57183330\n",
      "Iteration 524, loss = 0.57156697\n",
      "Iteration 525, loss = 0.57157267\n",
      "Iteration 526, loss = 0.57070125\n",
      "Iteration 527, loss = 0.57042724\n",
      "Iteration 528, loss = 0.57009496\n",
      "Iteration 529, loss = 0.57016006\n",
      "Iteration 530, loss = 0.56980555\n",
      "Iteration 531, loss = 0.56956122\n",
      "Iteration 532, loss = 0.56926214\n",
      "Iteration 533, loss = 0.56874148\n",
      "Iteration 534, loss = 0.56831366\n",
      "Iteration 535, loss = 0.56924076\n",
      "Iteration 536, loss = 0.57000317\n",
      "Iteration 537, loss = 0.57037335\n",
      "Iteration 538, loss = 0.56914342\n",
      "Iteration 539, loss = 0.56775897\n",
      "Iteration 540, loss = 0.56736006\n",
      "Iteration 541, loss = 0.56910013\n",
      "Iteration 542, loss = 0.56796840\n",
      "Iteration 543, loss = 0.56713793\n",
      "Iteration 544, loss = 0.56651782\n",
      "Iteration 545, loss = 0.56643193\n",
      "Iteration 546, loss = 0.56655645\n",
      "Iteration 547, loss = 0.56636672\n",
      "Iteration 548, loss = 0.56604746\n",
      "Iteration 549, loss = 0.56581158\n",
      "Iteration 550, loss = 0.56544485\n",
      "Iteration 551, loss = 0.56515463\n",
      "Iteration 552, loss = 0.56482091\n",
      "Iteration 553, loss = 0.56474984\n",
      "Iteration 554, loss = 0.56508711\n",
      "Iteration 555, loss = 0.56534101\n",
      "Iteration 556, loss = 0.56451624\n",
      "Iteration 557, loss = 0.56432414\n",
      "Iteration 558, loss = 0.56419249\n",
      "Iteration 559, loss = 0.56391210\n",
      "Iteration 560, loss = 0.56371598\n",
      "Iteration 561, loss = 0.56341544\n",
      "Iteration 562, loss = 0.56316222\n",
      "Iteration 563, loss = 0.56299758\n",
      "Iteration 564, loss = 0.56290984\n",
      "Iteration 565, loss = 0.56293888\n",
      "Iteration 566, loss = 0.56269582\n",
      "Iteration 567, loss = 0.56258822\n",
      "Iteration 568, loss = 0.56216657\n",
      "Iteration 569, loss = 0.56219466\n",
      "Iteration 570, loss = 0.56213868\n",
      "Iteration 571, loss = 0.56205453\n",
      "Iteration 572, loss = 0.56248580\n",
      "Iteration 573, loss = 0.56254538\n",
      "Iteration 574, loss = 0.56223073\n",
      "Iteration 575, loss = 0.56171577\n",
      "Iteration 576, loss = 0.56126480\n",
      "Iteration 577, loss = 0.56077142\n",
      "Iteration 578, loss = 0.56065217\n",
      "Iteration 579, loss = 0.56044093\n",
      "Iteration 580, loss = 0.56034016\n",
      "Iteration 581, loss = 0.56025064\n",
      "Iteration 582, loss = 0.56011195\n",
      "Iteration 583, loss = 0.55999509\n",
      "Iteration 584, loss = 0.55985505\n",
      "Iteration 585, loss = 0.55983706\n",
      "Iteration 586, loss = 0.55985073\n",
      "Iteration 587, loss = 0.56014096\n",
      "Iteration 588, loss = 0.56015436\n",
      "Iteration 589, loss = 0.55986174\n",
      "Iteration 590, loss = 0.55969610\n",
      "Iteration 591, loss = 0.55914652\n",
      "Iteration 592, loss = 0.55896718\n",
      "Iteration 593, loss = 0.55871169\n",
      "Iteration 594, loss = 0.55850831\n",
      "Iteration 595, loss = 0.55842831\n",
      "Iteration 596, loss = 0.55857980\n",
      "Iteration 597, loss = 0.55929372\n",
      "Iteration 598, loss = 0.55907359\n",
      "Iteration 599, loss = 0.55850169\n",
      "Iteration 600, loss = 0.55827277\n",
      "Iteration 601, loss = 0.55794286\n",
      "Iteration 602, loss = 0.55725399\n",
      "Iteration 603, loss = 0.55781611\n",
      "Iteration 604, loss = 0.55861280\n",
      "Iteration 605, loss = 0.55848435\n",
      "Iteration 606, loss = 0.55803106\n",
      "Iteration 607, loss = 0.55709792\n",
      "Iteration 608, loss = 0.55665352\n",
      "Iteration 609, loss = 0.55691538\n",
      "Iteration 610, loss = 0.55812115\n",
      "Iteration 611, loss = 0.55931722\n",
      "Iteration 612, loss = 0.55979057\n",
      "Iteration 613, loss = 0.55833094\n",
      "Iteration 614, loss = 0.55762319\n",
      "Iteration 615, loss = 0.55581525\n",
      "Iteration 616, loss = 0.55615100\n",
      "Iteration 617, loss = 0.55726289\n",
      "Iteration 618, loss = 0.55731529\n",
      "Iteration 619, loss = 0.55715099\n",
      "Iteration 620, loss = 0.55660584\n",
      "Iteration 621, loss = 0.55591131\n",
      "Iteration 622, loss = 0.55542070\n",
      "Iteration 623, loss = 0.55519643\n",
      "Iteration 624, loss = 0.55713674\n",
      "Iteration 625, loss = 0.55899266\n",
      "Iteration 626, loss = 0.55933370\n",
      "Iteration 627, loss = 0.55800139\n",
      "Iteration 628, loss = 0.55623835\n",
      "Iteration 629, loss = 0.55447122\n",
      "Iteration 630, loss = 0.55535851\n",
      "Iteration 631, loss = 0.55570806\n",
      "Iteration 632, loss = 0.55662493\n",
      "Iteration 633, loss = 0.55779070\n",
      "Iteration 634, loss = 0.55798156\n",
      "Iteration 635, loss = 0.55711829\n",
      "Iteration 636, loss = 0.55574677\n",
      "Iteration 637, loss = 0.55463952\n",
      "Iteration 638, loss = 0.55326191\n",
      "Iteration 639, loss = 0.55499921\n",
      "Iteration 640, loss = 0.55685394\n",
      "Iteration 641, loss = 0.55728272\n",
      "Iteration 642, loss = 0.55647074\n",
      "Iteration 643, loss = 0.55492779\n",
      "Iteration 644, loss = 0.55387236\n",
      "Iteration 645, loss = 0.55334365\n",
      "Iteration 646, loss = 0.55403302\n",
      "Iteration 647, loss = 0.55433396\n",
      "Iteration 648, loss = 0.55523005\n",
      "Iteration 649, loss = 0.55491155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.83697450\n",
      "Iteration 2, loss = 1.71211129\n",
      "Iteration 3, loss = 1.59759439\n",
      "Iteration 4, loss = 1.48921909\n",
      "Iteration 5, loss = 1.38750032\n",
      "Iteration 6, loss = 1.29352692\n",
      "Iteration 7, loss = 1.21467542\n",
      "Iteration 8, loss = 1.13984522\n",
      "Iteration 9, loss = 1.07678786\n",
      "Iteration 10, loss = 1.02404591\n",
      "Iteration 11, loss = 0.97361154\n",
      "Iteration 12, loss = 0.93454899\n",
      "Iteration 13, loss = 0.90116563\n",
      "Iteration 14, loss = 0.87393870\n",
      "Iteration 15, loss = 0.85037820\n",
      "Iteration 16, loss = 0.82964364\n",
      "Iteration 17, loss = 0.81103061\n",
      "Iteration 18, loss = 0.79737853\n",
      "Iteration 19, loss = 0.78595859\n",
      "Iteration 20, loss = 0.77502114\n",
      "Iteration 21, loss = 0.76636202\n",
      "Iteration 22, loss = 0.75841343\n",
      "Iteration 23, loss = 0.75241246\n",
      "Iteration 24, loss = 0.74598545\n",
      "Iteration 25, loss = 0.73906330\n",
      "Iteration 26, loss = 0.73260273\n",
      "Iteration 27, loss = 0.72670372\n",
      "Iteration 28, loss = 0.72073878\n",
      "Iteration 29, loss = 0.71465489\n",
      "Iteration 30, loss = 0.70955780\n",
      "Iteration 31, loss = 0.70376368\n",
      "Iteration 32, loss = 0.69794069\n",
      "Iteration 33, loss = 0.69258762\n",
      "Iteration 34, loss = 0.68659306\n",
      "Iteration 35, loss = 0.68086931\n",
      "Iteration 36, loss = 0.67498691\n",
      "Iteration 37, loss = 0.66884137\n",
      "Iteration 38, loss = 0.66327713\n",
      "Iteration 39, loss = 0.65827994\n",
      "Iteration 40, loss = 0.65302632\n",
      "Iteration 41, loss = 0.64769106\n",
      "Iteration 42, loss = 0.64259735\n",
      "Iteration 43, loss = 0.63771188\n",
      "Iteration 44, loss = 0.63299420\n",
      "Iteration 45, loss = 0.62845110\n",
      "Iteration 46, loss = 0.62383234\n",
      "Iteration 47, loss = 0.61942131\n",
      "Iteration 48, loss = 0.61532289\n",
      "Iteration 49, loss = 0.61107007\n",
      "Iteration 50, loss = 0.60744036\n",
      "Iteration 51, loss = 0.60444135\n",
      "Iteration 52, loss = 0.60115960\n",
      "Iteration 53, loss = 0.59840155\n",
      "Iteration 54, loss = 0.59593956\n",
      "Iteration 55, loss = 0.59341048\n",
      "Iteration 56, loss = 0.59172098\n",
      "Iteration 57, loss = 0.58971653\n",
      "Iteration 58, loss = 0.58806937\n",
      "Iteration 59, loss = 0.58673767\n",
      "Iteration 60, loss = 0.58546092\n",
      "Iteration 61, loss = 0.58441328\n",
      "Iteration 62, loss = 0.58363598\n",
      "Iteration 63, loss = 0.58290763\n",
      "Iteration 64, loss = 0.58242348\n",
      "Iteration 65, loss = 0.58179728\n",
      "Iteration 66, loss = 0.58127270\n",
      "Iteration 67, loss = 0.58081236\n",
      "Iteration 68, loss = 0.58043394\n",
      "Iteration 69, loss = 0.57994252\n",
      "Iteration 70, loss = 0.57954487\n",
      "Iteration 71, loss = 0.57910274\n",
      "Iteration 72, loss = 0.57864960\n",
      "Iteration 73, loss = 0.57828906\n",
      "Iteration 74, loss = 0.57786180\n",
      "Iteration 75, loss = 0.57758207\n",
      "Iteration 76, loss = 0.57725188\n",
      "Iteration 77, loss = 0.57709289\n",
      "Iteration 78, loss = 0.57685039\n",
      "Iteration 79, loss = 0.57666358\n",
      "Iteration 80, loss = 0.57652228\n",
      "Iteration 81, loss = 0.57645645\n",
      "Iteration 82, loss = 0.57633008\n",
      "Iteration 83, loss = 0.57624612\n",
      "Iteration 84, loss = 0.57616414\n",
      "Iteration 85, loss = 0.57610999\n",
      "Iteration 86, loss = 0.57604416\n",
      "Iteration 87, loss = 0.57588627\n",
      "Iteration 88, loss = 0.57577513\n",
      "Iteration 89, loss = 0.57563170\n",
      "Iteration 90, loss = 0.57556997\n",
      "Iteration 91, loss = 0.57540608\n",
      "Iteration 92, loss = 0.57528388\n",
      "Iteration 93, loss = 0.57522001\n",
      "Iteration 94, loss = 0.57512389\n",
      "Iteration 95, loss = 0.57504902\n",
      "Iteration 96, loss = 0.57496849\n",
      "Iteration 97, loss = 0.57490974\n",
      "Iteration 98, loss = 0.57478391\n",
      "Iteration 99, loss = 0.57466393\n",
      "Iteration 100, loss = 0.57455069\n",
      "Iteration 101, loss = 0.57449876\n",
      "Iteration 102, loss = 0.57433264\n",
      "Iteration 103, loss = 0.57449446\n",
      "Iteration 104, loss = 0.57446471\n",
      "Iteration 105, loss = 0.57427436\n",
      "Iteration 106, loss = 0.57422224\n",
      "Iteration 107, loss = 0.57399912\n",
      "Iteration 108, loss = 0.57395474\n",
      "Iteration 109, loss = 0.57378188\n",
      "Iteration 110, loss = 0.57371581\n",
      "Iteration 111, loss = 0.57358708\n",
      "Iteration 112, loss = 0.57360452\n",
      "Iteration 113, loss = 0.57346210\n",
      "Iteration 114, loss = 0.57340354\n",
      "Iteration 115, loss = 0.57332549\n",
      "Iteration 116, loss = 0.57324181\n",
      "Iteration 117, loss = 0.57316687\n",
      "Iteration 118, loss = 0.57309140\n",
      "Iteration 119, loss = 0.57306998\n",
      "Iteration 120, loss = 0.57299280\n",
      "Iteration 121, loss = 0.57291946\n",
      "Iteration 122, loss = 0.57283239\n",
      "Iteration 123, loss = 0.57273882\n",
      "Iteration 124, loss = 0.57263452\n",
      "Iteration 125, loss = 0.57276837\n",
      "Iteration 126, loss = 0.57261053\n",
      "Iteration 127, loss = 0.57257820\n",
      "Iteration 128, loss = 0.57250230\n",
      "Iteration 129, loss = 0.57247225\n",
      "Iteration 130, loss = 0.57237049\n",
      "Iteration 131, loss = 0.57226691\n",
      "Iteration 132, loss = 0.57213166\n",
      "Iteration 133, loss = 0.57206985\n",
      "Iteration 134, loss = 0.57195726\n",
      "Iteration 135, loss = 0.57185706\n",
      "Iteration 136, loss = 0.57181061\n",
      "Iteration 137, loss = 0.57176397\n",
      "Iteration 138, loss = 0.57171105\n",
      "Iteration 139, loss = 0.57170216\n",
      "Iteration 140, loss = 0.57165893\n",
      "Iteration 141, loss = 0.57173317\n",
      "Iteration 142, loss = 0.57161420\n",
      "Iteration 143, loss = 0.57154763\n",
      "Iteration 144, loss = 0.57146366\n",
      "Iteration 145, loss = 0.57143174\n",
      "Iteration 146, loss = 0.57143676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 26.73730264\n",
      "Iteration 2, loss = 26.73730263\n",
      "Iteration 3, loss = 26.73730262\n",
      "Iteration 4, loss = 26.73730261\n",
      "Iteration 5, loss = 26.73730259\n",
      "Iteration 6, loss = 26.73730258\n",
      "Iteration 7, loss = 26.73730257\n",
      "Iteration 8, loss = 26.73730256\n",
      "Iteration 9, loss = 26.73730255\n",
      "Iteration 10, loss = 26.73730254\n",
      "Iteration 11, loss = 26.73730252\n",
      "Iteration 12, loss = 26.73730251\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 24.95997504\n",
      "Iteration 2, loss = 24.53848774\n",
      "Iteration 3, loss = 24.07204517\n",
      "Iteration 4, loss = 23.56567902\n",
      "Iteration 5, loss = 23.01389290\n",
      "Iteration 6, loss = 22.44723975\n",
      "Iteration 7, loss = 21.88131652\n",
      "Iteration 8, loss = 21.32170680\n",
      "Iteration 9, loss = 20.76405382\n",
      "Iteration 10, loss = 20.20020828\n",
      "Iteration 11, loss = 19.65276535\n",
      "Iteration 12, loss = 19.08645747\n",
      "Iteration 13, loss = 18.53104787\n",
      "Iteration 14, loss = 17.97938001\n",
      "Iteration 15, loss = 17.42328023\n",
      "Iteration 16, loss = 16.87259608\n",
      "Iteration 17, loss = 16.30739629\n",
      "Iteration 18, loss = 15.76517407\n",
      "Iteration 19, loss = 15.21005828\n",
      "Iteration 20, loss = 14.66266422\n",
      "Iteration 21, loss = 14.11864970\n",
      "Iteration 22, loss = 13.57916963\n",
      "Iteration 23, loss = 13.02257941\n",
      "Iteration 24, loss = 12.49801788\n",
      "Iteration 25, loss = 11.96140306\n",
      "Iteration 26, loss = 11.42740204\n",
      "Iteration 27, loss = 10.90303734\n",
      "Iteration 28, loss = 10.37764946\n",
      "Iteration 29, loss = 9.84963127\n",
      "Iteration 30, loss = 9.32775291\n",
      "Iteration 31, loss = 8.81613181\n",
      "Iteration 32, loss = 8.28224774\n",
      "Iteration 33, loss = 7.77662506\n",
      "Iteration 34, loss = 7.26439443\n",
      "Iteration 35, loss = 6.75249044\n",
      "Iteration 36, loss = 6.25327247\n",
      "Iteration 37, loss = 5.75694859\n",
      "Iteration 38, loss = 5.29139421\n",
      "Iteration 39, loss = 4.82881376\n",
      "Iteration 40, loss = 4.39126743\n",
      "Iteration 41, loss = 3.99329019\n",
      "Iteration 42, loss = 3.63575261\n",
      "Iteration 43, loss = 3.31161463\n",
      "Iteration 44, loss = 3.04680005\n",
      "Iteration 45, loss = 2.82826247\n",
      "Iteration 46, loss = 2.63912569\n",
      "Iteration 47, loss = 2.50534792\n",
      "Iteration 48, loss = 2.39300356\n",
      "Iteration 49, loss = 2.30711684\n",
      "Iteration 50, loss = 2.24476178\n",
      "Iteration 51, loss = 2.19150898\n",
      "Iteration 52, loss = 2.15448560\n",
      "Iteration 53, loss = 2.12299559\n",
      "Iteration 54, loss = 2.09799212\n",
      "Iteration 55, loss = 2.07646607\n",
      "Iteration 56, loss = 2.05824760\n",
      "Iteration 57, loss = 2.04131495\n",
      "Iteration 58, loss = 2.02962050\n",
      "Iteration 59, loss = 2.01464832\n",
      "Iteration 60, loss = 1.99958970\n",
      "Iteration 61, loss = 1.98760475\n",
      "Iteration 62, loss = 1.97510479\n",
      "Iteration 63, loss = 1.96294694\n",
      "Iteration 64, loss = 1.95120838\n",
      "Iteration 65, loss = 1.93751405\n",
      "Iteration 66, loss = 1.92415256\n",
      "Iteration 67, loss = 1.90876111\n",
      "Iteration 68, loss = 1.89262302\n",
      "Iteration 69, loss = 1.87647252\n",
      "Iteration 70, loss = 1.85865131\n",
      "Iteration 71, loss = 1.84165895\n",
      "Iteration 72, loss = 1.82094401\n",
      "Iteration 73, loss = 1.79707565\n",
      "Iteration 74, loss = 1.77440233\n",
      "Iteration 75, loss = 1.74832667\n",
      "Iteration 76, loss = 1.72153865\n",
      "Iteration 77, loss = 1.69467975\n",
      "Iteration 78, loss = 1.66960433\n",
      "Iteration 79, loss = 1.64138132\n",
      "Iteration 80, loss = 1.61540358\n",
      "Iteration 81, loss = 1.58926417\n",
      "Iteration 82, loss = 1.56077976\n",
      "Iteration 83, loss = 1.53477371\n",
      "Iteration 84, loss = 1.50272092\n",
      "Iteration 85, loss = 1.47232054\n",
      "Iteration 86, loss = 1.44436184\n",
      "Iteration 87, loss = 1.41403960\n",
      "Iteration 88, loss = 1.39090519\n",
      "Iteration 89, loss = 1.36976478\n",
      "Iteration 90, loss = 1.34792684\n",
      "Iteration 91, loss = 1.32753224\n",
      "Iteration 92, loss = 1.30310446\n",
      "Iteration 93, loss = 1.27989765\n",
      "Iteration 94, loss = 1.25437197\n",
      "Iteration 95, loss = 1.23069829\n",
      "Iteration 96, loss = 1.20546098\n",
      "Iteration 97, loss = 1.18334250\n",
      "Iteration 98, loss = 1.16251540\n",
      "Iteration 99, loss = 1.13893383\n",
      "Iteration 100, loss = 1.11727226\n",
      "Iteration 101, loss = 1.09485714\n",
      "Iteration 102, loss = 1.07053762\n",
      "Iteration 103, loss = 1.05075912\n",
      "Iteration 104, loss = 1.02877318\n",
      "Iteration 105, loss = 1.01016633\n",
      "Iteration 106, loss = 0.99623611\n",
      "Iteration 107, loss = 0.97849040\n",
      "Iteration 108, loss = 0.96077118\n",
      "Iteration 109, loss = 0.94558147\n",
      "Iteration 110, loss = 0.93110917\n",
      "Iteration 111, loss = 0.91805862\n",
      "Iteration 112, loss = 0.90576160\n",
      "Iteration 113, loss = 0.89409684\n",
      "Iteration 114, loss = 0.87973314\n",
      "Iteration 115, loss = 0.86755812\n",
      "Iteration 116, loss = 0.85600216\n",
      "Iteration 117, loss = 0.84589099\n",
      "Iteration 118, loss = 0.83514813\n",
      "Iteration 119, loss = 0.82564424\n",
      "Iteration 120, loss = 0.81711137\n",
      "Iteration 121, loss = 0.80953832\n",
      "Iteration 122, loss = 0.80154563\n",
      "Iteration 123, loss = 0.79448179\n",
      "Iteration 124, loss = 0.78813009\n",
      "Iteration 125, loss = 0.78132772\n",
      "Iteration 126, loss = 0.77645605\n",
      "Iteration 127, loss = 0.77230836\n",
      "Iteration 128, loss = 0.76664438\n",
      "Iteration 129, loss = 0.76095694\n",
      "Iteration 130, loss = 0.75490148\n",
      "Iteration 131, loss = 0.74891523\n",
      "Iteration 132, loss = 0.74238486\n",
      "Iteration 133, loss = 0.73729589\n",
      "Iteration 134, loss = 0.73138911\n",
      "Iteration 135, loss = 0.72552023\n",
      "Iteration 136, loss = 0.72101203\n",
      "Iteration 137, loss = 0.71499360\n",
      "Iteration 138, loss = 0.70944481\n",
      "Iteration 139, loss = 0.70438910\n",
      "Iteration 140, loss = 0.69955498\n",
      "Iteration 141, loss = 0.69510375\n",
      "Iteration 142, loss = 0.69433067\n",
      "Iteration 143, loss = 0.69089697\n",
      "Iteration 144, loss = 0.68666031\n",
      "Iteration 145, loss = 0.68295621\n",
      "Iteration 146, loss = 0.67933228\n",
      "Iteration 147, loss = 0.67679366\n",
      "Iteration 148, loss = 0.67344669\n",
      "Iteration 149, loss = 0.67079800\n",
      "Iteration 150, loss = 0.66836629\n",
      "Iteration 151, loss = 0.66609108\n",
      "Iteration 152, loss = 0.66475895\n",
      "Iteration 153, loss = 0.66376679\n",
      "Iteration 154, loss = 0.66237885\n",
      "Iteration 155, loss = 0.66078772\n",
      "Iteration 156, loss = 0.65912220\n",
      "Iteration 157, loss = 0.65765556\n",
      "Iteration 158, loss = 0.65642519\n",
      "Iteration 159, loss = 0.65524253\n",
      "Iteration 160, loss = 0.65392775\n",
      "Iteration 161, loss = 0.65262694\n",
      "Iteration 162, loss = 0.65160543\n",
      "Iteration 163, loss = 0.65138592\n",
      "Iteration 164, loss = 0.65092217\n",
      "Iteration 165, loss = 0.65027165\n",
      "Iteration 166, loss = 0.64940038\n",
      "Iteration 167, loss = 0.65029204\n",
      "Iteration 168, loss = 0.64873869\n",
      "Iteration 169, loss = 0.64737669\n",
      "Iteration 170, loss = 0.64696195\n",
      "Iteration 171, loss = 0.64721787\n",
      "Iteration 172, loss = 0.64747233\n",
      "Iteration 173, loss = 0.64705969\n",
      "Iteration 174, loss = 0.64673096\n",
      "Iteration 175, loss = 0.64411838\n",
      "Iteration 176, loss = 0.64365801\n",
      "Iteration 177, loss = 0.64330979\n",
      "Iteration 178, loss = 0.64321644\n",
      "Iteration 179, loss = 0.64280531\n",
      "Iteration 180, loss = 0.64245450\n",
      "Iteration 181, loss = 0.64136775\n",
      "Iteration 182, loss = 0.64096304\n",
      "Iteration 183, loss = 0.64043569\n",
      "Iteration 184, loss = 0.64012253\n",
      "Iteration 185, loss = 0.63980578\n",
      "Iteration 186, loss = 0.63990942\n",
      "Iteration 187, loss = 0.63951521\n",
      "Iteration 188, loss = 0.63892164\n",
      "Iteration 189, loss = 0.63777560\n",
      "Iteration 190, loss = 0.63723443\n",
      "Iteration 191, loss = 0.63733226\n",
      "Iteration 192, loss = 0.64018613\n",
      "Iteration 193, loss = 0.64229772\n",
      "Iteration 194, loss = 0.64189146\n",
      "Iteration 195, loss = 0.63788177\n",
      "Iteration 196, loss = 0.63527486\n",
      "Iteration 197, loss = 0.63388196\n",
      "Iteration 198, loss = 0.63612312\n",
      "Iteration 199, loss = 0.63999298\n",
      "Iteration 200, loss = 0.63718052\n",
      "Iteration 201, loss = 0.63450845\n",
      "Iteration 202, loss = 0.63343576\n",
      "Iteration 203, loss = 0.63267959\n",
      "Iteration 204, loss = 0.63193458\n",
      "Iteration 205, loss = 0.63147670\n",
      "Iteration 206, loss = 0.63164921\n",
      "Iteration 207, loss = 0.63231585\n",
      "Iteration 208, loss = 0.63304490\n",
      "Iteration 209, loss = 0.63361546\n",
      "Iteration 210, loss = 0.63367188\n",
      "Iteration 211, loss = 0.63208798\n",
      "Iteration 212, loss = 0.63159453\n",
      "Iteration 213, loss = 0.62943165\n",
      "Iteration 214, loss = 0.62908386\n",
      "Iteration 215, loss = 0.62943646\n",
      "Iteration 216, loss = 0.63027788\n",
      "Iteration 217, loss = 0.63168027\n",
      "Iteration 218, loss = 0.63173280\n",
      "Iteration 219, loss = 0.62998574\n",
      "Iteration 220, loss = 0.62945133\n",
      "Iteration 221, loss = 0.62825152\n",
      "Iteration 222, loss = 0.62814424\n",
      "Iteration 223, loss = 0.62722614\n",
      "Iteration 224, loss = 0.62610049\n",
      "Iteration 225, loss = 0.62756091\n",
      "Iteration 226, loss = 0.62712473\n",
      "Iteration 227, loss = 0.62637765\n",
      "Iteration 228, loss = 0.62528628\n",
      "Iteration 229, loss = 0.62460303\n",
      "Iteration 230, loss = 0.62530566\n",
      "Iteration 231, loss = 0.62529513\n",
      "Iteration 232, loss = 0.62487400\n",
      "Iteration 233, loss = 0.62435572\n",
      "Iteration 234, loss = 0.62308853\n",
      "Iteration 235, loss = 0.62195155\n",
      "Iteration 236, loss = 0.62761179\n",
      "Iteration 237, loss = 0.62920658\n",
      "Iteration 238, loss = 0.62759238\n",
      "Iteration 239, loss = 0.62363229\n",
      "Iteration 240, loss = 0.62313114\n",
      "Iteration 241, loss = 0.62282334\n",
      "Iteration 242, loss = 0.62286570\n",
      "Iteration 243, loss = 0.62197783\n",
      "Iteration 244, loss = 0.62048739\n",
      "Iteration 245, loss = 0.62020047\n",
      "Iteration 246, loss = 0.62265591\n",
      "Iteration 247, loss = 0.62429653\n",
      "Iteration 248, loss = 0.62240766\n",
      "Iteration 249, loss = 0.61888857\n",
      "Iteration 250, loss = 0.61922597\n",
      "Iteration 251, loss = 0.62492617\n",
      "Iteration 252, loss = 0.62593273\n",
      "Iteration 253, loss = 0.62276743\n",
      "Iteration 254, loss = 0.61838083\n",
      "Iteration 255, loss = 0.61802907\n",
      "Iteration 256, loss = 0.61915539\n",
      "Iteration 257, loss = 0.62157956\n",
      "Iteration 258, loss = 0.61962776\n",
      "Iteration 259, loss = 0.61634569\n",
      "Iteration 260, loss = 0.61736331\n",
      "Iteration 261, loss = 0.62004442\n",
      "Iteration 262, loss = 0.62452242\n",
      "Iteration 263, loss = 0.62744763\n",
      "Iteration 264, loss = 0.62587269\n",
      "Iteration 265, loss = 0.62067552\n",
      "Iteration 266, loss = 0.61784316\n",
      "Iteration 267, loss = 0.61637230\n",
      "Iteration 268, loss = 0.61487282\n",
      "Iteration 269, loss = 0.61443120\n",
      "Iteration 270, loss = 0.61519025\n",
      "Iteration 271, loss = 0.61668596\n",
      "Iteration 272, loss = 0.61653725\n",
      "Iteration 273, loss = 0.61502856\n",
      "Iteration 274, loss = 0.61335694\n",
      "Iteration 275, loss = 0.61323786\n",
      "Iteration 276, loss = 0.61555083\n",
      "Iteration 277, loss = 0.61893655\n",
      "Iteration 278, loss = 0.61861729\n",
      "Iteration 279, loss = 0.61546978\n",
      "Iteration 280, loss = 0.61150291\n",
      "Iteration 281, loss = 0.61165561\n",
      "Iteration 282, loss = 0.61406663\n",
      "Iteration 283, loss = 0.61498813\n",
      "Iteration 284, loss = 0.61324742\n",
      "Iteration 285, loss = 0.61027610\n",
      "Iteration 286, loss = 0.61158824\n",
      "Iteration 287, loss = 0.61261515\n",
      "Iteration 288, loss = 0.61178651\n",
      "Iteration 289, loss = 0.61043322\n",
      "Iteration 290, loss = 0.60945799\n",
      "Iteration 291, loss = 0.60943514\n",
      "Iteration 292, loss = 0.61009986\n",
      "Iteration 293, loss = 0.61036953\n",
      "Iteration 294, loss = 0.60943144\n",
      "Iteration 295, loss = 0.60825504\n",
      "Iteration 296, loss = 0.60871433\n",
      "Iteration 297, loss = 0.61167235\n",
      "Iteration 298, loss = 0.61189413\n",
      "Iteration 299, loss = 0.60956197\n",
      "Iteration 300, loss = 0.60821614\n",
      "Iteration 301, loss = 0.60847294\n",
      "Iteration 302, loss = 0.60766308\n",
      "Iteration 303, loss = 0.60682313\n",
      "Iteration 304, loss = 0.60854753\n",
      "Iteration 305, loss = 0.60669248\n",
      "Iteration 306, loss = 0.60715020\n",
      "Iteration 307, loss = 0.60679359\n",
      "Iteration 308, loss = 0.60781422\n",
      "Iteration 309, loss = 0.60712623\n",
      "Iteration 310, loss = 0.60602007\n",
      "Iteration 311, loss = 0.60542130\n",
      "Iteration 312, loss = 0.60526817\n",
      "Iteration 313, loss = 0.60517842\n",
      "Iteration 314, loss = 0.60485113\n",
      "Iteration 315, loss = 0.60482277\n",
      "Iteration 316, loss = 0.60469500\n",
      "Iteration 317, loss = 0.60460143\n",
      "Iteration 318, loss = 0.60435666\n",
      "Iteration 319, loss = 0.60440301\n",
      "Iteration 320, loss = 0.60429554\n",
      "Iteration 321, loss = 0.60395628\n",
      "Iteration 322, loss = 0.60368419\n",
      "Iteration 323, loss = 0.60352882\n",
      "Iteration 324, loss = 0.60332645\n",
      "Iteration 325, loss = 0.60312968\n",
      "Iteration 326, loss = 0.60279245\n",
      "Iteration 327, loss = 0.60351481\n",
      "Iteration 328, loss = 0.60366292\n",
      "Iteration 329, loss = 0.60281889\n",
      "Iteration 330, loss = 0.60227409\n",
      "Iteration 331, loss = 0.60212657\n",
      "Iteration 332, loss = 0.60276921\n",
      "Iteration 333, loss = 0.60351961\n",
      "Iteration 334, loss = 0.60378378\n",
      "Iteration 335, loss = 0.60163619\n",
      "Iteration 336, loss = 0.60104928\n",
      "Iteration 337, loss = 0.60472315\n",
      "Iteration 338, loss = 0.60779122\n",
      "Iteration 339, loss = 0.60735637\n",
      "Iteration 340, loss = 0.60328791\n",
      "Iteration 341, loss = 0.60074134\n",
      "Iteration 342, loss = 0.60313704\n",
      "Iteration 343, loss = 0.60425962\n",
      "Iteration 344, loss = 0.60403927\n",
      "Iteration 345, loss = 0.60359155\n",
      "Iteration 346, loss = 0.60156658\n",
      "Iteration 347, loss = 0.60014677\n",
      "Iteration 348, loss = 0.60045413\n",
      "Iteration 349, loss = 0.60186393\n",
      "Iteration 350, loss = 0.60242262\n",
      "Iteration 351, loss = 0.60068117\n",
      "Iteration 352, loss = 0.59849477\n",
      "Iteration 353, loss = 0.59928017\n",
      "Iteration 354, loss = 0.60316413\n",
      "Iteration 355, loss = 0.60767732\n",
      "Iteration 356, loss = 0.60514644\n",
      "Iteration 357, loss = 0.60048955\n",
      "Iteration 358, loss = 0.59835008\n",
      "Iteration 359, loss = 0.60045178\n",
      "Iteration 360, loss = 0.60190328\n",
      "Iteration 361, loss = 0.60091326\n",
      "Iteration 362, loss = 0.59814194\n",
      "Iteration 363, loss = 0.59886208\n",
      "Iteration 364, loss = 0.59829920\n",
      "Iteration 365, loss = 0.59895040\n",
      "Iteration 366, loss = 0.59956804\n",
      "Iteration 367, loss = 0.60075754\n",
      "Iteration 368, loss = 0.59967062\n",
      "Iteration 369, loss = 0.59725061\n",
      "Iteration 370, loss = 0.59617571\n",
      "Iteration 371, loss = 0.59645871\n",
      "Iteration 372, loss = 0.59674025\n",
      "Iteration 373, loss = 0.59751737\n",
      "Iteration 374, loss = 0.59738415\n",
      "Iteration 375, loss = 0.59631153\n",
      "Iteration 376, loss = 0.59579017\n",
      "Iteration 377, loss = 0.59527145\n",
      "Iteration 378, loss = 0.59539074\n",
      "Iteration 379, loss = 0.59686795\n",
      "Iteration 380, loss = 0.59730702\n",
      "Iteration 381, loss = 0.59547703\n",
      "Iteration 382, loss = 0.59568585\n",
      "Iteration 383, loss = 0.59547754\n",
      "Iteration 384, loss = 0.59433449\n",
      "Iteration 385, loss = 0.59425216\n",
      "Iteration 386, loss = 0.59504804\n",
      "Iteration 387, loss = 0.59584672\n",
      "Iteration 388, loss = 0.59590059\n",
      "Iteration 389, loss = 0.59458329\n",
      "Iteration 390, loss = 0.59365070\n",
      "Iteration 391, loss = 0.59454752\n",
      "Iteration 392, loss = 0.59570333\n",
      "Iteration 393, loss = 0.59529617\n",
      "Iteration 394, loss = 0.59385468\n",
      "Iteration 395, loss = 0.59419450\n",
      "Iteration 396, loss = 0.59319182\n",
      "Iteration 397, loss = 0.59301698\n",
      "Iteration 398, loss = 0.59265564\n",
      "Iteration 399, loss = 0.59304243\n",
      "Iteration 400, loss = 0.59348301\n",
      "Iteration 401, loss = 0.59372448\n",
      "Iteration 402, loss = 0.59406272\n",
      "Iteration 403, loss = 0.59260737\n",
      "Iteration 404, loss = 0.59278480\n",
      "Iteration 405, loss = 0.59228681\n",
      "Iteration 406, loss = 0.59154085\n",
      "Iteration 407, loss = 0.59147472\n",
      "Iteration 408, loss = 0.59209935\n",
      "Iteration 409, loss = 0.59200861\n",
      "Iteration 410, loss = 0.59165107\n",
      "Iteration 411, loss = 0.59143169\n",
      "Iteration 412, loss = 0.59081583\n",
      "Iteration 413, loss = 0.59164335\n",
      "Iteration 414, loss = 0.59208403\n",
      "Iteration 415, loss = 0.59172347\n",
      "Iteration 416, loss = 0.59116268\n",
      "Iteration 417, loss = 0.59020252\n",
      "Iteration 418, loss = 0.59024759\n",
      "Iteration 419, loss = 0.58979320\n",
      "Iteration 420, loss = 0.58966660\n",
      "Iteration 421, loss = 0.58986489\n",
      "Iteration 422, loss = 0.59035888\n",
      "Iteration 423, loss = 0.59111373\n",
      "Iteration 424, loss = 0.59023221\n",
      "Iteration 425, loss = 0.58903019\n",
      "Iteration 426, loss = 0.58965353\n",
      "Iteration 427, loss = 0.58902803\n",
      "Iteration 428, loss = 0.58856038\n",
      "Iteration 429, loss = 0.58863345\n",
      "Iteration 430, loss = 0.58882290\n",
      "Iteration 431, loss = 0.58869058\n",
      "Iteration 432, loss = 0.58872517\n",
      "Iteration 433, loss = 0.58789085\n",
      "Iteration 434, loss = 0.58798089\n",
      "Iteration 435, loss = 0.58767361\n",
      "Iteration 436, loss = 0.58954657\n",
      "Iteration 437, loss = 0.59143158\n",
      "Iteration 438, loss = 0.59261086\n",
      "Iteration 439, loss = 0.59148070\n",
      "Iteration 440, loss = 0.58819011\n",
      "Iteration 441, loss = 0.58652553\n",
      "Iteration 442, loss = 0.58608439\n",
      "Iteration 443, loss = 0.58682249\n",
      "Iteration 444, loss = 0.58772386\n",
      "Iteration 445, loss = 0.58680710\n",
      "Iteration 446, loss = 0.58557514\n",
      "Iteration 447, loss = 0.58607514\n",
      "Iteration 448, loss = 0.58563811\n",
      "Iteration 449, loss = 0.58492973\n",
      "Iteration 450, loss = 0.58445915\n",
      "Iteration 451, loss = 0.58451309\n",
      "Iteration 452, loss = 0.58489979\n",
      "Iteration 453, loss = 0.58538798\n",
      "Iteration 454, loss = 0.58590073\n",
      "Iteration 455, loss = 0.58529790\n",
      "Iteration 456, loss = 0.58457442\n",
      "Iteration 457, loss = 0.58352974\n",
      "Iteration 458, loss = 0.58324269\n",
      "Iteration 459, loss = 0.58406616\n",
      "Iteration 460, loss = 0.58477911\n",
      "Iteration 461, loss = 0.58450973\n",
      "Iteration 462, loss = 0.58358390\n",
      "Iteration 463, loss = 0.58325677\n",
      "Iteration 464, loss = 0.58256226\n",
      "Iteration 465, loss = 0.58296376\n",
      "Iteration 466, loss = 0.58219253\n",
      "Iteration 467, loss = 0.58206202\n",
      "Iteration 468, loss = 0.58232846\n",
      "Iteration 469, loss = 0.58209654\n",
      "Iteration 470, loss = 0.58139573\n",
      "Iteration 471, loss = 0.58121805\n",
      "Iteration 472, loss = 0.58182501\n",
      "Iteration 473, loss = 0.58301767\n",
      "Iteration 474, loss = 0.58176977\n",
      "Iteration 475, loss = 0.58116347\n",
      "Iteration 476, loss = 0.58096178\n",
      "Iteration 477, loss = 0.58121440\n",
      "Iteration 478, loss = 0.58050376\n",
      "Iteration 479, loss = 0.58061133\n",
      "Iteration 480, loss = 0.58052959\n",
      "Iteration 481, loss = 0.58033070\n",
      "Iteration 482, loss = 0.58014929\n",
      "Iteration 483, loss = 0.57995550\n",
      "Iteration 484, loss = 0.57969096\n",
      "Iteration 485, loss = 0.57995519\n",
      "Iteration 486, loss = 0.57991093\n",
      "Iteration 487, loss = 0.57917883\n",
      "Iteration 488, loss = 0.57955284\n",
      "Iteration 489, loss = 0.58223982\n",
      "Iteration 490, loss = 0.58373762\n",
      "Iteration 491, loss = 0.58124829\n",
      "Iteration 492, loss = 0.57971149\n",
      "Iteration 493, loss = 0.58091558\n",
      "Iteration 494, loss = 0.58130817\n",
      "Iteration 495, loss = 0.58030537\n",
      "Iteration 496, loss = 0.57920467\n",
      "Iteration 497, loss = 0.57976370\n",
      "Iteration 498, loss = 0.58006093\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.01517987\n",
      "Iteration 2, loss = 18.38503532\n",
      "Iteration 3, loss = 17.73952826\n",
      "Iteration 4, loss = 17.10524019\n",
      "Iteration 5, loss = 16.46182642\n",
      "Iteration 6, loss = 15.82806049\n",
      "Iteration 7, loss = 15.19018091\n",
      "Iteration 8, loss = 14.55378864\n",
      "Iteration 9, loss = 13.90600654\n",
      "Iteration 10, loss = 13.27364428\n",
      "Iteration 11, loss = 12.63039551\n",
      "Iteration 12, loss = 11.98878670\n",
      "Iteration 13, loss = 11.35339347\n",
      "Iteration 14, loss = 10.71603087\n",
      "Iteration 15, loss = 10.07918257\n",
      "Iteration 16, loss = 9.44630845\n",
      "Iteration 17, loss = 8.81021090\n",
      "Iteration 18, loss = 8.18033094\n",
      "Iteration 19, loss = 7.55391473\n",
      "Iteration 20, loss = 6.93316043\n",
      "Iteration 21, loss = 6.30914308\n",
      "Iteration 22, loss = 5.70544849\n",
      "Iteration 23, loss = 5.11751342\n",
      "Iteration 24, loss = 4.55711369\n",
      "Iteration 25, loss = 4.02498767\n",
      "Iteration 26, loss = 3.53849338\n",
      "Iteration 27, loss = 3.12044006\n",
      "Iteration 28, loss = 2.75517158\n",
      "Iteration 29, loss = 2.46176833\n",
      "Iteration 30, loss = 2.21715061\n",
      "Iteration 31, loss = 2.04061702\n",
      "Iteration 32, loss = 1.89885326\n",
      "Iteration 33, loss = 1.80733927\n",
      "Iteration 34, loss = 1.73437808\n",
      "Iteration 35, loss = 1.69466231\n",
      "Iteration 36, loss = 1.66328206\n",
      "Iteration 37, loss = 1.64460577\n",
      "Iteration 38, loss = 1.63314071\n",
      "Iteration 39, loss = 1.63001964\n",
      "Iteration 40, loss = 1.62431813\n",
      "Iteration 41, loss = 1.62123895\n",
      "Iteration 42, loss = 1.61894643\n",
      "Iteration 43, loss = 1.61529847\n",
      "Iteration 44, loss = 1.61146103\n",
      "Iteration 45, loss = 1.60583740\n",
      "Iteration 46, loss = 1.59835168\n",
      "Iteration 47, loss = 1.59027192\n",
      "Iteration 48, loss = 1.58141384\n",
      "Iteration 49, loss = 1.57121233\n",
      "Iteration 50, loss = 1.56172190\n",
      "Iteration 51, loss = 1.55075914\n",
      "Iteration 52, loss = 1.53977638\n",
      "Iteration 53, loss = 1.52764275\n",
      "Iteration 54, loss = 1.51706640\n",
      "Iteration 55, loss = 1.50733502\n",
      "Iteration 56, loss = 1.49644080\n",
      "Iteration 57, loss = 1.48651399\n",
      "Iteration 58, loss = 1.47767069\n",
      "Iteration 59, loss = 1.46814190\n",
      "Iteration 60, loss = 1.46052003\n",
      "Iteration 61, loss = 1.45183148\n",
      "Iteration 62, loss = 1.44334232\n",
      "Iteration 63, loss = 1.43471603\n",
      "Iteration 64, loss = 1.42679606\n",
      "Iteration 65, loss = 1.41849511\n",
      "Iteration 66, loss = 1.40974979\n",
      "Iteration 67, loss = 1.40171994\n",
      "Iteration 68, loss = 1.39309964\n",
      "Iteration 69, loss = 1.38489759\n",
      "Iteration 70, loss = 1.37682366\n",
      "Iteration 71, loss = 1.36842903\n",
      "Iteration 72, loss = 1.36097969\n",
      "Iteration 73, loss = 1.35320431\n",
      "Iteration 74, loss = 1.34541894\n",
      "Iteration 75, loss = 1.33768769\n",
      "Iteration 76, loss = 1.33035824\n",
      "Iteration 77, loss = 1.32246564\n",
      "Iteration 78, loss = 1.31504590\n",
      "Iteration 79, loss = 1.30740715\n",
      "Iteration 80, loss = 1.29974771\n",
      "Iteration 81, loss = 1.29223692\n",
      "Iteration 82, loss = 1.28440717\n",
      "Iteration 83, loss = 1.27700455\n",
      "Iteration 84, loss = 1.26978570\n",
      "Iteration 85, loss = 1.26175600\n",
      "Iteration 86, loss = 1.25403560\n",
      "Iteration 87, loss = 1.24675407\n",
      "Iteration 88, loss = 1.23917297\n",
      "Iteration 89, loss = 1.23147646\n",
      "Iteration 90, loss = 1.22445471\n",
      "Iteration 91, loss = 1.21691454\n",
      "Iteration 92, loss = 1.20877489\n",
      "Iteration 93, loss = 1.20164099\n",
      "Iteration 94, loss = 1.19335134\n",
      "Iteration 95, loss = 1.18588028\n",
      "Iteration 96, loss = 1.17857253\n",
      "Iteration 97, loss = 1.17121641\n",
      "Iteration 98, loss = 1.16406078\n",
      "Iteration 99, loss = 1.15688790\n",
      "Iteration 100, loss = 1.15040194\n",
      "Iteration 101, loss = 1.14300071\n",
      "Iteration 102, loss = 1.13612236\n",
      "Iteration 103, loss = 1.12916074\n",
      "Iteration 104, loss = 1.12205736\n",
      "Iteration 105, loss = 1.11520067\n",
      "Iteration 106, loss = 1.10882382\n",
      "Iteration 107, loss = 1.10200618\n",
      "Iteration 108, loss = 1.09596185\n",
      "Iteration 109, loss = 1.08956692\n",
      "Iteration 110, loss = 1.08353668\n",
      "Iteration 111, loss = 1.07741625\n",
      "Iteration 112, loss = 1.07130037\n",
      "Iteration 113, loss = 1.06507740\n",
      "Iteration 114, loss = 1.05930041\n",
      "Iteration 115, loss = 1.05318023\n",
      "Iteration 116, loss = 1.04721978\n",
      "Iteration 117, loss = 1.04111012\n",
      "Iteration 118, loss = 1.03470814\n",
      "Iteration 119, loss = 1.02831112\n",
      "Iteration 120, loss = 1.02238319\n",
      "Iteration 121, loss = 1.01633787\n",
      "Iteration 122, loss = 1.01069250\n",
      "Iteration 123, loss = 1.00650851\n",
      "Iteration 124, loss = 1.00114353\n",
      "Iteration 125, loss = 0.99670887\n",
      "Iteration 126, loss = 0.99060316\n",
      "Iteration 127, loss = 0.98428078\n",
      "Iteration 128, loss = 0.97753982\n",
      "Iteration 129, loss = 0.97181809\n",
      "Iteration 130, loss = 0.96581059\n",
      "Iteration 131, loss = 0.96095087\n",
      "Iteration 132, loss = 0.95563921\n",
      "Iteration 133, loss = 0.95134357\n",
      "Iteration 134, loss = 0.94720429\n",
      "Iteration 135, loss = 0.94354348\n",
      "Iteration 136, loss = 0.93941659\n",
      "Iteration 137, loss = 0.93553133\n",
      "Iteration 138, loss = 0.93066510\n",
      "Iteration 139, loss = 0.92586174\n",
      "Iteration 140, loss = 0.91948640\n",
      "Iteration 141, loss = 0.91451826\n",
      "Iteration 142, loss = 0.90979297\n",
      "Iteration 143, loss = 0.90433302\n",
      "Iteration 144, loss = 0.89991746\n",
      "Iteration 145, loss = 0.89523971\n",
      "Iteration 146, loss = 0.89115720\n",
      "Iteration 147, loss = 0.88668049\n",
      "Iteration 148, loss = 0.88256694\n",
      "Iteration 149, loss = 0.87870686\n",
      "Iteration 150, loss = 0.87483991\n",
      "Iteration 151, loss = 0.87066673\n",
      "Iteration 152, loss = 0.86683972\n",
      "Iteration 153, loss = 0.86304655\n",
      "Iteration 154, loss = 0.85934616\n",
      "Iteration 155, loss = 0.85583178\n",
      "Iteration 156, loss = 0.85220806\n",
      "Iteration 157, loss = 0.84873032\n",
      "Iteration 158, loss = 0.84470339\n",
      "Iteration 159, loss = 0.84127362\n",
      "Iteration 160, loss = 0.83730500\n",
      "Iteration 161, loss = 0.83377443\n",
      "Iteration 162, loss = 0.83021519\n",
      "Iteration 163, loss = 0.82634935\n",
      "Iteration 164, loss = 0.82316899\n",
      "Iteration 165, loss = 0.81959446\n",
      "Iteration 166, loss = 0.81636430\n",
      "Iteration 167, loss = 0.81307594\n",
      "Iteration 168, loss = 0.80900130\n",
      "Iteration 169, loss = 0.80547126\n",
      "Iteration 170, loss = 0.80222486\n",
      "Iteration 171, loss = 0.79858858\n",
      "Iteration 172, loss = 0.79207377\n",
      "Iteration 173, loss = 0.78346960\n",
      "Iteration 174, loss = 0.78032659\n",
      "Iteration 175, loss = 0.77775861\n",
      "Iteration 176, loss = 0.77136302\n",
      "Iteration 177, loss = 0.76214930\n",
      "Iteration 178, loss = 0.75542115\n",
      "Iteration 179, loss = 0.75478157\n",
      "Iteration 180, loss = 0.75135476\n",
      "Iteration 181, loss = 0.74394235\n",
      "Iteration 182, loss = 0.73641099\n",
      "Iteration 183, loss = 0.73184109\n",
      "Iteration 184, loss = 0.73128126\n",
      "Iteration 185, loss = 0.72739276\n",
      "Iteration 186, loss = 0.72269677\n",
      "Iteration 187, loss = 0.71657077\n",
      "Iteration 188, loss = 0.71100782\n",
      "Iteration 189, loss = 0.70561481\n",
      "Iteration 190, loss = 0.70154266\n",
      "Iteration 191, loss = 0.69770364\n",
      "Iteration 192, loss = 0.69469996\n",
      "Iteration 193, loss = 0.68956722\n",
      "Iteration 194, loss = 0.68432818\n",
      "Iteration 195, loss = 0.67942399\n",
      "Iteration 196, loss = 0.67632216\n",
      "Iteration 197, loss = 0.67303695\n",
      "Iteration 198, loss = 0.67020024\n",
      "Iteration 199, loss = 0.66621831\n",
      "Iteration 200, loss = 0.65877954\n",
      "Iteration 201, loss = 0.65972372\n",
      "Iteration 202, loss = 0.65773760\n",
      "Iteration 203, loss = 0.65339693\n",
      "Iteration 204, loss = 0.64719901\n",
      "Iteration 205, loss = 0.64295759\n",
      "Iteration 206, loss = 0.64010061\n",
      "Iteration 207, loss = 0.63677429\n",
      "Iteration 208, loss = 0.63276549\n",
      "Iteration 209, loss = 0.62989453\n",
      "Iteration 210, loss = 0.62984235\n",
      "Iteration 211, loss = 0.62814217\n",
      "Iteration 212, loss = 0.62427882\n",
      "Iteration 213, loss = 0.62019893\n",
      "Iteration 214, loss = 0.61566904\n",
      "Iteration 215, loss = 0.61448421\n",
      "Iteration 216, loss = 0.61194889\n",
      "Iteration 217, loss = 0.60913145\n",
      "Iteration 218, loss = 0.60645167\n",
      "Iteration 219, loss = 0.60360913\n",
      "Iteration 220, loss = 0.60079318\n",
      "Iteration 221, loss = 0.59914560\n",
      "Iteration 222, loss = 0.59778878\n",
      "Iteration 223, loss = 0.59624411\n",
      "Iteration 224, loss = 0.59428052\n",
      "Iteration 225, loss = 0.59210844\n",
      "Iteration 226, loss = 0.58942631\n",
      "Iteration 227, loss = 0.58746705\n",
      "Iteration 228, loss = 0.58554701\n",
      "Iteration 229, loss = 0.58383132\n",
      "Iteration 230, loss = 0.58153117\n",
      "Iteration 231, loss = 0.58054495\n",
      "Iteration 232, loss = 0.57885954\n",
      "Iteration 233, loss = 0.57686587\n",
      "Iteration 234, loss = 0.57552642\n",
      "Iteration 235, loss = 0.57435202\n",
      "Iteration 236, loss = 0.57353317\n",
      "Iteration 237, loss = 0.57175874\n",
      "Iteration 238, loss = 0.56993559\n",
      "Iteration 239, loss = 0.57170653\n",
      "Iteration 240, loss = 0.57030989\n",
      "Iteration 241, loss = 0.56834518\n",
      "Iteration 242, loss = 0.56637317\n",
      "Iteration 243, loss = 0.56553621\n",
      "Iteration 244, loss = 0.56536999\n",
      "Iteration 245, loss = 0.56436768\n",
      "Iteration 246, loss = 0.56288089\n",
      "Iteration 247, loss = 0.56199579\n",
      "Iteration 248, loss = 0.56135523\n",
      "Iteration 249, loss = 0.56080369\n",
      "Iteration 250, loss = 0.56048547\n",
      "Iteration 251, loss = 0.56066483\n",
      "Iteration 252, loss = 0.56087986\n",
      "Iteration 253, loss = 0.55973715\n",
      "Iteration 254, loss = 0.55755956\n",
      "Iteration 255, loss = 0.55633223\n",
      "Iteration 256, loss = 0.55683448\n",
      "Iteration 257, loss = 0.55850530\n",
      "Iteration 258, loss = 0.55603124\n",
      "Iteration 259, loss = 0.55318541\n",
      "Iteration 260, loss = 0.55698637\n",
      "Iteration 261, loss = 0.56052643\n",
      "Iteration 262, loss = 0.55778818\n",
      "Iteration 263, loss = 0.55340375\n",
      "Iteration 264, loss = 0.55409460\n",
      "Iteration 265, loss = 0.55419268\n",
      "Iteration 266, loss = 0.55379238\n",
      "Iteration 267, loss = 0.55215185\n",
      "Iteration 268, loss = 0.55110052\n",
      "Iteration 269, loss = 0.55000830\n",
      "Iteration 270, loss = 0.55024165\n",
      "Iteration 271, loss = 0.54996463\n",
      "Iteration 272, loss = 0.54960972\n",
      "Iteration 273, loss = 0.54882965\n",
      "Iteration 274, loss = 0.54847166\n",
      "Iteration 275, loss = 0.54830463\n",
      "Iteration 276, loss = 0.54782795\n",
      "Iteration 277, loss = 0.54781082\n",
      "Iteration 278, loss = 0.54773174\n",
      "Iteration 279, loss = 0.54714268\n",
      "Iteration 280, loss = 0.54663197\n",
      "Iteration 281, loss = 0.54656798\n",
      "Iteration 282, loss = 0.54713544\n",
      "Iteration 283, loss = 0.54820657\n",
      "Iteration 284, loss = 0.54867042\n",
      "Iteration 285, loss = 0.54873257\n",
      "Iteration 286, loss = 0.54756252\n",
      "Iteration 287, loss = 0.54610181\n",
      "Iteration 288, loss = 0.54515316\n",
      "Iteration 289, loss = 0.54759725\n",
      "Iteration 290, loss = 0.54782846\n",
      "Iteration 291, loss = 0.54689466\n",
      "Iteration 292, loss = 0.54612687\n",
      "Iteration 293, loss = 0.54526210\n",
      "Iteration 294, loss = 0.54498001\n",
      "Iteration 295, loss = 0.54483179\n",
      "Iteration 296, loss = 0.54457510\n",
      "Iteration 297, loss = 0.54405175\n",
      "Iteration 298, loss = 0.54405732\n",
      "Iteration 299, loss = 0.54414270\n",
      "Iteration 300, loss = 0.54407695\n",
      "Iteration 301, loss = 0.54445543\n",
      "Iteration 302, loss = 0.54407246\n",
      "Iteration 303, loss = 0.54308948\n",
      "Iteration 304, loss = 0.54398783\n",
      "Iteration 305, loss = 0.54400810\n",
      "Iteration 306, loss = 0.54389063\n",
      "Iteration 307, loss = 0.54295478\n",
      "Iteration 308, loss = 0.54284670\n",
      "Iteration 309, loss = 0.54393479\n",
      "Iteration 310, loss = 0.54541034\n",
      "Iteration 311, loss = 0.54519456\n",
      "Iteration 312, loss = 0.54384492\n",
      "Iteration 313, loss = 0.54301361\n",
      "Iteration 314, loss = 0.54263807\n",
      "Iteration 315, loss = 0.54277710\n",
      "Iteration 316, loss = 0.54276871\n",
      "Iteration 317, loss = 0.54222898\n",
      "Iteration 318, loss = 0.54214931\n",
      "Iteration 319, loss = 0.54421757\n",
      "Iteration 320, loss = 0.54432627\n",
      "Iteration 321, loss = 0.54374077\n",
      "Iteration 322, loss = 0.54215712\n",
      "Iteration 323, loss = 0.54208930\n",
      "Iteration 324, loss = 0.54214852\n",
      "Iteration 325, loss = 0.54224872\n",
      "Iteration 326, loss = 0.54219046\n",
      "Iteration 327, loss = 0.54200308\n",
      "Iteration 328, loss = 0.54179039\n",
      "Iteration 329, loss = 0.54155501\n",
      "Iteration 330, loss = 0.54240650\n",
      "Iteration 331, loss = 0.54192388\n",
      "Iteration 332, loss = 0.54198803\n",
      "Iteration 333, loss = 0.54147276\n",
      "Iteration 334, loss = 0.54212411\n",
      "Iteration 335, loss = 0.54163215\n",
      "Iteration 336, loss = 0.54115123\n",
      "Iteration 337, loss = 0.54131975\n",
      "Iteration 338, loss = 0.54176488\n",
      "Iteration 339, loss = 0.54139517\n",
      "Iteration 340, loss = 0.54088597\n",
      "Iteration 341, loss = 0.54142418\n",
      "Iteration 342, loss = 0.54330673\n",
      "Iteration 343, loss = 0.54414039\n",
      "Iteration 344, loss = 0.54404706\n",
      "Iteration 345, loss = 0.54239150\n",
      "Iteration 346, loss = 0.54257108\n",
      "Iteration 347, loss = 0.54160001\n",
      "Iteration 348, loss = 0.54037730\n",
      "Iteration 349, loss = 0.54213890\n",
      "Iteration 350, loss = 0.54854331\n",
      "Iteration 351, loss = 0.55073050\n",
      "Iteration 352, loss = 0.54589947\n",
      "Iteration 353, loss = 0.54119809\n",
      "Iteration 354, loss = 0.54152161\n",
      "Iteration 355, loss = 0.54383601\n",
      "Iteration 356, loss = 0.54439181\n",
      "Iteration 357, loss = 0.54114273\n",
      "Iteration 358, loss = 0.54373623\n",
      "Iteration 359, loss = 0.54167989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 8.97465254\n",
      "Iteration 2, loss = 8.90276977\n",
      "Iteration 3, loss = 8.82120605\n",
      "Iteration 4, loss = 8.72426130\n",
      "Iteration 5, loss = 8.62092953\n",
      "Iteration 6, loss = 8.50971749\n",
      "Iteration 7, loss = 8.38610183\n",
      "Iteration 8, loss = 8.25763806\n",
      "Iteration 9, loss = 8.13165568\n",
      "Iteration 10, loss = 7.99014911\n",
      "Iteration 11, loss = 7.86162663\n",
      "Iteration 12, loss = 7.72041318\n",
      "Iteration 13, loss = 7.57320740\n",
      "Iteration 14, loss = 7.43593056\n",
      "Iteration 15, loss = 7.29508628\n",
      "Iteration 16, loss = 7.15174597\n",
      "Iteration 17, loss = 7.01093244\n",
      "Iteration 18, loss = 6.87027922\n",
      "Iteration 19, loss = 6.72974489\n",
      "Iteration 20, loss = 6.57940965\n",
      "Iteration 21, loss = 6.44187381\n",
      "Iteration 22, loss = 6.29459358\n",
      "Iteration 23, loss = 6.14949050\n",
      "Iteration 24, loss = 5.99823100\n",
      "Iteration 25, loss = 5.84623041\n",
      "Iteration 26, loss = 5.69538811\n",
      "Iteration 27, loss = 5.53961573\n",
      "Iteration 28, loss = 5.38144433\n",
      "Iteration 29, loss = 5.21886789\n",
      "Iteration 30, loss = 5.05333752\n",
      "Iteration 31, loss = 4.88354833\n",
      "Iteration 32, loss = 4.72007258\n",
      "Iteration 33, loss = 4.54697880\n",
      "Iteration 34, loss = 4.37473513\n",
      "Iteration 35, loss = 4.20866791\n",
      "Iteration 36, loss = 4.04017053\n",
      "Iteration 37, loss = 3.86476828\n",
      "Iteration 38, loss = 3.69445742\n",
      "Iteration 39, loss = 3.52273268\n",
      "Iteration 40, loss = 3.34970124\n",
      "Iteration 41, loss = 3.17737198\n",
      "Iteration 42, loss = 2.99788170\n",
      "Iteration 43, loss = 2.82374449\n",
      "Iteration 44, loss = 2.64943146\n",
      "Iteration 45, loss = 2.47816449\n",
      "Iteration 46, loss = 2.30826136\n",
      "Iteration 47, loss = 2.13344845\n",
      "Iteration 48, loss = 1.96629382\n",
      "Iteration 49, loss = 1.81020228\n",
      "Iteration 50, loss = 1.64816368\n",
      "Iteration 51, loss = 1.50207392\n",
      "Iteration 52, loss = 1.36858588\n",
      "Iteration 53, loss = 1.24282123\n",
      "Iteration 54, loss = 1.14076123\n",
      "Iteration 55, loss = 1.05868685\n",
      "Iteration 56, loss = 1.00348298\n",
      "Iteration 57, loss = 0.97149992\n",
      "Iteration 58, loss = 0.95640710\n",
      "Iteration 59, loss = 0.95589526\n",
      "Iteration 60, loss = 0.96589923\n",
      "Iteration 61, loss = 0.97175451\n",
      "Iteration 62, loss = 0.97468978\n",
      "Iteration 63, loss = 0.96785297\n",
      "Iteration 64, loss = 0.95506368\n",
      "Iteration 65, loss = 0.93988674\n",
      "Iteration 66, loss = 0.92041017\n",
      "Iteration 67, loss = 0.90177864\n",
      "Iteration 68, loss = 0.88640387\n",
      "Iteration 69, loss = 0.87380043\n",
      "Iteration 70, loss = 0.86589289\n",
      "Iteration 71, loss = 0.86104275\n",
      "Iteration 72, loss = 0.85546359\n",
      "Iteration 73, loss = 0.85089092\n",
      "Iteration 74, loss = 0.84506513\n",
      "Iteration 75, loss = 0.83894189\n",
      "Iteration 76, loss = 0.83170047\n",
      "Iteration 77, loss = 0.82399626\n",
      "Iteration 78, loss = 0.81610186\n",
      "Iteration 79, loss = 0.80926040\n",
      "Iteration 80, loss = 0.80226237\n",
      "Iteration 81, loss = 0.79531765\n",
      "Iteration 82, loss = 0.78910434\n",
      "Iteration 83, loss = 0.78314896\n",
      "Iteration 84, loss = 0.77762007\n",
      "Iteration 85, loss = 0.77271981\n",
      "Iteration 86, loss = 0.76737384\n",
      "Iteration 87, loss = 0.76312656\n",
      "Iteration 88, loss = 0.75843984\n",
      "Iteration 89, loss = 0.75548557\n",
      "Iteration 90, loss = 0.75169625\n",
      "Iteration 91, loss = 0.74737275\n",
      "Iteration 92, loss = 0.74376052\n",
      "Iteration 93, loss = 0.73883897\n",
      "Iteration 94, loss = 0.73271479\n",
      "Iteration 95, loss = 0.72734717\n",
      "Iteration 96, loss = 0.72171899\n",
      "Iteration 97, loss = 0.71728597\n",
      "Iteration 98, loss = 0.71303466\n",
      "Iteration 99, loss = 0.70877709\n",
      "Iteration 100, loss = 0.70561321\n",
      "Iteration 101, loss = 0.70169271\n",
      "Iteration 102, loss = 0.69814946\n",
      "Iteration 103, loss = 0.69477667\n",
      "Iteration 104, loss = 0.69160468\n",
      "Iteration 105, loss = 0.68788469\n",
      "Iteration 106, loss = 0.68495573\n",
      "Iteration 107, loss = 0.68154918\n",
      "Iteration 108, loss = 0.67876362\n",
      "Iteration 109, loss = 0.67567461\n",
      "Iteration 110, loss = 0.67318323\n",
      "Iteration 111, loss = 0.67038603\n",
      "Iteration 112, loss = 0.66806162\n",
      "Iteration 113, loss = 0.66628471\n",
      "Iteration 114, loss = 0.66345376\n",
      "Iteration 115, loss = 0.66147629\n",
      "Iteration 116, loss = 0.65872353\n",
      "Iteration 117, loss = 0.65668001\n",
      "Iteration 118, loss = 0.65474162\n",
      "Iteration 119, loss = 0.65275275\n",
      "Iteration 120, loss = 0.65127398\n",
      "Iteration 121, loss = 0.64942629\n",
      "Iteration 122, loss = 0.64688071\n",
      "Iteration 123, loss = 0.64523985\n",
      "Iteration 124, loss = 0.64309545\n",
      "Iteration 125, loss = 0.64120684\n",
      "Iteration 126, loss = 0.63934094\n",
      "Iteration 127, loss = 0.63747562\n",
      "Iteration 128, loss = 0.63593002\n",
      "Iteration 129, loss = 0.63456731\n",
      "Iteration 130, loss = 0.63288292\n",
      "Iteration 131, loss = 0.63148238\n",
      "Iteration 132, loss = 0.63002739\n",
      "Iteration 133, loss = 0.62880114\n",
      "Iteration 134, loss = 0.62713404\n",
      "Iteration 135, loss = 0.62540841\n",
      "Iteration 136, loss = 0.62424389\n",
      "Iteration 137, loss = 0.62280329\n",
      "Iteration 138, loss = 0.62135376\n",
      "Iteration 139, loss = 0.61975073\n",
      "Iteration 140, loss = 0.61908287\n",
      "Iteration 141, loss = 0.61768639\n",
      "Iteration 142, loss = 0.61696747\n",
      "Iteration 143, loss = 0.61618444\n",
      "Iteration 144, loss = 0.61530941\n",
      "Iteration 145, loss = 0.61439831\n",
      "Iteration 146, loss = 0.61268107\n",
      "Iteration 147, loss = 0.61116907\n",
      "Iteration 148, loss = 0.60953753\n",
      "Iteration 149, loss = 0.60922852\n",
      "Iteration 150, loss = 0.61208648\n",
      "Iteration 151, loss = 0.61172876\n",
      "Iteration 152, loss = 0.61006117\n",
      "Iteration 153, loss = 0.60773182\n",
      "Iteration 154, loss = 0.60479284\n",
      "Iteration 155, loss = 0.60379424\n",
      "Iteration 156, loss = 0.60286575\n",
      "Iteration 157, loss = 0.60214969\n",
      "Iteration 158, loss = 0.60168069\n",
      "Iteration 159, loss = 0.60081565\n",
      "Iteration 160, loss = 0.59993379\n",
      "Iteration 161, loss = 0.59902916\n",
      "Iteration 162, loss = 0.59810125\n",
      "Iteration 163, loss = 0.59705428\n",
      "Iteration 164, loss = 0.59647789\n",
      "Iteration 165, loss = 0.59590351\n",
      "Iteration 166, loss = 0.59568946\n",
      "Iteration 167, loss = 0.59509225\n",
      "Iteration 168, loss = 0.59405997\n",
      "Iteration 169, loss = 0.59377402\n",
      "Iteration 170, loss = 0.59233191\n",
      "Iteration 171, loss = 0.59179116\n",
      "Iteration 172, loss = 0.59131788\n",
      "Iteration 173, loss = 0.59047909\n",
      "Iteration 174, loss = 0.58969780\n",
      "Iteration 175, loss = 0.58863673\n",
      "Iteration 176, loss = 0.58787540\n",
      "Iteration 177, loss = 0.58703606\n",
      "Iteration 178, loss = 0.58643476\n",
      "Iteration 179, loss = 0.58520033\n",
      "Iteration 180, loss = 0.58408016\n",
      "Iteration 181, loss = 0.58298201\n",
      "Iteration 182, loss = 0.58218263\n",
      "Iteration 183, loss = 0.58298241\n",
      "Iteration 184, loss = 0.58305470\n",
      "Iteration 185, loss = 0.58285051\n",
      "Iteration 186, loss = 0.58101795\n",
      "Iteration 187, loss = 0.57903892\n",
      "Iteration 188, loss = 0.57584718\n",
      "Iteration 189, loss = 0.57528706\n",
      "Iteration 190, loss = 0.57585795\n",
      "Iteration 191, loss = 0.57729326\n",
      "Iteration 192, loss = 0.57621435\n",
      "Iteration 193, loss = 0.57298107\n",
      "Iteration 194, loss = 0.57058711\n",
      "Iteration 195, loss = 0.57030854\n",
      "Iteration 196, loss = 0.57150359\n",
      "Iteration 197, loss = 0.56952652\n",
      "Iteration 198, loss = 0.56721810\n",
      "Iteration 199, loss = 0.56728889\n",
      "Iteration 200, loss = 0.56926091\n",
      "Iteration 201, loss = 0.56954620\n",
      "Iteration 202, loss = 0.56840361\n",
      "Iteration 203, loss = 0.56648474\n",
      "Iteration 204, loss = 0.56456276\n",
      "Iteration 205, loss = 0.56484914\n",
      "Iteration 206, loss = 0.56540579\n",
      "Iteration 207, loss = 0.56523110\n",
      "Iteration 208, loss = 0.56420614\n",
      "Iteration 209, loss = 0.56277833\n",
      "Iteration 210, loss = 0.56249521\n",
      "Iteration 211, loss = 0.56263893\n",
      "Iteration 212, loss = 0.56256026\n",
      "Iteration 213, loss = 0.56131969\n",
      "Iteration 214, loss = 0.56189521\n",
      "Iteration 215, loss = 0.56076568\n",
      "Iteration 216, loss = 0.56080875\n",
      "Iteration 217, loss = 0.56034212\n",
      "Iteration 218, loss = 0.56011705\n",
      "Iteration 219, loss = 0.55925985\n",
      "Iteration 220, loss = 0.55906211\n",
      "Iteration 221, loss = 0.55909731\n",
      "Iteration 222, loss = 0.55898972\n",
      "Iteration 223, loss = 0.55906733\n",
      "Iteration 224, loss = 0.55847924\n",
      "Iteration 225, loss = 0.55802287\n",
      "Iteration 226, loss = 0.55688046\n",
      "Iteration 227, loss = 0.55672285\n",
      "Iteration 228, loss = 0.55663997\n",
      "Iteration 229, loss = 0.55671281\n",
      "Iteration 230, loss = 0.55652091\n",
      "Iteration 231, loss = 0.55518562\n",
      "Iteration 232, loss = 0.55472578\n",
      "Iteration 233, loss = 0.55462397\n",
      "Iteration 234, loss = 0.55440870\n",
      "Iteration 235, loss = 0.55378931\n",
      "Iteration 236, loss = 0.55293131\n",
      "Iteration 237, loss = 0.55271872\n",
      "Iteration 238, loss = 0.55317538\n",
      "Iteration 239, loss = 0.55393480\n",
      "Iteration 240, loss = 0.55312985\n",
      "Iteration 241, loss = 0.55238093\n",
      "Iteration 242, loss = 0.55103696\n",
      "Iteration 243, loss = 0.55085926\n",
      "Iteration 244, loss = 0.55036567\n",
      "Iteration 245, loss = 0.55057054\n",
      "Iteration 246, loss = 0.55058227\n",
      "Iteration 247, loss = 0.55030134\n",
      "Iteration 248, loss = 0.55052913\n",
      "Iteration 249, loss = 0.54975933\n",
      "Iteration 250, loss = 0.54902553\n",
      "Iteration 251, loss = 0.54881117\n",
      "Iteration 252, loss = 0.55049189\n",
      "Iteration 253, loss = 0.55126135\n",
      "Iteration 254, loss = 0.55062683\n",
      "Iteration 255, loss = 0.54926231\n",
      "Iteration 256, loss = 0.54842782\n",
      "Iteration 257, loss = 0.54759137\n",
      "Iteration 258, loss = 0.54754902\n",
      "Iteration 259, loss = 0.54843227\n",
      "Iteration 260, loss = 0.54942582\n",
      "Iteration 261, loss = 0.54922504\n",
      "Iteration 262, loss = 0.54770911\n",
      "Iteration 263, loss = 0.54690969\n",
      "Iteration 264, loss = 0.54606058\n",
      "Iteration 265, loss = 0.54694983\n",
      "Iteration 266, loss = 0.54745603\n",
      "Iteration 267, loss = 0.54767373\n",
      "Iteration 268, loss = 0.54645035\n",
      "Iteration 269, loss = 0.54512140\n",
      "Iteration 270, loss = 0.54562391\n",
      "Iteration 271, loss = 0.54946259\n",
      "Iteration 272, loss = 0.55285809\n",
      "Iteration 273, loss = 0.55117064\n",
      "Iteration 274, loss = 0.54787297\n",
      "Iteration 275, loss = 0.54502693\n",
      "Iteration 276, loss = 0.54543102\n",
      "Iteration 277, loss = 0.54617583\n",
      "Iteration 278, loss = 0.54705527\n",
      "Iteration 279, loss = 0.54690179\n",
      "Iteration 280, loss = 0.54627299\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.74063241\n",
      "Iteration 2, loss = 2.34519274\n",
      "Iteration 3, loss = 1.95145852\n",
      "Iteration 4, loss = 1.56315993\n",
      "Iteration 5, loss = 1.20124424\n",
      "Iteration 6, loss = 0.88863332\n",
      "Iteration 7, loss = 0.69322153\n",
      "Iteration 8, loss = 0.65131823\n",
      "Iteration 9, loss = 0.75189397\n",
      "Iteration 10, loss = 0.86334772\n",
      "Iteration 11, loss = 0.87089082\n",
      "Iteration 12, loss = 0.79095021\n",
      "Iteration 13, loss = 0.69044464\n",
      "Iteration 14, loss = 0.63010877\n",
      "Iteration 15, loss = 0.61180435\n",
      "Iteration 16, loss = 0.63218009\n",
      "Iteration 17, loss = 0.65370429\n",
      "Iteration 18, loss = 0.66054467\n",
      "Iteration 19, loss = 0.64737030\n",
      "Iteration 20, loss = 0.62294817\n",
      "Iteration 21, loss = 0.59454496\n",
      "Iteration 22, loss = 0.58177532\n",
      "Iteration 23, loss = 0.58750701\n",
      "Iteration 24, loss = 0.59518311\n",
      "Iteration 25, loss = 0.59582011\n",
      "Iteration 26, loss = 0.58761594\n",
      "Iteration 27, loss = 0.57443683\n",
      "Iteration 28, loss = 0.56526668\n",
      "Iteration 29, loss = 0.55902667\n",
      "Iteration 30, loss = 0.55708037\n",
      "Iteration 31, loss = 0.55658148\n",
      "Iteration 32, loss = 0.55407419\n",
      "Iteration 33, loss = 0.54997790\n",
      "Iteration 34, loss = 0.54715629\n",
      "Iteration 35, loss = 0.54536244\n",
      "Iteration 36, loss = 0.54470353\n",
      "Iteration 37, loss = 0.54287904\n",
      "Iteration 38, loss = 0.54149242\n",
      "Iteration 39, loss = 0.53895899\n",
      "Iteration 40, loss = 0.53873881\n",
      "Iteration 41, loss = 0.53979462\n",
      "Iteration 42, loss = 0.54037521\n",
      "Iteration 43, loss = 0.53943100\n",
      "Iteration 44, loss = 0.53690529\n",
      "Iteration 45, loss = 0.53452329\n",
      "Iteration 46, loss = 0.53277982\n",
      "Iteration 47, loss = 0.53183847\n",
      "Iteration 48, loss = 0.53111200\n",
      "Iteration 49, loss = 0.53128794\n",
      "Iteration 50, loss = 0.53009385\n",
      "Iteration 51, loss = 0.52966701\n",
      "Iteration 52, loss = 0.52908446\n",
      "Iteration 53, loss = 0.52872397\n",
      "Iteration 54, loss = 0.52803654\n",
      "Iteration 55, loss = 0.52845093\n",
      "Iteration 56, loss = 0.52687322\n",
      "Iteration 57, loss = 0.52606048\n",
      "Iteration 58, loss = 0.52648635\n",
      "Iteration 59, loss = 0.52650228\n",
      "Iteration 60, loss = 0.52540851\n",
      "Iteration 61, loss = 0.52502550\n",
      "Iteration 62, loss = 0.52470600\n",
      "Iteration 63, loss = 0.52550483\n",
      "Iteration 64, loss = 0.52474766\n",
      "Iteration 65, loss = 0.52386988\n",
      "Iteration 66, loss = 0.52316485\n",
      "Iteration 67, loss = 0.52305067\n",
      "Iteration 68, loss = 0.52262313\n",
      "Iteration 69, loss = 0.52262622\n",
      "Iteration 70, loss = 0.52261605\n",
      "Iteration 71, loss = 0.52185180\n",
      "Iteration 72, loss = 0.52176468\n",
      "Iteration 73, loss = 0.52254111\n",
      "Iteration 74, loss = 0.52144728\n",
      "Iteration 75, loss = 0.52151765\n",
      "Iteration 76, loss = 0.52194889\n",
      "Iteration 77, loss = 0.52041596\n",
      "Iteration 78, loss = 0.52090598\n",
      "Iteration 79, loss = 0.52424729\n",
      "Iteration 80, loss = 0.52434787\n",
      "Iteration 81, loss = 0.52167983\n",
      "Iteration 82, loss = 0.52012330\n",
      "Iteration 83, loss = 0.51994054\n",
      "Iteration 84, loss = 0.51956009\n",
      "Iteration 85, loss = 0.51953177\n",
      "Iteration 86, loss = 0.51938836\n",
      "Iteration 87, loss = 0.51948152\n",
      "Iteration 88, loss = 0.52174864\n",
      "Iteration 89, loss = 0.52700268\n",
      "Iteration 90, loss = 0.52868457\n",
      "Iteration 91, loss = 0.52692060\n",
      "Iteration 92, loss = 0.52088425\n",
      "Iteration 93, loss = 0.51813556\n",
      "Iteration 94, loss = 0.52217019\n",
      "Iteration 95, loss = 0.52179323\n",
      "Iteration 96, loss = 0.52033399\n",
      "Iteration 97, loss = 0.51795934\n",
      "Iteration 98, loss = 0.51782330\n",
      "Iteration 99, loss = 0.51717736\n",
      "Iteration 100, loss = 0.51682823\n",
      "Iteration 101, loss = 0.51675225\n",
      "Iteration 102, loss = 0.51719485\n",
      "Iteration 103, loss = 0.51857728\n",
      "Iteration 104, loss = 0.51833015\n",
      "Iteration 105, loss = 0.51722755\n",
      "Iteration 106, loss = 0.51606582\n",
      "Iteration 107, loss = 0.51578389\n",
      "Iteration 108, loss = 0.51623421\n",
      "Iteration 109, loss = 0.51791201\n",
      "Iteration 110, loss = 0.51640451\n",
      "Iteration 111, loss = 0.51478247\n",
      "Iteration 112, loss = 0.51792799\n",
      "Iteration 113, loss = 0.52016853\n",
      "Iteration 114, loss = 0.51654528\n",
      "Iteration 115, loss = 0.51763002\n",
      "Iteration 116, loss = 0.51756415\n",
      "Iteration 117, loss = 0.51851718\n",
      "Iteration 118, loss = 0.51693758\n",
      "Iteration 119, loss = 0.51493198\n",
      "Iteration 120, loss = 0.51557883\n",
      "Iteration 121, loss = 0.51509289\n",
      "Iteration 122, loss = 0.51407168\n",
      "Iteration 123, loss = 0.51412651\n",
      "Iteration 124, loss = 0.51569078\n",
      "Iteration 125, loss = 0.51326012\n",
      "Iteration 126, loss = 0.51792704\n",
      "Iteration 127, loss = 0.51782388\n",
      "Iteration 128, loss = 0.51419880\n",
      "Iteration 129, loss = 0.51313047\n",
      "Iteration 130, loss = 0.51498815\n",
      "Iteration 131, loss = 0.51524515\n",
      "Iteration 132, loss = 0.51230063\n",
      "Iteration 133, loss = 0.51320057\n",
      "Iteration 134, loss = 0.51548993\n",
      "Iteration 135, loss = 0.51362299\n",
      "Iteration 136, loss = 0.51118212\n",
      "Iteration 137, loss = 0.51421145\n",
      "Iteration 138, loss = 0.51478846\n",
      "Iteration 139, loss = 0.51271967\n",
      "Iteration 140, loss = 0.51130963\n",
      "Iteration 141, loss = 0.51105859\n",
      "Iteration 142, loss = 0.51126781\n",
      "Iteration 143, loss = 0.51434521\n",
      "Iteration 144, loss = 0.51259483\n",
      "Iteration 145, loss = 0.51151537\n",
      "Iteration 146, loss = 0.51219629\n",
      "Iteration 147, loss = 0.51187572\n",
      "Iteration 148, loss = 0.51085831\n",
      "Iteration 149, loss = 0.51043680\n",
      "Iteration 150, loss = 0.51191109\n",
      "Iteration 151, loss = 0.51388477\n",
      "Iteration 152, loss = 0.51375737\n",
      "Iteration 153, loss = 0.51006037\n",
      "Iteration 154, loss = 0.51046805\n",
      "Iteration 155, loss = 0.50990677\n",
      "Iteration 156, loss = 0.50925491\n",
      "Iteration 157, loss = 0.51057155\n",
      "Iteration 158, loss = 0.51190847\n",
      "Iteration 159, loss = 0.51042091\n",
      "Iteration 160, loss = 0.50848932\n",
      "Iteration 161, loss = 0.51317195\n",
      "Iteration 162, loss = 0.51747999\n",
      "Iteration 163, loss = 0.51375039\n",
      "Iteration 164, loss = 0.50941630\n",
      "Iteration 165, loss = 0.50965220\n",
      "Iteration 166, loss = 0.51167006\n",
      "Iteration 167, loss = 0.51186173\n",
      "Iteration 168, loss = 0.51118736\n",
      "Iteration 169, loss = 0.51058274\n",
      "Iteration 170, loss = 0.51051250\n",
      "Iteration 171, loss = 0.50846016\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 12.51799191\n",
      "Iteration 2, loss = 11.18224090\n",
      "Iteration 3, loss = 9.85368594\n",
      "Iteration 4, loss = 8.53492798\n",
      "Iteration 5, loss = 7.20069647\n",
      "Iteration 6, loss = 5.89609795\n",
      "Iteration 7, loss = 4.59930088\n",
      "Iteration 8, loss = 3.34131034\n",
      "Iteration 9, loss = 2.22513498\n",
      "Iteration 10, loss = 1.39890389\n",
      "Iteration 11, loss = 1.03363061\n",
      "Iteration 12, loss = 1.03448687\n",
      "Iteration 13, loss = 1.15669209\n",
      "Iteration 14, loss = 1.27956419\n",
      "Iteration 15, loss = 1.36141944\n",
      "Iteration 16, loss = 1.40388233\n",
      "Iteration 17, loss = 1.40226189\n",
      "Iteration 18, loss = 1.36146455\n",
      "Iteration 19, loss = 1.28655507\n",
      "Iteration 20, loss = 1.18816554\n",
      "Iteration 21, loss = 1.08587186\n",
      "Iteration 22, loss = 0.97178152\n",
      "Iteration 23, loss = 0.88241798\n",
      "Iteration 24, loss = 0.80668416\n",
      "Iteration 25, loss = 0.77007992\n",
      "Iteration 26, loss = 0.76238495\n",
      "Iteration 27, loss = 0.76576031\n",
      "Iteration 28, loss = 0.76933828\n",
      "Iteration 29, loss = 0.75046586\n",
      "Iteration 30, loss = 0.71732189\n",
      "Iteration 31, loss = 0.67980601\n",
      "Iteration 32, loss = 0.64685227\n",
      "Iteration 33, loss = 0.62299785\n",
      "Iteration 34, loss = 0.60750059\n",
      "Iteration 35, loss = 0.59910440\n",
      "Iteration 36, loss = 0.59097862\n",
      "Iteration 37, loss = 0.58020150\n",
      "Iteration 38, loss = 0.56748771\n",
      "Iteration 39, loss = 0.55571002\n",
      "Iteration 40, loss = 0.54631696\n",
      "Iteration 41, loss = 0.54196252\n",
      "Iteration 42, loss = 0.54140630\n",
      "Iteration 43, loss = 0.53997330\n",
      "Iteration 44, loss = 0.53739565\n",
      "Iteration 45, loss = 0.53301786\n",
      "Iteration 46, loss = 0.53022363\n",
      "Iteration 47, loss = 0.53020818\n",
      "Iteration 48, loss = 0.53006863\n",
      "Iteration 49, loss = 0.53048424\n",
      "Iteration 50, loss = 0.53070176\n",
      "Iteration 51, loss = 0.53082524\n",
      "Iteration 52, loss = 0.52903460\n",
      "Iteration 53, loss = 0.52697008\n",
      "Iteration 54, loss = 0.52700578\n",
      "Iteration 55, loss = 0.52723256\n",
      "Iteration 56, loss = 0.52841342\n",
      "Iteration 57, loss = 0.53074590\n",
      "Iteration 58, loss = 0.53132198\n",
      "Iteration 59, loss = 0.52949017\n",
      "Iteration 60, loss = 0.52705414\n",
      "Iteration 61, loss = 0.52563097\n",
      "Iteration 62, loss = 0.52524819\n",
      "Iteration 63, loss = 0.52556304\n",
      "Iteration 64, loss = 0.52731661\n",
      "Iteration 65, loss = 0.52751257\n",
      "Iteration 66, loss = 0.52546454\n",
      "Iteration 67, loss = 0.52511207\n",
      "Iteration 68, loss = 0.52378138\n",
      "Iteration 69, loss = 0.52525921\n",
      "Iteration 70, loss = 0.52534920\n",
      "Iteration 71, loss = 0.52434850\n",
      "Iteration 72, loss = 0.52389121\n",
      "Iteration 73, loss = 0.52267365\n",
      "Iteration 74, loss = 0.52258670\n",
      "Iteration 75, loss = 0.52227829\n",
      "Iteration 76, loss = 0.52216937\n",
      "Iteration 77, loss = 0.52185435\n",
      "Iteration 78, loss = 0.52221546\n",
      "Iteration 79, loss = 0.52260767\n",
      "Iteration 80, loss = 0.52276094\n",
      "Iteration 81, loss = 0.52278818\n",
      "Iteration 82, loss = 0.52265822\n",
      "Iteration 83, loss = 0.52332998\n",
      "Iteration 84, loss = 0.52392131\n",
      "Iteration 85, loss = 0.52416379\n",
      "Iteration 86, loss = 0.52454932\n",
      "Iteration 87, loss = 0.52356445\n",
      "Iteration 88, loss = 0.52203671\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.27511234\n",
      "Iteration 2, loss = 3.90592866\n",
      "Iteration 3, loss = 3.52243748\n",
      "Iteration 4, loss = 3.16776470\n",
      "Iteration 5, loss = 2.79824092\n",
      "Iteration 6, loss = 2.44065709\n",
      "Iteration 7, loss = 2.07500647\n",
      "Iteration 8, loss = 1.72363286\n",
      "Iteration 9, loss = 1.39721424\n",
      "Iteration 10, loss = 1.12364542\n",
      "Iteration 11, loss = 1.00577868\n",
      "Iteration 12, loss = 0.97980522\n",
      "Iteration 13, loss = 1.04299891\n",
      "Iteration 14, loss = 1.07364197\n",
      "Iteration 15, loss = 1.03410384\n",
      "Iteration 16, loss = 0.94646688\n",
      "Iteration 17, loss = 0.86076751\n",
      "Iteration 18, loss = 0.79841035\n",
      "Iteration 19, loss = 0.77327551\n",
      "Iteration 20, loss = 0.76796690\n",
      "Iteration 21, loss = 0.75937346\n",
      "Iteration 22, loss = 0.74080042\n",
      "Iteration 23, loss = 0.71204899\n",
      "Iteration 24, loss = 0.68142052\n",
      "Iteration 25, loss = 0.66074690\n",
      "Iteration 26, loss = 0.64360735\n",
      "Iteration 27, loss = 0.64116336\n",
      "Iteration 28, loss = 0.63276456\n",
      "Iteration 29, loss = 0.61966176\n",
      "Iteration 30, loss = 0.60848016\n",
      "Iteration 31, loss = 0.60047390\n",
      "Iteration 32, loss = 0.59427774\n",
      "Iteration 33, loss = 0.58953433\n",
      "Iteration 34, loss = 0.58570815\n",
      "Iteration 35, loss = 0.57946413\n",
      "Iteration 36, loss = 0.57556324\n",
      "Iteration 37, loss = 0.57271113\n",
      "Iteration 38, loss = 0.56937376\n",
      "Iteration 39, loss = 0.56649190\n",
      "Iteration 40, loss = 0.56598319\n",
      "Iteration 41, loss = 0.56354935\n",
      "Iteration 42, loss = 0.56174398\n",
      "Iteration 43, loss = 0.55933960\n",
      "Iteration 44, loss = 0.55792781\n",
      "Iteration 45, loss = 0.55666344\n",
      "Iteration 46, loss = 0.55564148\n",
      "Iteration 47, loss = 0.55447989\n",
      "Iteration 48, loss = 0.55327527\n",
      "Iteration 49, loss = 0.55242188\n",
      "Iteration 50, loss = 0.55090863\n",
      "Iteration 51, loss = 0.55011117\n",
      "Iteration 52, loss = 0.54904771\n",
      "Iteration 53, loss = 0.54811622\n",
      "Iteration 54, loss = 0.54706325\n",
      "Iteration 55, loss = 0.54594246\n",
      "Iteration 56, loss = 0.54498635\n",
      "Iteration 57, loss = 0.54412634\n",
      "Iteration 58, loss = 0.54344673\n",
      "Iteration 59, loss = 0.54231798\n",
      "Iteration 60, loss = 0.54123310\n",
      "Iteration 61, loss = 0.54039719\n",
      "Iteration 62, loss = 0.54019430\n",
      "Iteration 63, loss = 0.53912047\n",
      "Iteration 64, loss = 0.53846942\n",
      "Iteration 65, loss = 0.53763588\n",
      "Iteration 66, loss = 0.53675629\n",
      "Iteration 67, loss = 0.53650094\n",
      "Iteration 68, loss = 0.53655246\n",
      "Iteration 69, loss = 0.53644306\n",
      "Iteration 70, loss = 0.53648596\n",
      "Iteration 71, loss = 0.53538595\n",
      "Iteration 72, loss = 0.53460093\n",
      "Iteration 73, loss = 0.53283139\n",
      "Iteration 74, loss = 0.53451417\n",
      "Iteration 75, loss = 0.53564057\n",
      "Iteration 76, loss = 0.53453583\n",
      "Iteration 77, loss = 0.53170976\n",
      "Iteration 78, loss = 0.53092400\n",
      "Iteration 79, loss = 0.53382572\n",
      "Iteration 80, loss = 0.53577228\n",
      "Iteration 81, loss = 0.53463419\n",
      "Iteration 82, loss = 0.53215966\n",
      "Iteration 83, loss = 0.53043340\n",
      "Iteration 84, loss = 0.52896362\n",
      "Iteration 85, loss = 0.53169698\n",
      "Iteration 86, loss = 0.53401540\n",
      "Iteration 87, loss = 0.53379694\n",
      "Iteration 88, loss = 0.53235135\n",
      "Iteration 89, loss = 0.53002469\n",
      "Iteration 90, loss = 0.52771147\n",
      "Iteration 91, loss = 0.52739451\n",
      "Iteration 92, loss = 0.52743547\n",
      "Iteration 93, loss = 0.52723442\n",
      "Iteration 94, loss = 0.52656657\n",
      "Iteration 95, loss = 0.52573488\n",
      "Iteration 96, loss = 0.52548079\n",
      "Iteration 97, loss = 0.52656004\n",
      "Iteration 98, loss = 0.52605573\n",
      "Iteration 99, loss = 0.52443426\n",
      "Iteration 100, loss = 0.52458162\n",
      "Iteration 101, loss = 0.52791144\n",
      "Iteration 102, loss = 0.52390608\n",
      "Iteration 103, loss = 0.52402356\n",
      "Iteration 104, loss = 0.53192018\n",
      "Iteration 105, loss = 0.53914608\n",
      "Iteration 106, loss = 0.53575885\n",
      "Iteration 107, loss = 0.52425735\n",
      "Iteration 108, loss = 0.52419403\n",
      "Iteration 109, loss = 0.53134615\n",
      "Iteration 110, loss = 0.53683858\n",
      "Iteration 111, loss = 0.53530344\n",
      "Iteration 112, loss = 0.53028518\n",
      "Iteration 113, loss = 0.52281238\n",
      "Iteration 114, loss = 0.52248939\n",
      "Iteration 115, loss = 0.52283517\n",
      "Iteration 116, loss = 0.52153938\n",
      "Iteration 117, loss = 0.52352268\n",
      "Iteration 118, loss = 0.52303977\n",
      "Iteration 119, loss = 0.52112773\n",
      "Iteration 120, loss = 0.52034572\n",
      "Iteration 121, loss = 0.52102351\n",
      "Iteration 122, loss = 0.52086692\n",
      "Iteration 123, loss = 0.52022989\n",
      "Iteration 124, loss = 0.52015313\n",
      "Iteration 125, loss = 0.52103190\n",
      "Iteration 126, loss = 0.52061945\n",
      "Iteration 127, loss = 0.51919961\n",
      "Iteration 128, loss = 0.51976280\n",
      "Iteration 129, loss = 0.51979609\n",
      "Iteration 130, loss = 0.51952127\n",
      "Iteration 131, loss = 0.51864870\n",
      "Iteration 132, loss = 0.51847881\n",
      "Iteration 133, loss = 0.51912376\n",
      "Iteration 134, loss = 0.51994411\n",
      "Iteration 135, loss = 0.51950358\n",
      "Iteration 136, loss = 0.51815061\n",
      "Iteration 137, loss = 0.51781442\n",
      "Iteration 138, loss = 0.51751208\n",
      "Iteration 139, loss = 0.51747084\n",
      "Iteration 140, loss = 0.51749468\n",
      "Iteration 141, loss = 0.51684324\n",
      "Iteration 142, loss = 0.51689966\n",
      "Iteration 143, loss = 0.51745852\n",
      "Iteration 144, loss = 0.51825474\n",
      "Iteration 145, loss = 0.51801539\n",
      "Iteration 146, loss = 0.51752137\n",
      "Iteration 147, loss = 0.51611940\n",
      "Iteration 148, loss = 0.51600566\n",
      "Iteration 149, loss = 0.51607413\n",
      "Iteration 150, loss = 0.51573016\n",
      "Iteration 151, loss = 0.51676357\n",
      "Iteration 152, loss = 0.51880668\n",
      "Iteration 153, loss = 0.51792974\n",
      "Iteration 154, loss = 0.51535876\n",
      "Iteration 155, loss = 0.51638278\n",
      "Iteration 156, loss = 0.51615163\n",
      "Iteration 157, loss = 0.51548095\n",
      "Iteration 158, loss = 0.51466905\n",
      "Iteration 159, loss = 0.51481855\n",
      "Iteration 160, loss = 0.51729409\n",
      "Iteration 161, loss = 0.51703614\n",
      "Iteration 162, loss = 0.51578792\n",
      "Iteration 163, loss = 0.51459200\n",
      "Iteration 164, loss = 0.51488718\n",
      "Iteration 165, loss = 0.51534943\n",
      "Iteration 166, loss = 0.51713119\n",
      "Iteration 167, loss = 0.51743847\n",
      "Iteration 168, loss = 0.51500590\n",
      "Iteration 169, loss = 0.51382738\n",
      "Iteration 170, loss = 0.51400605\n",
      "Iteration 171, loss = 0.51649975\n",
      "Iteration 172, loss = 0.51458256\n",
      "Iteration 173, loss = 0.51533664\n",
      "Iteration 174, loss = 0.51432966\n",
      "Iteration 175, loss = 0.51338109\n",
      "Iteration 176, loss = 0.51474020\n",
      "Iteration 177, loss = 0.51435003\n",
      "Iteration 178, loss = 0.51336380\n",
      "Iteration 179, loss = 0.51329432\n",
      "Iteration 180, loss = 0.51304688\n",
      "Iteration 181, loss = 0.51580723\n",
      "Iteration 182, loss = 0.51577454\n",
      "Iteration 183, loss = 0.51274280\n",
      "Iteration 184, loss = 0.51346498\n",
      "Iteration 185, loss = 0.51703423\n",
      "Iteration 186, loss = 0.51906336\n",
      "Iteration 187, loss = 0.51415675\n",
      "Iteration 188, loss = 0.51310599\n",
      "Iteration 189, loss = 0.51939802\n",
      "Iteration 190, loss = 0.51886297\n",
      "Iteration 191, loss = 0.51201486\n",
      "Iteration 192, loss = 0.51674927\n",
      "Iteration 193, loss = 0.51839765\n",
      "Iteration 194, loss = 0.51460652\n",
      "Iteration 195, loss = 0.51149028\n",
      "Iteration 196, loss = 0.51386026\n",
      "Iteration 197, loss = 0.51539579\n",
      "Iteration 198, loss = 0.51190400\n",
      "Iteration 199, loss = 0.51346986\n",
      "Iteration 200, loss = 0.51493358\n",
      "Iteration 201, loss = 0.51443994\n",
      "Iteration 202, loss = 0.51443005\n",
      "Iteration 203, loss = 0.51208553\n",
      "Iteration 204, loss = 0.51121201\n",
      "Iteration 205, loss = 0.51230730\n",
      "Iteration 206, loss = 0.51300083\n",
      "Iteration 207, loss = 0.51096112\n",
      "Iteration 208, loss = 0.51170579\n",
      "Iteration 209, loss = 0.51818824\n",
      "Iteration 210, loss = 0.51332220\n",
      "Iteration 211, loss = 0.51244794\n",
      "Iteration 212, loss = 0.52058230\n",
      "Iteration 213, loss = 0.51971828\n",
      "Iteration 214, loss = 0.51170597\n",
      "Iteration 215, loss = 0.51220702\n",
      "Iteration 216, loss = 0.51391407\n",
      "Iteration 217, loss = 0.51190183\n",
      "Iteration 218, loss = 0.51116928\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.35541121\n",
      "Iteration 2, loss = 6.86506170\n",
      "Iteration 3, loss = 6.35699955\n",
      "Iteration 4, loss = 5.86928232\n",
      "Iteration 5, loss = 5.37181357\n",
      "Iteration 6, loss = 4.87166903\n",
      "Iteration 7, loss = 4.37217072\n",
      "Iteration 8, loss = 3.87525952\n",
      "Iteration 9, loss = 3.38237934\n",
      "Iteration 10, loss = 2.88571349\n",
      "Iteration 11, loss = 2.39342427\n",
      "Iteration 12, loss = 1.92533227\n",
      "Iteration 13, loss = 1.45102847\n",
      "Iteration 14, loss = 1.11680037\n",
      "Iteration 15, loss = 1.04735505\n",
      "Iteration 16, loss = 1.27085594\n",
      "Iteration 17, loss = 1.41051738\n",
      "Iteration 18, loss = 1.35076951\n",
      "Iteration 19, loss = 1.16393637\n",
      "Iteration 20, loss = 0.98797675\n",
      "Iteration 21, loss = 0.91857215\n",
      "Iteration 22, loss = 0.92893722\n",
      "Iteration 23, loss = 0.95876677\n",
      "Iteration 24, loss = 0.96478881\n",
      "Iteration 25, loss = 0.94071432\n",
      "Iteration 26, loss = 0.88827115\n",
      "Iteration 27, loss = 0.83702964\n",
      "Iteration 28, loss = 0.79603824\n",
      "Iteration 29, loss = 0.78276023\n",
      "Iteration 30, loss = 0.79408465\n",
      "Iteration 31, loss = 0.79817978\n",
      "Iteration 32, loss = 0.77237107\n",
      "Iteration 33, loss = 0.73352243\n",
      "Iteration 34, loss = 0.70845158\n",
      "Iteration 35, loss = 0.70049789\n",
      "Iteration 36, loss = 0.69681138\n",
      "Iteration 37, loss = 0.68824693\n",
      "Iteration 38, loss = 0.67357610\n",
      "Iteration 39, loss = 0.65588149\n",
      "Iteration 40, loss = 0.64146517\n",
      "Iteration 41, loss = 0.63065387\n",
      "Iteration 42, loss = 0.62073334\n",
      "Iteration 43, loss = 0.61232387\n",
      "Iteration 44, loss = 0.60469984\n",
      "Iteration 45, loss = 0.59772149\n",
      "Iteration 46, loss = 0.59126680\n",
      "Iteration 47, loss = 0.58474739\n",
      "Iteration 48, loss = 0.58052849\n",
      "Iteration 49, loss = 0.57685423\n",
      "Iteration 50, loss = 0.57170030\n",
      "Iteration 51, loss = 0.56683001\n",
      "Iteration 52, loss = 0.56423377\n",
      "Iteration 53, loss = 0.56034365\n",
      "Iteration 54, loss = 0.55834410\n",
      "Iteration 55, loss = 0.55582492\n",
      "Iteration 56, loss = 0.55435493\n",
      "Iteration 57, loss = 0.55297929\n",
      "Iteration 58, loss = 0.55200134\n",
      "Iteration 59, loss = 0.55078556\n",
      "Iteration 60, loss = 0.55172379\n",
      "Iteration 61, loss = 0.55206530\n",
      "Iteration 62, loss = 0.55051913\n",
      "Iteration 63, loss = 0.54758665\n",
      "Iteration 64, loss = 0.54749790\n",
      "Iteration 65, loss = 0.55021585\n",
      "Iteration 66, loss = 0.55280612\n",
      "Iteration 67, loss = 0.54871084\n",
      "Iteration 68, loss = 0.54418764\n",
      "Iteration 69, loss = 0.54851579\n",
      "Iteration 70, loss = 0.55427344\n",
      "Iteration 71, loss = 0.55518757\n",
      "Iteration 72, loss = 0.55013228\n",
      "Iteration 73, loss = 0.54367224\n",
      "Iteration 74, loss = 0.54379577\n",
      "Iteration 75, loss = 0.54914433\n",
      "Iteration 76, loss = 0.55001367\n",
      "Iteration 77, loss = 0.54409478\n",
      "Iteration 78, loss = 0.54108474\n",
      "Iteration 79, loss = 0.54711153\n",
      "Iteration 80, loss = 0.55341882\n",
      "Iteration 81, loss = 0.55312103\n",
      "Iteration 82, loss = 0.54617546\n",
      "Iteration 83, loss = 0.54017721\n",
      "Iteration 84, loss = 0.54698948\n",
      "Iteration 85, loss = 0.55040430\n",
      "Iteration 86, loss = 0.54602670\n",
      "Iteration 87, loss = 0.54162132\n",
      "Iteration 88, loss = 0.54031938\n",
      "Iteration 89, loss = 0.54392501\n",
      "Iteration 90, loss = 0.54317367\n",
      "Iteration 91, loss = 0.54147831\n",
      "Iteration 92, loss = 0.53986225\n",
      "Iteration 93, loss = 0.54076496\n",
      "Iteration 94, loss = 0.54238530\n",
      "Iteration 95, loss = 0.54251905\n",
      "Iteration 96, loss = 0.54061999\n",
      "Iteration 97, loss = 0.54044716\n",
      "Iteration 98, loss = 0.53974239\n",
      "Iteration 99, loss = 0.53908220\n",
      "Iteration 100, loss = 0.53842870\n",
      "Iteration 101, loss = 0.53900110\n",
      "Iteration 102, loss = 0.53916425\n",
      "Iteration 103, loss = 0.53840854\n",
      "Iteration 104, loss = 0.53849721\n",
      "Iteration 105, loss = 0.53819148\n",
      "Iteration 106, loss = 0.53949772\n",
      "Iteration 107, loss = 0.54063718\n",
      "Iteration 108, loss = 0.54067526\n",
      "Iteration 109, loss = 0.53802598\n",
      "Iteration 110, loss = 0.53851299\n",
      "Iteration 111, loss = 0.53868761\n",
      "Iteration 112, loss = 0.53878122\n",
      "Iteration 113, loss = 0.53782656\n",
      "Iteration 114, loss = 0.53648263\n",
      "Iteration 115, loss = 0.53695332\n",
      "Iteration 116, loss = 0.53816647\n",
      "Iteration 117, loss = 0.53823600\n",
      "Iteration 118, loss = 0.53690524\n",
      "Iteration 119, loss = 0.53572328\n",
      "Iteration 120, loss = 0.53736322\n",
      "Iteration 121, loss = 0.53676377\n",
      "Iteration 122, loss = 0.53516454\n",
      "Iteration 123, loss = 0.53473368\n",
      "Iteration 124, loss = 0.53446048\n",
      "Iteration 125, loss = 0.53412258\n",
      "Iteration 126, loss = 0.53415833\n",
      "Iteration 127, loss = 0.53372393\n",
      "Iteration 128, loss = 0.53346403\n",
      "Iteration 129, loss = 0.53407484\n",
      "Iteration 130, loss = 0.53552754\n",
      "Iteration 131, loss = 0.53396750\n",
      "Iteration 132, loss = 0.53220571\n",
      "Iteration 133, loss = 0.53365345\n",
      "Iteration 134, loss = 0.53370146\n",
      "Iteration 135, loss = 0.53232176\n",
      "Iteration 136, loss = 0.53120214\n",
      "Iteration 137, loss = 0.53149832\n",
      "Iteration 138, loss = 0.53119820\n",
      "Iteration 139, loss = 0.53087657\n",
      "Iteration 140, loss = 0.53060765\n",
      "Iteration 141, loss = 0.53029112\n",
      "Iteration 142, loss = 0.53030600\n",
      "Iteration 143, loss = 0.53008917\n",
      "Iteration 144, loss = 0.53054111\n",
      "Iteration 145, loss = 0.52988149\n",
      "Iteration 146, loss = 0.52951513\n",
      "Iteration 147, loss = 0.52968366\n",
      "Iteration 148, loss = 0.52939126\n",
      "Iteration 149, loss = 0.52941591\n",
      "Iteration 150, loss = 0.52914014\n",
      "Iteration 151, loss = 0.52921901\n",
      "Iteration 152, loss = 0.53186446\n",
      "Iteration 153, loss = 0.53429598\n",
      "Iteration 154, loss = 0.53272983\n",
      "Iteration 155, loss = 0.52844834\n",
      "Iteration 156, loss = 0.52827051\n",
      "Iteration 157, loss = 0.52814148\n",
      "Iteration 158, loss = 0.52744948\n",
      "Iteration 159, loss = 0.52805357\n",
      "Iteration 160, loss = 0.53063500\n",
      "Iteration 161, loss = 0.53112179\n",
      "Iteration 162, loss = 0.52826223\n",
      "Iteration 163, loss = 0.52780608\n",
      "Iteration 164, loss = 0.52712094\n",
      "Iteration 165, loss = 0.52663889\n",
      "Iteration 166, loss = 0.52741453\n",
      "Iteration 167, loss = 0.52941927\n",
      "Iteration 168, loss = 0.53172986\n",
      "Iteration 169, loss = 0.52828799\n",
      "Iteration 170, loss = 0.52736616\n",
      "Iteration 171, loss = 0.52905377\n",
      "Iteration 172, loss = 0.52869467\n",
      "Iteration 173, loss = 0.52636400\n",
      "Iteration 174, loss = 0.52551617\n",
      "Iteration 175, loss = 0.52948784\n",
      "Iteration 176, loss = 0.53027682\n",
      "Iteration 177, loss = 0.52626999\n",
      "Iteration 178, loss = 0.52645056\n",
      "Iteration 179, loss = 0.53085236\n",
      "Iteration 180, loss = 0.52550613\n",
      "Iteration 181, loss = 0.52568362\n",
      "Iteration 182, loss = 0.53693737\n",
      "Iteration 183, loss = 0.54086487\n",
      "Iteration 184, loss = 0.53383512\n",
      "Iteration 185, loss = 0.52614127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75815829\n",
      "Iteration 2, loss = 0.55501700\n",
      "Iteration 3, loss = 0.60433475\n",
      "Iteration 4, loss = 0.64155784\n",
      "Iteration 5, loss = 0.60910457\n",
      "Iteration 6, loss = 0.55378794\n",
      "Iteration 7, loss = 0.53033717\n",
      "Iteration 8, loss = 0.55493241\n",
      "Iteration 9, loss = 0.56079245\n",
      "Iteration 10, loss = 0.53572097\n",
      "Iteration 11, loss = 0.51920229\n",
      "Iteration 12, loss = 0.52767716\n",
      "Iteration 13, loss = 0.54210261\n",
      "Iteration 14, loss = 0.53871740\n",
      "Iteration 15, loss = 0.52074732\n",
      "Iteration 16, loss = 0.51203655\n",
      "Iteration 17, loss = 0.53063746\n",
      "Iteration 18, loss = 0.54001861\n",
      "Iteration 19, loss = 0.52703454\n",
      "Iteration 20, loss = 0.51073164\n",
      "Iteration 21, loss = 0.51394793\n",
      "Iteration 22, loss = 0.52345920\n",
      "Iteration 23, loss = 0.52567894\n",
      "Iteration 24, loss = 0.51575134\n",
      "Iteration 25, loss = 0.50990148\n",
      "Iteration 26, loss = 0.51287599\n",
      "Iteration 27, loss = 0.51459501\n",
      "Iteration 28, loss = 0.50953343\n",
      "Iteration 29, loss = 0.50682443\n",
      "Iteration 30, loss = 0.50832353\n",
      "Iteration 31, loss = 0.51124247\n",
      "Iteration 32, loss = 0.50913087\n",
      "Iteration 33, loss = 0.50493733\n",
      "Iteration 34, loss = 0.50615963\n",
      "Iteration 35, loss = 0.50613191\n",
      "Iteration 36, loss = 0.50438673\n",
      "Iteration 37, loss = 0.50394813\n",
      "Iteration 38, loss = 0.50537421\n",
      "Iteration 39, loss = 0.50738310\n",
      "Iteration 40, loss = 0.50721905\n",
      "Iteration 41, loss = 0.50398907\n",
      "Iteration 42, loss = 0.50238728\n",
      "Iteration 43, loss = 0.50457651\n",
      "Iteration 44, loss = 0.50764753\n",
      "Iteration 45, loss = 0.50582468\n",
      "Iteration 46, loss = 0.50293251\n",
      "Iteration 47, loss = 0.50246067\n",
      "Iteration 48, loss = 0.50258934\n",
      "Iteration 49, loss = 0.50222621\n",
      "Iteration 50, loss = 0.50148187\n",
      "Iteration 51, loss = 0.50133962\n",
      "Iteration 52, loss = 0.50262965\n",
      "Iteration 53, loss = 0.50472829\n",
      "Iteration 54, loss = 0.50442294\n",
      "Iteration 55, loss = 0.50186124\n",
      "Iteration 56, loss = 0.50160017\n",
      "Iteration 57, loss = 0.50393430\n",
      "Iteration 58, loss = 0.50400793\n",
      "Iteration 59, loss = 0.50141662\n",
      "Iteration 60, loss = 0.50160729\n",
      "Iteration 61, loss = 0.50133971\n",
      "Iteration 62, loss = 0.49902883\n",
      "Iteration 63, loss = 0.49985215\n",
      "Iteration 64, loss = 0.50535379\n",
      "Iteration 65, loss = 0.50286662\n",
      "Iteration 66, loss = 0.49968625\n",
      "Iteration 67, loss = 0.50484731\n",
      "Iteration 68, loss = 0.50674269\n",
      "Iteration 69, loss = 0.50166533\n",
      "Iteration 70, loss = 0.49851086\n",
      "Iteration 71, loss = 0.50215743\n",
      "Iteration 72, loss = 0.50473178\n",
      "Iteration 73, loss = 0.50052354\n",
      "Iteration 74, loss = 0.49814185\n",
      "Iteration 75, loss = 0.50216982\n",
      "Iteration 76, loss = 0.50146555\n",
      "Iteration 77, loss = 0.49664387\n",
      "Iteration 78, loss = 0.50074341\n",
      "Iteration 79, loss = 0.50810853\n",
      "Iteration 80, loss = 0.50838694\n",
      "Iteration 81, loss = 0.49690773\n",
      "Iteration 82, loss = 0.50599445\n",
      "Iteration 83, loss = 0.51639129\n",
      "Iteration 84, loss = 0.50270459\n",
      "Iteration 85, loss = 0.49487642\n",
      "Iteration 86, loss = 0.51147962\n",
      "Iteration 87, loss = 0.52600370\n",
      "Iteration 88, loss = 0.51261156\n",
      "Iteration 89, loss = 0.49753504\n",
      "Iteration 90, loss = 0.50226935\n",
      "Iteration 91, loss = 0.50760075\n",
      "Iteration 92, loss = 0.49995854\n",
      "Iteration 93, loss = 0.49912954\n",
      "Iteration 94, loss = 0.49791577\n",
      "Iteration 95, loss = 0.49571061\n",
      "Iteration 96, loss = 0.49525736\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 12.49721896\n",
      "Iteration 2, loss = 11.32032144\n",
      "Iteration 3, loss = 10.13601845\n",
      "Iteration 4, loss = 8.98209371\n",
      "Iteration 5, loss = 7.78825143\n",
      "Iteration 6, loss = 6.62541066\n",
      "Iteration 7, loss = 5.44867349\n",
      "Iteration 8, loss = 4.28210758\n",
      "Iteration 9, loss = 3.14849007\n",
      "Iteration 10, loss = 2.11364589\n",
      "Iteration 11, loss = 1.26454639\n",
      "Iteration 12, loss = 0.85856685\n",
      "Iteration 13, loss = 0.84216996\n",
      "Iteration 14, loss = 1.01446682\n",
      "Iteration 15, loss = 1.17069991\n",
      "Iteration 16, loss = 1.26949219\n",
      "Iteration 17, loss = 1.31376303\n",
      "Iteration 18, loss = 1.31278273\n",
      "Iteration 19, loss = 1.27390847\n",
      "Iteration 20, loss = 1.19880240\n",
      "Iteration 21, loss = 1.10073715\n",
      "Iteration 22, loss = 0.99234500\n",
      "Iteration 23, loss = 0.87176429\n",
      "Iteration 24, loss = 0.77315529\n",
      "Iteration 25, loss = 0.70648671\n",
      "Iteration 26, loss = 0.68850474\n",
      "Iteration 27, loss = 0.70780275\n",
      "Iteration 28, loss = 0.74303375\n",
      "Iteration 29, loss = 0.75262924\n",
      "Iteration 30, loss = 0.72876237\n",
      "Iteration 31, loss = 0.68624578\n",
      "Iteration 32, loss = 0.64692820\n",
      "Iteration 33, loss = 0.62391771\n",
      "Iteration 34, loss = 0.61801730\n",
      "Iteration 35, loss = 0.62007880\n",
      "Iteration 36, loss = 0.62153734\n",
      "Iteration 37, loss = 0.61882710\n",
      "Iteration 38, loss = 0.61075270\n",
      "Iteration 39, loss = 0.59847803\n",
      "Iteration 40, loss = 0.58348998\n",
      "Iteration 41, loss = 0.57482289\n",
      "Iteration 42, loss = 0.56871686\n",
      "Iteration 43, loss = 0.57131488\n",
      "Iteration 44, loss = 0.57479052\n",
      "Iteration 45, loss = 0.57329462\n",
      "Iteration 46, loss = 0.56752999\n",
      "Iteration 47, loss = 0.55978288\n",
      "Iteration 48, loss = 0.55468533\n",
      "Iteration 49, loss = 0.55384689\n",
      "Iteration 50, loss = 0.55539509\n",
      "Iteration 51, loss = 0.55735042\n",
      "Iteration 52, loss = 0.55699633\n",
      "Iteration 53, loss = 0.55484095\n",
      "Iteration 54, loss = 0.55215976\n",
      "Iteration 55, loss = 0.55015467\n",
      "Iteration 56, loss = 0.54952874\n",
      "Iteration 57, loss = 0.54924688\n",
      "Iteration 58, loss = 0.55015893\n",
      "Iteration 59, loss = 0.55188796\n",
      "Iteration 60, loss = 0.55120372\n",
      "Iteration 61, loss = 0.54928182\n",
      "Iteration 62, loss = 0.54807911\n",
      "Iteration 63, loss = 0.54783128\n",
      "Iteration 64, loss = 0.54821299\n",
      "Iteration 65, loss = 0.54933332\n",
      "Iteration 66, loss = 0.55073578\n",
      "Iteration 67, loss = 0.55128010\n",
      "Iteration 68, loss = 0.54995176\n",
      "Iteration 69, loss = 0.54728704\n",
      "Iteration 70, loss = 0.54615499\n",
      "Iteration 71, loss = 0.54675927\n",
      "Iteration 72, loss = 0.54711747\n",
      "Iteration 73, loss = 0.54656821\n",
      "Iteration 74, loss = 0.54608253\n",
      "Iteration 75, loss = 0.54547387\n",
      "Iteration 76, loss = 0.54530014\n",
      "Iteration 77, loss = 0.54531106\n",
      "Iteration 78, loss = 0.54500133\n",
      "Iteration 79, loss = 0.54486798\n",
      "Iteration 80, loss = 0.54471824\n",
      "Iteration 81, loss = 0.54505869\n",
      "Iteration 82, loss = 0.54544826\n",
      "Iteration 83, loss = 0.54538837\n",
      "Iteration 84, loss = 0.54454879\n",
      "Iteration 85, loss = 0.54431553\n",
      "Iteration 86, loss = 0.54421256\n",
      "Iteration 87, loss = 0.54380287\n",
      "Iteration 88, loss = 0.54338572\n",
      "Iteration 89, loss = 0.54271685\n",
      "Iteration 90, loss = 0.54248004\n",
      "Iteration 91, loss = 0.54238917\n",
      "Iteration 92, loss = 0.54242085\n",
      "Iteration 93, loss = 0.54240096\n",
      "Iteration 94, loss = 0.54223121\n",
      "Iteration 95, loss = 0.54153035\n",
      "Iteration 96, loss = 0.54194016\n",
      "Iteration 97, loss = 0.54157657\n",
      "Iteration 98, loss = 0.54181172\n",
      "Iteration 99, loss = 0.54175387\n",
      "Iteration 100, loss = 0.54164183\n",
      "Iteration 101, loss = 0.54111048\n",
      "Iteration 102, loss = 0.54121990\n",
      "Iteration 103, loss = 0.54035214\n",
      "Iteration 104, loss = 0.54100245\n",
      "Iteration 105, loss = 0.54010087\n",
      "Iteration 106, loss = 0.53951837\n",
      "Iteration 107, loss = 0.54044354\n",
      "Iteration 108, loss = 0.54182242\n",
      "Iteration 109, loss = 0.54172744\n",
      "Iteration 110, loss = 0.54046575\n",
      "Iteration 111, loss = 0.53947406\n",
      "Iteration 112, loss = 0.53887132\n",
      "Iteration 113, loss = 0.53880907\n",
      "Iteration 114, loss = 0.54014105\n",
      "Iteration 115, loss = 0.54028034\n",
      "Iteration 116, loss = 0.53962386\n",
      "Iteration 117, loss = 0.53894096\n",
      "Iteration 118, loss = 0.53810725\n",
      "Iteration 119, loss = 0.53824398\n",
      "Iteration 120, loss = 0.53778297\n",
      "Iteration 121, loss = 0.53796854\n",
      "Iteration 122, loss = 0.53758839\n",
      "Iteration 123, loss = 0.53766738\n",
      "Iteration 124, loss = 0.53790778\n",
      "Iteration 125, loss = 0.53749168\n",
      "Iteration 126, loss = 0.53724577\n",
      "Iteration 127, loss = 0.53703726\n",
      "Iteration 128, loss = 0.53689122\n",
      "Iteration 129, loss = 0.53675196\n",
      "Iteration 130, loss = 0.53658270\n",
      "Iteration 131, loss = 0.53705469\n",
      "Iteration 132, loss = 0.53659307\n",
      "Iteration 133, loss = 0.53630722\n",
      "Iteration 134, loss = 0.53689645\n",
      "Iteration 135, loss = 0.53704692\n",
      "Iteration 136, loss = 0.53627084\n",
      "Iteration 137, loss = 0.53528824\n",
      "Iteration 138, loss = 0.53749966\n",
      "Iteration 139, loss = 0.53711120\n",
      "Iteration 140, loss = 0.53630427\n",
      "Iteration 141, loss = 0.53512338\n",
      "Iteration 142, loss = 0.53497716\n",
      "Iteration 143, loss = 0.53479345\n",
      "Iteration 144, loss = 0.53504388\n",
      "Iteration 145, loss = 0.53540174\n",
      "Iteration 146, loss = 0.53471004\n",
      "Iteration 147, loss = 0.53343471\n",
      "Iteration 148, loss = 0.53300757\n",
      "Iteration 149, loss = 0.53319137\n",
      "Iteration 150, loss = 0.53347686\n",
      "Iteration 151, loss = 0.53368672\n",
      "Iteration 152, loss = 0.53408569\n",
      "Iteration 153, loss = 0.53358291\n",
      "Iteration 154, loss = 0.53197729\n",
      "Iteration 155, loss = 0.53129561\n",
      "Iteration 156, loss = 0.53151540\n",
      "Iteration 157, loss = 0.53132285\n",
      "Iteration 158, loss = 0.53112874\n",
      "Iteration 159, loss = 0.53063752\n",
      "Iteration 160, loss = 0.53042986\n",
      "Iteration 161, loss = 0.53040219\n",
      "Iteration 162, loss = 0.53035761\n",
      "Iteration 163, loss = 0.53024231\n",
      "Iteration 164, loss = 0.53024295\n",
      "Iteration 165, loss = 0.53000908\n",
      "Iteration 166, loss = 0.53022125\n",
      "Iteration 167, loss = 0.52998919\n",
      "Iteration 168, loss = 0.53127951\n",
      "Iteration 169, loss = 0.53174912\n",
      "Iteration 170, loss = 0.53092231\n",
      "Iteration 171, loss = 0.52980957\n",
      "Iteration 172, loss = 0.52925324\n",
      "Iteration 173, loss = 0.52915918\n",
      "Iteration 174, loss = 0.52985379\n",
      "Iteration 175, loss = 0.53085508\n",
      "Iteration 176, loss = 0.53153522\n",
      "Iteration 177, loss = 0.53094291\n",
      "Iteration 178, loss = 0.53163961\n",
      "Iteration 179, loss = 0.53091091\n",
      "Iteration 180, loss = 0.52992691\n",
      "Iteration 181, loss = 0.52874367\n",
      "Iteration 182, loss = 0.52855642\n",
      "Iteration 183, loss = 0.52848594\n",
      "Iteration 184, loss = 0.52882598\n",
      "Iteration 185, loss = 0.52817256\n",
      "Iteration 186, loss = 0.52813084\n",
      "Iteration 187, loss = 0.53045055\n",
      "Iteration 188, loss = 0.53223300\n",
      "Iteration 189, loss = 0.53233185\n",
      "Iteration 190, loss = 0.52990099\n",
      "Iteration 191, loss = 0.52961709\n",
      "Iteration 192, loss = 0.52856499\n",
      "Iteration 193, loss = 0.52800145\n",
      "Iteration 194, loss = 0.52962231\n",
      "Iteration 195, loss = 0.52987085\n",
      "Iteration 196, loss = 0.52894441\n",
      "Iteration 197, loss = 0.52809262\n",
      "Iteration 198, loss = 0.52766928\n",
      "Iteration 199, loss = 0.52790505\n",
      "Iteration 200, loss = 0.52810004\n",
      "Iteration 201, loss = 0.52813745\n",
      "Iteration 202, loss = 0.52784236\n",
      "Iteration 203, loss = 0.52763458\n",
      "Iteration 204, loss = 0.52765509\n",
      "Iteration 205, loss = 0.52773322\n",
      "Iteration 206, loss = 0.52777892\n",
      "Iteration 207, loss = 0.52744172\n",
      "Iteration 208, loss = 0.52842843\n",
      "Iteration 209, loss = 0.52716153\n",
      "Iteration 210, loss = 0.52742118\n",
      "Iteration 211, loss = 0.52844597\n",
      "Iteration 212, loss = 0.52871835\n",
      "Iteration 213, loss = 0.52817616\n",
      "Iteration 214, loss = 0.52695621\n",
      "Iteration 215, loss = 0.52654943\n",
      "Iteration 216, loss = 0.52662134\n",
      "Iteration 217, loss = 0.52813654\n",
      "Iteration 218, loss = 0.52876016\n",
      "Iteration 219, loss = 0.52797401\n",
      "Iteration 220, loss = 0.52739881\n",
      "Iteration 221, loss = 0.52754304\n",
      "Iteration 222, loss = 0.52617551\n",
      "Iteration 223, loss = 0.52647611\n",
      "Iteration 224, loss = 0.52632606\n",
      "Iteration 225, loss = 0.52647405\n",
      "Iteration 226, loss = 0.52662466\n",
      "Iteration 227, loss = 0.52599216\n",
      "Iteration 228, loss = 0.52542253\n",
      "Iteration 229, loss = 0.52602841\n",
      "Iteration 230, loss = 0.52908690\n",
      "Iteration 231, loss = 0.53064546\n",
      "Iteration 232, loss = 0.52823337\n",
      "Iteration 233, loss = 0.52655655\n",
      "Iteration 234, loss = 0.52594738\n",
      "Iteration 235, loss = 0.52661225\n",
      "Iteration 236, loss = 0.52689707\n",
      "Iteration 237, loss = 0.52665930\n",
      "Iteration 238, loss = 0.52740600\n",
      "Iteration 239, loss = 0.52780592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 8.31906835\n",
      "Iteration 2, loss = 6.57915541\n",
      "Iteration 3, loss = 4.87719323\n",
      "Iteration 4, loss = 3.16988442\n",
      "Iteration 5, loss = 1.61796339\n",
      "Iteration 6, loss = 0.71835421\n",
      "Iteration 7, loss = 0.71206047\n",
      "Iteration 8, loss = 0.96139040\n",
      "Iteration 9, loss = 1.16553237\n",
      "Iteration 10, loss = 1.30583300\n",
      "Iteration 11, loss = 1.36903463\n",
      "Iteration 12, loss = 1.36493287\n",
      "Iteration 13, loss = 1.30490705\n",
      "Iteration 14, loss = 1.19632247\n",
      "Iteration 15, loss = 1.06236057\n",
      "Iteration 16, loss = 0.90177758\n",
      "Iteration 17, loss = 0.76965399\n",
      "Iteration 18, loss = 0.66988508\n",
      "Iteration 19, loss = 0.63324700\n",
      "Iteration 20, loss = 0.65784208\n",
      "Iteration 21, loss = 0.69786114\n",
      "Iteration 22, loss = 0.70458430\n",
      "Iteration 23, loss = 0.67407657\n",
      "Iteration 24, loss = 0.63346161\n",
      "Iteration 25, loss = 0.60751670\n",
      "Iteration 26, loss = 0.59872376\n",
      "Iteration 27, loss = 0.60345598\n",
      "Iteration 28, loss = 0.61022481\n",
      "Iteration 29, loss = 0.61120061\n",
      "Iteration 30, loss = 0.60583027\n",
      "Iteration 31, loss = 0.59428170\n",
      "Iteration 32, loss = 0.57993184\n",
      "Iteration 33, loss = 0.57050442\n",
      "Iteration 34, loss = 0.57924719\n",
      "Iteration 35, loss = 0.58326335\n",
      "Iteration 36, loss = 0.57660091\n",
      "Iteration 37, loss = 0.56514053\n",
      "Iteration 38, loss = 0.56818374\n",
      "Iteration 39, loss = 0.57315268\n",
      "Iteration 40, loss = 0.57214263\n",
      "Iteration 41, loss = 0.56593507\n",
      "Iteration 42, loss = 0.56150358\n",
      "Iteration 43, loss = 0.55843870\n",
      "Iteration 44, loss = 0.56132023\n",
      "Iteration 45, loss = 0.56203392\n",
      "Iteration 46, loss = 0.56079758\n",
      "Iteration 47, loss = 0.55831095\n",
      "Iteration 48, loss = 0.55716254\n",
      "Iteration 49, loss = 0.55388816\n",
      "Iteration 50, loss = 0.55294031\n",
      "Iteration 51, loss = 0.55587855\n",
      "Iteration 52, loss = 0.55722793\n",
      "Iteration 53, loss = 0.55600479\n",
      "Iteration 54, loss = 0.55365155\n",
      "Iteration 55, loss = 0.55186684\n",
      "Iteration 56, loss = 0.54921466\n",
      "Iteration 57, loss = 0.54827536\n",
      "Iteration 58, loss = 0.54757167\n",
      "Iteration 59, loss = 0.54803882\n",
      "Iteration 60, loss = 0.54882828\n",
      "Iteration 61, loss = 0.54927421\n",
      "Iteration 62, loss = 0.54838453\n",
      "Iteration 63, loss = 0.54630014\n",
      "Iteration 64, loss = 0.54435614\n",
      "Iteration 65, loss = 0.54385966\n",
      "Iteration 66, loss = 0.54336052\n",
      "Iteration 67, loss = 0.54308135\n",
      "Iteration 68, loss = 0.54282046\n",
      "Iteration 69, loss = 0.54282217\n",
      "Iteration 70, loss = 0.54427484\n",
      "Iteration 71, loss = 0.54525136\n",
      "Iteration 72, loss = 0.54310618\n",
      "Iteration 73, loss = 0.53999401\n",
      "Iteration 74, loss = 0.54119796\n",
      "Iteration 75, loss = 0.54655875\n",
      "Iteration 76, loss = 0.54977710\n",
      "Iteration 77, loss = 0.54333746\n",
      "Iteration 78, loss = 0.53908711\n",
      "Iteration 79, loss = 0.54231660\n",
      "Iteration 80, loss = 0.54523430\n",
      "Iteration 81, loss = 0.54365752\n",
      "Iteration 82, loss = 0.53925203\n",
      "Iteration 83, loss = 0.53781157\n",
      "Iteration 84, loss = 0.53954185\n",
      "Iteration 85, loss = 0.53984602\n",
      "Iteration 86, loss = 0.53773021\n",
      "Iteration 87, loss = 0.53607354\n",
      "Iteration 88, loss = 0.53657962\n",
      "Iteration 89, loss = 0.53660533\n",
      "Iteration 90, loss = 0.53540572\n",
      "Iteration 91, loss = 0.53546653\n",
      "Iteration 92, loss = 0.53623643\n",
      "Iteration 93, loss = 0.53648744\n",
      "Iteration 94, loss = 0.53596160\n",
      "Iteration 95, loss = 0.53454423\n",
      "Iteration 96, loss = 0.53471865\n",
      "Iteration 97, loss = 0.53464683\n",
      "Iteration 98, loss = 0.53417394\n",
      "Iteration 99, loss = 0.53392604\n",
      "Iteration 100, loss = 0.53365769\n",
      "Iteration 101, loss = 0.53314949\n",
      "Iteration 102, loss = 0.53332285\n",
      "Iteration 103, loss = 0.53265801\n",
      "Iteration 104, loss = 0.53352388\n",
      "Iteration 105, loss = 0.53385276\n",
      "Iteration 106, loss = 0.53265345\n",
      "Iteration 107, loss = 0.53212357\n",
      "Iteration 108, loss = 0.53271574\n",
      "Iteration 109, loss = 0.53269890\n",
      "Iteration 110, loss = 0.53223621\n",
      "Iteration 111, loss = 0.53186774\n",
      "Iteration 112, loss = 0.53098402\n",
      "Iteration 113, loss = 0.53163221\n",
      "Iteration 114, loss = 0.53147374\n",
      "Iteration 115, loss = 0.53045241\n",
      "Iteration 116, loss = 0.53107711\n",
      "Iteration 117, loss = 0.53198280\n",
      "Iteration 118, loss = 0.53170911\n",
      "Iteration 119, loss = 0.53159511\n",
      "Iteration 120, loss = 0.53067543\n",
      "Iteration 121, loss = 0.53003379\n",
      "Iteration 122, loss = 0.52936554\n",
      "Iteration 123, loss = 0.52912724\n",
      "Iteration 124, loss = 0.52888366\n",
      "Iteration 125, loss = 0.52856819\n",
      "Iteration 126, loss = 0.52865828\n",
      "Iteration 127, loss = 0.52873020\n",
      "Iteration 128, loss = 0.52833664\n",
      "Iteration 129, loss = 0.52813117\n",
      "Iteration 130, loss = 0.52806615\n",
      "Iteration 131, loss = 0.52846889\n",
      "Iteration 132, loss = 0.52889607\n",
      "Iteration 133, loss = 0.52855373\n",
      "Iteration 134, loss = 0.52944132\n",
      "Iteration 135, loss = 0.52961646\n",
      "Iteration 136, loss = 0.52897772\n",
      "Iteration 137, loss = 0.52710928\n",
      "Iteration 138, loss = 0.52822583\n",
      "Iteration 139, loss = 0.52788074\n",
      "Iteration 140, loss = 0.52696367\n",
      "Iteration 141, loss = 0.52707112\n",
      "Iteration 142, loss = 0.52768255\n",
      "Iteration 143, loss = 0.52718402\n",
      "Iteration 144, loss = 0.52631916\n",
      "Iteration 145, loss = 0.52758666\n",
      "Iteration 146, loss = 0.52739993\n",
      "Iteration 147, loss = 0.52671468\n",
      "Iteration 148, loss = 0.52618236\n",
      "Iteration 149, loss = 0.52629269\n",
      "Iteration 150, loss = 0.52696129\n",
      "Iteration 151, loss = 0.52694621\n",
      "Iteration 152, loss = 0.52689910\n",
      "Iteration 153, loss = 0.52636350\n",
      "Iteration 154, loss = 0.52629788\n",
      "Iteration 155, loss = 0.52615305\n",
      "Iteration 156, loss = 0.52584487\n",
      "Iteration 157, loss = 0.52599193\n",
      "Iteration 158, loss = 0.52549414\n",
      "Iteration 159, loss = 0.52523836\n",
      "Iteration 160, loss = 0.52494458\n",
      "Iteration 161, loss = 0.52678785\n",
      "Iteration 162, loss = 0.52525475\n",
      "Iteration 163, loss = 0.52509109\n",
      "Iteration 164, loss = 0.52654013\n",
      "Iteration 165, loss = 0.52595826\n",
      "Iteration 166, loss = 0.52395937\n",
      "Iteration 167, loss = 0.52600385\n",
      "Iteration 168, loss = 0.52862995\n",
      "Iteration 169, loss = 0.52625363\n",
      "Iteration 170, loss = 0.52458414\n",
      "Iteration 171, loss = 0.53252576\n",
      "Iteration 172, loss = 0.53611753\n",
      "Iteration 173, loss = 0.52938634\n",
      "Iteration 174, loss = 0.52502391\n",
      "Iteration 175, loss = 0.52482055\n",
      "Iteration 176, loss = 0.52416576\n",
      "Iteration 177, loss = 0.52411706\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.31990325\n",
      "Iteration 2, loss = 6.89922023\n",
      "Iteration 3, loss = 6.48124124\n",
      "Iteration 4, loss = 6.06359426\n",
      "Iteration 5, loss = 5.64188199\n",
      "Iteration 6, loss = 5.22927386\n",
      "Iteration 7, loss = 4.82141804\n",
      "Iteration 8, loss = 4.40717786\n",
      "Iteration 9, loss = 4.00254628\n",
      "Iteration 10, loss = 3.61941165\n",
      "Iteration 11, loss = 3.20209637\n",
      "Iteration 12, loss = 2.83065570\n",
      "Iteration 13, loss = 2.43020954\n",
      "Iteration 14, loss = 2.06354193\n",
      "Iteration 15, loss = 1.66115349\n",
      "Iteration 16, loss = 1.27523971\n",
      "Iteration 17, loss = 0.92313449\n",
      "Iteration 18, loss = 0.63854397\n",
      "Iteration 19, loss = 0.55908188\n",
      "Iteration 20, loss = 0.70683282\n",
      "Iteration 21, loss = 0.87584341\n",
      "Iteration 22, loss = 0.87310153\n",
      "Iteration 23, loss = 0.73582870\n",
      "Iteration 24, loss = 0.59434099\n",
      "Iteration 25, loss = 0.52690529\n",
      "Iteration 26, loss = 0.54053260\n",
      "Iteration 27, loss = 0.57977855\n",
      "Iteration 28, loss = 0.60637103\n",
      "Iteration 29, loss = 0.60641400\n",
      "Iteration 30, loss = 0.58484168\n",
      "Iteration 31, loss = 0.55165950\n",
      "Iteration 32, loss = 0.52914090\n",
      "Iteration 33, loss = 0.51946606\n",
      "Iteration 34, loss = 0.52507377\n",
      "Iteration 35, loss = 0.53953474\n",
      "Iteration 36, loss = 0.54760427\n",
      "Iteration 37, loss = 0.54299718\n",
      "Iteration 38, loss = 0.52947896\n",
      "Iteration 39, loss = 0.51972974\n",
      "Iteration 40, loss = 0.51752527\n",
      "Iteration 41, loss = 0.51922360\n",
      "Iteration 42, loss = 0.52140641\n",
      "Iteration 43, loss = 0.52281363\n",
      "Iteration 44, loss = 0.52205138\n",
      "Iteration 45, loss = 0.51873238\n",
      "Iteration 46, loss = 0.51602489\n",
      "Iteration 47, loss = 0.51327137\n",
      "Iteration 48, loss = 0.51344432\n",
      "Iteration 49, loss = 0.51268491\n",
      "Iteration 50, loss = 0.51253971\n",
      "Iteration 51, loss = 0.51230055\n",
      "Iteration 52, loss = 0.51177139\n",
      "Iteration 53, loss = 0.51092774\n",
      "Iteration 54, loss = 0.51005277\n",
      "Iteration 55, loss = 0.50968502\n",
      "Iteration 56, loss = 0.51049315\n",
      "Iteration 57, loss = 0.51146374\n",
      "Iteration 58, loss = 0.51081637\n",
      "Iteration 59, loss = 0.50871039\n",
      "Iteration 60, loss = 0.50784481\n",
      "Iteration 61, loss = 0.50906690\n",
      "Iteration 62, loss = 0.50915605\n",
      "Iteration 63, loss = 0.50846556\n",
      "Iteration 64, loss = 0.50696665\n",
      "Iteration 65, loss = 0.50571307\n",
      "Iteration 66, loss = 0.50598559\n",
      "Iteration 67, loss = 0.50825940\n",
      "Iteration 68, loss = 0.50983890\n",
      "Iteration 69, loss = 0.50862979\n",
      "Iteration 70, loss = 0.50538019\n",
      "Iteration 71, loss = 0.50524780\n",
      "Iteration 72, loss = 0.50591653\n",
      "Iteration 73, loss = 0.50601951\n",
      "Iteration 74, loss = 0.50514093\n",
      "Iteration 75, loss = 0.50397572\n",
      "Iteration 76, loss = 0.50329785\n",
      "Iteration 77, loss = 0.50299690\n",
      "Iteration 78, loss = 0.50239772\n",
      "Iteration 79, loss = 0.50255695\n",
      "Iteration 80, loss = 0.50225788\n",
      "Iteration 81, loss = 0.50244212\n",
      "Iteration 82, loss = 0.50120602\n",
      "Iteration 83, loss = 0.50150377\n",
      "Iteration 84, loss = 0.50260899\n",
      "Iteration 85, loss = 0.50177486\n",
      "Iteration 86, loss = 0.50025407\n",
      "Iteration 87, loss = 0.49963888\n",
      "Iteration 88, loss = 0.50179374\n",
      "Iteration 89, loss = 0.50514181\n",
      "Iteration 90, loss = 0.50810542\n",
      "Iteration 91, loss = 0.50661790\n",
      "Iteration 92, loss = 0.50064849\n",
      "Iteration 93, loss = 0.49830507\n",
      "Iteration 94, loss = 0.49914956\n",
      "Iteration 95, loss = 0.50268660\n",
      "Iteration 96, loss = 0.50446973\n",
      "Iteration 97, loss = 0.50284510\n",
      "Iteration 98, loss = 0.49997284\n",
      "Iteration 99, loss = 0.49762592\n",
      "Iteration 100, loss = 0.49776548\n",
      "Iteration 101, loss = 0.49759059\n",
      "Iteration 102, loss = 0.49747223\n",
      "Iteration 103, loss = 0.49684153\n",
      "Iteration 104, loss = 0.49671012\n",
      "Iteration 105, loss = 0.49668460\n",
      "Iteration 106, loss = 0.49689932\n",
      "Iteration 107, loss = 0.49688091\n",
      "Iteration 108, loss = 0.49594377\n",
      "Iteration 109, loss = 0.49580938\n",
      "Iteration 110, loss = 0.49556900\n",
      "Iteration 111, loss = 0.49548575\n",
      "Iteration 112, loss = 0.49518654\n",
      "Iteration 113, loss = 0.49592875\n",
      "Iteration 114, loss = 0.49572235\n",
      "Iteration 115, loss = 0.49482908\n",
      "Iteration 116, loss = 0.49528221\n",
      "Iteration 117, loss = 0.49457629\n",
      "Iteration 118, loss = 0.49448156\n",
      "Iteration 119, loss = 0.49511415\n",
      "Iteration 120, loss = 0.49544956\n",
      "Iteration 121, loss = 0.49421591\n",
      "Iteration 122, loss = 0.49399402\n",
      "Iteration 123, loss = 0.49445226\n",
      "Iteration 124, loss = 0.49509026\n",
      "Iteration 125, loss = 0.49716468\n",
      "Iteration 126, loss = 0.49751039\n",
      "Iteration 127, loss = 0.49527381\n",
      "Iteration 128, loss = 0.49448653\n",
      "Iteration 129, loss = 0.49388428\n",
      "Iteration 130, loss = 0.49431233\n",
      "Iteration 131, loss = 0.49325568\n",
      "Iteration 132, loss = 0.49309635\n",
      "Iteration 133, loss = 0.49459978\n",
      "Iteration 134, loss = 0.49348888\n",
      "Iteration 135, loss = 0.49272472\n",
      "Iteration 136, loss = 0.49446071\n",
      "Iteration 137, loss = 0.49262479\n",
      "Iteration 138, loss = 0.49245242\n",
      "Iteration 139, loss = 0.49513956\n",
      "Iteration 140, loss = 0.49683800\n",
      "Iteration 141, loss = 0.49300239\n",
      "Iteration 142, loss = 0.49190676\n",
      "Iteration 143, loss = 0.49679534\n",
      "Iteration 144, loss = 0.50081065\n",
      "Iteration 145, loss = 0.49818628\n",
      "Iteration 146, loss = 0.49176829\n",
      "Iteration 147, loss = 0.49225257\n",
      "Iteration 148, loss = 0.49406774\n",
      "Iteration 149, loss = 0.49332758\n",
      "Iteration 150, loss = 0.49161319\n",
      "Iteration 151, loss = 0.49186809\n",
      "Iteration 152, loss = 0.49215453\n",
      "Iteration 153, loss = 0.49160734\n",
      "Iteration 154, loss = 0.49154997\n",
      "Iteration 155, loss = 0.49188664\n",
      "Iteration 156, loss = 0.49190950\n",
      "Iteration 157, loss = 0.49254462\n",
      "Iteration 158, loss = 0.49292565\n",
      "Iteration 159, loss = 0.49305060\n",
      "Iteration 160, loss = 0.49181972\n",
      "Iteration 161, loss = 0.49199041\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 9.15863631\n",
      "Iteration 2, loss = 9.15863629\n",
      "Iteration 3, loss = 9.15863626\n",
      "Iteration 4, loss = 9.15863624\n",
      "Iteration 5, loss = 9.15863622\n",
      "Iteration 6, loss = 9.15863619\n",
      "Iteration 7, loss = 9.15863617\n",
      "Iteration 8, loss = 9.15863615\n",
      "Iteration 9, loss = 9.15863613\n",
      "Iteration 10, loss = 9.15863611\n",
      "Iteration 11, loss = 9.15385152\n",
      "Iteration 12, loss = 9.15092122\n",
      "Iteration 13, loss = 9.12982980\n",
      "Iteration 14, loss = 9.07627421\n",
      "Iteration 15, loss = 8.95209371\n",
      "Iteration 16, loss = 8.72664895\n",
      "Iteration 17, loss = 8.44688747\n",
      "Iteration 18, loss = 8.07665941\n",
      "Iteration 19, loss = 7.65136322\n",
      "Iteration 20, loss = 7.22286398\n",
      "Iteration 21, loss = 6.77534171\n",
      "Iteration 22, loss = 6.34430948\n",
      "Iteration 23, loss = 5.87277682\n",
      "Iteration 24, loss = 5.42924304\n",
      "Iteration 25, loss = 5.00556779\n",
      "Iteration 26, loss = 4.54741207\n",
      "Iteration 27, loss = 4.10612628\n",
      "Iteration 28, loss = 3.68228541\n",
      "Iteration 29, loss = 3.24453334\n",
      "Iteration 30, loss = 2.81268837\n",
      "Iteration 31, loss = 2.38310138\n",
      "Iteration 32, loss = 1.98453767\n",
      "Iteration 33, loss = 1.57488811\n",
      "Iteration 34, loss = 1.22889838\n",
      "Iteration 35, loss = 0.94642467\n",
      "Iteration 36, loss = 0.79259627\n",
      "Iteration 37, loss = 0.76617532\n",
      "Iteration 38, loss = 0.85828124\n",
      "Iteration 39, loss = 0.94119071\n",
      "Iteration 40, loss = 0.95462072\n",
      "Iteration 41, loss = 0.89811996\n",
      "Iteration 42, loss = 0.80989993\n",
      "Iteration 43, loss = 0.74406909\n",
      "Iteration 44, loss = 0.70910176\n",
      "Iteration 45, loss = 0.71485157\n",
      "Iteration 46, loss = 0.72576315\n",
      "Iteration 47, loss = 0.73640473\n",
      "Iteration 48, loss = 0.73457809\n",
      "Iteration 49, loss = 0.72237327\n",
      "Iteration 50, loss = 0.70420402\n",
      "Iteration 51, loss = 0.68182890\n",
      "Iteration 52, loss = 0.66205898\n",
      "Iteration 53, loss = 0.64929983\n",
      "Iteration 54, loss = 0.64547655\n",
      "Iteration 55, loss = 0.64333655\n",
      "Iteration 56, loss = 0.64230250\n",
      "Iteration 57, loss = 0.63652667\n",
      "Iteration 58, loss = 0.62671680\n",
      "Iteration 59, loss = 0.61663921\n",
      "Iteration 60, loss = 0.61181686\n",
      "Iteration 61, loss = 0.61059522\n",
      "Iteration 62, loss = 0.60600472\n",
      "Iteration 63, loss = 0.60092559\n",
      "Iteration 64, loss = 0.59567027\n",
      "Iteration 65, loss = 0.59095979\n",
      "Iteration 66, loss = 0.58592783\n",
      "Iteration 67, loss = 0.58056918\n",
      "Iteration 68, loss = 0.57667044\n",
      "Iteration 69, loss = 0.57239554\n",
      "Iteration 70, loss = 0.56896297\n",
      "Iteration 71, loss = 0.56500579\n",
      "Iteration 72, loss = 0.56257984\n",
      "Iteration 73, loss = 0.55960222\n",
      "Iteration 74, loss = 0.55660937\n",
      "Iteration 75, loss = 0.55340722\n",
      "Iteration 76, loss = 0.55088493\n",
      "Iteration 77, loss = 0.54777822\n",
      "Iteration 78, loss = 0.54474165\n",
      "Iteration 79, loss = 0.54269557\n",
      "Iteration 80, loss = 0.53967477\n",
      "Iteration 81, loss = 0.53742740\n",
      "Iteration 82, loss = 0.53556430\n",
      "Iteration 83, loss = 0.53381402\n",
      "Iteration 84, loss = 0.53203787\n",
      "Iteration 85, loss = 0.53033870\n",
      "Iteration 86, loss = 0.52854972\n",
      "Iteration 87, loss = 0.52674014\n",
      "Iteration 88, loss = 0.52543422\n",
      "Iteration 89, loss = 0.52378437\n",
      "Iteration 90, loss = 0.52265292\n",
      "Iteration 91, loss = 0.52186526\n",
      "Iteration 92, loss = 0.52117395\n",
      "Iteration 93, loss = 0.52022809\n",
      "Iteration 94, loss = 0.52044435\n",
      "Iteration 95, loss = 0.52022966\n",
      "Iteration 96, loss = 0.51852887\n",
      "Iteration 97, loss = 0.51618136\n",
      "Iteration 98, loss = 0.51515176\n",
      "Iteration 99, loss = 0.51356946\n",
      "Iteration 100, loss = 0.51267512\n",
      "Iteration 101, loss = 0.51182472\n",
      "Iteration 102, loss = 0.51171684\n",
      "Iteration 103, loss = 0.51274068\n",
      "Iteration 104, loss = 0.51412693\n",
      "Iteration 105, loss = 0.51455854\n",
      "Iteration 106, loss = 0.51306162\n",
      "Iteration 107, loss = 0.51051187\n",
      "Iteration 108, loss = 0.50992971\n",
      "Iteration 109, loss = 0.50718665\n",
      "Iteration 110, loss = 0.50682308\n",
      "Iteration 111, loss = 0.50782886\n",
      "Iteration 112, loss = 0.50818816\n",
      "Iteration 113, loss = 0.50640240\n",
      "Iteration 114, loss = 0.50547976\n",
      "Iteration 115, loss = 0.50524037\n",
      "Iteration 116, loss = 0.50548690\n",
      "Iteration 117, loss = 0.50458882\n",
      "Iteration 118, loss = 0.50373660\n",
      "Iteration 119, loss = 0.50529720\n",
      "Iteration 120, loss = 0.50459103\n",
      "Iteration 121, loss = 0.50391026\n",
      "Iteration 122, loss = 0.50328703\n",
      "Iteration 123, loss = 0.50346032\n",
      "Iteration 124, loss = 0.50312040\n",
      "Iteration 125, loss = 0.50280142\n",
      "Iteration 126, loss = 0.50269282\n",
      "Iteration 127, loss = 0.50251006\n",
      "Iteration 128, loss = 0.50215253\n",
      "Iteration 129, loss = 0.50219784\n",
      "Iteration 130, loss = 0.50225449\n",
      "Iteration 131, loss = 0.50218002\n",
      "Iteration 132, loss = 0.50171940\n",
      "Iteration 133, loss = 0.50146754\n",
      "Iteration 134, loss = 0.50135842\n",
      "Iteration 135, loss = 0.50122227\n",
      "Iteration 136, loss = 0.50117476\n",
      "Iteration 137, loss = 0.50119453\n",
      "Iteration 138, loss = 0.50118631\n",
      "Iteration 139, loss = 0.50602994\n",
      "Iteration 140, loss = 0.50429422\n",
      "Iteration 141, loss = 0.50149214\n",
      "Iteration 142, loss = 0.50073103\n",
      "Iteration 143, loss = 0.50172159\n",
      "Iteration 144, loss = 0.50198648\n",
      "Iteration 145, loss = 0.50154964\n",
      "Iteration 146, loss = 0.50200641\n",
      "Iteration 147, loss = 0.50139663\n",
      "Iteration 148, loss = 0.49960196\n",
      "Iteration 149, loss = 0.50192707\n",
      "Iteration 150, loss = 0.50613091\n",
      "Iteration 151, loss = 0.50696220\n",
      "Iteration 152, loss = 0.50262170\n",
      "Iteration 153, loss = 0.49948191\n",
      "Iteration 154, loss = 0.50221277\n",
      "Iteration 155, loss = 0.51093178\n",
      "Iteration 156, loss = 0.51424439\n",
      "Iteration 157, loss = 0.51051539\n",
      "Iteration 158, loss = 0.50197338\n",
      "Iteration 159, loss = 0.49995062\n",
      "Iteration 160, loss = 0.50242222\n",
      "Iteration 161, loss = 0.50467247\n",
      "Iteration 162, loss = 0.50379004\n",
      "Iteration 163, loss = 0.50073894\n",
      "Iteration 164, loss = 0.49991221\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70447924\n",
      "Iteration 2, loss = 0.61827491\n",
      "Iteration 3, loss = 0.61062155\n",
      "Iteration 4, loss = 0.59494966\n",
      "Iteration 5, loss = 0.58423175\n",
      "Iteration 6, loss = 0.58065755\n",
      "Iteration 7, loss = 0.56758543\n",
      "Iteration 8, loss = 0.55815343\n",
      "Iteration 9, loss = 0.55510199\n",
      "Iteration 10, loss = 0.54918826\n",
      "Iteration 11, loss = 0.54722132\n",
      "Iteration 12, loss = 0.54254992\n",
      "Iteration 13, loss = 0.53982484\n",
      "Iteration 14, loss = 0.53730698\n",
      "Iteration 15, loss = 0.53367460\n",
      "Iteration 16, loss = 0.53035509\n",
      "Iteration 17, loss = 0.53592324\n",
      "Iteration 18, loss = 0.53537115\n",
      "Iteration 19, loss = 0.52878620\n",
      "Iteration 20, loss = 0.53164718\n",
      "Iteration 21, loss = 0.53183164\n",
      "Iteration 22, loss = 0.52698863\n",
      "Iteration 23, loss = 0.53161213\n",
      "Iteration 24, loss = 0.54105367\n",
      "Iteration 25, loss = 0.53476316\n",
      "Iteration 26, loss = 0.52664533\n",
      "Iteration 27, loss = 0.53781471\n",
      "Iteration 28, loss = 0.54127804\n",
      "Iteration 29, loss = 0.53241768\n",
      "Iteration 30, loss = 0.52491299\n",
      "Iteration 31, loss = 0.52519424\n",
      "Iteration 32, loss = 0.52642044\n",
      "Iteration 33, loss = 0.52381013\n",
      "Iteration 34, loss = 0.52509342\n",
      "Iteration 35, loss = 0.52381862\n",
      "Iteration 36, loss = 0.52187254\n",
      "Iteration 37, loss = 0.52409216\n",
      "Iteration 38, loss = 0.52446198\n",
      "Iteration 39, loss = 0.52267602\n",
      "Iteration 40, loss = 0.52271594\n",
      "Iteration 41, loss = 0.51972851\n",
      "Iteration 42, loss = 0.52210774\n",
      "Iteration 43, loss = 0.53439245\n",
      "Iteration 44, loss = 0.53382115\n",
      "Iteration 45, loss = 0.52262177\n",
      "Iteration 46, loss = 0.52111904\n",
      "Iteration 47, loss = 0.52051153\n",
      "Iteration 48, loss = 0.51984312\n",
      "Iteration 49, loss = 0.52411333\n",
      "Iteration 50, loss = 0.51902205\n",
      "Iteration 51, loss = 0.51972570\n",
      "Iteration 52, loss = 0.53820430\n",
      "Iteration 53, loss = 0.53457845\n",
      "Iteration 54, loss = 0.51303917\n",
      "Iteration 55, loss = 0.54035254\n",
      "Iteration 56, loss = 0.56557812\n",
      "Iteration 57, loss = 0.53794848\n",
      "Iteration 58, loss = 0.51960906\n",
      "Iteration 59, loss = 0.54695625\n",
      "Iteration 60, loss = 0.54567642\n",
      "Iteration 61, loss = 0.52339988\n",
      "Iteration 62, loss = 0.53121574\n",
      "Iteration 63, loss = 0.52803708\n",
      "Iteration 64, loss = 0.51275966\n",
      "Iteration 65, loss = 0.53854825\n",
      "Iteration 66, loss = 0.54014606\n",
      "Iteration 67, loss = 0.51914921\n",
      "Iteration 68, loss = 0.51817742\n",
      "Iteration 69, loss = 0.52991889\n",
      "Iteration 70, loss = 0.53151465\n",
      "Iteration 71, loss = 0.52010071\n",
      "Iteration 72, loss = 0.51613858\n",
      "Iteration 73, loss = 0.51823437\n",
      "Iteration 74, loss = 0.51998817\n",
      "Iteration 75, loss = 0.51745975\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 16.98801411\n",
      "Iteration 2, loss = 15.00443945\n",
      "Iteration 3, loss = 13.02266597\n",
      "Iteration 4, loss = 11.05034446\n",
      "Iteration 5, loss = 9.08994338\n",
      "Iteration 6, loss = 7.14808762\n",
      "Iteration 7, loss = 5.23978713\n",
      "Iteration 8, loss = 3.39694612\n",
      "Iteration 9, loss = 1.81200869\n",
      "Iteration 10, loss = 0.91020259\n",
      "Iteration 11, loss = 0.90976020\n",
      "Iteration 12, loss = 1.22855261\n",
      "Iteration 13, loss = 1.48690198\n",
      "Iteration 14, loss = 1.65860614\n",
      "Iteration 15, loss = 1.75467731\n",
      "Iteration 16, loss = 1.78334213\n",
      "Iteration 17, loss = 1.75322017\n",
      "Iteration 18, loss = 1.67365140\n",
      "Iteration 19, loss = 1.54841592\n",
      "Iteration 20, loss = 1.39964861\n",
      "Iteration 21, loss = 1.21534971\n",
      "Iteration 22, loss = 1.02499473\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.74776694\n",
      "Iteration 2, loss = 0.78182578\n",
      "Iteration 3, loss = 0.60796369\n",
      "Iteration 4, loss = 0.72933665\n",
      "Iteration 5, loss = 0.78776209\n",
      "Iteration 6, loss = 0.75454528\n",
      "Iteration 7, loss = 0.66844552\n",
      "Iteration 8, loss = 0.58669270\n",
      "Iteration 9, loss = 0.56114657\n",
      "Iteration 10, loss = 0.59347031\n",
      "Iteration 11, loss = 0.61102257\n",
      "Iteration 12, loss = 0.58341904\n",
      "Iteration 13, loss = 0.54976521\n",
      "Iteration 14, loss = 0.54704329\n",
      "Iteration 15, loss = 0.55774974\n",
      "Iteration 16, loss = 0.56306379\n",
      "Iteration 17, loss = 0.55426316\n",
      "Iteration 18, loss = 0.53974136\n",
      "Iteration 19, loss = 0.53918194\n",
      "Iteration 20, loss = 0.54384171\n",
      "Iteration 21, loss = 0.54429385\n",
      "Iteration 22, loss = 0.53728081\n",
      "Iteration 23, loss = 0.53525238\n",
      "Iteration 24, loss = 0.53484313\n",
      "Iteration 25, loss = 0.53507836\n",
      "Iteration 26, loss = 0.53275805\n",
      "Iteration 27, loss = 0.53148131\n",
      "Iteration 28, loss = 0.52954813\n",
      "Iteration 29, loss = 0.52730314\n",
      "Iteration 30, loss = 0.52725049\n",
      "Iteration 31, loss = 0.52672254\n",
      "Iteration 32, loss = 0.52564745\n",
      "Iteration 33, loss = 0.52416384\n",
      "Iteration 34, loss = 0.52439649\n",
      "Iteration 35, loss = 0.52433725\n",
      "Iteration 36, loss = 0.52360315\n",
      "Iteration 37, loss = 0.52325403\n",
      "Iteration 38, loss = 0.52301630\n",
      "Iteration 39, loss = 0.52287293\n",
      "Iteration 40, loss = 0.52116778\n",
      "Iteration 41, loss = 0.52372504\n",
      "Iteration 42, loss = 0.52307289\n",
      "Iteration 43, loss = 0.51893210\n",
      "Iteration 44, loss = 0.52131289\n",
      "Iteration 45, loss = 0.52018715\n",
      "Iteration 46, loss = 0.51723220\n",
      "Iteration 47, loss = 0.51758064\n",
      "Iteration 48, loss = 0.51940832\n",
      "Iteration 49, loss = 0.51856134\n",
      "Iteration 50, loss = 0.51556557\n",
      "Iteration 51, loss = 0.51507360\n",
      "Iteration 52, loss = 0.51445313\n",
      "Iteration 53, loss = 0.51448954\n",
      "Iteration 54, loss = 0.51447229\n",
      "Iteration 55, loss = 0.51424457\n",
      "Iteration 56, loss = 0.51463319\n",
      "Iteration 57, loss = 0.51435447\n",
      "Iteration 58, loss = 0.51288185\n",
      "Iteration 59, loss = 0.51183159\n",
      "Iteration 60, loss = 0.51259259\n",
      "Iteration 61, loss = 0.51986578\n",
      "Iteration 62, loss = 0.51826240\n",
      "Iteration 63, loss = 0.51289834\n",
      "Iteration 64, loss = 0.51142661\n",
      "Iteration 65, loss = 0.51460584\n",
      "Iteration 66, loss = 0.51688422\n",
      "Iteration 67, loss = 0.51292189\n",
      "Iteration 68, loss = 0.51062524\n",
      "Iteration 69, loss = 0.50994240\n",
      "Iteration 70, loss = 0.51022865\n",
      "Iteration 71, loss = 0.51022761\n",
      "Iteration 72, loss = 0.51055653\n",
      "Iteration 73, loss = 0.51112599\n",
      "Iteration 74, loss = 0.51021405\n",
      "Iteration 75, loss = 0.51042593\n",
      "Iteration 76, loss = 0.50959617\n",
      "Iteration 77, loss = 0.51188364\n",
      "Iteration 78, loss = 0.51180313\n",
      "Iteration 79, loss = 0.50892919\n",
      "Iteration 80, loss = 0.50820396\n",
      "Iteration 81, loss = 0.50943602\n",
      "Iteration 82, loss = 0.50992379\n",
      "Iteration 83, loss = 0.51185584\n",
      "Iteration 84, loss = 0.50761007\n",
      "Iteration 85, loss = 0.50911071\n",
      "Iteration 86, loss = 0.52036755\n",
      "Iteration 87, loss = 0.51797896\n",
      "Iteration 88, loss = 0.50667373\n",
      "Iteration 89, loss = 0.51155419\n",
      "Iteration 90, loss = 0.51408255\n",
      "Iteration 91, loss = 0.50594719\n",
      "Iteration 92, loss = 0.50609304\n",
      "Iteration 93, loss = 0.51317385\n",
      "Iteration 94, loss = 0.50793295\n",
      "Iteration 95, loss = 0.50342945\n",
      "Iteration 96, loss = 0.51490172\n",
      "Iteration 97, loss = 0.51631506\n",
      "Iteration 98, loss = 0.50522535\n",
      "Iteration 99, loss = 0.50800538\n",
      "Iteration 100, loss = 0.51219184\n",
      "Iteration 101, loss = 0.50719632\n",
      "Iteration 102, loss = 0.50203152\n",
      "Iteration 103, loss = 0.50803793\n",
      "Iteration 104, loss = 0.51282585\n",
      "Iteration 105, loss = 0.50596238\n",
      "Iteration 106, loss = 0.50335357\n",
      "Iteration 107, loss = 0.51163262\n",
      "Iteration 108, loss = 0.50828385\n",
      "Iteration 109, loss = 0.50127005\n",
      "Iteration 110, loss = 0.50654224\n",
      "Iteration 111, loss = 0.50748974\n",
      "Iteration 112, loss = 0.50234500\n",
      "Iteration 113, loss = 0.50125013\n",
      "Iteration 114, loss = 0.50656729\n",
      "Iteration 115, loss = 0.50610964\n",
      "Iteration 116, loss = 0.50185604\n",
      "Iteration 117, loss = 0.50443972\n",
      "Iteration 118, loss = 0.50371654\n",
      "Iteration 119, loss = 0.50387687\n",
      "Iteration 120, loss = 0.50173604\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63185061\n",
      "Iteration 2, loss = 0.64030159\n",
      "Iteration 3, loss = 0.58629905\n",
      "Iteration 4, loss = 0.58076728\n",
      "Iteration 5, loss = 0.55472360\n",
      "Iteration 6, loss = 0.56198543\n",
      "Iteration 7, loss = 0.56376291\n",
      "Iteration 8, loss = 0.54020449\n",
      "Iteration 9, loss = 0.53443893\n",
      "Iteration 10, loss = 0.54955723\n",
      "Iteration 11, loss = 0.53571032\n",
      "Iteration 12, loss = 0.52614753\n",
      "Iteration 13, loss = 0.55354011\n",
      "Iteration 14, loss = 0.54128181\n",
      "Iteration 15, loss = 0.52022145\n",
      "Iteration 16, loss = 0.54145263\n",
      "Iteration 17, loss = 0.53727687\n",
      "Iteration 18, loss = 0.52152745\n",
      "Iteration 19, loss = 0.52595416\n",
      "Iteration 20, loss = 0.52550345\n",
      "Iteration 21, loss = 0.52264668\n",
      "Iteration 22, loss = 0.52506015\n",
      "Iteration 23, loss = 0.52359650\n",
      "Iteration 24, loss = 0.51797828\n",
      "Iteration 25, loss = 0.52616303\n",
      "Iteration 26, loss = 0.53170877\n",
      "Iteration 27, loss = 0.51847965\n",
      "Iteration 28, loss = 0.51992399\n",
      "Iteration 29, loss = 0.52779284\n",
      "Iteration 30, loss = 0.51700726\n",
      "Iteration 31, loss = 0.52113432\n",
      "Iteration 32, loss = 0.52939718\n",
      "Iteration 33, loss = 0.51920719\n",
      "Iteration 34, loss = 0.52153859\n",
      "Iteration 35, loss = 0.52286113\n",
      "Iteration 36, loss = 0.51811379\n",
      "Iteration 37, loss = 0.51610419\n",
      "Iteration 38, loss = 0.52001412\n",
      "Iteration 39, loss = 0.51747870\n",
      "Iteration 40, loss = 0.51651479\n",
      "Iteration 41, loss = 0.51903070\n",
      "Iteration 42, loss = 0.51673540\n",
      "Iteration 43, loss = 0.52104508\n",
      "Iteration 44, loss = 0.51341443\n",
      "Iteration 45, loss = 0.51778870\n",
      "Iteration 46, loss = 0.51616871\n",
      "Iteration 47, loss = 0.51481681\n",
      "Iteration 48, loss = 0.53055170\n",
      "Iteration 49, loss = 0.52314635\n",
      "Iteration 50, loss = 0.51311589\n",
      "Iteration 51, loss = 0.51617377\n",
      "Iteration 52, loss = 0.51819147\n",
      "Iteration 53, loss = 0.50857491\n",
      "Iteration 54, loss = 0.53831074\n",
      "Iteration 55, loss = 0.53651357\n",
      "Iteration 56, loss = 0.50905284\n",
      "Iteration 57, loss = 0.53792123\n",
      "Iteration 58, loss = 0.54513362\n",
      "Iteration 59, loss = 0.51690351\n",
      "Iteration 60, loss = 0.51901648\n",
      "Iteration 61, loss = 0.51685661\n",
      "Iteration 62, loss = 0.50976358\n",
      "Iteration 63, loss = 0.52893359\n",
      "Iteration 64, loss = 0.53039400\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73611207\n",
      "Iteration 2, loss = 0.74080146\n",
      "Iteration 3, loss = 0.63095875\n",
      "Iteration 4, loss = 0.64407640\n",
      "Iteration 5, loss = 0.60808857\n",
      "Iteration 6, loss = 0.57411828\n",
      "Iteration 7, loss = 0.56479981\n",
      "Iteration 8, loss = 0.55707990\n",
      "Iteration 9, loss = 0.55285618\n",
      "Iteration 10, loss = 0.55301069\n",
      "Iteration 11, loss = 0.55486592\n",
      "Iteration 12, loss = 0.55530637\n",
      "Iteration 13, loss = 0.54933446\n",
      "Iteration 14, loss = 0.54921829\n",
      "Iteration 15, loss = 0.54572747\n",
      "Iteration 16, loss = 0.54429593\n",
      "Iteration 17, loss = 0.54459032\n",
      "Iteration 18, loss = 0.54125720\n",
      "Iteration 19, loss = 0.53985373\n",
      "Iteration 20, loss = 0.53895174\n",
      "Iteration 21, loss = 0.53683401\n",
      "Iteration 22, loss = 0.53661268\n",
      "Iteration 23, loss = 0.53452323\n",
      "Iteration 24, loss = 0.53291298\n",
      "Iteration 25, loss = 0.53207406\n",
      "Iteration 26, loss = 0.53107759\n",
      "Iteration 27, loss = 0.53110438\n",
      "Iteration 28, loss = 0.53186738\n",
      "Iteration 29, loss = 0.53370781\n",
      "Iteration 30, loss = 0.53441141\n",
      "Iteration 31, loss = 0.52997593\n",
      "Iteration 32, loss = 0.53308314\n",
      "Iteration 33, loss = 0.53036863\n",
      "Iteration 34, loss = 0.52788414\n",
      "Iteration 35, loss = 0.52822412\n",
      "Iteration 36, loss = 0.53115993\n",
      "Iteration 37, loss = 0.53010255\n",
      "Iteration 38, loss = 0.52656918\n",
      "Iteration 39, loss = 0.52960258\n",
      "Iteration 40, loss = 0.52461178\n",
      "Iteration 41, loss = 0.52723809\n",
      "Iteration 42, loss = 0.53498865\n",
      "Iteration 43, loss = 0.52967871\n",
      "Iteration 44, loss = 0.52479942\n",
      "Iteration 45, loss = 0.52442014\n",
      "Iteration 46, loss = 0.52263899\n",
      "Iteration 47, loss = 0.52726837\n",
      "Iteration 48, loss = 0.52689162\n",
      "Iteration 49, loss = 0.52603723\n",
      "Iteration 50, loss = 0.53398194\n",
      "Iteration 51, loss = 0.52428911\n",
      "Iteration 52, loss = 0.53574186\n",
      "Iteration 53, loss = 0.52424309\n",
      "Iteration 54, loss = 0.52494970\n",
      "Iteration 55, loss = 0.52818222\n",
      "Iteration 56, loss = 0.52204724\n",
      "Iteration 57, loss = 0.52295480\n",
      "Iteration 58, loss = 0.52455469\n",
      "Iteration 59, loss = 0.52569146\n",
      "Iteration 60, loss = 0.51936605\n",
      "Iteration 61, loss = 0.53033810\n",
      "Iteration 62, loss = 0.52429335\n",
      "Iteration 63, loss = 0.52029015\n",
      "Iteration 64, loss = 0.52091537\n",
      "Iteration 65, loss = 0.51939197\n",
      "Iteration 66, loss = 0.51851033\n",
      "Iteration 67, loss = 0.51836471\n",
      "Iteration 68, loss = 0.52542744\n",
      "Iteration 69, loss = 0.51870892\n",
      "Iteration 70, loss = 0.54144809\n",
      "Iteration 71, loss = 0.52513198\n",
      "Iteration 72, loss = 0.52510301\n",
      "Iteration 73, loss = 0.53915683\n",
      "Iteration 74, loss = 0.52269088\n",
      "Iteration 75, loss = 0.51957271\n",
      "Iteration 76, loss = 0.52055781\n",
      "Iteration 77, loss = 0.51772030\n",
      "Iteration 78, loss = 0.52765353\n",
      "Iteration 79, loss = 0.51990854\n",
      "Iteration 80, loss = 0.51743648\n",
      "Iteration 81, loss = 0.52431044\n",
      "Iteration 82, loss = 0.51779523\n",
      "Iteration 83, loss = 0.51707548\n",
      "Iteration 84, loss = 0.52322933\n",
      "Iteration 85, loss = 0.52405809\n",
      "Iteration 86, loss = 0.51988434\n",
      "Iteration 87, loss = 0.51588164\n",
      "Iteration 88, loss = 0.51886259\n",
      "Iteration 89, loss = 0.51279106\n",
      "Iteration 90, loss = 0.52911964\n",
      "Iteration 91, loss = 0.53201716\n",
      "Iteration 92, loss = 0.51880467\n",
      "Iteration 93, loss = 0.51505459\n",
      "Iteration 94, loss = 0.52359787\n",
      "Iteration 95, loss = 0.52012544\n",
      "Iteration 96, loss = 0.51535536\n",
      "Iteration 97, loss = 0.51410314\n",
      "Iteration 98, loss = 0.51802528\n",
      "Iteration 99, loss = 0.52157628\n",
      "Iteration 100, loss = 0.51875321\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.78268545\n",
      "Iteration 2, loss = 5.03191023\n",
      "Iteration 3, loss = 4.32735996\n",
      "Iteration 4, loss = 3.61240390\n",
      "Iteration 5, loss = 2.90222303\n",
      "Iteration 6, loss = 2.16260569\n",
      "Iteration 7, loss = 1.50133466\n",
      "Iteration 8, loss = 0.87383850\n",
      "Iteration 9, loss = 0.69124877\n",
      "Iteration 10, loss = 1.14135419\n",
      "Iteration 11, loss = 1.38266588\n",
      "Iteration 12, loss = 1.13045602\n",
      "Iteration 13, loss = 0.78382303\n",
      "Iteration 14, loss = 0.63638295\n",
      "Iteration 15, loss = 0.69675115\n",
      "Iteration 16, loss = 0.75977006\n",
      "Iteration 17, loss = 0.77252818\n",
      "Iteration 18, loss = 0.73059451\n",
      "Iteration 19, loss = 0.65766942\n",
      "Iteration 20, loss = 0.59126354\n",
      "Iteration 21, loss = 0.58039448\n",
      "Iteration 22, loss = 0.60045599\n",
      "Iteration 23, loss = 0.60646698\n",
      "Iteration 24, loss = 0.57580512\n",
      "Iteration 25, loss = 0.54982318\n",
      "Iteration 26, loss = 0.54204122\n",
      "Iteration 27, loss = 0.54633177\n",
      "Iteration 28, loss = 0.54929279\n",
      "Iteration 29, loss = 0.54361639\n",
      "Iteration 30, loss = 0.53033381\n",
      "Iteration 31, loss = 0.52281897\n",
      "Iteration 32, loss = 0.52300231\n",
      "Iteration 33, loss = 0.52745874\n",
      "Iteration 34, loss = 0.52342370\n",
      "Iteration 35, loss = 0.51798954\n",
      "Iteration 36, loss = 0.51858480\n",
      "Iteration 37, loss = 0.52048864\n",
      "Iteration 38, loss = 0.51855500\n",
      "Iteration 39, loss = 0.51487937\n",
      "Iteration 40, loss = 0.51343134\n",
      "Iteration 41, loss = 0.51274297\n",
      "Iteration 42, loss = 0.51247364\n",
      "Iteration 43, loss = 0.51163839\n",
      "Iteration 44, loss = 0.51059319\n",
      "Iteration 45, loss = 0.51024097\n",
      "Iteration 46, loss = 0.51020102\n",
      "Iteration 47, loss = 0.51106277\n",
      "Iteration 48, loss = 0.51067457\n",
      "Iteration 49, loss = 0.50866369\n",
      "Iteration 50, loss = 0.50801663\n",
      "Iteration 51, loss = 0.51084920\n",
      "Iteration 52, loss = 0.50828804\n",
      "Iteration 53, loss = 0.50584372\n",
      "Iteration 54, loss = 0.50994050\n",
      "Iteration 55, loss = 0.50811549\n",
      "Iteration 56, loss = 0.50336583\n",
      "Iteration 57, loss = 0.50913463\n",
      "Iteration 58, loss = 0.51482634\n",
      "Iteration 59, loss = 0.51010490\n",
      "Iteration 60, loss = 0.50355620\n",
      "Iteration 61, loss = 0.50504993\n",
      "Iteration 62, loss = 0.50542666\n",
      "Iteration 63, loss = 0.50284239\n",
      "Iteration 64, loss = 0.50190179\n",
      "Iteration 65, loss = 0.50216004\n",
      "Iteration 66, loss = 0.50160544\n",
      "Iteration 67, loss = 0.50013471\n",
      "Iteration 68, loss = 0.49905004\n",
      "Iteration 69, loss = 0.50177491\n",
      "Iteration 70, loss = 0.49961876\n",
      "Iteration 71, loss = 0.49848792\n",
      "Iteration 72, loss = 0.49900104\n",
      "Iteration 73, loss = 0.50061887\n",
      "Iteration 74, loss = 0.49952756\n",
      "Iteration 75, loss = 0.49756117\n",
      "Iteration 76, loss = 0.49713929\n",
      "Iteration 77, loss = 0.49711544\n",
      "Iteration 78, loss = 0.49622735\n",
      "Iteration 79, loss = 0.49582186\n",
      "Iteration 80, loss = 0.49509258\n",
      "Iteration 81, loss = 0.49579357\n",
      "Iteration 82, loss = 0.49826450\n",
      "Iteration 83, loss = 0.49774598\n",
      "Iteration 84, loss = 0.49503434\n",
      "Iteration 85, loss = 0.49468767\n",
      "Iteration 86, loss = 0.49710662\n",
      "Iteration 87, loss = 0.49714413\n",
      "Iteration 88, loss = 0.49352763\n",
      "Iteration 89, loss = 0.49312368\n",
      "Iteration 90, loss = 0.49971241\n",
      "Iteration 91, loss = 0.49913419\n",
      "Iteration 92, loss = 0.49161529\n",
      "Iteration 93, loss = 0.49140624\n",
      "Iteration 94, loss = 0.50437983\n",
      "Iteration 95, loss = 0.50260183\n",
      "Iteration 96, loss = 0.49114082\n",
      "Iteration 97, loss = 0.49349678\n",
      "Iteration 98, loss = 0.50674916\n",
      "Iteration 99, loss = 0.50395479\n",
      "Iteration 100, loss = 0.49241765\n",
      "Iteration 101, loss = 0.49342267\n",
      "Iteration 102, loss = 0.49386652\n",
      "Iteration 103, loss = 0.49183169\n",
      "Iteration 104, loss = 0.48931956\n",
      "Iteration 105, loss = 0.48885751\n",
      "Iteration 106, loss = 0.48888430\n",
      "Iteration 107, loss = 0.48831717\n",
      "Iteration 108, loss = 0.48889691\n",
      "Iteration 109, loss = 0.48803579\n",
      "Iteration 110, loss = 0.48837586\n",
      "Iteration 111, loss = 0.49273728\n",
      "Iteration 112, loss = 0.49369904\n",
      "Iteration 113, loss = 0.49008898\n",
      "Iteration 114, loss = 0.48881501\n",
      "Iteration 115, loss = 0.48708700\n",
      "Iteration 116, loss = 0.48675258\n",
      "Iteration 117, loss = 0.48812865\n",
      "Iteration 118, loss = 0.49128604\n",
      "Iteration 119, loss = 0.48765953\n",
      "Iteration 120, loss = 0.48674806\n",
      "Iteration 121, loss = 0.49024515\n",
      "Iteration 122, loss = 0.48900277\n",
      "Iteration 123, loss = 0.48527821\n",
      "Iteration 124, loss = 0.48525743\n",
      "Iteration 125, loss = 0.48593864\n",
      "Iteration 126, loss = 0.48589165\n",
      "Iteration 127, loss = 0.48596186\n",
      "Iteration 128, loss = 0.48763442\n",
      "Iteration 129, loss = 0.48760271\n",
      "Iteration 130, loss = 0.48526765\n",
      "Iteration 131, loss = 0.48840050\n",
      "Iteration 132, loss = 0.48859952\n",
      "Iteration 133, loss = 0.48426241\n",
      "Iteration 134, loss = 0.48818550\n",
      "Iteration 135, loss = 0.49117143\n",
      "Iteration 136, loss = 0.48508974\n",
      "Iteration 137, loss = 0.48397527\n",
      "Iteration 138, loss = 0.49312980\n",
      "Iteration 139, loss = 0.49068473\n",
      "Iteration 140, loss = 0.48241648\n",
      "Iteration 141, loss = 0.48607975\n",
      "Iteration 142, loss = 0.49164595\n",
      "Iteration 143, loss = 0.48978419\n",
      "Iteration 144, loss = 0.48422153\n",
      "Iteration 145, loss = 0.48371197\n",
      "Iteration 146, loss = 0.48421846\n",
      "Iteration 147, loss = 0.48335709\n",
      "Iteration 148, loss = 0.48254875\n",
      "Iteration 149, loss = 0.48249462\n",
      "Iteration 150, loss = 0.48337393\n",
      "Iteration 151, loss = 0.48397468\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.11664414\n",
      "Iteration 2, loss = 5.32109041\n",
      "Iteration 3, loss = 3.56646496\n",
      "Iteration 4, loss = 1.89065286\n",
      "Iteration 5, loss = 0.78056997\n",
      "Iteration 6, loss = 0.63067971\n",
      "Iteration 7, loss = 0.84647868\n",
      "Iteration 8, loss = 1.04604267\n",
      "Iteration 9, loss = 1.17334890\n",
      "Iteration 10, loss = 1.21939873\n",
      "Iteration 11, loss = 1.19613397\n",
      "Iteration 12, loss = 1.11239672\n",
      "Iteration 13, loss = 0.99977020\n",
      "Iteration 14, loss = 0.85329937\n",
      "Iteration 15, loss = 0.72154672\n",
      "Iteration 16, loss = 0.61728297\n",
      "Iteration 17, loss = 0.58750453\n",
      "Iteration 18, loss = 0.61591414\n",
      "Iteration 19, loss = 0.66353427\n",
      "Iteration 20, loss = 0.68444970\n",
      "Iteration 21, loss = 0.65713654\n",
      "Iteration 22, loss = 0.60344589\n",
      "Iteration 23, loss = 0.56330105\n",
      "Iteration 24, loss = 0.54603064\n",
      "Iteration 25, loss = 0.55341768\n",
      "Iteration 26, loss = 0.56770538\n",
      "Iteration 27, loss = 0.57173522\n",
      "Iteration 28, loss = 0.56275440\n",
      "Iteration 29, loss = 0.54667322\n",
      "Iteration 30, loss = 0.52824990\n",
      "Iteration 31, loss = 0.51837643\n",
      "Iteration 32, loss = 0.52061103\n",
      "Iteration 33, loss = 0.52442543\n",
      "Iteration 34, loss = 0.52732006\n",
      "Iteration 35, loss = 0.52332731\n",
      "Iteration 36, loss = 0.51383119\n",
      "Iteration 37, loss = 0.50857846\n",
      "Iteration 38, loss = 0.50643410\n",
      "Iteration 39, loss = 0.50806769\n",
      "Iteration 40, loss = 0.51114212\n",
      "Iteration 41, loss = 0.50964204\n",
      "Iteration 42, loss = 0.50616429\n",
      "Iteration 43, loss = 0.50319711\n",
      "Iteration 44, loss = 0.50231435\n",
      "Iteration 45, loss = 0.50160639\n",
      "Iteration 46, loss = 0.50117178\n",
      "Iteration 47, loss = 0.50114282\n",
      "Iteration 48, loss = 0.50214460\n",
      "Iteration 49, loss = 0.50168774\n",
      "Iteration 50, loss = 0.50064202\n",
      "Iteration 51, loss = 0.49992308\n",
      "Iteration 52, loss = 0.49954298\n",
      "Iteration 53, loss = 0.49960215\n",
      "Iteration 54, loss = 0.49885764\n",
      "Iteration 55, loss = 0.49845804\n",
      "Iteration 56, loss = 0.49882462\n",
      "Iteration 57, loss = 0.50133303\n",
      "Iteration 58, loss = 0.50156992\n",
      "Iteration 59, loss = 0.49796676\n",
      "Iteration 60, loss = 0.49822020\n",
      "Iteration 61, loss = 0.50010651\n",
      "Iteration 62, loss = 0.49819093\n",
      "Iteration 63, loss = 0.49502887\n",
      "Iteration 64, loss = 0.49535565\n",
      "Iteration 65, loss = 0.49948356\n",
      "Iteration 66, loss = 0.50413978\n",
      "Iteration 67, loss = 0.50083703\n",
      "Iteration 68, loss = 0.49536183\n",
      "Iteration 69, loss = 0.49548804\n",
      "Iteration 70, loss = 0.49542167\n",
      "Iteration 71, loss = 0.49448600\n",
      "Iteration 72, loss = 0.49365212\n",
      "Iteration 73, loss = 0.49394528\n",
      "Iteration 74, loss = 0.49609036\n",
      "Iteration 75, loss = 0.49429713\n",
      "Iteration 76, loss = 0.49210743\n",
      "Iteration 77, loss = 0.49362623\n",
      "Iteration 78, loss = 0.49472005\n",
      "Iteration 79, loss = 0.49337211\n",
      "Iteration 80, loss = 0.49228961\n",
      "Iteration 81, loss = 0.49146572\n",
      "Iteration 82, loss = 0.49164384\n",
      "Iteration 83, loss = 0.49157333\n",
      "Iteration 84, loss = 0.49055625\n",
      "Iteration 85, loss = 0.49039807\n",
      "Iteration 86, loss = 0.48997126\n",
      "Iteration 87, loss = 0.49054560\n",
      "Iteration 88, loss = 0.48937504\n",
      "Iteration 89, loss = 0.48931005\n",
      "Iteration 90, loss = 0.49092843\n",
      "Iteration 91, loss = 0.49396608\n",
      "Iteration 92, loss = 0.49407244\n",
      "Iteration 93, loss = 0.49147406\n",
      "Iteration 94, loss = 0.48933093\n",
      "Iteration 95, loss = 0.48859916\n",
      "Iteration 96, loss = 0.48967279\n",
      "Iteration 97, loss = 0.49156198\n",
      "Iteration 98, loss = 0.49125593\n",
      "Iteration 99, loss = 0.48923283\n",
      "Iteration 100, loss = 0.48628900\n",
      "Iteration 101, loss = 0.49156496\n",
      "Iteration 102, loss = 0.49282992\n",
      "Iteration 103, loss = 0.48869086\n",
      "Iteration 104, loss = 0.48900867\n",
      "Iteration 105, loss = 0.48795866\n",
      "Iteration 106, loss = 0.48663710\n",
      "Iteration 107, loss = 0.48583364\n",
      "Iteration 108, loss = 0.48649593\n",
      "Iteration 109, loss = 0.48699111\n",
      "Iteration 110, loss = 0.48680937\n",
      "Iteration 111, loss = 0.48640535\n",
      "Iteration 112, loss = 0.48533592\n",
      "Iteration 113, loss = 0.48625300\n",
      "Iteration 114, loss = 0.48669683\n",
      "Iteration 115, loss = 0.48589542\n",
      "Iteration 116, loss = 0.48578878\n",
      "Iteration 117, loss = 0.48721933\n",
      "Iteration 118, loss = 0.48825860\n",
      "Iteration 119, loss = 0.48651145\n",
      "Iteration 120, loss = 0.48482556\n",
      "Iteration 121, loss = 0.48503882\n",
      "Iteration 122, loss = 0.48665354\n",
      "Iteration 123, loss = 0.48679041\n",
      "Iteration 124, loss = 0.48503211\n",
      "Iteration 125, loss = 0.48307993\n",
      "Iteration 126, loss = 0.48385130\n",
      "Iteration 127, loss = 0.48353876\n",
      "Iteration 128, loss = 0.48237024\n",
      "Iteration 129, loss = 0.48251030\n",
      "Iteration 130, loss = 0.48509756\n",
      "Iteration 131, loss = 0.48507351\n",
      "Iteration 132, loss = 0.48290959\n",
      "Iteration 133, loss = 0.48150644\n",
      "Iteration 134, loss = 0.48265877\n",
      "Iteration 135, loss = 0.48100400\n",
      "Iteration 136, loss = 0.48126837\n",
      "Iteration 137, loss = 0.48459281\n",
      "Iteration 138, loss = 0.48651749\n",
      "Iteration 139, loss = 0.48421545\n",
      "Iteration 140, loss = 0.48235991\n",
      "Iteration 141, loss = 0.48127767\n",
      "Iteration 142, loss = 0.47999090\n",
      "Iteration 143, loss = 0.47997723\n",
      "Iteration 144, loss = 0.48204920\n",
      "Iteration 145, loss = 0.48111878\n",
      "Iteration 146, loss = 0.48048898\n",
      "Iteration 147, loss = 0.47950796\n",
      "Iteration 148, loss = 0.47989253\n",
      "Iteration 149, loss = 0.47958434\n",
      "Iteration 150, loss = 0.47906025\n",
      "Iteration 151, loss = 0.47983587\n",
      "Iteration 152, loss = 0.48002274\n",
      "Iteration 153, loss = 0.47907519\n",
      "Iteration 154, loss = 0.47825768\n",
      "Iteration 155, loss = 0.47788322\n",
      "Iteration 156, loss = 0.47781910\n",
      "Iteration 157, loss = 0.47843589\n",
      "Iteration 158, loss = 0.47787406\n",
      "Iteration 159, loss = 0.47961586\n",
      "Iteration 160, loss = 0.47891468\n",
      "Iteration 161, loss = 0.47710873\n",
      "Iteration 162, loss = 0.47741515\n",
      "Iteration 163, loss = 0.47671566\n",
      "Iteration 164, loss = 0.47682052\n",
      "Iteration 165, loss = 0.47711387\n",
      "Iteration 166, loss = 0.47710627\n",
      "Iteration 167, loss = 0.47680266\n",
      "Iteration 168, loss = 0.47659247\n",
      "Iteration 169, loss = 0.47684318\n",
      "Iteration 170, loss = 0.47645982\n",
      "Iteration 171, loss = 0.47630646\n",
      "Iteration 172, loss = 0.47599152\n",
      "Iteration 173, loss = 0.47560237\n",
      "Iteration 174, loss = 0.47507044\n",
      "Iteration 175, loss = 0.47554199\n",
      "Iteration 176, loss = 0.47609371\n",
      "Iteration 177, loss = 0.47472886\n",
      "Iteration 178, loss = 0.47517643\n",
      "Iteration 179, loss = 0.47761292\n",
      "Iteration 180, loss = 0.47721078\n",
      "Iteration 181, loss = 0.47451558\n",
      "Iteration 182, loss = 0.47422088\n",
      "Iteration 183, loss = 0.47797396\n",
      "Iteration 184, loss = 0.48466924\n",
      "Iteration 185, loss = 0.47492554\n",
      "Iteration 186, loss = 0.47752587\n",
      "Iteration 187, loss = 0.49393734\n",
      "Iteration 188, loss = 0.49035508\n",
      "Iteration 189, loss = 0.47675538\n",
      "Iteration 190, loss = 0.47393510\n",
      "Iteration 191, loss = 0.47460093\n",
      "Iteration 192, loss = 0.47435267\n",
      "Iteration 193, loss = 0.47351401\n",
      "Iteration 194, loss = 0.47211243\n",
      "Iteration 195, loss = 0.47361169\n",
      "Iteration 196, loss = 0.47528907\n",
      "Iteration 197, loss = 0.47353329\n",
      "Iteration 198, loss = 0.47367253\n",
      "Iteration 199, loss = 0.47107862\n",
      "Iteration 200, loss = 0.47590584\n",
      "Iteration 201, loss = 0.47640489\n",
      "Iteration 202, loss = 0.47181914\n",
      "Iteration 203, loss = 0.47058389\n",
      "Iteration 204, loss = 0.47201014\n",
      "Iteration 205, loss = 0.47379346\n",
      "Iteration 206, loss = 0.47219587\n",
      "Iteration 207, loss = 0.47101501\n",
      "Iteration 208, loss = 0.47121489\n",
      "Iteration 209, loss = 0.47103153\n",
      "Iteration 210, loss = 0.46999047\n",
      "Iteration 211, loss = 0.47121624\n",
      "Iteration 212, loss = 0.46993449\n",
      "Iteration 213, loss = 0.46987755\n",
      "Iteration 214, loss = 0.47118842\n",
      "Iteration 215, loss = 0.47349970\n",
      "Iteration 216, loss = 0.47148639\n",
      "Iteration 217, loss = 0.46826490\n",
      "Iteration 218, loss = 0.47168962\n",
      "Iteration 219, loss = 0.47518680\n",
      "Iteration 220, loss = 0.47191698\n",
      "Iteration 221, loss = 0.46916874\n",
      "Iteration 222, loss = 0.46957296\n",
      "Iteration 223, loss = 0.47101186\n",
      "Iteration 224, loss = 0.47472448\n",
      "Iteration 225, loss = 0.47277080\n",
      "Iteration 226, loss = 0.47022904\n",
      "Iteration 227, loss = 0.47314265\n",
      "Iteration 228, loss = 0.47377500\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.19285557\n",
      "Iteration 2, loss = 0.70657607\n",
      "Iteration 3, loss = 0.75239631\n",
      "Iteration 4, loss = 0.69851348\n",
      "Iteration 5, loss = 0.60889237\n",
      "Iteration 6, loss = 0.59033974\n",
      "Iteration 7, loss = 0.61459566\n",
      "Iteration 8, loss = 0.59123405\n",
      "Iteration 9, loss = 0.55674694\n",
      "Iteration 10, loss = 0.56202409\n",
      "Iteration 11, loss = 0.56144596\n",
      "Iteration 12, loss = 0.54811044\n",
      "Iteration 13, loss = 0.53985634\n",
      "Iteration 14, loss = 0.54008968\n",
      "Iteration 15, loss = 0.53929722\n",
      "Iteration 16, loss = 0.53626108\n",
      "Iteration 17, loss = 0.53990996\n",
      "Iteration 18, loss = 0.54361710\n",
      "Iteration 19, loss = 0.53714105\n",
      "Iteration 20, loss = 0.53329517\n",
      "Iteration 21, loss = 0.53701266\n",
      "Iteration 22, loss = 0.53386242\n",
      "Iteration 23, loss = 0.53137570\n",
      "Iteration 24, loss = 0.52953429\n",
      "Iteration 25, loss = 0.52740621\n",
      "Iteration 26, loss = 0.52610807\n",
      "Iteration 27, loss = 0.52423043\n",
      "Iteration 28, loss = 0.52399729\n",
      "Iteration 29, loss = 0.52312058\n",
      "Iteration 30, loss = 0.52156737\n",
      "Iteration 31, loss = 0.52152894\n",
      "Iteration 32, loss = 0.52355882\n",
      "Iteration 33, loss = 0.51817416\n",
      "Iteration 34, loss = 0.52290539\n",
      "Iteration 35, loss = 0.53188844\n",
      "Iteration 36, loss = 0.51336077\n",
      "Iteration 37, loss = 0.52768214\n",
      "Iteration 38, loss = 0.54704025\n",
      "Iteration 39, loss = 0.52231170\n",
      "Iteration 40, loss = 0.51519051\n",
      "Iteration 41, loss = 0.54463875\n",
      "Iteration 42, loss = 0.53027187\n",
      "Iteration 43, loss = 0.51491114\n",
      "Iteration 44, loss = 0.51589765\n",
      "Iteration 45, loss = 0.50874373\n",
      "Iteration 46, loss = 0.51036835\n",
      "Iteration 47, loss = 0.50992848\n",
      "Iteration 48, loss = 0.50704557\n",
      "Iteration 49, loss = 0.51333491\n",
      "Iteration 50, loss = 0.50858133\n",
      "Iteration 51, loss = 0.50528776\n",
      "Iteration 52, loss = 0.50576928\n",
      "Iteration 53, loss = 0.50408719\n",
      "Iteration 54, loss = 0.50193404\n",
      "Iteration 55, loss = 0.50627817\n",
      "Iteration 56, loss = 0.50241343\n",
      "Iteration 57, loss = 0.50317729\n",
      "Iteration 58, loss = 0.50062452\n",
      "Iteration 59, loss = 0.49922194\n",
      "Iteration 60, loss = 0.51383079\n",
      "Iteration 61, loss = 0.50991311\n",
      "Iteration 62, loss = 0.49525502\n",
      "Iteration 63, loss = 0.50633469\n",
      "Iteration 64, loss = 0.51817678\n",
      "Iteration 65, loss = 0.50678763\n",
      "Iteration 66, loss = 0.49593950\n",
      "Iteration 67, loss = 0.49792580\n",
      "Iteration 68, loss = 0.49708582\n",
      "Iteration 69, loss = 0.49469180\n",
      "Iteration 70, loss = 0.50116823\n",
      "Iteration 71, loss = 0.49686344\n",
      "Iteration 72, loss = 0.49496649\n",
      "Iteration 73, loss = 0.49764109\n",
      "Iteration 74, loss = 0.49477099\n",
      "Iteration 75, loss = 0.49197049\n",
      "Iteration 76, loss = 0.49269081\n",
      "Iteration 77, loss = 0.49251644\n",
      "Iteration 78, loss = 0.49026458\n",
      "Iteration 79, loss = 0.49180642\n",
      "Iteration 80, loss = 0.48948940\n",
      "Iteration 81, loss = 0.48934375\n",
      "Iteration 82, loss = 0.49148694\n",
      "Iteration 83, loss = 0.49010301\n",
      "Iteration 84, loss = 0.48752921\n",
      "Iteration 85, loss = 0.49662784\n",
      "Iteration 86, loss = 0.49193815\n",
      "Iteration 87, loss = 0.48999095\n",
      "Iteration 88, loss = 0.48887319\n",
      "Iteration 89, loss = 0.48528229\n",
      "Iteration 90, loss = 0.49387226\n",
      "Iteration 91, loss = 0.49280290\n",
      "Iteration 92, loss = 0.48490461\n",
      "Iteration 93, loss = 0.49174753\n",
      "Iteration 94, loss = 0.48804517\n",
      "Iteration 95, loss = 0.48406287\n",
      "Iteration 96, loss = 0.49450113\n",
      "Iteration 97, loss = 0.48700008\n",
      "Iteration 98, loss = 0.48723949\n",
      "Iteration 99, loss = 0.50038250\n",
      "Iteration 100, loss = 0.49371971\n",
      "Iteration 101, loss = 0.48668000\n",
      "Iteration 102, loss = 0.48941549\n",
      "Iteration 103, loss = 0.48518219\n",
      "Iteration 104, loss = 0.48619803\n",
      "Iteration 105, loss = 0.48749843\n",
      "Iteration 106, loss = 0.48400588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.78113849\n",
      "Iteration 2, loss = 1.83398878\n",
      "Iteration 3, loss = 1.40153388\n",
      "Iteration 4, loss = 1.29397938\n",
      "Iteration 5, loss = 1.34272572\n",
      "Iteration 6, loss = 1.39149141\n",
      "Iteration 7, loss = 1.36101316\n",
      "Iteration 8, loss = 1.24612530\n",
      "Iteration 9, loss = 1.07858705\n",
      "Iteration 10, loss = 0.95386741\n",
      "Iteration 11, loss = 0.87925086\n",
      "Iteration 12, loss = 0.86668043\n",
      "Iteration 13, loss = 0.83613747\n",
      "Iteration 14, loss = 0.76780067\n",
      "Iteration 15, loss = 0.69125709\n",
      "Iteration 16, loss = 0.65388396\n",
      "Iteration 17, loss = 0.63299418\n",
      "Iteration 18, loss = 0.60727278\n",
      "Iteration 19, loss = 0.57306649\n",
      "Iteration 20, loss = 0.55096244\n",
      "Iteration 21, loss = 0.55081179\n",
      "Iteration 22, loss = 0.54137139\n",
      "Iteration 23, loss = 0.52662294\n",
      "Iteration 24, loss = 0.51718896\n",
      "Iteration 25, loss = 0.51559802\n",
      "Iteration 26, loss = 0.51168180\n",
      "Iteration 27, loss = 0.50853652\n",
      "Iteration 28, loss = 0.50726790\n",
      "Iteration 29, loss = 0.50670524\n",
      "Iteration 30, loss = 0.50559404\n",
      "Iteration 31, loss = 0.50632568\n",
      "Iteration 32, loss = 0.50614788\n",
      "Iteration 33, loss = 0.50617305\n",
      "Iteration 34, loss = 0.50601854\n",
      "Iteration 35, loss = 0.50502394\n",
      "Iteration 36, loss = 0.50439972\n",
      "Iteration 37, loss = 0.50377267\n",
      "Iteration 38, loss = 0.50296541\n",
      "Iteration 39, loss = 0.50318095\n",
      "Iteration 40, loss = 0.50080430\n",
      "Iteration 41, loss = 0.49963889\n",
      "Iteration 42, loss = 0.50020137\n",
      "Iteration 43, loss = 0.49841365\n",
      "Iteration 44, loss = 0.49967362\n",
      "Iteration 45, loss = 0.50218625\n",
      "Iteration 46, loss = 0.49885681\n",
      "Iteration 47, loss = 0.49754721\n",
      "Iteration 48, loss = 0.49838357\n",
      "Iteration 49, loss = 0.49779992\n",
      "Iteration 50, loss = 0.49705539\n",
      "Iteration 51, loss = 0.49999393\n",
      "Iteration 52, loss = 0.49889430\n",
      "Iteration 53, loss = 0.49662514\n",
      "Iteration 54, loss = 0.49544849\n",
      "Iteration 55, loss = 0.49536823\n",
      "Iteration 56, loss = 0.49513961\n",
      "Iteration 57, loss = 0.49524436\n",
      "Iteration 58, loss = 0.49593650\n",
      "Iteration 59, loss = 0.49481444\n",
      "Iteration 60, loss = 0.49369431\n",
      "Iteration 61, loss = 0.49988308\n",
      "Iteration 62, loss = 0.50086943\n",
      "Iteration 63, loss = 0.49310699\n",
      "Iteration 64, loss = 0.49507200\n",
      "Iteration 65, loss = 0.49851735\n",
      "Iteration 66, loss = 0.49359455\n",
      "Iteration 67, loss = 0.49350895\n",
      "Iteration 68, loss = 0.49635183\n",
      "Iteration 69, loss = 0.49583131\n",
      "Iteration 70, loss = 0.49237190\n",
      "Iteration 71, loss = 0.49252553\n",
      "Iteration 72, loss = 0.49159527\n",
      "Iteration 73, loss = 0.49279460\n",
      "Iteration 74, loss = 0.50219104\n",
      "Iteration 75, loss = 0.50005908\n",
      "Iteration 76, loss = 0.49102151\n",
      "Iteration 77, loss = 0.49259277\n",
      "Iteration 78, loss = 0.49915907\n",
      "Iteration 79, loss = 0.49308547\n",
      "Iteration 80, loss = 0.49213189\n",
      "Iteration 81, loss = 0.49749069\n",
      "Iteration 82, loss = 0.49190809\n",
      "Iteration 83, loss = 0.49327903\n",
      "Iteration 84, loss = 0.49083310\n",
      "Iteration 85, loss = 0.49080753\n",
      "Iteration 86, loss = 0.48895889\n",
      "Iteration 87, loss = 0.49050760\n",
      "Iteration 88, loss = 0.49371160\n",
      "Iteration 89, loss = 0.49104747\n",
      "Iteration 90, loss = 0.48947062\n",
      "Iteration 91, loss = 0.49014845\n",
      "Iteration 92, loss = 0.48858991\n",
      "Iteration 93, loss = 0.48935089\n",
      "Iteration 94, loss = 0.49100160\n",
      "Iteration 95, loss = 0.49014252\n",
      "Iteration 96, loss = 0.48808753\n",
      "Iteration 97, loss = 0.48804953\n",
      "Iteration 98, loss = 0.48810792\n",
      "Iteration 99, loss = 0.49022625\n",
      "Iteration 100, loss = 0.48970251\n",
      "Iteration 101, loss = 0.49827750\n",
      "Iteration 102, loss = 0.49076866\n",
      "Iteration 103, loss = 0.49193786\n",
      "Iteration 104, loss = 0.49645771\n",
      "Iteration 105, loss = 0.49055014\n",
      "Iteration 106, loss = 0.48912102\n",
      "Iteration 107, loss = 0.49018914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 14.59740199\n",
      "Iteration 2, loss = 13.04692339\n",
      "Iteration 3, loss = 11.48998341\n",
      "Iteration 4, loss = 9.96509921\n",
      "Iteration 5, loss = 8.41933649\n",
      "Iteration 6, loss = 6.86651809\n",
      "Iteration 7, loss = 5.34680438\n",
      "Iteration 8, loss = 3.82601525\n",
      "Iteration 9, loss = 2.39856723\n",
      "Iteration 10, loss = 1.23931027\n",
      "Iteration 11, loss = 0.74657812\n",
      "Iteration 12, loss = 0.87762507\n",
      "Iteration 13, loss = 1.13437134\n",
      "Iteration 14, loss = 1.33827030\n",
      "Iteration 15, loss = 1.45981318\n",
      "Iteration 16, loss = 1.50550891\n",
      "Iteration 17, loss = 1.48359899\n",
      "Iteration 18, loss = 1.40953303\n",
      "Iteration 19, loss = 1.29334427\n",
      "Iteration 20, loss = 1.15398956\n",
      "Iteration 21, loss = 1.00467153\n",
      "Iteration 22, loss = 0.84817193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.03641680\n",
      "Iteration 2, loss = 2.48575094\n",
      "Iteration 3, loss = 1.93824247\n",
      "Iteration 4, loss = 1.40118710\n",
      "Iteration 5, loss = 0.89280745\n",
      "Iteration 6, loss = 0.58209004\n",
      "Iteration 7, loss = 0.73067731\n",
      "Iteration 8, loss = 0.88844025\n",
      "Iteration 9, loss = 0.76735281\n",
      "Iteration 10, loss = 0.58719068\n",
      "Iteration 11, loss = 0.54656356\n",
      "Iteration 12, loss = 0.59597359\n",
      "Iteration 13, loss = 0.63144210\n",
      "Iteration 14, loss = 0.62251945\n",
      "Iteration 15, loss = 0.58264898\n",
      "Iteration 16, loss = 0.54296231\n",
      "Iteration 17, loss = 0.52341333\n",
      "Iteration 18, loss = 0.52754029\n",
      "Iteration 19, loss = 0.54341764\n",
      "Iteration 20, loss = 0.54132811\n",
      "Iteration 21, loss = 0.52509405\n",
      "Iteration 22, loss = 0.51253832\n",
      "Iteration 23, loss = 0.51069133\n",
      "Iteration 24, loss = 0.51324951\n",
      "Iteration 25, loss = 0.51561528\n",
      "Iteration 26, loss = 0.51511234\n",
      "Iteration 27, loss = 0.51014300\n",
      "Iteration 28, loss = 0.50219421\n",
      "Iteration 29, loss = 0.50239395\n",
      "Iteration 30, loss = 0.51285144\n",
      "Iteration 31, loss = 0.51239374\n",
      "Iteration 32, loss = 0.50047911\n",
      "Iteration 33, loss = 0.50169708\n",
      "Iteration 34, loss = 0.51124409\n",
      "Iteration 35, loss = 0.50624995\n",
      "Iteration 36, loss = 0.49918172\n",
      "Iteration 37, loss = 0.49982612\n",
      "Iteration 38, loss = 0.50461156\n",
      "Iteration 39, loss = 0.50334360\n",
      "Iteration 40, loss = 0.49926710\n",
      "Iteration 41, loss = 0.49652987\n",
      "Iteration 42, loss = 0.49600488\n",
      "Iteration 43, loss = 0.49580382\n",
      "Iteration 44, loss = 0.49627464\n",
      "Iteration 45, loss = 0.49561825\n",
      "Iteration 46, loss = 0.49509030\n",
      "Iteration 47, loss = 0.49474276\n",
      "Iteration 48, loss = 0.49473795\n",
      "Iteration 49, loss = 0.49440812\n",
      "Iteration 50, loss = 0.49484723\n",
      "Iteration 51, loss = 0.49502309\n",
      "Iteration 52, loss = 0.49436762\n",
      "Iteration 53, loss = 0.49479565\n",
      "Iteration 54, loss = 0.49662856\n",
      "Iteration 55, loss = 0.49705034\n",
      "Iteration 56, loss = 0.49583534\n",
      "Iteration 57, loss = 0.49473623\n",
      "Iteration 58, loss = 0.49340792\n",
      "Iteration 59, loss = 0.49394539\n",
      "Iteration 60, loss = 0.49658787\n",
      "Iteration 61, loss = 0.49596939\n",
      "Iteration 62, loss = 0.49311112\n",
      "Iteration 63, loss = 0.49345525\n",
      "Iteration 64, loss = 0.49656061\n",
      "Iteration 65, loss = 0.49734816\n",
      "Iteration 66, loss = 0.49425214\n",
      "Iteration 67, loss = 0.49233662\n",
      "Iteration 68, loss = 0.49436484\n",
      "Iteration 69, loss = 0.49816029\n",
      "Iteration 70, loss = 0.49704035\n",
      "Iteration 71, loss = 0.49271765\n",
      "Iteration 72, loss = 0.49392654\n",
      "Iteration 73, loss = 0.49353558\n",
      "Iteration 74, loss = 0.49168275\n",
      "Iteration 75, loss = 0.49239916\n",
      "Iteration 76, loss = 0.49517591\n",
      "Iteration 77, loss = 0.49454907\n",
      "Iteration 78, loss = 0.49191092\n",
      "Iteration 79, loss = 0.49438417\n",
      "Iteration 80, loss = 0.49786296\n",
      "Iteration 81, loss = 0.49684595\n",
      "Iteration 82, loss = 0.49279347\n",
      "Iteration 83, loss = 0.49199073\n",
      "Iteration 84, loss = 0.49085616\n",
      "Iteration 85, loss = 0.49232009\n",
      "Iteration 86, loss = 0.49488904\n",
      "Iteration 87, loss = 0.49406136\n",
      "Iteration 88, loss = 0.49119600\n",
      "Iteration 89, loss = 0.48982962\n",
      "Iteration 90, loss = 0.49450662\n",
      "Iteration 91, loss = 0.49626247\n",
      "Iteration 92, loss = 0.49346158\n",
      "Iteration 93, loss = 0.49093906\n",
      "Iteration 94, loss = 0.49146553\n",
      "Iteration 95, loss = 0.49139799\n",
      "Iteration 96, loss = 0.49084743\n",
      "Iteration 97, loss = 0.49413083\n",
      "Iteration 98, loss = 0.49177033\n",
      "Iteration 99, loss = 0.49070420\n",
      "Iteration 100, loss = 0.49042707\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79803857\n",
      "Iteration 2, loss = 0.72214711\n",
      "Iteration 3, loss = 0.68346691\n",
      "Iteration 4, loss = 0.62227504\n",
      "Iteration 5, loss = 0.58031127\n",
      "Iteration 6, loss = 0.55375831\n",
      "Iteration 7, loss = 0.53915729\n",
      "Iteration 8, loss = 0.52828540\n",
      "Iteration 9, loss = 0.53110174\n",
      "Iteration 10, loss = 0.52883201\n",
      "Iteration 11, loss = 0.52962890\n",
      "Iteration 12, loss = 0.53297939\n",
      "Iteration 13, loss = 0.53722824\n",
      "Iteration 14, loss = 0.52822491\n",
      "Iteration 15, loss = 0.54137689\n",
      "Iteration 16, loss = 0.51949208\n",
      "Iteration 17, loss = 0.52387852\n",
      "Iteration 18, loss = 0.52157211\n",
      "Iteration 19, loss = 0.51747398\n",
      "Iteration 20, loss = 0.51897555\n",
      "Iteration 21, loss = 0.51317898\n",
      "Iteration 22, loss = 0.51466232\n",
      "Iteration 23, loss = 0.51378509\n",
      "Iteration 24, loss = 0.51087756\n",
      "Iteration 25, loss = 0.51023454\n",
      "Iteration 26, loss = 0.50962822\n",
      "Iteration 27, loss = 0.50904924\n",
      "Iteration 28, loss = 0.51183524\n",
      "Iteration 29, loss = 0.51355523\n",
      "Iteration 30, loss = 0.51122452\n",
      "Iteration 31, loss = 0.51299698\n",
      "Iteration 32, loss = 0.50948967\n",
      "Iteration 33, loss = 0.50757540\n",
      "Iteration 34, loss = 0.50859743\n",
      "Iteration 35, loss = 0.53769640\n",
      "Iteration 36, loss = 0.51416625\n",
      "Iteration 37, loss = 0.53214899\n",
      "Iteration 38, loss = 0.52455109\n",
      "Iteration 39, loss = 0.50777415\n",
      "Iteration 40, loss = 0.51383337\n",
      "Iteration 41, loss = 0.50483173\n",
      "Iteration 42, loss = 0.52430302\n",
      "Iteration 43, loss = 0.50500448\n",
      "Iteration 44, loss = 0.51719105\n",
      "Iteration 45, loss = 0.51157135\n",
      "Iteration 46, loss = 0.50826495\n",
      "Iteration 47, loss = 0.51078331\n",
      "Iteration 48, loss = 0.50912630\n",
      "Iteration 49, loss = 0.52401346\n",
      "Iteration 50, loss = 0.50699025\n",
      "Iteration 51, loss = 0.50515464\n",
      "Iteration 52, loss = 0.50459001\n",
      "Iteration 53, loss = 0.50247026\n",
      "Iteration 54, loss = 0.50036984\n",
      "Iteration 55, loss = 0.50602533\n",
      "Iteration 56, loss = 0.50082609\n",
      "Iteration 57, loss = 0.49872095\n",
      "Iteration 58, loss = 0.49791004\n",
      "Iteration 59, loss = 0.50139050\n",
      "Iteration 60, loss = 0.50084226\n",
      "Iteration 61, loss = 0.49888597\n",
      "Iteration 62, loss = 0.49712145\n",
      "Iteration 63, loss = 0.50146418\n",
      "Iteration 64, loss = 0.49677543\n",
      "Iteration 65, loss = 0.50314447\n",
      "Iteration 66, loss = 0.49972797\n",
      "Iteration 67, loss = 0.50048398\n",
      "Iteration 68, loss = 0.49924569\n",
      "Iteration 69, loss = 0.49528411\n",
      "Iteration 70, loss = 0.49883092\n",
      "Iteration 71, loss = 0.49643428\n",
      "Iteration 72, loss = 0.51142672\n",
      "Iteration 73, loss = 0.50411916\n",
      "Iteration 74, loss = 0.49537869\n",
      "Iteration 75, loss = 0.49443288\n",
      "Iteration 76, loss = 0.49326760\n",
      "Iteration 77, loss = 0.49284988\n",
      "Iteration 78, loss = 0.49288275\n",
      "Iteration 79, loss = 0.49207328\n",
      "Iteration 80, loss = 0.49362146\n",
      "Iteration 81, loss = 0.49479306\n",
      "Iteration 82, loss = 0.49746229\n",
      "Iteration 83, loss = 0.49643902\n",
      "Iteration 84, loss = 0.49483352\n",
      "Iteration 85, loss = 0.49123641\n",
      "Iteration 86, loss = 0.49813701\n",
      "Iteration 87, loss = 0.49460969\n",
      "Iteration 88, loss = 0.49695373\n",
      "Iteration 89, loss = 0.50222534\n",
      "Iteration 90, loss = 0.49138641\n",
      "Iteration 91, loss = 0.49674630\n",
      "Iteration 92, loss = 0.50083014\n",
      "Iteration 93, loss = 0.49835715\n",
      "Iteration 94, loss = 0.49622553\n",
      "Iteration 95, loss = 0.49582818\n",
      "Iteration 96, loss = 0.48986796\n",
      "Iteration 97, loss = 0.48931069\n",
      "Iteration 98, loss = 0.48915072\n",
      "Iteration 99, loss = 0.49020187\n",
      "Iteration 100, loss = 0.48909010\n",
      "Iteration 101, loss = 0.49096021\n",
      "Iteration 102, loss = 0.49265615\n",
      "Iteration 103, loss = 0.49178886\n",
      "Iteration 104, loss = 0.49103925\n",
      "Iteration 105, loss = 0.48927321\n",
      "Iteration 106, loss = 0.49895184\n",
      "Iteration 107, loss = 0.49024241\n",
      "Iteration 108, loss = 0.50536351\n",
      "Iteration 109, loss = 0.48934459\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50106613\n",
      "Iteration 2, loss = 0.84572367\n",
      "Iteration 3, loss = 0.90167325\n",
      "Iteration 4, loss = 0.84067868\n",
      "Iteration 5, loss = 0.71552290\n",
      "Iteration 6, loss = 0.71720038\n",
      "Iteration 7, loss = 0.69890071\n",
      "Iteration 8, loss = 0.64730732\n",
      "Iteration 9, loss = 0.62379157\n",
      "Iteration 10, loss = 0.62587654\n",
      "Iteration 11, loss = 0.59988283\n",
      "Iteration 12, loss = 0.59295999\n",
      "Iteration 13, loss = 0.60114225\n",
      "Iteration 14, loss = 0.57891826\n",
      "Iteration 15, loss = 0.59260234\n",
      "Iteration 16, loss = 0.60435726\n",
      "Iteration 17, loss = 0.57325611\n",
      "Iteration 18, loss = 0.57493872\n",
      "Iteration 19, loss = 0.59525158\n",
      "Iteration 20, loss = 0.57950993\n",
      "Iteration 21, loss = 0.56715163\n",
      "Iteration 22, loss = 0.57951917\n",
      "Iteration 23, loss = 0.57077543\n",
      "Iteration 24, loss = 0.55586897\n",
      "Iteration 25, loss = 0.57253932\n",
      "Iteration 26, loss = 0.56489090\n",
      "Iteration 27, loss = 0.55179652\n",
      "Iteration 28, loss = 0.57475338\n",
      "Iteration 29, loss = 0.56622516\n",
      "Iteration 30, loss = 0.55157347\n",
      "Iteration 31, loss = 0.55792449\n",
      "Iteration 32, loss = 0.55365077\n",
      "Iteration 33, loss = 0.54590112\n",
      "Iteration 34, loss = 0.54554360\n",
      "Iteration 35, loss = 0.54518037\n",
      "Iteration 36, loss = 0.54349270\n",
      "Iteration 37, loss = 0.54316717\n",
      "Iteration 38, loss = 0.54224126\n",
      "Iteration 39, loss = 0.54606048\n",
      "Iteration 40, loss = 0.54404303\n",
      "Iteration 41, loss = 0.54541449\n",
      "Iteration 42, loss = 0.54223036\n",
      "Iteration 43, loss = 0.54407068\n",
      "Iteration 44, loss = 0.54138629\n",
      "Iteration 45, loss = 0.54070080\n",
      "Iteration 46, loss = 0.54387277\n",
      "Iteration 47, loss = 0.53428514\n",
      "Iteration 48, loss = 0.56360800\n",
      "Iteration 49, loss = 0.54631535\n",
      "Iteration 50, loss = 0.54823533\n",
      "Iteration 51, loss = 0.57131618\n",
      "Iteration 52, loss = 0.53155946\n",
      "Iteration 53, loss = 0.58636230\n",
      "Iteration 54, loss = 0.56992012\n",
      "Iteration 55, loss = 0.54441735\n",
      "Iteration 56, loss = 0.56267266\n",
      "Iteration 57, loss = 0.53959466\n",
      "Iteration 58, loss = 0.54074327\n",
      "Iteration 59, loss = 0.54256281\n",
      "Iteration 60, loss = 0.53216499\n",
      "Iteration 61, loss = 0.53166025\n",
      "Iteration 62, loss = 0.53201768\n",
      "Iteration 63, loss = 0.53116032\n",
      "Iteration 64, loss = 0.53099528\n",
      "Iteration 65, loss = 0.53120196\n",
      "Iteration 66, loss = 0.53289216\n",
      "Iteration 67, loss = 0.53101780\n",
      "Iteration 68, loss = 0.52994153\n",
      "Iteration 69, loss = 0.53139299\n",
      "Iteration 70, loss = 0.53024735\n",
      "Iteration 71, loss = 0.53114033\n",
      "Iteration 72, loss = 0.52843457\n",
      "Iteration 73, loss = 0.52991940\n",
      "Iteration 74, loss = 0.53272346\n",
      "Iteration 75, loss = 0.53071927\n",
      "Iteration 76, loss = 0.53244886\n",
      "Iteration 77, loss = 0.53218779\n",
      "Iteration 78, loss = 0.52798165\n",
      "Iteration 79, loss = 0.53583709\n",
      "Iteration 80, loss = 0.53106640\n",
      "Iteration 81, loss = 0.52678221\n",
      "Iteration 82, loss = 0.52644663\n",
      "Iteration 83, loss = 0.52899938\n",
      "Iteration 84, loss = 0.52843243\n",
      "Iteration 85, loss = 0.52687130\n",
      "Iteration 86, loss = 0.52775347\n",
      "Iteration 87, loss = 0.52407442\n",
      "Iteration 88, loss = 0.53188853\n",
      "Iteration 89, loss = 0.53294587\n",
      "Iteration 90, loss = 0.52586299\n",
      "Iteration 91, loss = 0.52838707\n",
      "Iteration 92, loss = 0.52394865\n",
      "Iteration 93, loss = 0.52707924\n",
      "Iteration 94, loss = 0.52275024\n",
      "Iteration 95, loss = 0.53478335\n",
      "Iteration 96, loss = 0.52540057\n",
      "Iteration 97, loss = 0.53051263\n",
      "Iteration 98, loss = 0.52439525\n",
      "Iteration 99, loss = 0.53780808\n",
      "Iteration 100, loss = 0.52359003\n",
      "Iteration 101, loss = 0.54795211\n",
      "Iteration 102, loss = 0.54378946\n",
      "Iteration 103, loss = 0.53110446\n",
      "Iteration 104, loss = 0.56431101\n",
      "Iteration 105, loss = 0.52298492\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.45063171\n",
      "Iteration 2, loss = 1.41250247\n",
      "Iteration 3, loss = 0.63477104\n",
      "Iteration 4, loss = 1.05635502\n",
      "Iteration 5, loss = 1.09757987\n",
      "Iteration 6, loss = 0.67489882\n",
      "Iteration 7, loss = 0.61942781\n",
      "Iteration 8, loss = 0.74420787\n",
      "Iteration 9, loss = 0.79377414\n",
      "Iteration 10, loss = 0.72489395\n",
      "Iteration 11, loss = 0.61025665\n",
      "Iteration 12, loss = 0.56997709\n",
      "Iteration 13, loss = 0.65207921\n",
      "Iteration 14, loss = 0.68219006\n",
      "Iteration 15, loss = 0.59200403\n",
      "Iteration 16, loss = 0.55098932\n",
      "Iteration 17, loss = 0.60368525\n",
      "Iteration 18, loss = 0.63697681\n",
      "Iteration 19, loss = 0.59955052\n",
      "Iteration 20, loss = 0.54683466\n",
      "Iteration 21, loss = 0.56124887\n",
      "Iteration 22, loss = 0.58023358\n",
      "Iteration 23, loss = 0.55612453\n",
      "Iteration 24, loss = 0.53823513\n",
      "Iteration 25, loss = 0.54425902\n",
      "Iteration 26, loss = 0.54458507\n",
      "Iteration 27, loss = 0.53633048\n",
      "Iteration 28, loss = 0.53139745\n",
      "Iteration 29, loss = 0.54450145\n",
      "Iteration 30, loss = 0.55300066\n",
      "Iteration 31, loss = 0.53992719\n",
      "Iteration 32, loss = 0.53147481\n",
      "Iteration 33, loss = 0.53564129\n",
      "Iteration 34, loss = 0.53547991\n",
      "Iteration 35, loss = 0.52930146\n",
      "Iteration 36, loss = 0.52931856\n",
      "Iteration 37, loss = 0.53313532\n",
      "Iteration 38, loss = 0.53147432\n",
      "Iteration 39, loss = 0.52636350\n",
      "Iteration 40, loss = 0.52703575\n",
      "Iteration 41, loss = 0.53134306\n",
      "Iteration 42, loss = 0.52864991\n",
      "Iteration 43, loss = 0.52448451\n",
      "Iteration 44, loss = 0.52323743\n",
      "Iteration 45, loss = 0.52268331\n",
      "Iteration 46, loss = 0.52145638\n",
      "Iteration 47, loss = 0.52115866\n",
      "Iteration 48, loss = 0.52096219\n",
      "Iteration 49, loss = 0.51919407\n",
      "Iteration 50, loss = 0.52024524\n",
      "Iteration 51, loss = 0.52636555\n",
      "Iteration 52, loss = 0.52324857\n",
      "Iteration 53, loss = 0.51711935\n",
      "Iteration 54, loss = 0.52305676\n",
      "Iteration 55, loss = 0.52108725\n",
      "Iteration 56, loss = 0.51714207\n",
      "Iteration 57, loss = 0.52542484\n",
      "Iteration 58, loss = 0.51888305\n",
      "Iteration 59, loss = 0.51619904\n",
      "Iteration 60, loss = 0.52817629\n",
      "Iteration 61, loss = 0.53070551\n",
      "Iteration 62, loss = 0.51747194\n",
      "Iteration 63, loss = 0.51514649\n",
      "Iteration 64, loss = 0.52529671\n",
      "Iteration 65, loss = 0.51586138\n",
      "Iteration 66, loss = 0.51660265\n",
      "Iteration 67, loss = 0.52192649\n",
      "Iteration 68, loss = 0.51507883\n",
      "Iteration 69, loss = 0.51810812\n",
      "Iteration 70, loss = 0.51121382\n",
      "Iteration 71, loss = 0.51816150\n",
      "Iteration 72, loss = 0.52289711\n",
      "Iteration 73, loss = 0.50809976\n",
      "Iteration 74, loss = 0.53110446\n",
      "Iteration 75, loss = 0.54032418\n",
      "Iteration 76, loss = 0.51805428\n",
      "Iteration 77, loss = 0.51856916\n",
      "Iteration 78, loss = 0.51267784\n",
      "Iteration 79, loss = 0.51273176\n",
      "Iteration 80, loss = 0.52527375\n",
      "Iteration 81, loss = 0.51475966\n",
      "Iteration 82, loss = 0.51322692\n",
      "Iteration 83, loss = 0.51699374\n",
      "Iteration 84, loss = 0.51468791\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.45455483\n",
      "Iteration 2, loss = 2.56034758\n",
      "Iteration 3, loss = 1.72958001\n",
      "Iteration 4, loss = 0.90570717\n",
      "Iteration 5, loss = 0.66360830\n",
      "Iteration 6, loss = 1.18311449\n",
      "Iteration 7, loss = 1.07315559\n",
      "Iteration 8, loss = 0.67873083\n",
      "Iteration 9, loss = 0.57289242\n",
      "Iteration 10, loss = 0.70640597\n",
      "Iteration 11, loss = 0.77655097\n",
      "Iteration 12, loss = 0.73081755\n",
      "Iteration 13, loss = 0.61612860\n",
      "Iteration 14, loss = 0.53257271\n",
      "Iteration 15, loss = 0.57074869\n",
      "Iteration 16, loss = 0.63291485\n",
      "Iteration 17, loss = 0.61447449\n",
      "Iteration 18, loss = 0.54942290\n",
      "Iteration 19, loss = 0.52578965\n",
      "Iteration 20, loss = 0.55189188\n",
      "Iteration 21, loss = 0.57170797\n",
      "Iteration 22, loss = 0.56301336\n",
      "Iteration 23, loss = 0.53839345\n",
      "Iteration 24, loss = 0.52478021\n",
      "Iteration 25, loss = 0.52457556\n",
      "Iteration 26, loss = 0.52964937\n",
      "Iteration 27, loss = 0.52785365\n",
      "Iteration 28, loss = 0.52023478\n",
      "Iteration 29, loss = 0.52238173\n",
      "Iteration 30, loss = 0.52143566\n",
      "Iteration 31, loss = 0.51681827\n",
      "Iteration 32, loss = 0.51715544\n",
      "Iteration 33, loss = 0.51857894\n",
      "Iteration 34, loss = 0.51729997\n",
      "Iteration 35, loss = 0.51436602\n",
      "Iteration 36, loss = 0.51245459\n",
      "Iteration 37, loss = 0.51663457\n",
      "Iteration 38, loss = 0.52019126\n",
      "Iteration 39, loss = 0.51797350\n",
      "Iteration 40, loss = 0.51350650\n",
      "Iteration 41, loss = 0.51283098\n",
      "Iteration 42, loss = 0.51377970\n",
      "Iteration 43, loss = 0.51218417\n",
      "Iteration 44, loss = 0.51207845\n",
      "Iteration 45, loss = 0.51916379\n",
      "Iteration 46, loss = 0.51762773\n",
      "Iteration 47, loss = 0.51143912\n",
      "Iteration 48, loss = 0.51493222\n",
      "Iteration 49, loss = 0.52233761\n",
      "Iteration 50, loss = 0.51654712\n",
      "Iteration 51, loss = 0.50914087\n",
      "Iteration 52, loss = 0.51292893\n",
      "Iteration 53, loss = 0.51517349\n",
      "Iteration 54, loss = 0.50941737\n",
      "Iteration 55, loss = 0.51053574\n",
      "Iteration 56, loss = 0.51420556\n",
      "Iteration 57, loss = 0.50883951\n",
      "Iteration 58, loss = 0.50920701\n",
      "Iteration 59, loss = 0.51352874\n",
      "Iteration 60, loss = 0.51357671\n",
      "Iteration 61, loss = 0.50875558\n",
      "Iteration 62, loss = 0.50652648\n",
      "Iteration 63, loss = 0.51261174\n",
      "Iteration 64, loss = 0.51102989\n",
      "Iteration 65, loss = 0.50753914\n",
      "Iteration 66, loss = 0.50717424\n",
      "Iteration 67, loss = 0.50525763\n",
      "Iteration 68, loss = 0.50777117\n",
      "Iteration 69, loss = 0.51239673\n",
      "Iteration 70, loss = 0.50831587\n",
      "Iteration 71, loss = 0.50802818\n",
      "Iteration 72, loss = 0.50671404\n",
      "Iteration 73, loss = 0.50598801\n",
      "Iteration 74, loss = 0.50466564\n",
      "Iteration 75, loss = 0.50519046\n",
      "Iteration 76, loss = 0.50419644\n",
      "Iteration 77, loss = 0.50534944\n",
      "Iteration 78, loss = 0.50619217\n",
      "Iteration 79, loss = 0.50271306\n",
      "Iteration 80, loss = 0.51103528\n",
      "Iteration 81, loss = 0.51695716\n",
      "Iteration 82, loss = 0.51029609\n",
      "Iteration 83, loss = 0.50503374\n",
      "Iteration 84, loss = 0.50420824\n",
      "Iteration 85, loss = 0.50268751\n",
      "Iteration 86, loss = 0.50345455\n",
      "Iteration 87, loss = 0.50336576\n",
      "Iteration 88, loss = 0.50187656\n",
      "Iteration 89, loss = 0.50429209\n",
      "Iteration 90, loss = 0.50529522\n",
      "Iteration 91, loss = 0.50221592\n",
      "Iteration 92, loss = 0.50605635\n",
      "Iteration 93, loss = 0.50887176\n",
      "Iteration 94, loss = 0.50472385\n",
      "Iteration 95, loss = 0.50175411\n",
      "Iteration 96, loss = 0.50496651\n",
      "Iteration 97, loss = 0.50138601\n",
      "Iteration 98, loss = 0.50416145\n",
      "Iteration 99, loss = 0.50891071\n",
      "Iteration 100, loss = 0.50274861\n",
      "Iteration 101, loss = 0.50425500\n",
      "Iteration 102, loss = 0.50264942\n",
      "Iteration 103, loss = 0.50047680\n",
      "Iteration 104, loss = 0.50153769\n",
      "Iteration 105, loss = 0.50079121\n",
      "Iteration 106, loss = 0.49972160\n",
      "Iteration 107, loss = 0.49921126\n",
      "Iteration 108, loss = 0.49948323\n",
      "Iteration 109, loss = 0.49929802\n",
      "Iteration 110, loss = 0.49876473\n",
      "Iteration 111, loss = 0.49925373\n",
      "Iteration 112, loss = 0.49948685\n",
      "Iteration 113, loss = 0.49983168\n",
      "Iteration 114, loss = 0.49859818\n",
      "Iteration 115, loss = 0.49855297\n",
      "Iteration 116, loss = 0.49850426\n",
      "Iteration 117, loss = 0.49840079\n",
      "Iteration 118, loss = 0.50023137\n",
      "Iteration 119, loss = 0.50174834\n",
      "Iteration 120, loss = 0.49912643\n",
      "Iteration 121, loss = 0.50132184\n",
      "Iteration 122, loss = 0.49908316\n",
      "Iteration 123, loss = 0.50017811\n",
      "Iteration 124, loss = 0.50044141\n",
      "Iteration 125, loss = 0.49977626\n",
      "Iteration 126, loss = 0.49988327\n",
      "Iteration 127, loss = 0.50036773\n",
      "Iteration 128, loss = 0.49790054\n",
      "Iteration 129, loss = 0.49792674\n",
      "Iteration 130, loss = 0.49688619\n",
      "Iteration 131, loss = 0.49715017\n",
      "Iteration 132, loss = 0.50223775\n",
      "Iteration 133, loss = 0.49854972\n",
      "Iteration 134, loss = 0.49806519\n",
      "Iteration 135, loss = 0.50150565\n",
      "Iteration 136, loss = 0.49810068\n",
      "Iteration 137, loss = 0.49973085\n",
      "Iteration 138, loss = 0.49651581\n",
      "Iteration 139, loss = 0.50149634\n",
      "Iteration 140, loss = 0.50002975\n",
      "Iteration 141, loss = 0.49551886\n",
      "Iteration 142, loss = 0.49866545\n",
      "Iteration 143, loss = 0.50535700\n",
      "Iteration 144, loss = 0.50152905\n",
      "Iteration 145, loss = 0.49544330\n",
      "Iteration 146, loss = 0.50054894\n",
      "Iteration 147, loss = 0.50459961\n",
      "Iteration 148, loss = 0.49864105\n",
      "Iteration 149, loss = 0.49503989\n",
      "Iteration 150, loss = 0.49516788\n",
      "Iteration 151, loss = 0.49567989\n",
      "Iteration 152, loss = 0.49615429\n",
      "Iteration 153, loss = 0.49603281\n",
      "Iteration 154, loss = 0.49847251\n",
      "Iteration 155, loss = 0.50251309\n",
      "Iteration 156, loss = 0.49744882\n",
      "Iteration 157, loss = 0.49570025\n",
      "Iteration 158, loss = 0.49595752\n",
      "Iteration 159, loss = 0.49533971\n",
      "Iteration 160, loss = 0.49601374\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.86074941\n",
      "Iteration 2, loss = 0.81643120\n",
      "Iteration 3, loss = 0.69577452\n",
      "Iteration 4, loss = 0.65525230\n",
      "Iteration 5, loss = 0.59761255\n",
      "Iteration 6, loss = 0.58280688\n",
      "Iteration 7, loss = 0.54814890\n",
      "Iteration 8, loss = 0.53140684\n",
      "Iteration 9, loss = 0.52723964\n",
      "Iteration 10, loss = 0.52606543\n",
      "Iteration 11, loss = 0.52738449\n",
      "Iteration 12, loss = 0.52649665\n",
      "Iteration 13, loss = 0.52836285\n",
      "Iteration 14, loss = 0.52436709\n",
      "Iteration 15, loss = 0.52090843\n",
      "Iteration 16, loss = 0.51987585\n",
      "Iteration 17, loss = 0.52325165\n",
      "Iteration 18, loss = 0.51624795\n",
      "Iteration 19, loss = 0.51665934\n",
      "Iteration 20, loss = 0.51306327\n",
      "Iteration 21, loss = 0.51401334\n",
      "Iteration 22, loss = 0.51425990\n",
      "Iteration 23, loss = 0.51086154\n",
      "Iteration 24, loss = 0.53011107\n",
      "Iteration 25, loss = 0.50935324\n",
      "Iteration 26, loss = 0.53191607\n",
      "Iteration 27, loss = 0.50923772\n",
      "Iteration 28, loss = 0.54739035\n",
      "Iteration 29, loss = 0.52187558\n",
      "Iteration 30, loss = 0.52563178\n",
      "Iteration 31, loss = 0.51827464\n",
      "Iteration 32, loss = 0.51672949\n",
      "Iteration 33, loss = 0.53596645\n",
      "Iteration 34, loss = 0.51415667\n",
      "Iteration 35, loss = 0.51449923\n",
      "Iteration 36, loss = 0.50532833\n",
      "Iteration 37, loss = 0.51472140\n",
      "Iteration 38, loss = 0.52122298\n",
      "Iteration 39, loss = 0.51192081\n",
      "Iteration 40, loss = 0.50692894\n",
      "Iteration 41, loss = 0.50841940\n",
      "Iteration 42, loss = 0.52671413\n",
      "Iteration 43, loss = 0.50697287\n",
      "Iteration 44, loss = 0.52650115\n",
      "Iteration 45, loss = 0.50687340\n",
      "Iteration 46, loss = 0.52518845\n",
      "Iteration 47, loss = 0.52406386\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.79962896\n",
      "Iteration 2, loss = 0.64966658\n",
      "Iteration 3, loss = 0.85952660\n",
      "Iteration 4, loss = 1.01577100\n",
      "Iteration 5, loss = 0.94828646\n",
      "Iteration 6, loss = 0.74369510\n",
      "Iteration 7, loss = 0.59532202\n",
      "Iteration 8, loss = 0.61068900\n",
      "Iteration 9, loss = 0.69936931\n",
      "Iteration 10, loss = 0.64676218\n",
      "Iteration 11, loss = 0.55270292\n",
      "Iteration 12, loss = 0.55367683\n",
      "Iteration 13, loss = 0.61519447\n",
      "Iteration 14, loss = 0.62765242\n",
      "Iteration 15, loss = 0.58029956\n",
      "Iteration 16, loss = 0.53910086\n",
      "Iteration 17, loss = 0.54915351\n",
      "Iteration 18, loss = 0.55278333\n",
      "Iteration 19, loss = 0.53289679\n",
      "Iteration 20, loss = 0.52335980\n",
      "Iteration 21, loss = 0.52734399\n",
      "Iteration 22, loss = 0.52849474\n",
      "Iteration 23, loss = 0.52207848\n",
      "Iteration 24, loss = 0.51665012\n",
      "Iteration 25, loss = 0.51766664\n",
      "Iteration 26, loss = 0.51891709\n",
      "Iteration 27, loss = 0.51618828\n",
      "Iteration 28, loss = 0.51143288\n",
      "Iteration 29, loss = 0.51026030\n",
      "Iteration 30, loss = 0.51044593\n",
      "Iteration 31, loss = 0.51328940\n",
      "Iteration 32, loss = 0.51375128\n",
      "Iteration 33, loss = 0.50936916\n",
      "Iteration 34, loss = 0.50664937\n",
      "Iteration 35, loss = 0.51361265\n",
      "Iteration 36, loss = 0.51538201\n",
      "Iteration 37, loss = 0.50623005\n",
      "Iteration 38, loss = 0.50908448\n",
      "Iteration 39, loss = 0.50718796\n",
      "Iteration 40, loss = 0.50500937\n",
      "Iteration 41, loss = 0.50538085\n",
      "Iteration 42, loss = 0.50474247\n",
      "Iteration 43, loss = 0.50490169\n",
      "Iteration 44, loss = 0.50268995\n",
      "Iteration 45, loss = 0.50332558\n",
      "Iteration 46, loss = 0.50233231\n",
      "Iteration 47, loss = 0.50531288\n",
      "Iteration 48, loss = 0.50601480\n",
      "Iteration 49, loss = 0.50225825\n",
      "Iteration 50, loss = 0.50172497\n",
      "Iteration 51, loss = 0.50198438\n",
      "Iteration 52, loss = 0.50425036\n",
      "Iteration 53, loss = 0.50360335\n",
      "Iteration 54, loss = 0.49958735\n",
      "Iteration 55, loss = 0.49920177\n",
      "Iteration 56, loss = 0.49963338\n",
      "Iteration 57, loss = 0.49824692\n",
      "Iteration 58, loss = 0.50054861\n",
      "Iteration 59, loss = 0.50053814\n",
      "Iteration 60, loss = 0.49742839\n",
      "Iteration 61, loss = 0.49985190\n",
      "Iteration 62, loss = 0.49924967\n",
      "Iteration 63, loss = 0.49685099\n",
      "Iteration 64, loss = 0.49826790\n",
      "Iteration 65, loss = 0.50271559\n",
      "Iteration 66, loss = 0.50202455\n",
      "Iteration 67, loss = 0.49556262\n",
      "Iteration 68, loss = 0.49830839\n",
      "Iteration 69, loss = 0.49814881\n",
      "Iteration 70, loss = 0.49711823\n",
      "Iteration 71, loss = 0.49891556\n",
      "Iteration 72, loss = 0.49649071\n",
      "Iteration 73, loss = 0.49438048\n",
      "Iteration 74, loss = 0.49421616\n",
      "Iteration 75, loss = 0.49497354\n",
      "Iteration 76, loss = 0.49440889\n",
      "Iteration 77, loss = 0.49435442\n",
      "Iteration 78, loss = 0.49350010\n",
      "Iteration 79, loss = 0.49235724\n",
      "Iteration 80, loss = 0.49305435\n",
      "Iteration 81, loss = 0.49284215\n",
      "Iteration 82, loss = 0.49424619\n",
      "Iteration 83, loss = 0.49473598\n",
      "Iteration 84, loss = 0.49631061\n",
      "Iteration 85, loss = 0.49080977\n",
      "Iteration 86, loss = 0.49984160\n",
      "Iteration 87, loss = 0.49674534\n",
      "Iteration 88, loss = 0.49574952\n",
      "Iteration 89, loss = 0.50096812\n",
      "Iteration 90, loss = 0.49216893\n",
      "Iteration 91, loss = 0.49349888\n",
      "Iteration 92, loss = 0.49747928\n",
      "Iteration 93, loss = 0.49342010\n",
      "Iteration 94, loss = 0.49265351\n",
      "Iteration 95, loss = 0.49282155\n",
      "Iteration 96, loss = 0.49011184\n",
      "Iteration 97, loss = 0.49236893\n",
      "Iteration 98, loss = 0.49962760\n",
      "Iteration 99, loss = 0.48940987\n",
      "Iteration 100, loss = 0.49794333\n",
      "Iteration 101, loss = 0.50572283\n",
      "Iteration 102, loss = 0.49037940\n",
      "Iteration 103, loss = 0.49386977\n",
      "Iteration 104, loss = 0.50251914\n",
      "Iteration 105, loss = 0.49629698\n",
      "Iteration 106, loss = 0.49069133\n",
      "Iteration 107, loss = 0.48968012\n",
      "Iteration 108, loss = 0.48914063\n",
      "Iteration 109, loss = 0.48907555\n",
      "Iteration 110, loss = 0.48827790\n",
      "Iteration 111, loss = 0.49508811\n",
      "Iteration 112, loss = 0.49420587\n",
      "Iteration 113, loss = 0.48806865\n",
      "Iteration 114, loss = 0.49029541\n",
      "Iteration 115, loss = 0.49022733\n",
      "Iteration 116, loss = 0.48638271\n",
      "Iteration 117, loss = 0.49185823\n",
      "Iteration 118, loss = 0.48810663\n",
      "Iteration 119, loss = 0.49132087\n",
      "Iteration 120, loss = 0.49243037\n",
      "Iteration 121, loss = 0.48817691\n",
      "Iteration 122, loss = 0.49625289\n",
      "Iteration 123, loss = 0.49216517\n",
      "Iteration 124, loss = 0.48875366\n",
      "Iteration 125, loss = 0.48822966\n",
      "Iteration 126, loss = 0.48942493\n",
      "Iteration 127, loss = 0.48512313\n",
      "Iteration 128, loss = 0.50148835\n",
      "Iteration 129, loss = 0.48229315\n",
      "Iteration 130, loss = 0.51284006\n",
      "Iteration 131, loss = 0.53862952\n",
      "Iteration 132, loss = 0.48994652\n",
      "Iteration 133, loss = 0.49711097\n",
      "Iteration 134, loss = 0.49000129\n",
      "Iteration 135, loss = 0.49427501\n",
      "Iteration 136, loss = 0.49694651\n",
      "Iteration 137, loss = 0.48753026\n",
      "Iteration 138, loss = 0.49876782\n",
      "Iteration 139, loss = 0.48216686\n",
      "Iteration 140, loss = 0.50693952\n",
      "Iteration 141, loss = 0.50675484\n",
      "Iteration 142, loss = 0.48736659\n",
      "Iteration 143, loss = 0.48588532\n",
      "Iteration 144, loss = 0.48564020\n",
      "Iteration 145, loss = 0.49571055\n",
      "Iteration 146, loss = 0.49116222\n",
      "Iteration 147, loss = 0.48448448\n",
      "Iteration 148, loss = 0.48245254\n",
      "Iteration 149, loss = 0.48917245\n",
      "Iteration 150, loss = 0.48770337\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.18799912\n",
      "Iteration 2, loss = 1.24180202\n",
      "Iteration 3, loss = 0.60007358\n",
      "Iteration 4, loss = 0.89380052\n",
      "Iteration 5, loss = 0.79026009\n",
      "Iteration 6, loss = 0.55049482\n",
      "Iteration 7, loss = 0.61917534\n",
      "Iteration 8, loss = 0.72467418\n",
      "Iteration 9, loss = 0.69509520\n",
      "Iteration 10, loss = 0.58181917\n",
      "Iteration 11, loss = 0.54994793\n",
      "Iteration 12, loss = 0.61888831\n",
      "Iteration 13, loss = 0.61927988\n",
      "Iteration 14, loss = 0.55338798\n",
      "Iteration 15, loss = 0.54993289\n",
      "Iteration 16, loss = 0.57139249\n",
      "Iteration 17, loss = 0.57066535\n",
      "Iteration 18, loss = 0.54625130\n",
      "Iteration 19, loss = 0.53666834\n",
      "Iteration 20, loss = 0.54386480\n",
      "Iteration 21, loss = 0.54231596\n",
      "Iteration 22, loss = 0.53065843\n",
      "Iteration 23, loss = 0.52896491\n",
      "Iteration 24, loss = 0.53001699\n",
      "Iteration 25, loss = 0.52777745\n",
      "Iteration 26, loss = 0.52750121\n",
      "Iteration 27, loss = 0.52711592\n",
      "Iteration 28, loss = 0.52543246\n",
      "Iteration 29, loss = 0.52534599\n",
      "Iteration 30, loss = 0.52496082\n",
      "Iteration 31, loss = 0.52848283\n",
      "Iteration 32, loss = 0.52223044\n",
      "Iteration 33, loss = 0.52262181\n",
      "Iteration 34, loss = 0.53111393\n",
      "Iteration 35, loss = 0.52497143\n",
      "Iteration 36, loss = 0.51932574\n",
      "Iteration 37, loss = 0.52787792\n",
      "Iteration 38, loss = 0.52601548\n",
      "Iteration 39, loss = 0.52140237\n",
      "Iteration 40, loss = 0.52295225\n",
      "Iteration 41, loss = 0.52337667\n",
      "Iteration 42, loss = 0.51858634\n",
      "Iteration 43, loss = 0.51730187\n",
      "Iteration 44, loss = 0.51758741\n",
      "Iteration 45, loss = 0.51564288\n",
      "Iteration 46, loss = 0.51543585\n",
      "Iteration 47, loss = 0.51507488\n",
      "Iteration 48, loss = 0.51469606\n",
      "Iteration 49, loss = 0.51433918\n",
      "Iteration 50, loss = 0.51431421\n",
      "Iteration 51, loss = 0.51655949\n",
      "Iteration 52, loss = 0.52211976\n",
      "Iteration 53, loss = 0.51420992\n",
      "Iteration 54, loss = 0.51614727\n",
      "Iteration 55, loss = 0.52435113\n",
      "Iteration 56, loss = 0.51746314\n",
      "Iteration 57, loss = 0.51419040\n",
      "Iteration 58, loss = 0.51492284\n",
      "Iteration 59, loss = 0.51361165\n",
      "Iteration 60, loss = 0.51270345\n",
      "Iteration 61, loss = 0.52564070\n",
      "Iteration 62, loss = 0.52511354\n",
      "Iteration 63, loss = 0.50910965\n",
      "Iteration 64, loss = 0.52171713\n",
      "Iteration 65, loss = 0.54348105\n",
      "Iteration 66, loss = 0.52524618\n",
      "Iteration 67, loss = 0.51495323\n",
      "Iteration 68, loss = 0.53060036\n",
      "Iteration 69, loss = 0.52434475\n",
      "Iteration 70, loss = 0.51320181\n",
      "Iteration 71, loss = 0.51193018\n",
      "Iteration 72, loss = 0.51228734\n",
      "Iteration 73, loss = 0.51128853\n",
      "Iteration 74, loss = 0.51338999\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09018984\n",
      "Iteration 2, loss = 0.61995242\n",
      "Iteration 3, loss = 0.80194599\n",
      "Iteration 4, loss = 0.83793989\n",
      "Iteration 5, loss = 0.72584257\n",
      "Iteration 6, loss = 0.57726608\n",
      "Iteration 7, loss = 0.57216473\n",
      "Iteration 8, loss = 0.62226088\n",
      "Iteration 9, loss = 0.58878139\n",
      "Iteration 10, loss = 0.54259909\n",
      "Iteration 11, loss = 0.53906030\n",
      "Iteration 12, loss = 0.54694367\n",
      "Iteration 13, loss = 0.53476615\n",
      "Iteration 14, loss = 0.52484089\n",
      "Iteration 15, loss = 0.53035729\n",
      "Iteration 16, loss = 0.53251296\n",
      "Iteration 17, loss = 0.52583438\n",
      "Iteration 18, loss = 0.51905063\n",
      "Iteration 19, loss = 0.52261493\n",
      "Iteration 20, loss = 0.52328484\n",
      "Iteration 21, loss = 0.51916375\n",
      "Iteration 22, loss = 0.51535472\n",
      "Iteration 23, loss = 0.51412198\n",
      "Iteration 24, loss = 0.51532721\n",
      "Iteration 25, loss = 0.51587123\n",
      "Iteration 26, loss = 0.51473324\n",
      "Iteration 27, loss = 0.51188927\n",
      "Iteration 28, loss = 0.51071870\n",
      "Iteration 29, loss = 0.51084795\n",
      "Iteration 30, loss = 0.50966212\n",
      "Iteration 31, loss = 0.51057308\n",
      "Iteration 32, loss = 0.51296626\n",
      "Iteration 33, loss = 0.51078428\n",
      "Iteration 34, loss = 0.50750549\n",
      "Iteration 35, loss = 0.50984302\n",
      "Iteration 36, loss = 0.50936139\n",
      "Iteration 37, loss = 0.50594871\n",
      "Iteration 38, loss = 0.50828268\n",
      "Iteration 39, loss = 0.50797320\n",
      "Iteration 40, loss = 0.50704770\n",
      "Iteration 41, loss = 0.51084048\n",
      "Iteration 42, loss = 0.50856765\n",
      "Iteration 43, loss = 0.50405390\n",
      "Iteration 44, loss = 0.50678111\n",
      "Iteration 45, loss = 0.51228410\n",
      "Iteration 46, loss = 0.51219053\n",
      "Iteration 47, loss = 0.50732324\n",
      "Iteration 48, loss = 0.50279913\n",
      "Iteration 49, loss = 0.50273481\n",
      "Iteration 50, loss = 0.50417857\n",
      "Iteration 51, loss = 0.50033380\n",
      "Iteration 52, loss = 0.50707381\n",
      "Iteration 53, loss = 0.50641466\n",
      "Iteration 54, loss = 0.50028870\n",
      "Iteration 55, loss = 0.50509616\n",
      "Iteration 56, loss = 0.50754151\n",
      "Iteration 57, loss = 0.50124228\n",
      "Iteration 58, loss = 0.49889309\n",
      "Iteration 59, loss = 0.50081418\n",
      "Iteration 60, loss = 0.50035051\n",
      "Iteration 61, loss = 0.49833488\n",
      "Iteration 62, loss = 0.51420180\n",
      "Iteration 63, loss = 0.51607103\n",
      "Iteration 64, loss = 0.50126424\n",
      "Iteration 65, loss = 0.49895920\n",
      "Iteration 66, loss = 0.50528484\n",
      "Iteration 67, loss = 0.50444954\n",
      "Iteration 68, loss = 0.49914533\n",
      "Iteration 69, loss = 0.51074432\n",
      "Iteration 70, loss = 0.49625917\n",
      "Iteration 71, loss = 0.50444590\n",
      "Iteration 72, loss = 0.53031934\n",
      "Iteration 73, loss = 0.50235840\n",
      "Iteration 74, loss = 0.51074550\n",
      "Iteration 75, loss = 0.51942648\n",
      "Iteration 76, loss = 0.49357381\n",
      "Iteration 77, loss = 0.51943280\n",
      "Iteration 78, loss = 0.51578929\n",
      "Iteration 79, loss = 0.49206477\n",
      "Iteration 80, loss = 0.51532454\n",
      "Iteration 81, loss = 0.51134056\n",
      "Iteration 82, loss = 0.49339923\n",
      "Iteration 83, loss = 0.50281529\n",
      "Iteration 84, loss = 0.50325005\n",
      "Iteration 85, loss = 0.49348005\n",
      "Iteration 86, loss = 0.49371601\n",
      "Iteration 87, loss = 0.49219660\n",
      "Iteration 88, loss = 0.49161871\n",
      "Iteration 89, loss = 0.49142957\n",
      "Iteration 90, loss = 0.49125539\n",
      "Iteration 91, loss = 0.49184384\n",
      "Iteration 92, loss = 0.49167454\n",
      "Iteration 93, loss = 0.49640338\n",
      "Iteration 94, loss = 0.49357724\n",
      "Iteration 95, loss = 0.48892983\n",
      "Iteration 96, loss = 0.49263934\n",
      "Iteration 97, loss = 0.49795197\n",
      "Iteration 98, loss = 0.49471483\n",
      "Iteration 99, loss = 0.49035176\n",
      "Iteration 100, loss = 0.48952559\n",
      "Iteration 101, loss = 0.48946699\n",
      "Iteration 102, loss = 0.49440807\n",
      "Iteration 103, loss = 0.49454379\n",
      "Iteration 104, loss = 0.48881046\n",
      "Iteration 105, loss = 0.48791710\n",
      "Iteration 106, loss = 0.48883209\n",
      "Iteration 107, loss = 0.48923721\n",
      "Iteration 108, loss = 0.48585196\n",
      "Iteration 109, loss = 0.48971538\n",
      "Iteration 110, loss = 0.49633034\n",
      "Iteration 111, loss = 0.48967744\n",
      "Iteration 112, loss = 0.48841313\n",
      "Iteration 113, loss = 0.48824725\n",
      "Iteration 114, loss = 0.48569468\n",
      "Iteration 115, loss = 0.48671732\n",
      "Iteration 116, loss = 0.48708205\n",
      "Iteration 117, loss = 0.48387190\n",
      "Iteration 118, loss = 0.48973477\n",
      "Iteration 119, loss = 0.49178782\n",
      "Iteration 120, loss = 0.48664724\n",
      "Iteration 121, loss = 0.48844485\n",
      "Iteration 122, loss = 0.48286635\n",
      "Iteration 123, loss = 0.49811458\n",
      "Iteration 124, loss = 0.52008901\n",
      "Iteration 125, loss = 0.49075362\n",
      "Iteration 126, loss = 0.48898707\n",
      "Iteration 127, loss = 0.49991932\n",
      "Iteration 128, loss = 0.48734142\n",
      "Iteration 129, loss = 0.48945007\n",
      "Iteration 130, loss = 0.50639564\n",
      "Iteration 131, loss = 0.48545412\n",
      "Iteration 132, loss = 0.50179159\n",
      "Iteration 133, loss = 0.49322433\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 11.74113349\n",
      "Iteration 2, loss = 9.10288422\n",
      "Iteration 3, loss = 6.51954817\n",
      "Iteration 4, loss = 3.93830122\n",
      "Iteration 5, loss = 1.74177380\n",
      "Iteration 6, loss = 0.95345591\n",
      "Iteration 7, loss = 1.24234255\n",
      "Iteration 8, loss = 1.59527658\n",
      "Iteration 9, loss = 1.84967238\n",
      "Iteration 10, loss = 1.96843149\n",
      "Iteration 11, loss = 1.97979947\n",
      "Iteration 12, loss = 1.90622405\n",
      "Iteration 13, loss = 1.75813850\n",
      "Iteration 14, loss = 1.57204369\n",
      "Iteration 15, loss = 1.33476801\n",
      "Iteration 16, loss = 1.06183446\n",
      "Iteration 17, loss = 0.82831012\n",
      "Iteration 18, loss = 0.70300419\n",
      "Iteration 19, loss = 0.68383043\n",
      "Iteration 20, loss = 0.80548803\n",
      "Iteration 21, loss = 0.85985323\n",
      "Iteration 22, loss = 0.78417899\n",
      "Iteration 23, loss = 0.65880184\n",
      "Iteration 24, loss = 0.60372826\n",
      "Iteration 25, loss = 0.60608725\n",
      "Iteration 26, loss = 0.62439852\n",
      "Iteration 27, loss = 0.62117822\n",
      "Iteration 28, loss = 0.59699075\n",
      "Iteration 29, loss = 0.56440650\n",
      "Iteration 30, loss = 0.54899692\n",
      "Iteration 31, loss = 0.55740788\n",
      "Iteration 32, loss = 0.57413176\n",
      "Iteration 33, loss = 0.57299669\n",
      "Iteration 34, loss = 0.55472375\n",
      "Iteration 35, loss = 0.53990119\n",
      "Iteration 36, loss = 0.53029836\n",
      "Iteration 37, loss = 0.53120620\n",
      "Iteration 38, loss = 0.53380694\n",
      "Iteration 39, loss = 0.52884556\n",
      "Iteration 40, loss = 0.52340525\n",
      "Iteration 41, loss = 0.52047756\n",
      "Iteration 42, loss = 0.52200050\n",
      "Iteration 43, loss = 0.51981821\n",
      "Iteration 44, loss = 0.51625312\n",
      "Iteration 45, loss = 0.51519719\n",
      "Iteration 46, loss = 0.51704417\n",
      "Iteration 47, loss = 0.51792161\n",
      "Iteration 48, loss = 0.51597722\n",
      "Iteration 49, loss = 0.51402420\n",
      "Iteration 50, loss = 0.51272647\n",
      "Iteration 51, loss = 0.51172954\n",
      "Iteration 52, loss = 0.51138225\n",
      "Iteration 53, loss = 0.51163825\n",
      "Iteration 54, loss = 0.51358549\n",
      "Iteration 55, loss = 0.51261832\n",
      "Iteration 56, loss = 0.51057294\n",
      "Iteration 57, loss = 0.50920481\n",
      "Iteration 58, loss = 0.50801191\n",
      "Iteration 59, loss = 0.50658885\n",
      "Iteration 60, loss = 0.50672864\n",
      "Iteration 61, loss = 0.50565379\n",
      "Iteration 62, loss = 0.50381245\n",
      "Iteration 63, loss = 0.51121016\n",
      "Iteration 64, loss = 0.51456829\n",
      "Iteration 65, loss = 0.50680215\n",
      "Iteration 66, loss = 0.50246054\n",
      "Iteration 67, loss = 0.50823905\n",
      "Iteration 68, loss = 0.50838874\n",
      "Iteration 69, loss = 0.50321999\n",
      "Iteration 70, loss = 0.50201242\n",
      "Iteration 71, loss = 0.50089880\n",
      "Iteration 72, loss = 0.50014275\n",
      "Iteration 73, loss = 0.50047989\n",
      "Iteration 74, loss = 0.50129284\n",
      "Iteration 75, loss = 0.50000544\n",
      "Iteration 76, loss = 0.49984980\n",
      "Iteration 77, loss = 0.50005005\n",
      "Iteration 78, loss = 0.49944769\n",
      "Iteration 79, loss = 0.49923526\n",
      "Iteration 80, loss = 0.49818742\n",
      "Iteration 81, loss = 0.49759982\n",
      "Iteration 82, loss = 0.49801025\n",
      "Iteration 83, loss = 0.49758320\n",
      "Iteration 84, loss = 0.49607078\n",
      "Iteration 85, loss = 0.49572120\n",
      "Iteration 86, loss = 0.49547540\n",
      "Iteration 87, loss = 0.49471906\n",
      "Iteration 88, loss = 0.49653854\n",
      "Iteration 89, loss = 0.49571240\n",
      "Iteration 90, loss = 0.49395339\n",
      "Iteration 91, loss = 0.49569654\n",
      "Iteration 92, loss = 0.49694557\n",
      "Iteration 93, loss = 0.49508826\n",
      "Iteration 94, loss = 0.49252951\n",
      "Iteration 95, loss = 0.49428766\n",
      "Iteration 96, loss = 0.49477367\n",
      "Iteration 97, loss = 0.49526324\n",
      "Iteration 98, loss = 0.49145373\n",
      "Iteration 99, loss = 0.49121984\n",
      "Iteration 100, loss = 0.49101392\n",
      "Iteration 101, loss = 0.49088121\n",
      "Iteration 102, loss = 0.49032146\n",
      "Iteration 103, loss = 0.49116012\n",
      "Iteration 104, loss = 0.49178985\n",
      "Iteration 105, loss = 0.49087117\n",
      "Iteration 106, loss = 0.48825218\n",
      "Iteration 107, loss = 0.49010822\n",
      "Iteration 108, loss = 0.48859781\n",
      "Iteration 109, loss = 0.48771890\n",
      "Iteration 110, loss = 0.48952872\n",
      "Iteration 111, loss = 0.49173257\n",
      "Iteration 112, loss = 0.49022343\n",
      "Iteration 113, loss = 0.48567174\n",
      "Iteration 114, loss = 0.48757177\n",
      "Iteration 115, loss = 0.48923535\n",
      "Iteration 116, loss = 0.48476396\n",
      "Iteration 117, loss = 0.48755476\n",
      "Iteration 118, loss = 0.49154800\n",
      "Iteration 119, loss = 0.48815497\n",
      "Iteration 120, loss = 0.48395637\n",
      "Iteration 121, loss = 0.48515320\n",
      "Iteration 122, loss = 0.48689792\n",
      "Iteration 123, loss = 0.48563026\n",
      "Iteration 124, loss = 0.48473417\n",
      "Iteration 125, loss = 0.48340409\n",
      "Iteration 126, loss = 0.48254307\n",
      "Iteration 127, loss = 0.48261534\n",
      "Iteration 128, loss = 0.48460983\n",
      "Iteration 129, loss = 0.48619305\n",
      "Iteration 130, loss = 0.48402408\n",
      "Iteration 131, loss = 0.48350899\n",
      "Iteration 132, loss = 0.48332727\n",
      "Iteration 133, loss = 0.48159163\n",
      "Iteration 134, loss = 0.48170898\n",
      "Iteration 135, loss = 0.48122181\n",
      "Iteration 136, loss = 0.48083161\n",
      "Iteration 137, loss = 0.48061583\n",
      "Iteration 138, loss = 0.48038888\n",
      "Iteration 139, loss = 0.48208970\n",
      "Iteration 140, loss = 0.48030165\n",
      "Iteration 141, loss = 0.48152670\n",
      "Iteration 142, loss = 0.48551357\n",
      "Iteration 143, loss = 0.48082348\n",
      "Iteration 144, loss = 0.48441708\n",
      "Iteration 145, loss = 0.48687481\n",
      "Iteration 146, loss = 0.48193982\n",
      "Iteration 147, loss = 0.48104769\n",
      "Iteration 148, loss = 0.48239016\n",
      "Iteration 149, loss = 0.48129034\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.28680656\n",
      "Iteration 2, loss = 0.77311390\n",
      "Iteration 3, loss = 0.97456452\n",
      "Iteration 4, loss = 0.87256611\n",
      "Iteration 5, loss = 0.65476154\n",
      "Iteration 6, loss = 0.60243235\n",
      "Iteration 7, loss = 0.66808471\n",
      "Iteration 8, loss = 0.55900290\n",
      "Iteration 9, loss = 0.53557604\n",
      "Iteration 10, loss = 0.58696343\n",
      "Iteration 11, loss = 0.57886760\n",
      "Iteration 12, loss = 0.52246373\n",
      "Iteration 13, loss = 0.51256642\n",
      "Iteration 14, loss = 0.55594907\n",
      "Iteration 15, loss = 0.55806805\n",
      "Iteration 16, loss = 0.51863636\n",
      "Iteration 17, loss = 0.51501334\n",
      "Iteration 18, loss = 0.53513082\n",
      "Iteration 19, loss = 0.53567777\n",
      "Iteration 20, loss = 0.51879912\n",
      "Iteration 21, loss = 0.50275226\n",
      "Iteration 22, loss = 0.50345375\n",
      "Iteration 23, loss = 0.50176994\n",
      "Iteration 24, loss = 0.49787674\n",
      "Iteration 25, loss = 0.50848991\n",
      "Iteration 26, loss = 0.50625681\n",
      "Iteration 27, loss = 0.49452105\n",
      "Iteration 28, loss = 0.49405498\n",
      "Iteration 29, loss = 0.49474957\n",
      "Iteration 30, loss = 0.49024938\n",
      "Iteration 31, loss = 0.49252328\n",
      "Iteration 32, loss = 0.49450937\n",
      "Iteration 33, loss = 0.48946209\n",
      "Iteration 34, loss = 0.48902054\n",
      "Iteration 35, loss = 0.48771851\n",
      "Iteration 36, loss = 0.48475490\n",
      "Iteration 37, loss = 0.49703307\n",
      "Iteration 38, loss = 0.49878202\n",
      "Iteration 39, loss = 0.48324382\n",
      "Iteration 40, loss = 0.48761861\n",
      "Iteration 41, loss = 0.49440816\n",
      "Iteration 42, loss = 0.48422793\n",
      "Iteration 43, loss = 0.47763442\n",
      "Iteration 44, loss = 0.48253089\n",
      "Iteration 45, loss = 0.48691497\n",
      "Iteration 46, loss = 0.48109582\n",
      "Iteration 47, loss = 0.47956282\n",
      "Iteration 48, loss = 0.47847379\n",
      "Iteration 49, loss = 0.47598183\n",
      "Iteration 50, loss = 0.47660729\n",
      "Iteration 51, loss = 0.47474491\n",
      "Iteration 52, loss = 0.47631424\n",
      "Iteration 53, loss = 0.47612211\n",
      "Iteration 54, loss = 0.47369494\n",
      "Iteration 55, loss = 0.47581912\n",
      "Iteration 56, loss = 0.47512000\n",
      "Iteration 57, loss = 0.47211755\n",
      "Iteration 58, loss = 0.47141635\n",
      "Iteration 59, loss = 0.47149062\n",
      "Iteration 60, loss = 0.47062961\n",
      "Iteration 61, loss = 0.47099128\n",
      "Iteration 62, loss = 0.46983192\n",
      "Iteration 63, loss = 0.47063448\n",
      "Iteration 64, loss = 0.46968039\n",
      "Iteration 65, loss = 0.46866105\n",
      "Iteration 66, loss = 0.47753075\n",
      "Iteration 67, loss = 0.47636203\n",
      "Iteration 68, loss = 0.46946190\n",
      "Iteration 69, loss = 0.49375104\n",
      "Iteration 70, loss = 0.47981416\n",
      "Iteration 71, loss = 0.46850033\n",
      "Iteration 72, loss = 0.47890501\n",
      "Iteration 73, loss = 0.47124062\n",
      "Iteration 74, loss = 0.48646684\n",
      "Iteration 75, loss = 0.47039271\n",
      "Iteration 76, loss = 0.48256900\n",
      "Iteration 77, loss = 0.47211979\n",
      "Iteration 78, loss = 0.47263657\n",
      "Iteration 79, loss = 0.47519564\n",
      "Iteration 80, loss = 0.46521605\n",
      "Iteration 81, loss = 0.46987485\n",
      "Iteration 82, loss = 0.46643324\n",
      "Iteration 83, loss = 0.46168311\n",
      "Iteration 84, loss = 0.48493340\n",
      "Iteration 85, loss = 0.48761042\n",
      "Iteration 86, loss = 0.46859882\n",
      "Iteration 87, loss = 0.48688498\n",
      "Iteration 88, loss = 0.46251455\n",
      "Iteration 89, loss = 0.47862107\n",
      "Iteration 90, loss = 0.47377809\n",
      "Iteration 91, loss = 0.46516190\n",
      "Iteration 92, loss = 0.49105242\n",
      "Iteration 93, loss = 0.47088326\n",
      "Iteration 94, loss = 0.46593702\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.45901784\n",
      "Iteration 2, loss = 0.58915405\n",
      "Iteration 3, loss = 0.89757484\n",
      "Iteration 4, loss = 0.56731237\n",
      "Iteration 5, loss = 0.61784686\n",
      "Iteration 6, loss = 0.64802064\n",
      "Iteration 7, loss = 0.56185467\n",
      "Iteration 8, loss = 0.53226711\n",
      "Iteration 9, loss = 0.56677369\n",
      "Iteration 10, loss = 0.53755619\n",
      "Iteration 11, loss = 0.52010012\n",
      "Iteration 12, loss = 0.53674014\n",
      "Iteration 13, loss = 0.53058618\n",
      "Iteration 14, loss = 0.52051108\n",
      "Iteration 15, loss = 0.51742029\n",
      "Iteration 16, loss = 0.51707499\n",
      "Iteration 17, loss = 0.51357825\n",
      "Iteration 18, loss = 0.51791671\n",
      "Iteration 19, loss = 0.51731967\n",
      "Iteration 20, loss = 0.51199568\n",
      "Iteration 21, loss = 0.51534460\n",
      "Iteration 22, loss = 0.51745179\n",
      "Iteration 23, loss = 0.51329249\n",
      "Iteration 24, loss = 0.51183649\n",
      "Iteration 25, loss = 0.51174725\n",
      "Iteration 26, loss = 0.51499347\n",
      "Iteration 27, loss = 0.51410809\n",
      "Iteration 28, loss = 0.51400598\n",
      "Iteration 29, loss = 0.51002295\n",
      "Iteration 30, loss = 0.51266106\n",
      "Iteration 31, loss = 0.50956894\n",
      "Iteration 32, loss = 0.51251744\n",
      "Iteration 33, loss = 0.51147680\n",
      "Iteration 34, loss = 0.50790283\n",
      "Iteration 35, loss = 0.51404384\n",
      "Iteration 36, loss = 0.51121847\n",
      "Iteration 37, loss = 0.50532707\n",
      "Iteration 38, loss = 0.51181861\n",
      "Iteration 39, loss = 0.52401437\n",
      "Iteration 40, loss = 0.51999461\n",
      "Iteration 41, loss = 0.50857850\n",
      "Iteration 42, loss = 0.51415009\n",
      "Iteration 43, loss = 0.50991939\n",
      "Iteration 44, loss = 0.50424454\n",
      "Iteration 45, loss = 0.50634118\n",
      "Iteration 46, loss = 0.50583469\n",
      "Iteration 47, loss = 0.50393489\n",
      "Iteration 48, loss = 0.51428841\n",
      "Iteration 49, loss = 0.50529279\n",
      "Iteration 50, loss = 0.51838904\n",
      "Iteration 51, loss = 0.51422897\n",
      "Iteration 52, loss = 0.50809869\n",
      "Iteration 53, loss = 0.50324157\n",
      "Iteration 54, loss = 0.50304451\n",
      "Iteration 55, loss = 0.50605041\n",
      "Iteration 56, loss = 0.50336502\n",
      "Iteration 57, loss = 0.50198431\n",
      "Iteration 58, loss = 0.50198301\n",
      "Iteration 59, loss = 0.50038265\n",
      "Iteration 60, loss = 0.50573060\n",
      "Iteration 61, loss = 0.51130402\n",
      "Iteration 62, loss = 0.50130392\n",
      "Iteration 63, loss = 0.50564210\n",
      "Iteration 64, loss = 0.50464976\n",
      "Iteration 65, loss = 0.50006114\n",
      "Iteration 66, loss = 0.50120005\n",
      "Iteration 67, loss = 0.49974979\n",
      "Iteration 68, loss = 0.50453188\n",
      "Iteration 69, loss = 0.50154570\n",
      "Iteration 70, loss = 0.50365783\n",
      "Iteration 71, loss = 0.49941277\n",
      "Iteration 72, loss = 0.50561599\n",
      "Iteration 73, loss = 0.51896502\n",
      "Iteration 74, loss = 0.49642113\n",
      "Iteration 75, loss = 0.51320967\n",
      "Iteration 76, loss = 0.50859746\n",
      "Iteration 77, loss = 0.51208223\n",
      "Iteration 78, loss = 0.51696311\n",
      "Iteration 79, loss = 0.49989124\n",
      "Iteration 80, loss = 0.50817560\n",
      "Iteration 81, loss = 0.49991349\n",
      "Iteration 82, loss = 0.50503205\n",
      "Iteration 83, loss = 0.49888028\n",
      "Iteration 84, loss = 0.50830141\n",
      "Iteration 85, loss = 0.49278802\n",
      "Iteration 86, loss = 0.52819757\n",
      "Iteration 87, loss = 0.50391747\n",
      "Iteration 88, loss = 0.51633331\n",
      "Iteration 89, loss = 0.51418962\n",
      "Iteration 90, loss = 0.49684746\n",
      "Iteration 91, loss = 0.54571348\n",
      "Iteration 92, loss = 0.49916366\n",
      "Iteration 93, loss = 0.51866948\n",
      "Iteration 94, loss = 0.50922993\n",
      "Iteration 95, loss = 0.50723677\n",
      "Iteration 96, loss = 0.53392067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.38308434\n",
      "Iteration 2, loss = 0.59671942\n",
      "Iteration 3, loss = 0.78667297\n",
      "Iteration 4, loss = 0.53232889\n",
      "Iteration 5, loss = 0.65805434\n",
      "Iteration 6, loss = 0.64221507\n",
      "Iteration 7, loss = 0.53271666\n",
      "Iteration 8, loss = 0.57490022\n",
      "Iteration 9, loss = 0.60347701\n",
      "Iteration 10, loss = 0.51701793\n",
      "Iteration 11, loss = 0.54993496\n",
      "Iteration 12, loss = 0.59356410\n",
      "Iteration 13, loss = 0.54779451\n",
      "Iteration 14, loss = 0.51486272\n",
      "Iteration 15, loss = 0.55019074\n",
      "Iteration 16, loss = 0.52556461\n",
      "Iteration 17, loss = 0.51313871\n",
      "Iteration 18, loss = 0.52979400\n",
      "Iteration 19, loss = 0.51911708\n",
      "Iteration 20, loss = 0.50834358\n",
      "Iteration 21, loss = 0.50783970\n",
      "Iteration 22, loss = 0.50512455\n",
      "Iteration 23, loss = 0.50134200\n",
      "Iteration 24, loss = 0.50156510\n",
      "Iteration 25, loss = 0.50293509\n",
      "Iteration 26, loss = 0.49913956\n",
      "Iteration 27, loss = 0.49762980\n",
      "Iteration 28, loss = 0.50125785\n",
      "Iteration 29, loss = 0.49958267\n",
      "Iteration 30, loss = 0.49572367\n",
      "Iteration 31, loss = 0.49727704\n",
      "Iteration 32, loss = 0.49726967\n",
      "Iteration 33, loss = 0.49949986\n",
      "Iteration 34, loss = 0.49288068\n",
      "Iteration 35, loss = 0.50316331\n",
      "Iteration 36, loss = 0.49598500\n",
      "Iteration 37, loss = 0.49555207\n",
      "Iteration 38, loss = 0.50172422\n",
      "Iteration 39, loss = 0.49249711\n",
      "Iteration 40, loss = 0.49154445\n",
      "Iteration 41, loss = 0.49335335\n",
      "Iteration 42, loss = 0.49119303\n",
      "Iteration 43, loss = 0.49276901\n",
      "Iteration 44, loss = 0.48841273\n",
      "Iteration 45, loss = 0.49274210\n",
      "Iteration 46, loss = 0.48859535\n",
      "Iteration 47, loss = 0.50092314\n",
      "Iteration 48, loss = 0.49208035\n",
      "Iteration 49, loss = 0.48740073\n",
      "Iteration 50, loss = 0.49774547\n",
      "Iteration 51, loss = 0.49022913\n",
      "Iteration 52, loss = 0.48595185\n",
      "Iteration 53, loss = 0.49350966\n",
      "Iteration 54, loss = 0.48581448\n",
      "Iteration 55, loss = 0.48921752\n",
      "Iteration 56, loss = 0.49626648\n",
      "Iteration 57, loss = 0.48125114\n",
      "Iteration 58, loss = 0.50653682\n",
      "Iteration 59, loss = 0.49377999\n",
      "Iteration 60, loss = 0.50640382\n",
      "Iteration 61, loss = 0.50927215\n",
      "Iteration 62, loss = 0.49031270\n",
      "Iteration 63, loss = 0.52928218\n",
      "Iteration 64, loss = 0.48755645\n",
      "Iteration 65, loss = 0.51576850\n",
      "Iteration 66, loss = 0.49595794\n",
      "Iteration 67, loss = 0.48652120\n",
      "Iteration 68, loss = 0.51373796\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63089909\n",
      "Iteration 2, loss = 0.59984372\n",
      "Iteration 3, loss = 0.57388696\n",
      "Iteration 4, loss = 0.52600732\n",
      "Iteration 5, loss = 0.55297327\n",
      "Iteration 6, loss = 0.52943288\n",
      "Iteration 7, loss = 0.52435322\n",
      "Iteration 8, loss = 0.52910101\n",
      "Iteration 9, loss = 0.52198733\n",
      "Iteration 10, loss = 0.52293939\n",
      "Iteration 11, loss = 0.51405593\n",
      "Iteration 12, loss = 0.51384252\n",
      "Iteration 13, loss = 0.50965303\n",
      "Iteration 14, loss = 0.51123040\n",
      "Iteration 15, loss = 0.51058559\n",
      "Iteration 16, loss = 0.50744456\n",
      "Iteration 17, loss = 0.50969229\n",
      "Iteration 18, loss = 0.50656558\n",
      "Iteration 19, loss = 0.50497436\n",
      "Iteration 20, loss = 0.50357866\n",
      "Iteration 21, loss = 0.50584518\n",
      "Iteration 22, loss = 0.51305171\n",
      "Iteration 23, loss = 0.49906153\n",
      "Iteration 24, loss = 0.52485286\n",
      "Iteration 25, loss = 0.50687961\n",
      "Iteration 26, loss = 0.51613055\n",
      "Iteration 27, loss = 0.51630812\n",
      "Iteration 28, loss = 0.50568341\n",
      "Iteration 29, loss = 0.53487304\n",
      "Iteration 30, loss = 0.50325553\n",
      "Iteration 31, loss = 0.51430293\n",
      "Iteration 32, loss = 0.50183905\n",
      "Iteration 33, loss = 0.51785360\n",
      "Iteration 34, loss = 0.50991042\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.03034523\n",
      "Iteration 2, loss = 0.94898877\n",
      "Iteration 3, loss = 0.98068488\n",
      "Iteration 4, loss = 1.44285406\n",
      "Iteration 5, loss = 1.45233981\n",
      "Iteration 6, loss = 1.16217380\n",
      "Iteration 7, loss = 0.71401147\n",
      "Iteration 8, loss = 0.59974398\n",
      "Iteration 9, loss = 0.94002400\n",
      "Iteration 10, loss = 0.87257639\n",
      "Iteration 11, loss = 0.60606469\n",
      "Iteration 12, loss = 0.60584322\n",
      "Iteration 13, loss = 0.69556641\n",
      "Iteration 14, loss = 0.69549976\n",
      "Iteration 15, loss = 0.61622766\n",
      "Iteration 16, loss = 0.56255792\n",
      "Iteration 17, loss = 0.59459688\n",
      "Iteration 18, loss = 0.60303189\n",
      "Iteration 19, loss = 0.56825644\n",
      "Iteration 20, loss = 0.54992181\n",
      "Iteration 21, loss = 0.56112861\n",
      "Iteration 22, loss = 0.55758731\n",
      "Iteration 23, loss = 0.54567984\n",
      "Iteration 24, loss = 0.54566115\n",
      "Iteration 25, loss = 0.54904907\n",
      "Iteration 26, loss = 0.54167311\n",
      "Iteration 27, loss = 0.54187153\n",
      "Iteration 28, loss = 0.55052290\n",
      "Iteration 29, loss = 0.54453110\n",
      "Iteration 30, loss = 0.54175361\n",
      "Iteration 31, loss = 0.54025835\n",
      "Iteration 32, loss = 0.53821460\n",
      "Iteration 33, loss = 0.53765684\n",
      "Iteration 34, loss = 0.53677182\n",
      "Iteration 35, loss = 0.53652929\n",
      "Iteration 36, loss = 0.53838761\n",
      "Iteration 37, loss = 0.53767953\n",
      "Iteration 38, loss = 0.53407882\n",
      "Iteration 39, loss = 0.53474965\n",
      "Iteration 40, loss = 0.54136976\n",
      "Iteration 41, loss = 0.54290247\n",
      "Iteration 42, loss = 0.53627337\n",
      "Iteration 43, loss = 0.53507379\n",
      "Iteration 44, loss = 0.53822279\n",
      "Iteration 45, loss = 0.53493296\n",
      "Iteration 46, loss = 0.53354650\n",
      "Iteration 47, loss = 0.53319078\n",
      "Iteration 48, loss = 0.53339353\n",
      "Iteration 49, loss = 0.53398502\n",
      "Iteration 50, loss = 0.53213825\n",
      "Iteration 51, loss = 0.52985023\n",
      "Iteration 52, loss = 0.53314191\n",
      "Iteration 53, loss = 0.53168037\n",
      "Iteration 54, loss = 0.52900039\n",
      "Iteration 55, loss = 0.53091090\n",
      "Iteration 56, loss = 0.52684520\n",
      "Iteration 57, loss = 0.53320563\n",
      "Iteration 58, loss = 0.53480261\n",
      "Iteration 59, loss = 0.52618705\n",
      "Iteration 60, loss = 0.53334236\n",
      "Iteration 61, loss = 0.52798614\n",
      "Iteration 62, loss = 0.52874489\n",
      "Iteration 63, loss = 0.55459846\n",
      "Iteration 64, loss = 0.54099997\n",
      "Iteration 65, loss = 0.52662527\n",
      "Iteration 66, loss = 0.53979044\n",
      "Iteration 67, loss = 0.53116606\n",
      "Iteration 68, loss = 0.52514324\n",
      "Iteration 69, loss = 0.53832303\n",
      "Iteration 70, loss = 0.52497916\n",
      "Iteration 71, loss = 0.53855965\n",
      "Iteration 72, loss = 0.54198879\n",
      "Iteration 73, loss = 0.52296905\n",
      "Iteration 74, loss = 0.52419715\n",
      "Iteration 75, loss = 0.52863024\n",
      "Iteration 76, loss = 0.52495327\n",
      "Iteration 77, loss = 0.52216219\n",
      "Iteration 78, loss = 0.52646690\n",
      "Iteration 79, loss = 0.52589842\n",
      "Iteration 80, loss = 0.52326746\n",
      "Iteration 81, loss = 0.52364614\n",
      "Iteration 82, loss = 0.52876863\n",
      "Iteration 83, loss = 0.52190196\n",
      "Iteration 84, loss = 0.53068347\n",
      "Iteration 85, loss = 0.52947734\n",
      "Iteration 86, loss = 0.52051687\n",
      "Iteration 87, loss = 0.52464093\n",
      "Iteration 88, loss = 0.52196550\n",
      "Iteration 89, loss = 0.52210792\n",
      "Iteration 90, loss = 0.52114060\n",
      "Iteration 91, loss = 0.52249793\n",
      "Iteration 92, loss = 0.52785745\n",
      "Iteration 93, loss = 0.52258958\n",
      "Iteration 94, loss = 0.52248046\n",
      "Iteration 95, loss = 0.52296243\n",
      "Iteration 96, loss = 0.51775704\n",
      "Iteration 97, loss = 0.52643847\n",
      "Iteration 98, loss = 0.53001097\n",
      "Iteration 99, loss = 0.51855583\n",
      "Iteration 100, loss = 0.53057466\n",
      "Iteration 101, loss = 0.52034376\n",
      "Iteration 102, loss = 0.52575551\n",
      "Iteration 103, loss = 0.52836185\n",
      "Iteration 104, loss = 0.51727673\n",
      "Iteration 105, loss = 0.53099580\n",
      "Iteration 106, loss = 0.51917481\n",
      "Iteration 107, loss = 0.53388437\n",
      "Iteration 108, loss = 0.53592368\n",
      "Iteration 109, loss = 0.51740040\n",
      "Iteration 110, loss = 0.52749743\n",
      "Iteration 111, loss = 0.52298664\n",
      "Iteration 112, loss = 0.51919664\n",
      "Iteration 113, loss = 0.51883999\n",
      "Iteration 114, loss = 0.51745931\n",
      "Iteration 115, loss = 0.52165179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.64726449\n",
      "Iteration 2, loss = 1.99708066\n",
      "Iteration 3, loss = 0.84831784\n",
      "Iteration 4, loss = 1.45673057\n",
      "Iteration 5, loss = 1.70129704\n",
      "Iteration 6, loss = 1.65833782\n",
      "Iteration 7, loss = 1.40805159\n",
      "Iteration 8, loss = 1.02213450\n",
      "Iteration 9, loss = 0.62955090\n",
      "Iteration 10, loss = 0.65347112\n",
      "Iteration 11, loss = 0.89818169\n",
      "Iteration 12, loss = 0.73655919\n",
      "Iteration 13, loss = 0.55297455\n",
      "Iteration 14, loss = 0.60743282\n",
      "Iteration 15, loss = 0.68215904\n",
      "Iteration 16, loss = 0.66518811\n",
      "Iteration 17, loss = 0.58305584\n",
      "Iteration 18, loss = 0.54203564\n",
      "Iteration 19, loss = 0.60403340\n",
      "Iteration 20, loss = 0.62239378\n",
      "Iteration 21, loss = 0.56256206\n",
      "Iteration 22, loss = 0.54329692\n",
      "Iteration 23, loss = 0.55573726\n",
      "Iteration 24, loss = 0.56066746\n",
      "Iteration 25, loss = 0.54104258\n",
      "Iteration 26, loss = 0.53524668\n",
      "Iteration 27, loss = 0.55993904\n",
      "Iteration 28, loss = 0.56019873\n",
      "Iteration 29, loss = 0.53590579\n",
      "Iteration 30, loss = 0.53275698\n",
      "Iteration 31, loss = 0.54345686\n",
      "Iteration 32, loss = 0.53637456\n",
      "Iteration 33, loss = 0.52668191\n",
      "Iteration 34, loss = 0.53945007\n",
      "Iteration 35, loss = 0.54588953\n",
      "Iteration 36, loss = 0.53302620\n",
      "Iteration 37, loss = 0.52692139\n",
      "Iteration 38, loss = 0.52676900\n",
      "Iteration 39, loss = 0.52484471\n",
      "Iteration 40, loss = 0.52516349\n",
      "Iteration 41, loss = 0.53141353\n",
      "Iteration 42, loss = 0.53008681\n",
      "Iteration 43, loss = 0.52584212\n",
      "Iteration 44, loss = 0.52560636\n",
      "Iteration 45, loss = 0.52536955\n",
      "Iteration 46, loss = 0.52279474\n",
      "Iteration 47, loss = 0.52205514\n",
      "Iteration 48, loss = 0.52162592\n",
      "Iteration 49, loss = 0.52167302\n",
      "Iteration 50, loss = 0.52199176\n",
      "Iteration 51, loss = 0.52122887\n",
      "Iteration 52, loss = 0.52059964\n",
      "Iteration 53, loss = 0.52266243\n",
      "Iteration 54, loss = 0.51991671\n",
      "Iteration 55, loss = 0.52022448\n",
      "Iteration 56, loss = 0.52034465\n",
      "Iteration 57, loss = 0.51963618\n",
      "Iteration 58, loss = 0.51872884\n",
      "Iteration 59, loss = 0.51830541\n",
      "Iteration 60, loss = 0.51976133\n",
      "Iteration 61, loss = 0.51839978\n",
      "Iteration 62, loss = 0.52493984\n",
      "Iteration 63, loss = 0.52233298\n",
      "Iteration 64, loss = 0.51884397\n",
      "Iteration 65, loss = 0.52260185\n",
      "Iteration 66, loss = 0.51613040\n",
      "Iteration 67, loss = 0.53051824\n",
      "Iteration 68, loss = 0.54185768\n",
      "Iteration 69, loss = 0.51632870\n",
      "Iteration 70, loss = 0.52554574\n",
      "Iteration 71, loss = 0.53701951\n",
      "Iteration 72, loss = 0.52289500\n",
      "Iteration 73, loss = 0.52058014\n",
      "Iteration 74, loss = 0.52625770\n",
      "Iteration 75, loss = 0.51533151\n",
      "Iteration 76, loss = 0.52452534\n",
      "Iteration 77, loss = 0.53416031\n",
      "Iteration 78, loss = 0.51888942\n",
      "Iteration 79, loss = 0.51959613\n",
      "Iteration 80, loss = 0.53089303\n",
      "Iteration 81, loss = 0.52027710\n",
      "Iteration 82, loss = 0.51424768\n",
      "Iteration 83, loss = 0.51607633\n",
      "Iteration 84, loss = 0.51688659\n",
      "Iteration 85, loss = 0.51601780\n",
      "Iteration 86, loss = 0.51511632\n",
      "Iteration 87, loss = 0.51658090\n",
      "Iteration 88, loss = 0.51448076\n",
      "Iteration 89, loss = 0.51433195\n",
      "Iteration 90, loss = 0.52335392\n",
      "Iteration 91, loss = 0.51822352\n",
      "Iteration 92, loss = 0.51546382\n",
      "Iteration 93, loss = 0.51387988\n",
      "Iteration 94, loss = 0.51443392\n",
      "Iteration 95, loss = 0.51555684\n",
      "Iteration 96, loss = 0.51234914\n",
      "Iteration 97, loss = 0.51677979\n",
      "Iteration 98, loss = 0.52629106\n",
      "Iteration 99, loss = 0.52545623\n",
      "Iteration 100, loss = 0.51737805\n",
      "Iteration 101, loss = 0.51400423\n",
      "Iteration 102, loss = 0.51464814\n",
      "Iteration 103, loss = 0.51304820\n",
      "Iteration 104, loss = 0.51335250\n",
      "Iteration 105, loss = 0.51440640\n",
      "Iteration 106, loss = 0.51351077\n",
      "Iteration 107, loss = 0.51194375\n",
      "Iteration 108, loss = 0.51586470\n",
      "Iteration 109, loss = 0.51391348\n",
      "Iteration 110, loss = 0.51043139\n",
      "Iteration 111, loss = 0.52248909\n",
      "Iteration 112, loss = 0.52463222\n",
      "Iteration 113, loss = 0.51139390\n",
      "Iteration 114, loss = 0.51656190\n",
      "Iteration 115, loss = 0.50969127\n",
      "Iteration 116, loss = 0.51918077\n",
      "Iteration 117, loss = 0.53163256\n",
      "Iteration 118, loss = 0.52116909\n",
      "Iteration 119, loss = 0.51454858\n",
      "Iteration 120, loss = 0.51340000\n",
      "Iteration 121, loss = 0.51224364\n",
      "Iteration 122, loss = 0.51394769\n",
      "Iteration 123, loss = 0.51113020\n",
      "Iteration 124, loss = 0.51396806\n",
      "Iteration 125, loss = 0.51016893\n",
      "Iteration 126, loss = 0.51404341\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.07354734\n",
      "Iteration 2, loss = 2.72264620\n",
      "Iteration 3, loss = 1.31386920\n",
      "Iteration 4, loss = 0.81912912\n",
      "Iteration 5, loss = 1.45678147\n",
      "Iteration 6, loss = 0.95759744\n",
      "Iteration 7, loss = 0.60901527\n",
      "Iteration 8, loss = 0.78097858\n",
      "Iteration 9, loss = 0.87422694\n",
      "Iteration 10, loss = 0.78493155\n",
      "Iteration 11, loss = 0.62947159\n",
      "Iteration 12, loss = 0.58786538\n",
      "Iteration 13, loss = 0.65131659\n",
      "Iteration 14, loss = 0.62343929\n",
      "Iteration 15, loss = 0.55998039\n",
      "Iteration 16, loss = 0.56900044\n",
      "Iteration 17, loss = 0.57638929\n",
      "Iteration 18, loss = 0.55710069\n",
      "Iteration 19, loss = 0.53930246\n",
      "Iteration 20, loss = 0.53282375\n",
      "Iteration 21, loss = 0.52929044\n",
      "Iteration 22, loss = 0.52646327\n",
      "Iteration 23, loss = 0.52562891\n",
      "Iteration 24, loss = 0.52417599\n",
      "Iteration 25, loss = 0.52487662\n",
      "Iteration 26, loss = 0.52652633\n",
      "Iteration 27, loss = 0.52388172\n",
      "Iteration 28, loss = 0.52325617\n",
      "Iteration 29, loss = 0.52051932\n",
      "Iteration 30, loss = 0.52285330\n",
      "Iteration 31, loss = 0.52131103\n",
      "Iteration 32, loss = 0.51605286\n",
      "Iteration 33, loss = 0.52566737\n",
      "Iteration 34, loss = 0.52916792\n",
      "Iteration 35, loss = 0.51497685\n",
      "Iteration 36, loss = 0.52306604\n",
      "Iteration 37, loss = 0.53193950\n",
      "Iteration 38, loss = 0.52286071\n",
      "Iteration 39, loss = 0.51412777\n",
      "Iteration 40, loss = 0.51736044\n",
      "Iteration 41, loss = 0.51349374\n",
      "Iteration 42, loss = 0.51599287\n",
      "Iteration 43, loss = 0.51567073\n",
      "Iteration 44, loss = 0.51202926\n",
      "Iteration 45, loss = 0.52885985\n",
      "Iteration 46, loss = 0.51162344\n",
      "Iteration 47, loss = 0.52662115\n",
      "Iteration 48, loss = 0.53739883\n",
      "Iteration 49, loss = 0.51354732\n",
      "Iteration 50, loss = 0.51655686\n",
      "Iteration 51, loss = 0.51581438\n",
      "Iteration 52, loss = 0.51071954\n",
      "Iteration 53, loss = 0.51780770\n",
      "Iteration 54, loss = 0.51514360\n",
      "Iteration 55, loss = 0.50988872\n",
      "Iteration 56, loss = 0.51187501\n",
      "Iteration 57, loss = 0.51738843\n",
      "Iteration 58, loss = 0.51455625\n",
      "Iteration 59, loss = 0.50906316\n",
      "Iteration 60, loss = 0.54367148\n",
      "Iteration 61, loss = 0.52214802\n",
      "Iteration 62, loss = 0.51775565\n",
      "Iteration 63, loss = 0.53058349\n",
      "Iteration 64, loss = 0.51122068\n",
      "Iteration 65, loss = 0.51264518\n",
      "Iteration 66, loss = 0.50713264\n",
      "Iteration 67, loss = 0.51308684\n",
      "Iteration 68, loss = 0.51255397\n",
      "Iteration 69, loss = 0.50451168\n",
      "Iteration 70, loss = 0.52296103\n",
      "Iteration 71, loss = 0.51980673\n",
      "Iteration 72, loss = 0.50693884\n",
      "Iteration 73, loss = 0.51022791\n",
      "Iteration 74, loss = 0.50420029\n",
      "Iteration 75, loss = 0.52322869\n",
      "Iteration 76, loss = 0.51463037\n",
      "Iteration 77, loss = 0.50795393\n",
      "Iteration 78, loss = 0.50898913\n",
      "Iteration 79, loss = 0.50412862\n",
      "Iteration 80, loss = 0.51173805\n",
      "Iteration 81, loss = 0.50766465\n",
      "Iteration 82, loss = 0.50501311\n",
      "Iteration 83, loss = 0.51036977\n",
      "Iteration 84, loss = 0.51280508\n",
      "Iteration 85, loss = 0.51514853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76293376\n",
      "Iteration 2, loss = 1.28944127\n",
      "Iteration 3, loss = 0.64208282\n",
      "Iteration 4, loss = 0.72510646\n",
      "Iteration 5, loss = 0.78515701\n",
      "Iteration 6, loss = 0.66114429\n",
      "Iteration 7, loss = 0.55129147\n",
      "Iteration 8, loss = 0.58625387\n",
      "Iteration 9, loss = 0.59532337\n",
      "Iteration 10, loss = 0.54604871\n",
      "Iteration 11, loss = 0.54762335\n",
      "Iteration 12, loss = 0.56216917\n",
      "Iteration 13, loss = 0.54468055\n",
      "Iteration 14, loss = 0.53570241\n",
      "Iteration 15, loss = 0.55286356\n",
      "Iteration 16, loss = 0.54965205\n",
      "Iteration 17, loss = 0.52988014\n",
      "Iteration 18, loss = 0.52958902\n",
      "Iteration 19, loss = 0.53554790\n",
      "Iteration 20, loss = 0.52469087\n",
      "Iteration 21, loss = 0.52238035\n",
      "Iteration 22, loss = 0.53149990\n",
      "Iteration 23, loss = 0.52620126\n",
      "Iteration 24, loss = 0.51966829\n",
      "Iteration 25, loss = 0.52009517\n",
      "Iteration 26, loss = 0.51912467\n",
      "Iteration 27, loss = 0.51618724\n",
      "Iteration 28, loss = 0.52162395\n",
      "Iteration 29, loss = 0.52013896\n",
      "Iteration 30, loss = 0.51564777\n",
      "Iteration 31, loss = 0.52045261\n",
      "Iteration 32, loss = 0.51478007\n",
      "Iteration 33, loss = 0.52677350\n",
      "Iteration 34, loss = 0.51614245\n",
      "Iteration 35, loss = 0.52817625\n",
      "Iteration 36, loss = 0.52730343\n",
      "Iteration 37, loss = 0.51487920\n",
      "Iteration 38, loss = 0.52957203\n",
      "Iteration 39, loss = 0.51632209\n",
      "Iteration 40, loss = 0.51344834\n",
      "Iteration 41, loss = 0.51507612\n",
      "Iteration 42, loss = 0.51062011\n",
      "Iteration 43, loss = 0.50989680\n",
      "Iteration 44, loss = 0.51234673\n",
      "Iteration 45, loss = 0.50989640\n",
      "Iteration 46, loss = 0.51209261\n",
      "Iteration 47, loss = 0.50893855\n",
      "Iteration 48, loss = 0.51329147\n",
      "Iteration 49, loss = 0.50811725\n",
      "Iteration 50, loss = 0.51239302\n",
      "Iteration 51, loss = 0.51446722\n",
      "Iteration 52, loss = 0.50829993\n",
      "Iteration 53, loss = 0.50726526\n",
      "Iteration 54, loss = 0.50613210\n",
      "Iteration 55, loss = 0.51191467\n",
      "Iteration 56, loss = 0.50343236\n",
      "Iteration 57, loss = 0.52879313\n",
      "Iteration 58, loss = 0.51240864\n",
      "Iteration 59, loss = 0.51926698\n",
      "Iteration 60, loss = 0.53884775\n",
      "Iteration 61, loss = 0.51498025\n",
      "Iteration 62, loss = 0.53228866\n",
      "Iteration 63, loss = 0.50628702\n",
      "Iteration 64, loss = 0.52058151\n",
      "Iteration 65, loss = 0.51949462\n",
      "Iteration 66, loss = 0.50585506\n",
      "Iteration 67, loss = 0.51074667\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61885122\n",
      "Iteration 2, loss = 0.54369870\n",
      "Iteration 3, loss = 0.63729656\n",
      "Iteration 4, loss = 0.55639538\n",
      "Iteration 5, loss = 0.52722254\n",
      "Iteration 6, loss = 0.57707792\n",
      "Iteration 7, loss = 0.50688668\n",
      "Iteration 8, loss = 0.55646203\n",
      "Iteration 9, loss = 0.56674219\n",
      "Iteration 10, loss = 0.51060022\n",
      "Iteration 11, loss = 0.53473166\n",
      "Iteration 12, loss = 0.52442815\n",
      "Iteration 13, loss = 0.51493301\n",
      "Iteration 14, loss = 0.51534050\n",
      "Iteration 15, loss = 0.49421055\n",
      "Iteration 16, loss = 0.51313476\n",
      "Iteration 17, loss = 0.52548961\n",
      "Iteration 18, loss = 0.49597776\n",
      "Iteration 19, loss = 0.50660423\n",
      "Iteration 20, loss = 0.50635418\n",
      "Iteration 21, loss = 0.49864001\n",
      "Iteration 22, loss = 0.50073551\n",
      "Iteration 23, loss = 0.49499143\n",
      "Iteration 24, loss = 0.49339357\n",
      "Iteration 25, loss = 0.49272380\n",
      "Iteration 26, loss = 0.49319122\n",
      "Iteration 27, loss = 0.49320485\n",
      "Iteration 28, loss = 0.49327892\n",
      "Iteration 29, loss = 0.49070756\n",
      "Iteration 30, loss = 0.49620464\n",
      "Iteration 31, loss = 0.49176892\n",
      "Iteration 32, loss = 0.49916767\n",
      "Iteration 33, loss = 0.51016239\n",
      "Iteration 34, loss = 0.49242285\n",
      "Iteration 35, loss = 0.49632830\n",
      "Iteration 36, loss = 0.49414106\n",
      "Iteration 37, loss = 0.49011108\n",
      "Iteration 38, loss = 0.48914419\n",
      "Iteration 39, loss = 0.48774199\n",
      "Iteration 40, loss = 0.49146675\n",
      "Iteration 41, loss = 0.49468114\n",
      "Iteration 42, loss = 0.49193114\n",
      "Iteration 43, loss = 0.50049370\n",
      "Iteration 44, loss = 0.48893768\n",
      "Iteration 45, loss = 0.48813619\n",
      "Iteration 46, loss = 0.48633084\n",
      "Iteration 47, loss = 0.49158884\n",
      "Iteration 48, loss = 0.49047594\n",
      "Iteration 49, loss = 0.48835332\n",
      "Iteration 50, loss = 0.49025704\n",
      "Iteration 51, loss = 0.48724746\n",
      "Iteration 52, loss = 0.48648966\n",
      "Iteration 53, loss = 0.48547330\n",
      "Iteration 54, loss = 0.48446947\n",
      "Iteration 55, loss = 0.48387895\n",
      "Iteration 56, loss = 0.48434897\n",
      "Iteration 57, loss = 0.48644704\n",
      "Iteration 58, loss = 0.48339675\n",
      "Iteration 59, loss = 0.48357248\n",
      "Iteration 60, loss = 0.48134384\n",
      "Iteration 61, loss = 0.49290959\n",
      "Iteration 62, loss = 0.48198413\n",
      "Iteration 63, loss = 0.50445767\n",
      "Iteration 64, loss = 0.47801208\n",
      "Iteration 65, loss = 0.52923477\n",
      "Iteration 66, loss = 0.48854991\n",
      "Iteration 67, loss = 0.50543360\n",
      "Iteration 68, loss = 0.49767782\n",
      "Iteration 69, loss = 0.49426487\n",
      "Iteration 70, loss = 0.49973652\n",
      "Iteration 71, loss = 0.48892008\n",
      "Iteration 72, loss = 0.49022955\n",
      "Iteration 73, loss = 0.48890287\n",
      "Iteration 74, loss = 0.50337910\n",
      "Iteration 75, loss = 0.48390279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.03982695\n",
      "Iteration 2, loss = 0.91577301\n",
      "Iteration 3, loss = 0.89269879\n",
      "Iteration 4, loss = 0.69004899\n",
      "Iteration 5, loss = 0.60229573\n",
      "Iteration 6, loss = 0.73992491\n",
      "Iteration 7, loss = 0.68907909\n",
      "Iteration 8, loss = 0.54682529\n",
      "Iteration 9, loss = 0.56008042\n",
      "Iteration 10, loss = 0.63065828\n",
      "Iteration 11, loss = 0.54245319\n",
      "Iteration 12, loss = 0.51919557\n",
      "Iteration 13, loss = 0.55154234\n",
      "Iteration 14, loss = 0.54841625\n",
      "Iteration 15, loss = 0.51330661\n",
      "Iteration 16, loss = 0.52059393\n",
      "Iteration 17, loss = 0.53129166\n",
      "Iteration 18, loss = 0.50185007\n",
      "Iteration 19, loss = 0.52110308\n",
      "Iteration 20, loss = 0.54109678\n",
      "Iteration 21, loss = 0.50819008\n",
      "Iteration 22, loss = 0.50544260\n",
      "Iteration 23, loss = 0.56161692\n",
      "Iteration 24, loss = 0.53124444\n",
      "Iteration 25, loss = 0.49962893\n",
      "Iteration 26, loss = 0.51536976\n",
      "Iteration 27, loss = 0.51118473\n",
      "Iteration 28, loss = 0.49509081\n",
      "Iteration 29, loss = 0.50411763\n",
      "Iteration 30, loss = 0.51311728\n",
      "Iteration 31, loss = 0.49670924\n",
      "Iteration 32, loss = 0.49733002\n",
      "Iteration 33, loss = 0.50859879\n",
      "Iteration 34, loss = 0.50000225\n",
      "Iteration 35, loss = 0.49939985\n",
      "Iteration 36, loss = 0.49576944\n",
      "Iteration 37, loss = 0.49118792\n",
      "Iteration 38, loss = 0.49156783\n",
      "Iteration 39, loss = 0.49076800\n",
      "Iteration 40, loss = 0.48993596\n",
      "Iteration 41, loss = 0.49602582\n",
      "Iteration 42, loss = 0.49080291\n",
      "Iteration 43, loss = 0.49592305\n",
      "Iteration 44, loss = 0.49364968\n",
      "Iteration 45, loss = 0.49023902\n",
      "Iteration 46, loss = 0.49098358\n",
      "Iteration 47, loss = 0.48672251\n",
      "Iteration 48, loss = 0.50000144\n",
      "Iteration 49, loss = 0.50183399\n",
      "Iteration 50, loss = 0.48795770\n",
      "Iteration 51, loss = 0.53096171\n",
      "Iteration 52, loss = 0.49825176\n",
      "Iteration 53, loss = 0.49634163\n",
      "Iteration 54, loss = 0.51395622\n",
      "Iteration 55, loss = 0.49261588\n",
      "Iteration 56, loss = 0.48539924\n",
      "Iteration 57, loss = 0.50182215\n",
      "Iteration 58, loss = 0.49466704\n",
      "Iteration 59, loss = 0.48642967\n",
      "Iteration 60, loss = 0.52373593\n",
      "Iteration 61, loss = 0.49917274\n",
      "Iteration 62, loss = 0.49477925\n",
      "Iteration 63, loss = 0.49631598\n",
      "Iteration 64, loss = 0.48691993\n",
      "Iteration 65, loss = 0.49356226\n",
      "Iteration 66, loss = 0.48811276\n",
      "Iteration 67, loss = 0.48322834\n",
      "Iteration 68, loss = 0.48336671\n",
      "Iteration 69, loss = 0.48275465\n",
      "Iteration 70, loss = 0.48664072\n",
      "Iteration 71, loss = 0.48371847\n",
      "Iteration 72, loss = 0.48649968\n",
      "Iteration 73, loss = 0.48162449\n",
      "Iteration 74, loss = 0.49345051\n",
      "Iteration 75, loss = 0.47947028\n",
      "Iteration 76, loss = 0.50912030\n",
      "Iteration 77, loss = 0.49691716\n",
      "Iteration 78, loss = 0.49123549\n",
      "Iteration 79, loss = 0.49923507\n",
      "Iteration 80, loss = 0.49843025\n",
      "Iteration 81, loss = 0.49253595\n",
      "Iteration 82, loss = 0.48202231\n",
      "Iteration 83, loss = 0.49419996\n",
      "Iteration 84, loss = 0.48874859\n",
      "Iteration 85, loss = 0.51154545\n",
      "Iteration 86, loss = 0.47954955\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74550896\n",
      "Iteration 2, loss = 0.56323268\n",
      "Iteration 3, loss = 0.61681300\n",
      "Iteration 4, loss = 0.55878060\n",
      "Iteration 5, loss = 0.57826771\n",
      "Iteration 6, loss = 0.55988342\n",
      "Iteration 7, loss = 0.53715704\n",
      "Iteration 8, loss = 0.53632328\n",
      "Iteration 9, loss = 0.52930995\n",
      "Iteration 10, loss = 0.53167026\n",
      "Iteration 11, loss = 0.52914632\n",
      "Iteration 12, loss = 0.52610421\n",
      "Iteration 13, loss = 0.53033964\n",
      "Iteration 14, loss = 0.52473857\n",
      "Iteration 15, loss = 0.52776306\n",
      "Iteration 16, loss = 0.52910682\n",
      "Iteration 17, loss = 0.51883203\n",
      "Iteration 18, loss = 0.53060825\n",
      "Iteration 19, loss = 0.52190956\n",
      "Iteration 20, loss = 0.52734045\n",
      "Iteration 21, loss = 0.51911237\n",
      "Iteration 22, loss = 0.52163623\n",
      "Iteration 23, loss = 0.53526267\n",
      "Iteration 24, loss = 0.53152918\n",
      "Iteration 25, loss = 0.51616352\n",
      "Iteration 26, loss = 0.51459307\n",
      "Iteration 27, loss = 0.51770115\n",
      "Iteration 28, loss = 0.52488768\n",
      "Iteration 29, loss = 0.54130951\n",
      "Iteration 30, loss = 0.51460703\n",
      "Iteration 31, loss = 0.51631289\n",
      "Iteration 32, loss = 0.51157904\n",
      "Iteration 33, loss = 0.51239189\n",
      "Iteration 34, loss = 0.51332857\n",
      "Iteration 35, loss = 0.51107745\n",
      "Iteration 36, loss = 0.51757403\n",
      "Iteration 37, loss = 0.51010554\n",
      "Iteration 38, loss = 0.52733468\n",
      "Iteration 39, loss = 0.51635722\n",
      "Iteration 40, loss = 0.52161018\n",
      "Iteration 41, loss = 0.51506595\n",
      "Iteration 42, loss = 0.50884115\n",
      "Iteration 43, loss = 0.52429465\n",
      "Iteration 44, loss = 0.52499295\n",
      "Iteration 45, loss = 0.50933962\n",
      "Iteration 46, loss = 0.51543433\n",
      "Iteration 47, loss = 0.51434455\n",
      "Iteration 48, loss = 0.51665384\n",
      "Iteration 49, loss = 0.51417065\n",
      "Iteration 50, loss = 0.52520387\n",
      "Iteration 51, loss = 0.52499377\n",
      "Iteration 52, loss = 0.51274677\n",
      "Iteration 53, loss = 0.50829607\n",
      "Iteration 54, loss = 0.51686263\n",
      "Iteration 55, loss = 0.51111170\n",
      "Iteration 56, loss = 0.55813822\n",
      "Iteration 57, loss = 0.51540127\n",
      "Iteration 58, loss = 0.54319367\n",
      "Iteration 59, loss = 0.51771614\n",
      "Iteration 60, loss = 0.50700719\n",
      "Iteration 61, loss = 0.51886003\n",
      "Iteration 62, loss = 0.50843696\n",
      "Iteration 63, loss = 0.52396880\n",
      "Iteration 64, loss = 0.50465332\n",
      "Iteration 65, loss = 0.52462061\n",
      "Iteration 66, loss = 0.50653674\n",
      "Iteration 67, loss = 0.51381810\n",
      "Iteration 68, loss = 0.50535752\n",
      "Iteration 69, loss = 0.52194875\n",
      "Iteration 70, loss = 0.50518036\n",
      "Iteration 71, loss = 0.52587143\n",
      "Iteration 72, loss = 0.50585782\n",
      "Iteration 73, loss = 0.54209976\n",
      "Iteration 74, loss = 0.51445796\n",
      "Iteration 75, loss = 0.52843381\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 8.72447219\n",
      "Iteration 2, loss = 4.88559310\n",
      "Iteration 3, loss = 1.42540560\n",
      "Iteration 4, loss = 1.11346512\n",
      "Iteration 5, loss = 1.78240250\n",
      "Iteration 6, loss = 2.11218943\n",
      "Iteration 7, loss = 2.14197239\n",
      "Iteration 8, loss = 1.96326806\n",
      "Iteration 9, loss = 1.62707849\n",
      "Iteration 10, loss = 1.18638482\n",
      "Iteration 11, loss = 0.75829203\n",
      "Iteration 12, loss = 0.60449990\n",
      "Iteration 13, loss = 0.89295143\n",
      "Iteration 14, loss = 0.87558651\n",
      "Iteration 15, loss = 0.61868748\n",
      "Iteration 16, loss = 0.54239109\n",
      "Iteration 17, loss = 0.62865373\n",
      "Iteration 18, loss = 0.66574217\n",
      "Iteration 19, loss = 0.62974729\n",
      "Iteration 20, loss = 0.56572477\n",
      "Iteration 21, loss = 0.52174766\n",
      "Iteration 22, loss = 0.55042523\n",
      "Iteration 23, loss = 0.58399761\n",
      "Iteration 24, loss = 0.55443619\n",
      "Iteration 25, loss = 0.52270373\n",
      "Iteration 26, loss = 0.53467790\n",
      "Iteration 27, loss = 0.56109471\n",
      "Iteration 28, loss = 0.56001347\n",
      "Iteration 29, loss = 0.53543824\n",
      "Iteration 30, loss = 0.52113930\n",
      "Iteration 31, loss = 0.52507163\n",
      "Iteration 32, loss = 0.52526406\n",
      "Iteration 33, loss = 0.52022111\n",
      "Iteration 34, loss = 0.51782522\n",
      "Iteration 35, loss = 0.51911333\n",
      "Iteration 36, loss = 0.51700689\n",
      "Iteration 37, loss = 0.51599094\n",
      "Iteration 38, loss = 0.51623683\n",
      "Iteration 39, loss = 0.51625685\n",
      "Iteration 40, loss = 0.51533230\n",
      "Iteration 41, loss = 0.51389263\n",
      "Iteration 42, loss = 0.51353215\n",
      "Iteration 43, loss = 0.51386219\n",
      "Iteration 44, loss = 0.51338714\n",
      "Iteration 45, loss = 0.51216557\n",
      "Iteration 46, loss = 0.51244638\n",
      "Iteration 47, loss = 0.51506954\n",
      "Iteration 48, loss = 0.51118693\n",
      "Iteration 49, loss = 0.51485085\n",
      "Iteration 50, loss = 0.51778400\n",
      "Iteration 51, loss = 0.51184944\n",
      "Iteration 52, loss = 0.51435809\n",
      "Iteration 53, loss = 0.51771515\n",
      "Iteration 54, loss = 0.51184125\n",
      "Iteration 55, loss = 0.50935681\n",
      "Iteration 56, loss = 0.51778036\n",
      "Iteration 57, loss = 0.51886592\n",
      "Iteration 58, loss = 0.51171140\n",
      "Iteration 59, loss = 0.50979886\n",
      "Iteration 60, loss = 0.51152502\n",
      "Iteration 61, loss = 0.50861807\n",
      "Iteration 62, loss = 0.50909014\n",
      "Iteration 63, loss = 0.51168824\n",
      "Iteration 64, loss = 0.50956128\n",
      "Iteration 65, loss = 0.50872949\n",
      "Iteration 66, loss = 0.50736339\n",
      "Iteration 67, loss = 0.50851482\n",
      "Iteration 68, loss = 0.50774360\n",
      "Iteration 69, loss = 0.50875082\n",
      "Iteration 70, loss = 0.50871740\n",
      "Iteration 71, loss = 0.50702407\n",
      "Iteration 72, loss = 0.50787500\n",
      "Iteration 73, loss = 0.50764245\n",
      "Iteration 74, loss = 0.50573174\n",
      "Iteration 75, loss = 0.50618184\n",
      "Iteration 76, loss = 0.51208054\n",
      "Iteration 77, loss = 0.50747349\n",
      "Iteration 78, loss = 0.50822838\n",
      "Iteration 79, loss = 0.50919549\n",
      "Iteration 80, loss = 0.50525073\n",
      "Iteration 81, loss = 0.50965547\n",
      "Iteration 82, loss = 0.50470344\n",
      "Iteration 83, loss = 0.50691176\n",
      "Iteration 84, loss = 0.51083680\n",
      "Iteration 85, loss = 0.50471316\n",
      "Iteration 86, loss = 0.50639199\n",
      "Iteration 87, loss = 0.50676461\n",
      "Iteration 88, loss = 0.50265460\n",
      "Iteration 89, loss = 0.50350798\n",
      "Iteration 90, loss = 0.50618910\n",
      "Iteration 91, loss = 0.50514981\n",
      "Iteration 92, loss = 0.50861329\n",
      "Iteration 93, loss = 0.50429403\n",
      "Iteration 94, loss = 0.50658524\n",
      "Iteration 95, loss = 0.51237495\n",
      "Iteration 96, loss = 0.50479097\n",
      "Iteration 97, loss = 0.50949837\n",
      "Iteration 98, loss = 0.50559393\n",
      "Iteration 99, loss = 0.50695280\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75220937\n",
      "Iteration 2, loss = 0.84546139\n",
      "Iteration 3, loss = 0.64404822\n",
      "Iteration 4, loss = 0.64532148\n",
      "Iteration 5, loss = 0.60855872\n",
      "Iteration 6, loss = 0.60010443\n",
      "Iteration 7, loss = 0.57426320\n",
      "Iteration 8, loss = 0.56601208\n",
      "Iteration 9, loss = 0.56678335\n",
      "Iteration 10, loss = 0.56692164\n",
      "Iteration 11, loss = 0.54903959\n",
      "Iteration 12, loss = 0.54842491\n",
      "Iteration 13, loss = 0.54568488\n",
      "Iteration 14, loss = 0.54491648\n",
      "Iteration 15, loss = 0.54367083\n",
      "Iteration 16, loss = 0.54299545\n",
      "Iteration 17, loss = 0.54522424\n",
      "Iteration 18, loss = 0.54278787\n",
      "Iteration 19, loss = 0.54373301\n",
      "Iteration 20, loss = 0.53797610\n",
      "Iteration 21, loss = 0.53668999\n",
      "Iteration 22, loss = 0.53498041\n",
      "Iteration 23, loss = 0.53449661\n",
      "Iteration 24, loss = 0.53550256\n",
      "Iteration 25, loss = 0.53702214\n",
      "Iteration 26, loss = 0.53133375\n",
      "Iteration 27, loss = 0.55282611\n",
      "Iteration 28, loss = 0.52598732\n",
      "Iteration 29, loss = 0.58590584\n",
      "Iteration 30, loss = 0.52970600\n",
      "Iteration 31, loss = 0.59117384\n",
      "Iteration 32, loss = 0.55532572\n",
      "Iteration 33, loss = 0.57824830\n",
      "Iteration 34, loss = 0.55835876\n",
      "Iteration 35, loss = 0.57088533\n",
      "Iteration 36, loss = 0.55260944\n",
      "Iteration 37, loss = 0.55801996\n",
      "Iteration 38, loss = 0.53320712\n",
      "Iteration 39, loss = 0.56016940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.15983525\n",
      "Iteration 2, loss = 0.90442860\n",
      "Iteration 3, loss = 0.63524027\n",
      "Iteration 4, loss = 0.73944345\n",
      "Iteration 5, loss = 0.70713129\n",
      "Iteration 6, loss = 0.60027580\n",
      "Iteration 7, loss = 0.61201683\n",
      "Iteration 8, loss = 0.56835893\n",
      "Iteration 9, loss = 0.60414370\n",
      "Iteration 10, loss = 0.60077701\n",
      "Iteration 11, loss = 0.57709826\n",
      "Iteration 12, loss = 0.58141000\n",
      "Iteration 13, loss = 0.56967635\n",
      "Iteration 14, loss = 0.59049717\n",
      "Iteration 15, loss = 0.56011566\n",
      "Iteration 16, loss = 0.55517818\n",
      "Iteration 17, loss = 0.56098903\n",
      "Iteration 18, loss = 0.55066720\n",
      "Iteration 19, loss = 0.54720220\n",
      "Iteration 20, loss = 0.54690665\n",
      "Iteration 21, loss = 0.54474945\n",
      "Iteration 22, loss = 0.54589599\n",
      "Iteration 23, loss = 0.54244438\n",
      "Iteration 24, loss = 0.54613823\n",
      "Iteration 25, loss = 0.55012415\n",
      "Iteration 26, loss = 0.54050330\n",
      "Iteration 27, loss = 0.54252877\n",
      "Iteration 28, loss = 0.54542198\n",
      "Iteration 29, loss = 0.53656734\n",
      "Iteration 30, loss = 0.55644596\n",
      "Iteration 31, loss = 0.54773858\n",
      "Iteration 32, loss = 0.54077407\n",
      "Iteration 33, loss = 0.53788944\n",
      "Iteration 34, loss = 0.53565755\n",
      "Iteration 35, loss = 0.53688915\n",
      "Iteration 36, loss = 0.53662750\n",
      "Iteration 37, loss = 0.53889940\n",
      "Iteration 38, loss = 0.53428728\n",
      "Iteration 39, loss = 0.53670633\n",
      "Iteration 40, loss = 0.53361008\n",
      "Iteration 41, loss = 0.55600824\n",
      "Iteration 42, loss = 0.53513371\n",
      "Iteration 43, loss = 0.54575048\n",
      "Iteration 44, loss = 0.54594058\n",
      "Iteration 45, loss = 0.53482029\n",
      "Iteration 46, loss = 0.53302844\n",
      "Iteration 47, loss = 0.53661946\n",
      "Iteration 48, loss = 0.53992896\n",
      "Iteration 49, loss = 0.53163753\n",
      "Iteration 50, loss = 0.53378074\n",
      "Iteration 51, loss = 0.53713808\n",
      "Iteration 52, loss = 0.53365151\n",
      "Iteration 53, loss = 0.54106144\n",
      "Iteration 54, loss = 0.54334475\n",
      "Iteration 55, loss = 0.53166207\n",
      "Iteration 56, loss = 0.53210473\n",
      "Iteration 57, loss = 0.53593979\n",
      "Iteration 58, loss = 0.53230804\n",
      "Iteration 59, loss = 0.54409558\n",
      "Iteration 60, loss = 0.53659279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.04151548\n",
      "Iteration 2, loss = 0.80913806\n",
      "Iteration 3, loss = 0.67595409\n",
      "Iteration 4, loss = 0.60813367\n",
      "Iteration 5, loss = 0.57634930\n",
      "Iteration 6, loss = 0.57517273\n",
      "Iteration 7, loss = 0.60370945\n",
      "Iteration 8, loss = 0.54922160\n",
      "Iteration 9, loss = 0.57596092\n",
      "Iteration 10, loss = 0.58812817\n",
      "Iteration 11, loss = 0.54687992\n",
      "Iteration 12, loss = 0.56223428\n",
      "Iteration 13, loss = 0.56687288\n",
      "Iteration 14, loss = 0.53906971\n",
      "Iteration 15, loss = 0.55218565\n",
      "Iteration 16, loss = 0.54296738\n",
      "Iteration 17, loss = 0.53549272\n",
      "Iteration 18, loss = 0.54954073\n",
      "Iteration 19, loss = 0.53628653\n",
      "Iteration 20, loss = 0.52804231\n",
      "Iteration 21, loss = 0.53946042\n",
      "Iteration 22, loss = 0.53107505\n",
      "Iteration 23, loss = 0.53140547\n",
      "Iteration 24, loss = 0.53412357\n",
      "Iteration 25, loss = 0.52538327\n",
      "Iteration 26, loss = 0.53405643\n",
      "Iteration 27, loss = 0.52931586\n",
      "Iteration 28, loss = 0.53124449\n",
      "Iteration 29, loss = 0.52083944\n",
      "Iteration 30, loss = 0.52486329\n",
      "Iteration 31, loss = 0.52491510\n",
      "Iteration 32, loss = 0.51987640\n",
      "Iteration 33, loss = 0.52257725\n",
      "Iteration 34, loss = 0.52118673\n",
      "Iteration 35, loss = 0.51823435\n",
      "Iteration 36, loss = 0.51811373\n",
      "Iteration 37, loss = 0.51794150\n",
      "Iteration 38, loss = 0.51821495\n",
      "Iteration 39, loss = 0.51726707\n",
      "Iteration 40, loss = 0.51769083\n",
      "Iteration 41, loss = 0.51677135\n",
      "Iteration 42, loss = 0.51538235\n",
      "Iteration 43, loss = 0.51748734\n",
      "Iteration 44, loss = 0.51984541\n",
      "Iteration 45, loss = 0.51468425\n",
      "Iteration 46, loss = 0.52244026\n",
      "Iteration 47, loss = 0.51396559\n",
      "Iteration 48, loss = 0.53229556\n",
      "Iteration 49, loss = 0.52841820\n",
      "Iteration 50, loss = 0.51317507\n",
      "Iteration 51, loss = 0.51386634\n",
      "Iteration 52, loss = 0.51277754\n",
      "Iteration 53, loss = 0.51366424\n",
      "Iteration 54, loss = 0.51244329\n",
      "Iteration 55, loss = 0.51313007\n",
      "Iteration 56, loss = 0.51352159\n",
      "Iteration 57, loss = 0.51376315\n",
      "Iteration 58, loss = 0.51312534\n",
      "Iteration 59, loss = 0.51262799\n",
      "Iteration 60, loss = 0.51129517\n",
      "Iteration 61, loss = 0.51631231\n",
      "Iteration 62, loss = 0.51809056\n",
      "Iteration 63, loss = 0.51613421\n",
      "Iteration 64, loss = 0.51783005\n",
      "Iteration 65, loss = 0.51285149\n",
      "Iteration 66, loss = 0.52545182\n",
      "Iteration 67, loss = 0.51710195\n",
      "Iteration 68, loss = 0.51666240\n",
      "Iteration 69, loss = 0.51660831\n",
      "Iteration 70, loss = 0.51241824\n",
      "Iteration 71, loss = 0.51595460\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 7.16688665\n",
      "Iteration 2, loss = 3.83218538\n",
      "Iteration 3, loss = 1.01582007\n",
      "Iteration 4, loss = 0.99453166\n",
      "Iteration 5, loss = 1.57502680\n",
      "Iteration 6, loss = 1.80512387\n",
      "Iteration 7, loss = 1.78750543\n",
      "Iteration 8, loss = 1.59064108\n",
      "Iteration 9, loss = 1.24936217\n",
      "Iteration 10, loss = 0.84302004\n",
      "Iteration 11, loss = 0.54804223\n",
      "Iteration 12, loss = 0.71782131\n",
      "Iteration 13, loss = 0.95527012\n",
      "Iteration 14, loss = 0.80342767\n",
      "Iteration 15, loss = 0.55749320\n",
      "Iteration 16, loss = 0.55248456\n",
      "Iteration 17, loss = 0.64645662\n",
      "Iteration 18, loss = 0.68564329\n",
      "Iteration 19, loss = 0.63678986\n",
      "Iteration 20, loss = 0.55121150\n",
      "Iteration 21, loss = 0.51523468\n",
      "Iteration 22, loss = 0.57179751\n",
      "Iteration 23, loss = 0.58492973\n",
      "Iteration 24, loss = 0.53645094\n",
      "Iteration 25, loss = 0.50774379\n",
      "Iteration 26, loss = 0.53307647\n",
      "Iteration 27, loss = 0.55320026\n",
      "Iteration 28, loss = 0.53806512\n",
      "Iteration 29, loss = 0.51296796\n",
      "Iteration 30, loss = 0.50349699\n",
      "Iteration 31, loss = 0.51496257\n",
      "Iteration 32, loss = 0.51153808\n",
      "Iteration 33, loss = 0.49960905\n",
      "Iteration 34, loss = 0.50333965\n",
      "Iteration 35, loss = 0.50711405\n",
      "Iteration 36, loss = 0.50237206\n",
      "Iteration 37, loss = 0.49504772\n",
      "Iteration 38, loss = 0.50060814\n",
      "Iteration 39, loss = 0.50044000\n",
      "Iteration 40, loss = 0.49395910\n",
      "Iteration 41, loss = 0.49728330\n",
      "Iteration 42, loss = 0.49914520\n",
      "Iteration 43, loss = 0.49446569\n",
      "Iteration 44, loss = 0.49075075\n",
      "Iteration 45, loss = 0.49290596\n",
      "Iteration 46, loss = 0.49398500\n",
      "Iteration 47, loss = 0.49085348\n",
      "Iteration 48, loss = 0.48950347\n",
      "Iteration 49, loss = 0.49192003\n",
      "Iteration 50, loss = 0.49692145\n",
      "Iteration 51, loss = 0.49417971\n",
      "Iteration 52, loss = 0.48764132\n",
      "Iteration 53, loss = 0.49480133\n",
      "Iteration 54, loss = 0.49714574\n",
      "Iteration 55, loss = 0.48968915\n",
      "Iteration 56, loss = 0.48715546\n",
      "Iteration 57, loss = 0.49184285\n",
      "Iteration 58, loss = 0.48897484\n",
      "Iteration 59, loss = 0.48621274\n",
      "Iteration 60, loss = 0.48590460\n",
      "Iteration 61, loss = 0.48610896\n",
      "Iteration 62, loss = 0.48370519\n",
      "Iteration 63, loss = 0.48249208\n",
      "Iteration 64, loss = 0.48904275\n",
      "Iteration 65, loss = 0.48623572\n",
      "Iteration 66, loss = 0.48258248\n",
      "Iteration 67, loss = 0.48816916\n",
      "Iteration 68, loss = 0.48423734\n",
      "Iteration 69, loss = 0.48075324\n",
      "Iteration 70, loss = 0.48496968\n",
      "Iteration 71, loss = 0.48355764\n",
      "Iteration 72, loss = 0.47966608\n",
      "Iteration 73, loss = 0.47896647\n",
      "Iteration 74, loss = 0.47863905\n",
      "Iteration 75, loss = 0.47836760\n",
      "Iteration 76, loss = 0.47741867\n",
      "Iteration 77, loss = 0.47720482\n",
      "Iteration 78, loss = 0.48071444\n",
      "Iteration 79, loss = 0.48487327\n",
      "Iteration 80, loss = 0.48135151\n",
      "Iteration 81, loss = 0.47675308\n",
      "Iteration 82, loss = 0.47590283\n",
      "Iteration 83, loss = 0.47629548\n",
      "Iteration 84, loss = 0.48097224\n",
      "Iteration 85, loss = 0.47676967\n",
      "Iteration 86, loss = 0.47858867\n",
      "Iteration 87, loss = 0.47747546\n",
      "Iteration 88, loss = 0.47635230\n",
      "Iteration 89, loss = 0.47830417\n",
      "Iteration 90, loss = 0.47595164\n",
      "Iteration 91, loss = 0.47489821\n",
      "Iteration 92, loss = 0.47435046\n",
      "Iteration 93, loss = 0.47409131\n",
      "Iteration 94, loss = 0.47377445\n",
      "Iteration 95, loss = 0.47353852\n",
      "Iteration 96, loss = 0.47893941\n",
      "Iteration 97, loss = 0.47539526\n",
      "Iteration 98, loss = 0.47462236\n",
      "Iteration 99, loss = 0.47741743\n",
      "Iteration 100, loss = 0.47411479\n",
      "Iteration 101, loss = 0.47224627\n",
      "Iteration 102, loss = 0.47246182\n",
      "Iteration 103, loss = 0.47233831\n",
      "Iteration 104, loss = 0.47266803\n",
      "Iteration 105, loss = 0.47259966\n",
      "Iteration 106, loss = 0.47924476\n",
      "Iteration 107, loss = 0.47969917\n",
      "Iteration 108, loss = 0.47204201\n",
      "Iteration 109, loss = 0.47081033\n",
      "Iteration 110, loss = 0.47129652\n",
      "Iteration 111, loss = 0.47135681\n",
      "Iteration 112, loss = 0.47110104\n",
      "Iteration 113, loss = 0.47216853\n",
      "Iteration 114, loss = 0.48739258\n",
      "Iteration 115, loss = 0.48384141\n",
      "Iteration 116, loss = 0.47393794\n",
      "Iteration 117, loss = 0.48297423\n",
      "Iteration 118, loss = 0.47795812\n",
      "Iteration 119, loss = 0.47158279\n",
      "Iteration 120, loss = 0.47559918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94536140\n",
      "Iteration 2, loss = 1.06931664\n",
      "Iteration 3, loss = 0.97974087\n",
      "Iteration 4, loss = 0.60418527\n",
      "Iteration 5, loss = 0.72325872\n",
      "Iteration 6, loss = 0.57104708\n",
      "Iteration 7, loss = 0.58028318\n",
      "Iteration 8, loss = 0.64540765\n",
      "Iteration 9, loss = 0.56527428\n",
      "Iteration 10, loss = 0.52391296\n",
      "Iteration 11, loss = 0.59170072\n",
      "Iteration 12, loss = 0.53470955\n",
      "Iteration 13, loss = 0.52284229\n",
      "Iteration 14, loss = 0.55054830\n",
      "Iteration 15, loss = 0.52574070\n",
      "Iteration 16, loss = 0.51166820\n",
      "Iteration 17, loss = 0.51903923\n",
      "Iteration 18, loss = 0.50398326\n",
      "Iteration 19, loss = 0.50171936\n",
      "Iteration 20, loss = 0.49792669\n",
      "Iteration 21, loss = 0.49348446\n",
      "Iteration 22, loss = 0.50449428\n",
      "Iteration 23, loss = 0.49534454\n",
      "Iteration 24, loss = 0.49538267\n",
      "Iteration 25, loss = 0.49422997\n",
      "Iteration 26, loss = 0.49631337\n",
      "Iteration 27, loss = 0.49161380\n",
      "Iteration 28, loss = 0.48964385\n",
      "Iteration 29, loss = 0.49390709\n",
      "Iteration 30, loss = 0.48799557\n",
      "Iteration 31, loss = 0.49211555\n",
      "Iteration 32, loss = 0.49678093\n",
      "Iteration 33, loss = 0.48686671\n",
      "Iteration 34, loss = 0.48675930\n",
      "Iteration 35, loss = 0.48378488\n",
      "Iteration 36, loss = 0.48461551\n",
      "Iteration 37, loss = 0.48500823\n",
      "Iteration 38, loss = 0.48352948\n",
      "Iteration 39, loss = 0.48214698\n",
      "Iteration 40, loss = 0.48287891\n",
      "Iteration 41, loss = 0.48251951\n",
      "Iteration 42, loss = 0.50315285\n",
      "Iteration 43, loss = 0.48588815\n",
      "Iteration 44, loss = 0.49200484\n",
      "Iteration 45, loss = 0.48037150\n",
      "Iteration 46, loss = 0.48859522\n",
      "Iteration 47, loss = 0.48178755\n",
      "Iteration 48, loss = 0.48598509\n",
      "Iteration 49, loss = 0.47884107\n",
      "Iteration 50, loss = 0.48458589\n",
      "Iteration 51, loss = 0.49167115\n",
      "Iteration 52, loss = 0.47518962\n",
      "Iteration 53, loss = 0.50220142\n",
      "Iteration 54, loss = 0.48056853\n",
      "Iteration 55, loss = 0.49709132\n",
      "Iteration 56, loss = 0.48490089\n",
      "Iteration 57, loss = 0.48068983\n",
      "Iteration 58, loss = 0.48662017\n",
      "Iteration 59, loss = 0.48482790\n",
      "Iteration 60, loss = 0.50606926\n",
      "Iteration 61, loss = 0.47334250\n",
      "Iteration 62, loss = 0.48775163\n",
      "Iteration 63, loss = 0.48183713\n",
      "Iteration 64, loss = 0.47298945\n",
      "Iteration 65, loss = 0.47231671\n",
      "Iteration 66, loss = 0.47766411\n",
      "Iteration 67, loss = 0.47202589\n",
      "Iteration 68, loss = 0.47763493\n",
      "Iteration 69, loss = 0.47380859\n",
      "Iteration 70, loss = 0.47431495\n",
      "Iteration 71, loss = 0.47729180\n",
      "Iteration 72, loss = 0.47021066\n",
      "Iteration 73, loss = 0.47364675\n",
      "Iteration 74, loss = 0.46849987\n",
      "Iteration 75, loss = 0.48181542\n",
      "Iteration 76, loss = 0.48893458\n",
      "Iteration 77, loss = 0.47147874\n",
      "Iteration 78, loss = 0.47165062\n",
      "Iteration 79, loss = 0.46818888\n",
      "Iteration 80, loss = 0.46763985\n",
      "Iteration 81, loss = 0.46921524\n",
      "Iteration 82, loss = 0.47297303\n",
      "Iteration 83, loss = 0.46593140\n",
      "Iteration 84, loss = 0.48169909\n",
      "Iteration 85, loss = 0.49424677\n",
      "Iteration 86, loss = 0.47308317\n",
      "Iteration 87, loss = 0.50202968\n",
      "Iteration 88, loss = 0.47368814\n",
      "Iteration 89, loss = 0.46839095\n",
      "Iteration 90, loss = 0.46898483\n",
      "Iteration 91, loss = 0.46495086\n",
      "Iteration 92, loss = 0.46626270\n",
      "Iteration 93, loss = 0.46697818\n",
      "Iteration 94, loss = 0.46864199\n",
      "Iteration 95, loss = 0.46947931\n",
      "Iteration 96, loss = 0.46784104\n",
      "Iteration 97, loss = 0.46771160\n",
      "Iteration 98, loss = 0.46556046\n",
      "Iteration 99, loss = 0.46585439\n",
      "Iteration 100, loss = 0.46425676\n",
      "Iteration 101, loss = 0.46602300\n",
      "Iteration 102, loss = 0.46703760\n",
      "Iteration 103, loss = 0.46518895\n",
      "Iteration 104, loss = 0.46907400\n",
      "Iteration 105, loss = 0.46825519\n",
      "Iteration 106, loss = 0.46705136\n",
      "Iteration 107, loss = 0.46503990\n",
      "Iteration 108, loss = 0.46807405\n",
      "Iteration 109, loss = 0.47438874\n",
      "Iteration 110, loss = 0.48306477\n",
      "Iteration 111, loss = 0.46635538\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.50959330\n",
      "Iteration 2, loss = 0.61566367\n",
      "Iteration 3, loss = 0.78066811\n",
      "Iteration 4, loss = 0.56003326\n",
      "Iteration 5, loss = 0.63654246\n",
      "Iteration 6, loss = 0.59372623\n",
      "Iteration 7, loss = 0.52473772\n",
      "Iteration 8, loss = 0.54945018\n",
      "Iteration 9, loss = 0.54101300\n",
      "Iteration 10, loss = 0.50811213\n",
      "Iteration 11, loss = 0.53598452\n",
      "Iteration 12, loss = 0.55419259\n",
      "Iteration 13, loss = 0.51698573\n",
      "Iteration 14, loss = 0.51360928\n",
      "Iteration 15, loss = 0.55383821\n",
      "Iteration 16, loss = 0.51563155\n",
      "Iteration 17, loss = 0.52233389\n",
      "Iteration 18, loss = 0.52999567\n",
      "Iteration 19, loss = 0.50776385\n",
      "Iteration 20, loss = 0.50847229\n",
      "Iteration 21, loss = 0.51272484\n",
      "Iteration 22, loss = 0.51023397\n",
      "Iteration 23, loss = 0.50647485\n",
      "Iteration 24, loss = 0.50244674\n",
      "Iteration 25, loss = 0.50531902\n",
      "Iteration 26, loss = 0.50431333\n",
      "Iteration 27, loss = 0.50052277\n",
      "Iteration 28, loss = 0.50167780\n",
      "Iteration 29, loss = 0.50429254\n",
      "Iteration 30, loss = 0.50375430\n",
      "Iteration 31, loss = 0.50011523\n",
      "Iteration 32, loss = 0.50009665\n",
      "Iteration 33, loss = 0.50094773\n",
      "Iteration 34, loss = 0.49842902\n",
      "Iteration 35, loss = 0.50013718\n",
      "Iteration 36, loss = 0.49857919\n",
      "Iteration 37, loss = 0.49786691\n",
      "Iteration 38, loss = 0.49696200\n",
      "Iteration 39, loss = 0.49621458\n",
      "Iteration 40, loss = 0.49686372\n",
      "Iteration 41, loss = 0.50247543\n",
      "Iteration 42, loss = 0.50368001\n",
      "Iteration 43, loss = 0.49504270\n",
      "Iteration 44, loss = 0.50441026\n",
      "Iteration 45, loss = 0.49401282\n",
      "Iteration 46, loss = 0.51402714\n",
      "Iteration 47, loss = 0.49549928\n",
      "Iteration 48, loss = 0.51561108\n",
      "Iteration 49, loss = 0.51974054\n",
      "Iteration 50, loss = 0.49663488\n",
      "Iteration 51, loss = 0.51138942\n",
      "Iteration 52, loss = 0.49729919\n",
      "Iteration 53, loss = 0.49403057\n",
      "Iteration 54, loss = 0.49538906\n",
      "Iteration 55, loss = 0.49213751\n",
      "Iteration 56, loss = 0.49382911\n",
      "Iteration 57, loss = 0.49435056\n",
      "Iteration 58, loss = 0.49888968\n",
      "Iteration 59, loss = 0.49273512\n",
      "Iteration 60, loss = 0.49230182\n",
      "Iteration 61, loss = 0.49220888\n",
      "Iteration 62, loss = 0.49203055\n",
      "Iteration 63, loss = 0.49096747\n",
      "Iteration 64, loss = 0.49709757\n",
      "Iteration 65, loss = 0.49307354\n",
      "Iteration 66, loss = 0.50509201\n",
      "Iteration 67, loss = 0.50395796\n",
      "Iteration 68, loss = 0.49412763\n",
      "Iteration 69, loss = 0.48954317\n",
      "Iteration 70, loss = 0.49805623\n",
      "Iteration 71, loss = 0.49008062\n",
      "Iteration 72, loss = 0.49139415\n",
      "Iteration 73, loss = 0.49118434\n",
      "Iteration 74, loss = 0.49348925\n",
      "Iteration 75, loss = 0.49800487\n",
      "Iteration 76, loss = 0.48877240\n",
      "Iteration 77, loss = 0.50343791\n",
      "Iteration 78, loss = 0.50077739\n",
      "Iteration 79, loss = 0.49181016\n",
      "Iteration 80, loss = 0.51462951\n",
      "Iteration 81, loss = 0.49358987\n",
      "Iteration 82, loss = 0.49411777\n",
      "Iteration 83, loss = 0.48799530\n",
      "Iteration 84, loss = 0.50620751\n",
      "Iteration 85, loss = 0.49284580\n",
      "Iteration 86, loss = 0.48872519\n",
      "Iteration 87, loss = 0.48910186\n",
      "Iteration 88, loss = 0.48711898\n",
      "Iteration 89, loss = 0.49177011\n",
      "Iteration 90, loss = 0.49177622\n",
      "Iteration 91, loss = 0.49891965\n",
      "Iteration 92, loss = 0.48755137\n",
      "Iteration 93, loss = 0.48718107\n",
      "Iteration 94, loss = 0.48937320\n",
      "Iteration 95, loss = 0.49250009\n",
      "Iteration 96, loss = 0.48689056\n",
      "Iteration 97, loss = 0.48702078\n",
      "Iteration 98, loss = 0.48780254\n",
      "Iteration 99, loss = 0.49003401\n",
      "Iteration 100, loss = 0.49373358\n",
      "Iteration 101, loss = 0.48619859\n",
      "Iteration 102, loss = 0.50756735\n",
      "Iteration 103, loss = 0.49784370\n",
      "Iteration 104, loss = 0.49273284\n",
      "Iteration 105, loss = 0.48749081\n",
      "Iteration 106, loss = 0.50078224\n",
      "Iteration 107, loss = 0.48595062\n",
      "Iteration 108, loss = 0.50217261\n",
      "Iteration 109, loss = 0.49956223\n",
      "Iteration 110, loss = 0.49025025\n",
      "Iteration 111, loss = 0.48984785\n",
      "Iteration 112, loss = 0.48864482\n",
      "Iteration 113, loss = 0.48881218\n",
      "Iteration 114, loss = 0.48597635\n",
      "Iteration 115, loss = 0.48628042\n",
      "Iteration 116, loss = 0.49328782\n",
      "Iteration 117, loss = 0.50413626\n",
      "Iteration 118, loss = 0.50612732\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.64064829\n",
      "Iteration 2, loss = 0.84898628\n",
      "Iteration 3, loss = 0.90908582\n",
      "Iteration 4, loss = 0.69075963\n",
      "Iteration 5, loss = 0.78781294\n",
      "Iteration 6, loss = 0.63979156\n",
      "Iteration 7, loss = 0.64591617\n",
      "Iteration 8, loss = 0.69110742\n",
      "Iteration 9, loss = 0.57158668\n",
      "Iteration 10, loss = 0.60535827\n",
      "Iteration 11, loss = 0.55670346\n",
      "Iteration 12, loss = 0.56681914\n",
      "Iteration 13, loss = 0.61774917\n",
      "Iteration 14, loss = 0.55891237\n",
      "Iteration 15, loss = 0.55912403\n",
      "Iteration 16, loss = 0.56054852\n",
      "Iteration 17, loss = 0.54621413\n",
      "Iteration 18, loss = 0.54018342\n",
      "Iteration 19, loss = 0.53733790\n",
      "Iteration 20, loss = 0.53544171\n",
      "Iteration 21, loss = 0.53431802\n",
      "Iteration 22, loss = 0.53157376\n",
      "Iteration 23, loss = 0.53716967\n",
      "Iteration 24, loss = 0.53754591\n",
      "Iteration 25, loss = 0.52843807\n",
      "Iteration 26, loss = 0.54747095\n",
      "Iteration 27, loss = 0.53643190\n",
      "Iteration 28, loss = 0.53362218\n",
      "Iteration 29, loss = 0.53200429\n",
      "Iteration 30, loss = 0.52635598\n",
      "Iteration 31, loss = 0.52635539\n",
      "Iteration 32, loss = 0.52603854\n",
      "Iteration 33, loss = 0.52393032\n",
      "Iteration 34, loss = 0.52527985\n",
      "Iteration 35, loss = 0.52425435\n",
      "Iteration 36, loss = 0.52773586\n",
      "Iteration 37, loss = 0.52703030\n",
      "Iteration 38, loss = 0.52519152\n",
      "Iteration 39, loss = 0.52124711\n",
      "Iteration 40, loss = 0.53038990\n",
      "Iteration 41, loss = 0.51980525\n",
      "Iteration 42, loss = 0.54377246\n",
      "Iteration 43, loss = 0.52824565\n",
      "Iteration 44, loss = 0.54052466\n",
      "Iteration 45, loss = 0.52240737\n",
      "Iteration 46, loss = 0.54068818\n",
      "Iteration 47, loss = 0.54918550\n",
      "Iteration 48, loss = 0.52523919\n",
      "Iteration 49, loss = 0.54247504\n",
      "Iteration 50, loss = 0.51879831\n",
      "Iteration 51, loss = 0.53664659\n",
      "Iteration 52, loss = 0.51906799\n",
      "Iteration 53, loss = 0.54664944\n",
      "Iteration 54, loss = 0.52168545\n",
      "Iteration 55, loss = 0.56932846\n",
      "Iteration 56, loss = 0.52649256\n",
      "Iteration 57, loss = 0.53593196\n",
      "Iteration 58, loss = 0.54064344\n",
      "Iteration 59, loss = 0.52939285\n",
      "Iteration 60, loss = 0.53337706\n",
      "Iteration 61, loss = 0.52241103\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.60261079\n",
      "Iteration 2, loss = 0.62420200\n",
      "Iteration 3, loss = 0.90112077\n",
      "Iteration 4, loss = 0.58301018\n",
      "Iteration 5, loss = 0.67759988\n",
      "Iteration 6, loss = 0.59254657\n",
      "Iteration 7, loss = 0.56925519\n",
      "Iteration 8, loss = 0.61470011\n",
      "Iteration 9, loss = 0.53852494\n",
      "Iteration 10, loss = 0.59466815\n",
      "Iteration 11, loss = 0.58376763\n",
      "Iteration 12, loss = 0.53975865\n",
      "Iteration 13, loss = 0.56098728\n",
      "Iteration 14, loss = 0.54633745\n",
      "Iteration 15, loss = 0.53236072\n",
      "Iteration 16, loss = 0.57914164\n",
      "Iteration 17, loss = 0.56957022\n",
      "Iteration 18, loss = 0.53343555\n",
      "Iteration 19, loss = 0.55192756\n",
      "Iteration 20, loss = 0.53772690\n",
      "Iteration 21, loss = 0.53331950\n",
      "Iteration 22, loss = 0.53066311\n",
      "Iteration 23, loss = 0.52744295\n",
      "Iteration 24, loss = 0.52280439\n",
      "Iteration 25, loss = 0.52831573\n",
      "Iteration 26, loss = 0.53681624\n",
      "Iteration 27, loss = 0.52333550\n",
      "Iteration 28, loss = 0.53054019\n",
      "Iteration 29, loss = 0.52563416\n",
      "Iteration 30, loss = 0.52407248\n",
      "Iteration 31, loss = 0.52066000\n",
      "Iteration 32, loss = 0.52208773\n",
      "Iteration 33, loss = 0.52124652\n",
      "Iteration 34, loss = 0.51987507\n",
      "Iteration 35, loss = 0.51792379\n",
      "Iteration 36, loss = 0.51728746\n",
      "Iteration 37, loss = 0.51765362\n",
      "Iteration 38, loss = 0.51744366\n",
      "Iteration 39, loss = 0.51687355\n",
      "Iteration 40, loss = 0.51785940\n",
      "Iteration 41, loss = 0.51694646\n",
      "Iteration 42, loss = 0.51760237\n",
      "Iteration 43, loss = 0.51488916\n",
      "Iteration 44, loss = 0.51912291\n",
      "Iteration 45, loss = 0.51951183\n",
      "Iteration 46, loss = 0.51680466\n",
      "Iteration 47, loss = 0.51331464\n",
      "Iteration 48, loss = 0.51540908\n",
      "Iteration 49, loss = 0.51539331\n",
      "Iteration 50, loss = 0.51289380\n",
      "Iteration 51, loss = 0.51255351\n",
      "Iteration 52, loss = 0.51274089\n",
      "Iteration 53, loss = 0.51468953\n",
      "Iteration 54, loss = 0.51434670\n",
      "Iteration 55, loss = 0.51687259\n",
      "Iteration 56, loss = 0.51990414\n",
      "Iteration 57, loss = 0.52246904\n",
      "Iteration 58, loss = 0.51384208\n",
      "Iteration 59, loss = 0.51851273\n",
      "Iteration 60, loss = 0.51643823\n",
      "Iteration 61, loss = 0.51267137\n",
      "Iteration 62, loss = 0.51210206\n",
      "Iteration 63, loss = 0.51095034\n",
      "Iteration 64, loss = 0.51064639\n",
      "Iteration 65, loss = 0.51015929\n",
      "Iteration 66, loss = 0.51315022\n",
      "Iteration 67, loss = 0.51273847\n",
      "Iteration 68, loss = 0.51552922\n",
      "Iteration 69, loss = 0.52369348\n",
      "Iteration 70, loss = 0.51169756\n",
      "Iteration 71, loss = 0.53731523\n",
      "Iteration 72, loss = 0.51468893\n",
      "Iteration 73, loss = 0.51220791\n",
      "Iteration 74, loss = 0.51272364\n",
      "Iteration 75, loss = 0.51366786\n",
      "Iteration 76, loss = 0.51182414\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.13206326\n",
      "Iteration 2, loss = 0.89961647\n",
      "Iteration 3, loss = 1.08599433\n",
      "Iteration 4, loss = 1.57152890\n",
      "Iteration 5, loss = 1.65702133\n",
      "Iteration 6, loss = 1.48774570\n",
      "Iteration 7, loss = 1.16079906\n",
      "Iteration 8, loss = 0.72940787\n",
      "Iteration 9, loss = 0.54403652\n",
      "Iteration 10, loss = 0.87375579\n",
      "Iteration 11, loss = 0.87379364\n",
      "Iteration 12, loss = 0.57907683\n",
      "Iteration 13, loss = 0.56113094\n",
      "Iteration 14, loss = 0.67595679\n",
      "Iteration 15, loss = 0.71615390\n",
      "Iteration 16, loss = 0.64945473\n",
      "Iteration 17, loss = 0.54598934\n",
      "Iteration 18, loss = 0.54282271\n",
      "Iteration 19, loss = 0.61196633\n",
      "Iteration 20, loss = 0.58643024\n",
      "Iteration 21, loss = 0.52259405\n",
      "Iteration 22, loss = 0.52837203\n",
      "Iteration 23, loss = 0.54968631\n",
      "Iteration 24, loss = 0.53272319\n",
      "Iteration 25, loss = 0.51045727\n",
      "Iteration 26, loss = 0.52650098\n",
      "Iteration 27, loss = 0.53223997\n",
      "Iteration 28, loss = 0.50870097\n",
      "Iteration 29, loss = 0.51220724\n",
      "Iteration 30, loss = 0.51611124\n",
      "Iteration 31, loss = 0.50368916\n",
      "Iteration 32, loss = 0.50639251\n",
      "Iteration 33, loss = 0.50558220\n",
      "Iteration 34, loss = 0.50008895\n",
      "Iteration 35, loss = 0.49907324\n",
      "Iteration 36, loss = 0.49957510\n",
      "Iteration 37, loss = 0.49676710\n",
      "Iteration 38, loss = 0.49456009\n",
      "Iteration 39, loss = 0.50337305\n",
      "Iteration 40, loss = 0.50743241\n",
      "Iteration 41, loss = 0.50108871\n",
      "Iteration 42, loss = 0.49226658\n",
      "Iteration 43, loss = 0.49247028\n",
      "Iteration 44, loss = 0.49223092\n",
      "Iteration 45, loss = 0.49011636\n",
      "Iteration 46, loss = 0.49027444\n",
      "Iteration 47, loss = 0.48943999\n",
      "Iteration 48, loss = 0.49300145\n",
      "Iteration 49, loss = 0.49375473\n",
      "Iteration 50, loss = 0.49015373\n",
      "Iteration 51, loss = 0.48703710\n",
      "Iteration 52, loss = 0.48827083\n",
      "Iteration 53, loss = 0.48702468\n",
      "Iteration 54, loss = 0.48522314\n",
      "Iteration 55, loss = 0.48971058\n",
      "Iteration 56, loss = 0.48780077\n",
      "Iteration 57, loss = 0.48399774\n",
      "Iteration 58, loss = 0.48567718\n",
      "Iteration 59, loss = 0.48498987\n",
      "Iteration 60, loss = 0.49146290\n",
      "Iteration 61, loss = 0.48483066\n",
      "Iteration 62, loss = 0.48894509\n",
      "Iteration 63, loss = 0.50215999\n",
      "Iteration 64, loss = 0.48264933\n",
      "Iteration 65, loss = 0.49442546\n",
      "Iteration 66, loss = 0.50098152\n",
      "Iteration 67, loss = 0.48074473\n",
      "Iteration 68, loss = 0.49215931\n",
      "Iteration 69, loss = 0.50778377\n",
      "Iteration 70, loss = 0.47887940\n",
      "Iteration 71, loss = 0.49957144\n",
      "Iteration 72, loss = 0.51310272\n",
      "Iteration 73, loss = 0.48553187\n",
      "Iteration 74, loss = 0.48424125\n",
      "Iteration 75, loss = 0.50002931\n",
      "Iteration 76, loss = 0.48911144\n",
      "Iteration 77, loss = 0.47879215\n",
      "Iteration 78, loss = 0.48000288\n",
      "Iteration 79, loss = 0.48040477\n",
      "Iteration 80, loss = 0.47869971\n",
      "Iteration 81, loss = 0.47945181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.94032104\n",
      "Iteration 2, loss = 1.25982799\n",
      "Iteration 3, loss = 0.86436283\n",
      "Iteration 4, loss = 0.75511521\n",
      "Iteration 5, loss = 0.72388959\n",
      "Iteration 6, loss = 0.61810217\n",
      "Iteration 7, loss = 0.80769336\n",
      "Iteration 8, loss = 0.66194770\n",
      "Iteration 9, loss = 0.54670207\n",
      "Iteration 10, loss = 0.72663539\n",
      "Iteration 11, loss = 0.56662853\n",
      "Iteration 12, loss = 0.59693049\n",
      "Iteration 13, loss = 0.69481391\n",
      "Iteration 14, loss = 0.61532837\n",
      "Iteration 15, loss = 0.52019522\n",
      "Iteration 16, loss = 0.59764591\n",
      "Iteration 17, loss = 0.54700178\n",
      "Iteration 18, loss = 0.54010281\n",
      "Iteration 19, loss = 0.55247280\n",
      "Iteration 20, loss = 0.51081757\n",
      "Iteration 21, loss = 0.52617272\n",
      "Iteration 22, loss = 0.52238354\n",
      "Iteration 23, loss = 0.50684069\n",
      "Iteration 24, loss = 0.51348415\n",
      "Iteration 25, loss = 0.50469624\n",
      "Iteration 26, loss = 0.50558797\n",
      "Iteration 27, loss = 0.50649577\n",
      "Iteration 28, loss = 0.50184261\n",
      "Iteration 29, loss = 0.50447002\n",
      "Iteration 30, loss = 0.50291164\n",
      "Iteration 31, loss = 0.49976359\n",
      "Iteration 32, loss = 0.49898409\n",
      "Iteration 33, loss = 0.49938927\n",
      "Iteration 34, loss = 0.50607903\n",
      "Iteration 35, loss = 0.49985829\n",
      "Iteration 36, loss = 0.50711225\n",
      "Iteration 37, loss = 0.49511594\n",
      "Iteration 38, loss = 0.50928600\n",
      "Iteration 39, loss = 0.49727281\n",
      "Iteration 40, loss = 0.50830997\n",
      "Iteration 41, loss = 0.50455962\n",
      "Iteration 42, loss = 0.49691052\n",
      "Iteration 43, loss = 0.50107798\n",
      "Iteration 44, loss = 0.49415196\n",
      "Iteration 45, loss = 0.50035323\n",
      "Iteration 46, loss = 0.49713059\n",
      "Iteration 47, loss = 0.49457773\n",
      "Iteration 48, loss = 0.49707252\n",
      "Iteration 49, loss = 0.49452827\n",
      "Iteration 50, loss = 0.49168383\n",
      "Iteration 51, loss = 0.49323723\n",
      "Iteration 52, loss = 0.49688753\n",
      "Iteration 53, loss = 0.49414536\n",
      "Iteration 54, loss = 0.49220297\n",
      "Iteration 55, loss = 0.49068985\n",
      "Iteration 56, loss = 0.49564886\n",
      "Iteration 57, loss = 0.49061539\n",
      "Iteration 58, loss = 0.49076568\n",
      "Iteration 59, loss = 0.49082644\n",
      "Iteration 60, loss = 0.48905328\n",
      "Iteration 61, loss = 0.49071591\n",
      "Iteration 62, loss = 0.49055013\n",
      "Iteration 63, loss = 0.48928144\n",
      "Iteration 64, loss = 0.48638822\n",
      "Iteration 65, loss = 0.49922259\n",
      "Iteration 66, loss = 0.49448171\n",
      "Iteration 67, loss = 0.49076642\n",
      "Iteration 68, loss = 0.48552855\n",
      "Iteration 69, loss = 0.49115278\n",
      "Iteration 70, loss = 0.49564258\n",
      "Iteration 71, loss = 0.49155010\n",
      "Iteration 72, loss = 0.49810513\n",
      "Iteration 73, loss = 0.49301216\n",
      "Iteration 74, loss = 0.49369601\n",
      "Iteration 75, loss = 0.51711092\n",
      "Iteration 76, loss = 0.50421800\n",
      "Iteration 77, loss = 0.55863963\n",
      "Iteration 78, loss = 0.55938902\n",
      "Iteration 79, loss = 0.52237387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.27317612\n",
      "Iteration 2, loss = 0.93580645\n",
      "Iteration 3, loss = 0.62999145\n",
      "Iteration 4, loss = 0.75913980\n",
      "Iteration 5, loss = 0.65235283\n",
      "Iteration 6, loss = 0.59761260\n",
      "Iteration 7, loss = 0.67338018\n",
      "Iteration 8, loss = 0.58933249\n",
      "Iteration 9, loss = 0.58641581\n",
      "Iteration 10, loss = 0.58040032\n",
      "Iteration 11, loss = 0.55221119\n",
      "Iteration 12, loss = 0.57053267\n",
      "Iteration 13, loss = 0.54791246\n",
      "Iteration 14, loss = 0.54720806\n",
      "Iteration 15, loss = 0.55837900\n",
      "Iteration 16, loss = 0.53079290\n",
      "Iteration 17, loss = 0.55847042\n",
      "Iteration 18, loss = 0.54069220\n",
      "Iteration 19, loss = 0.54614256\n",
      "Iteration 20, loss = 0.54675389\n",
      "Iteration 21, loss = 0.53307041\n",
      "Iteration 22, loss = 0.53318997\n",
      "Iteration 23, loss = 0.52563053\n",
      "Iteration 24, loss = 0.53567127\n",
      "Iteration 25, loss = 0.53737398\n",
      "Iteration 26, loss = 0.52284703\n",
      "Iteration 27, loss = 0.53526854\n",
      "Iteration 28, loss = 0.53212629\n",
      "Iteration 29, loss = 0.52229553\n",
      "Iteration 30, loss = 0.53132216\n",
      "Iteration 31, loss = 0.52540091\n",
      "Iteration 32, loss = 0.52851280\n",
      "Iteration 33, loss = 0.51772410\n",
      "Iteration 34, loss = 0.52439419\n",
      "Iteration 35, loss = 0.52757017\n",
      "Iteration 36, loss = 0.53492629\n",
      "Iteration 37, loss = 0.53028772\n",
      "Iteration 38, loss = 0.52349500\n",
      "Iteration 39, loss = 0.53880459\n",
      "Iteration 40, loss = 0.51812326\n",
      "Iteration 41, loss = 0.53316313\n",
      "Iteration 42, loss = 0.51668102\n",
      "Iteration 43, loss = 0.52263919\n",
      "Iteration 44, loss = 0.51407583\n",
      "Iteration 45, loss = 0.53034171\n",
      "Iteration 46, loss = 0.52117798\n",
      "Iteration 47, loss = 0.51943993\n",
      "Iteration 48, loss = 0.51596690\n",
      "Iteration 49, loss = 0.51660577\n",
      "Iteration 50, loss = 0.51636922\n",
      "Iteration 51, loss = 0.51635484\n",
      "Iteration 52, loss = 0.51327305\n",
      "Iteration 53, loss = 0.52103301\n",
      "Iteration 54, loss = 0.50920500\n",
      "Iteration 55, loss = 0.53717848\n",
      "Iteration 56, loss = 0.50618748\n",
      "Iteration 57, loss = 0.58266062\n",
      "Iteration 58, loss = 0.53839298\n",
      "Iteration 59, loss = 0.52319574\n",
      "Iteration 60, loss = 0.52555766\n",
      "Iteration 61, loss = 0.51599801\n",
      "Iteration 62, loss = 0.53487292\n",
      "Iteration 63, loss = 0.51351581\n",
      "Iteration 64, loss = 0.52542634\n",
      "Iteration 65, loss = 0.50866645\n",
      "Iteration 66, loss = 0.50857218\n",
      "Iteration 67, loss = 0.50999548\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.79381232\n",
      "Iteration 2, loss = 1.19447435\n",
      "Iteration 3, loss = 0.63242930\n",
      "Iteration 4, loss = 0.75955417\n",
      "Iteration 5, loss = 0.60197240\n",
      "Iteration 6, loss = 0.72027535\n",
      "Iteration 7, loss = 0.57217755\n",
      "Iteration 8, loss = 0.65527493\n",
      "Iteration 9, loss = 0.59047868\n",
      "Iteration 10, loss = 0.58243984\n",
      "Iteration 11, loss = 0.62283089\n",
      "Iteration 12, loss = 0.55358539\n",
      "Iteration 13, loss = 0.59336834\n",
      "Iteration 14, loss = 0.56817429\n",
      "Iteration 15, loss = 0.54386220\n",
      "Iteration 16, loss = 0.53575025\n",
      "Iteration 17, loss = 0.53953902\n",
      "Iteration 18, loss = 0.53911927\n",
      "Iteration 19, loss = 0.54334291\n",
      "Iteration 20, loss = 0.54133359\n",
      "Iteration 21, loss = 0.53080987\n",
      "Iteration 22, loss = 0.53581964\n",
      "Iteration 23, loss = 0.53254381\n",
      "Iteration 24, loss = 0.51945654\n",
      "Iteration 25, loss = 0.54942665\n",
      "Iteration 26, loss = 0.53595678\n",
      "Iteration 27, loss = 0.52841386\n",
      "Iteration 28, loss = 0.51977423\n",
      "Iteration 29, loss = 0.52347852\n",
      "Iteration 30, loss = 0.52151345\n",
      "Iteration 31, loss = 0.51646775\n",
      "Iteration 32, loss = 0.51556202\n",
      "Iteration 33, loss = 0.51604308\n",
      "Iteration 34, loss = 0.51960648\n",
      "Iteration 35, loss = 0.51516331\n",
      "Iteration 36, loss = 0.51781703\n",
      "Iteration 37, loss = 0.51321593\n",
      "Iteration 38, loss = 0.51222184\n",
      "Iteration 39, loss = 0.51958332\n",
      "Iteration 40, loss = 0.52177002\n",
      "Iteration 41, loss = 0.51475173\n",
      "Iteration 42, loss = 0.51705119\n",
      "Iteration 43, loss = 0.51240829\n",
      "Iteration 44, loss = 0.50909689\n",
      "Iteration 45, loss = 0.54673557\n",
      "Iteration 46, loss = 0.52397734\n",
      "Iteration 47, loss = 0.54073876\n",
      "Iteration 48, loss = 0.51028736\n",
      "Iteration 49, loss = 0.52774489\n",
      "Iteration 50, loss = 0.51241038\n",
      "Iteration 51, loss = 0.51351197\n",
      "Iteration 52, loss = 0.51929653\n",
      "Iteration 53, loss = 0.50769287\n",
      "Iteration 54, loss = 0.50919612\n",
      "Iteration 55, loss = 0.50891454\n",
      "Iteration 56, loss = 0.51099050\n",
      "Iteration 57, loss = 0.50802916\n",
      "Iteration 58, loss = 0.52422361\n",
      "Iteration 59, loss = 0.50911686\n",
      "Iteration 60, loss = 0.54804747\n",
      "Iteration 61, loss = 0.50970411\n",
      "Iteration 62, loss = 0.54233789\n",
      "Iteration 63, loss = 0.52687292\n",
      "Iteration 64, loss = 0.61215507\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75414037\n",
      "Iteration 2, loss = 0.64579319\n",
      "Iteration 3, loss = 1.04371258\n",
      "Iteration 4, loss = 0.89240105\n",
      "Iteration 5, loss = 0.57849443\n",
      "Iteration 6, loss = 0.80906250\n",
      "Iteration 7, loss = 0.57578940\n",
      "Iteration 8, loss = 0.59028370\n",
      "Iteration 9, loss = 0.61132284\n",
      "Iteration 10, loss = 0.53230893\n",
      "Iteration 11, loss = 0.53458774\n",
      "Iteration 12, loss = 0.55175515\n",
      "Iteration 13, loss = 0.51589909\n",
      "Iteration 14, loss = 0.52362559\n",
      "Iteration 15, loss = 0.51832553\n",
      "Iteration 16, loss = 0.50807681\n",
      "Iteration 17, loss = 0.51267894\n",
      "Iteration 18, loss = 0.51557837\n",
      "Iteration 19, loss = 0.50846412\n",
      "Iteration 20, loss = 0.50686193\n",
      "Iteration 21, loss = 0.50551739\n",
      "Iteration 22, loss = 0.50621490\n",
      "Iteration 23, loss = 0.50897912\n",
      "Iteration 24, loss = 0.50226064\n",
      "Iteration 25, loss = 0.50253014\n",
      "Iteration 26, loss = 0.50010881\n",
      "Iteration 27, loss = 0.50176085\n",
      "Iteration 28, loss = 0.50003056\n",
      "Iteration 29, loss = 0.49976930\n",
      "Iteration 30, loss = 0.50155425\n",
      "Iteration 31, loss = 0.51365220\n",
      "Iteration 32, loss = 0.50129576\n",
      "Iteration 33, loss = 0.50649581\n",
      "Iteration 34, loss = 0.49814237\n",
      "Iteration 35, loss = 0.50414338\n",
      "Iteration 36, loss = 0.50173703\n",
      "Iteration 37, loss = 0.49741025\n",
      "Iteration 38, loss = 0.49792948\n",
      "Iteration 39, loss = 0.49803174\n",
      "Iteration 40, loss = 0.49836422\n",
      "Iteration 41, loss = 0.50275694\n",
      "Iteration 42, loss = 0.49924790\n",
      "Iteration 43, loss = 0.49666798\n",
      "Iteration 44, loss = 0.50014851\n",
      "Iteration 45, loss = 0.49918627\n",
      "Iteration 46, loss = 0.50577671\n",
      "Iteration 47, loss = 0.49389013\n",
      "Iteration 48, loss = 0.51319019\n",
      "Iteration 49, loss = 0.50067731\n",
      "Iteration 50, loss = 0.56802265\n",
      "Iteration 51, loss = 0.50125220\n",
      "Iteration 52, loss = 0.50385089\n",
      "Iteration 53, loss = 0.49930707\n",
      "Iteration 54, loss = 0.49984247\n",
      "Iteration 55, loss = 0.50222451\n",
      "Iteration 56, loss = 0.50002029\n",
      "Iteration 57, loss = 0.49686630\n",
      "Iteration 58, loss = 0.49558174\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.84148161\n",
      "Iteration 2, loss = 0.81657219\n",
      "Iteration 3, loss = 0.85025612\n",
      "Iteration 4, loss = 0.60139539\n",
      "Iteration 5, loss = 0.71485896\n",
      "Iteration 6, loss = 0.57659306\n",
      "Iteration 7, loss = 0.57483102\n",
      "Iteration 8, loss = 0.55240679\n",
      "Iteration 9, loss = 0.54837854\n",
      "Iteration 10, loss = 0.54061411\n",
      "Iteration 11, loss = 0.54130188\n",
      "Iteration 12, loss = 0.55280003\n",
      "Iteration 13, loss = 0.54301564\n",
      "Iteration 14, loss = 0.54820042\n",
      "Iteration 15, loss = 0.54284812\n",
      "Iteration 16, loss = 0.53145422\n",
      "Iteration 17, loss = 0.52954043\n",
      "Iteration 18, loss = 0.52961994\n",
      "Iteration 19, loss = 0.52783729\n",
      "Iteration 20, loss = 0.52786444\n",
      "Iteration 21, loss = 0.52686763\n",
      "Iteration 22, loss = 0.52908058\n",
      "Iteration 23, loss = 0.52692026\n",
      "Iteration 24, loss = 0.52368134\n",
      "Iteration 25, loss = 0.53826667\n",
      "Iteration 26, loss = 0.54083506\n",
      "Iteration 27, loss = 0.52426047\n",
      "Iteration 28, loss = 0.54461560\n",
      "Iteration 29, loss = 0.52334222\n",
      "Iteration 30, loss = 0.54139733\n",
      "Iteration 31, loss = 0.52013470\n",
      "Iteration 32, loss = 0.55972022\n",
      "Iteration 33, loss = 0.52676224\n",
      "Iteration 34, loss = 0.52208407\n",
      "Iteration 35, loss = 0.55054611\n",
      "Iteration 36, loss = 0.53946061\n",
      "Iteration 37, loss = 0.54878815\n",
      "Iteration 38, loss = 0.53492290\n",
      "Iteration 39, loss = 0.53761223\n",
      "Iteration 40, loss = 0.54422669\n",
      "Iteration 41, loss = 0.53577219\n",
      "Iteration 42, loss = 0.54133555\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.29722862\n",
      "Iteration 2, loss = 1.30707172\n",
      "Iteration 3, loss = 1.11693979\n",
      "Iteration 4, loss = 0.65070156\n",
      "Iteration 5, loss = 0.93467844\n",
      "Iteration 6, loss = 0.56926386\n",
      "Iteration 7, loss = 0.75734062\n",
      "Iteration 8, loss = 0.76834974\n",
      "Iteration 9, loss = 0.57789318\n",
      "Iteration 10, loss = 0.62107296\n",
      "Iteration 11, loss = 0.66861247\n",
      "Iteration 12, loss = 0.55404556\n",
      "Iteration 13, loss = 0.58428242\n",
      "Iteration 14, loss = 0.60029832\n",
      "Iteration 15, loss = 0.55537672\n",
      "Iteration 16, loss = 0.55747373\n",
      "Iteration 17, loss = 0.55329160\n",
      "Iteration 18, loss = 0.54149286\n",
      "Iteration 19, loss = 0.55825532\n",
      "Iteration 20, loss = 0.54192201\n",
      "Iteration 21, loss = 0.53437713\n",
      "Iteration 22, loss = 0.53603308\n",
      "Iteration 23, loss = 0.52780567\n",
      "Iteration 24, loss = 0.52618377\n",
      "Iteration 25, loss = 0.52612783\n",
      "Iteration 26, loss = 0.52518786\n",
      "Iteration 27, loss = 0.52923717\n",
      "Iteration 28, loss = 0.52704890\n",
      "Iteration 29, loss = 0.53107033\n",
      "Iteration 30, loss = 0.52420363\n",
      "Iteration 31, loss = 0.52339835\n",
      "Iteration 32, loss = 0.52030428\n",
      "Iteration 33, loss = 0.52558395\n",
      "Iteration 34, loss = 0.52180400\n",
      "Iteration 35, loss = 0.52636255\n",
      "Iteration 36, loss = 0.52715745\n",
      "Iteration 37, loss = 0.52275155\n",
      "Iteration 38, loss = 0.52515641\n",
      "Iteration 39, loss = 0.52005299\n",
      "Iteration 40, loss = 0.53242492\n",
      "Iteration 41, loss = 0.51429886\n",
      "Iteration 42, loss = 0.53357564\n",
      "Iteration 43, loss = 0.52209508\n",
      "Iteration 44, loss = 0.51784753\n",
      "Iteration 45, loss = 0.51791502\n",
      "Iteration 46, loss = 0.51323262\n",
      "Iteration 47, loss = 0.51795584\n",
      "Iteration 48, loss = 0.52459575\n",
      "Iteration 49, loss = 0.51065033\n",
      "Iteration 50, loss = 0.54456911\n",
      "Iteration 51, loss = 0.52138554\n",
      "Iteration 52, loss = 0.51068825\n",
      "Iteration 53, loss = 0.51441183\n",
      "Iteration 54, loss = 0.51419408\n",
      "Iteration 55, loss = 0.50921517\n",
      "Iteration 56, loss = 0.51486280\n",
      "Iteration 57, loss = 0.51646945\n",
      "Iteration 58, loss = 0.50579548\n",
      "Iteration 59, loss = 0.53446825\n",
      "Iteration 60, loss = 0.54139251\n",
      "Iteration 61, loss = 0.50731828\n",
      "Iteration 62, loss = 0.54536407\n",
      "Iteration 63, loss = 0.51079272\n",
      "Iteration 64, loss = 0.54615798\n",
      "Iteration 65, loss = 0.51239904\n",
      "Iteration 66, loss = 0.52673702\n",
      "Iteration 67, loss = 0.51051382\n",
      "Iteration 68, loss = 0.50921835\n",
      "Iteration 69, loss = 0.51034665\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.36380718\n",
      "Iteration 2, loss = 1.08713397\n",
      "Iteration 3, loss = 1.21245593\n",
      "Iteration 4, loss = 0.76875351\n",
      "Iteration 5, loss = 0.65042005\n",
      "Iteration 6, loss = 0.78811046\n",
      "Iteration 7, loss = 0.51642848\n",
      "Iteration 8, loss = 0.66638216\n",
      "Iteration 9, loss = 0.71957899\n",
      "Iteration 10, loss = 0.60462942\n",
      "Iteration 11, loss = 0.51333745\n",
      "Iteration 12, loss = 0.57529154\n",
      "Iteration 13, loss = 0.51790594\n",
      "Iteration 14, loss = 0.52891225\n",
      "Iteration 15, loss = 0.58139706\n",
      "Iteration 16, loss = 0.52869828\n",
      "Iteration 17, loss = 0.51634521\n",
      "Iteration 18, loss = 0.54921630\n",
      "Iteration 19, loss = 0.50314285\n",
      "Iteration 20, loss = 0.51202034\n",
      "Iteration 21, loss = 0.51446685\n",
      "Iteration 22, loss = 0.49410462\n",
      "Iteration 23, loss = 0.50680418\n",
      "Iteration 24, loss = 0.50875562\n",
      "Iteration 25, loss = 0.49628852\n",
      "Iteration 26, loss = 0.49473095\n",
      "Iteration 27, loss = 0.49330370\n",
      "Iteration 28, loss = 0.49215967\n",
      "Iteration 29, loss = 0.48944229\n",
      "Iteration 30, loss = 0.49540678\n",
      "Iteration 31, loss = 0.49664905\n",
      "Iteration 32, loss = 0.49590230\n",
      "Iteration 33, loss = 0.49252470\n",
      "Iteration 34, loss = 0.50422319\n",
      "Iteration 35, loss = 0.48937888\n",
      "Iteration 36, loss = 0.48708438\n",
      "Iteration 37, loss = 0.48877968\n",
      "Iteration 38, loss = 0.48449302\n",
      "Iteration 39, loss = 0.48983523\n",
      "Iteration 40, loss = 0.49076995\n",
      "Iteration 41, loss = 0.48754661\n",
      "Iteration 42, loss = 0.48543667\n",
      "Iteration 43, loss = 0.48407858\n",
      "Iteration 44, loss = 0.48427521\n",
      "Iteration 45, loss = 0.48370336\n",
      "Iteration 46, loss = 0.48514549\n",
      "Iteration 47, loss = 0.48398610\n",
      "Iteration 48, loss = 0.48431329\n",
      "Iteration 49, loss = 0.48334131\n",
      "Iteration 50, loss = 0.48467156\n",
      "Iteration 51, loss = 0.48412522\n",
      "Iteration 52, loss = 0.48285253\n",
      "Iteration 53, loss = 0.48225446\n",
      "Iteration 54, loss = 0.48264740\n",
      "Iteration 55, loss = 0.48437339\n",
      "Iteration 56, loss = 0.48844687\n",
      "Iteration 57, loss = 0.48109494\n",
      "Iteration 58, loss = 0.49193626\n",
      "Iteration 59, loss = 0.48687691\n",
      "Iteration 60, loss = 0.48247416\n",
      "Iteration 61, loss = 0.47952605\n",
      "Iteration 62, loss = 0.48013990\n",
      "Iteration 63, loss = 0.47770663\n",
      "Iteration 64, loss = 0.48553342\n",
      "Iteration 65, loss = 0.48455852\n",
      "Iteration 66, loss = 0.49955408\n",
      "Iteration 67, loss = 0.48578124\n",
      "Iteration 68, loss = 0.48002342\n",
      "Iteration 69, loss = 0.48773445\n",
      "Iteration 70, loss = 0.47449333\n",
      "Iteration 71, loss = 0.50626440\n",
      "Iteration 72, loss = 0.49346316\n",
      "Iteration 73, loss = 0.47677358\n",
      "Iteration 74, loss = 0.47888673\n",
      "Iteration 75, loss = 0.47852177\n",
      "Iteration 76, loss = 0.47534148\n",
      "Iteration 77, loss = 0.48305515\n",
      "Iteration 78, loss = 0.48035598\n",
      "Iteration 79, loss = 0.47423145\n",
      "Iteration 80, loss = 0.49587138\n",
      "Iteration 81, loss = 0.49654819\n",
      "Iteration 82, loss = 0.47540317\n",
      "Iteration 83, loss = 0.50956826\n",
      "Iteration 84, loss = 0.48773260\n",
      "Iteration 85, loss = 0.47639118\n",
      "Iteration 86, loss = 0.47542426\n",
      "Iteration 87, loss = 0.47567601\n",
      "Iteration 88, loss = 0.47285994\n",
      "Iteration 89, loss = 0.48672353\n",
      "Iteration 90, loss = 0.48221806\n",
      "Iteration 91, loss = 0.49873266\n",
      "Iteration 92, loss = 0.47600225\n",
      "Iteration 93, loss = 0.48116647\n",
      "Iteration 94, loss = 0.47739298\n",
      "Iteration 95, loss = 0.47719172\n",
      "Iteration 96, loss = 0.47712822\n",
      "Iteration 97, loss = 0.47565451\n",
      "Iteration 98, loss = 0.49089293\n",
      "Iteration 99, loss = 0.48225995\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.91023620\n",
      "Iteration 2, loss = 0.64587662\n",
      "Iteration 3, loss = 0.60933547\n",
      "Iteration 4, loss = 0.55075348\n",
      "Iteration 5, loss = 0.54985237\n",
      "Iteration 6, loss = 0.53609740\n",
      "Iteration 7, loss = 0.52888120\n",
      "Iteration 8, loss = 0.52724206\n",
      "Iteration 9, loss = 0.52186057\n",
      "Iteration 10, loss = 0.52431295\n",
      "Iteration 11, loss = 0.51250788\n",
      "Iteration 12, loss = 0.51857578\n",
      "Iteration 13, loss = 0.50793370\n",
      "Iteration 14, loss = 0.52220929\n",
      "Iteration 15, loss = 0.51011402\n",
      "Iteration 16, loss = 0.51994979\n",
      "Iteration 17, loss = 0.50578800\n",
      "Iteration 18, loss = 0.52037333\n",
      "Iteration 19, loss = 0.50250572\n",
      "Iteration 20, loss = 0.50733181\n",
      "Iteration 21, loss = 0.50760651\n",
      "Iteration 22, loss = 0.51077850\n",
      "Iteration 23, loss = 0.50849002\n",
      "Iteration 24, loss = 0.55725933\n",
      "Iteration 25, loss = 0.51039871\n",
      "Iteration 26, loss = 0.50602041\n",
      "Iteration 27, loss = 0.51567041\n",
      "Iteration 28, loss = 0.51746306\n",
      "Iteration 29, loss = 0.49288573\n",
      "Iteration 30, loss = 0.49517940\n",
      "Iteration 31, loss = 0.49596317\n",
      "Iteration 32, loss = 0.49316644\n",
      "Iteration 33, loss = 0.50574303\n",
      "Iteration 34, loss = 0.49525080\n",
      "Iteration 35, loss = 0.49481653\n",
      "Iteration 36, loss = 0.50538489\n",
      "Iteration 37, loss = 0.49981900\n",
      "Iteration 38, loss = 0.49554016\n",
      "Iteration 39, loss = 0.48686295\n",
      "Iteration 40, loss = 0.49996887\n",
      "Iteration 41, loss = 0.49390307\n",
      "Iteration 42, loss = 0.48852968\n",
      "Iteration 43, loss = 0.48861411\n",
      "Iteration 44, loss = 0.49246000\n",
      "Iteration 45, loss = 0.48589345\n",
      "Iteration 46, loss = 0.48572102\n",
      "Iteration 47, loss = 0.50118009\n",
      "Iteration 48, loss = 0.49511912\n",
      "Iteration 49, loss = 0.48486966\n",
      "Iteration 50, loss = 0.49160009\n",
      "Iteration 51, loss = 0.48589525\n",
      "Iteration 52, loss = 0.48558705\n",
      "Iteration 53, loss = 0.48655557\n",
      "Iteration 54, loss = 0.48571749\n",
      "Iteration 55, loss = 0.49813732\n",
      "Iteration 56, loss = 0.48681613\n",
      "Iteration 57, loss = 0.49080641\n",
      "Iteration 58, loss = 0.48686415\n",
      "Iteration 59, loss = 0.48730494\n",
      "Iteration 60, loss = 0.49240531\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.12735597\n",
      "Iteration 2, loss = 0.90870257\n",
      "Iteration 3, loss = 0.59925117\n",
      "Iteration 4, loss = 0.72289279\n",
      "Iteration 5, loss = 0.56861707\n",
      "Iteration 6, loss = 0.63208645\n",
      "Iteration 7, loss = 0.58948368\n",
      "Iteration 8, loss = 0.57723578\n",
      "Iteration 9, loss = 0.58266781\n",
      "Iteration 10, loss = 0.54824559\n",
      "Iteration 11, loss = 0.57840662\n",
      "Iteration 12, loss = 0.56321047\n",
      "Iteration 13, loss = 0.54198223\n",
      "Iteration 14, loss = 0.53261383\n",
      "Iteration 15, loss = 0.54684442\n",
      "Iteration 16, loss = 0.53821650\n",
      "Iteration 17, loss = 0.53571412\n",
      "Iteration 18, loss = 0.54486063\n",
      "Iteration 19, loss = 0.52540012\n",
      "Iteration 20, loss = 0.57856643\n",
      "Iteration 21, loss = 0.55595834\n",
      "Iteration 22, loss = 0.54528730\n",
      "Iteration 23, loss = 0.57262921\n",
      "Iteration 24, loss = 0.52619015\n",
      "Iteration 25, loss = 0.57293194\n",
      "Iteration 26, loss = 0.54287354\n",
      "Iteration 27, loss = 0.52890809\n",
      "Iteration 28, loss = 0.52350565\n",
      "Iteration 29, loss = 0.52368507\n",
      "Iteration 30, loss = 0.52071676\n",
      "Iteration 31, loss = 0.52004474\n",
      "Iteration 32, loss = 0.52076251\n",
      "Iteration 33, loss = 0.52276413\n",
      "Iteration 34, loss = 0.55345791\n",
      "Iteration 35, loss = 0.54141625\n",
      "Iteration 36, loss = 0.51914086\n",
      "Iteration 37, loss = 0.51891127\n",
      "Iteration 38, loss = 0.51769498\n",
      "Iteration 39, loss = 0.51701757\n",
      "Iteration 40, loss = 0.51935001\n",
      "Iteration 41, loss = 0.51893017\n",
      "Iteration 42, loss = 0.52134307\n",
      "Iteration 43, loss = 0.51919526\n",
      "Iteration 44, loss = 0.52039829\n",
      "Iteration 45, loss = 0.51721989\n",
      "Iteration 46, loss = 0.52207994\n",
      "Iteration 47, loss = 0.52584772\n",
      "Iteration 48, loss = 0.52208524\n",
      "Iteration 49, loss = 0.52921682\n",
      "Iteration 50, loss = 0.52423885\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.63477830\n",
      "Iteration 2, loss = 1.00901742\n",
      "Iteration 3, loss = 1.04844472\n",
      "Iteration 4, loss = 0.61676956\n",
      "Iteration 5, loss = 0.76740743\n",
      "Iteration 6, loss = 0.64349052\n",
      "Iteration 7, loss = 0.61573022\n",
      "Iteration 8, loss = 0.75072367\n",
      "Iteration 9, loss = 0.66382496\n",
      "Iteration 10, loss = 0.55349728\n",
      "Iteration 11, loss = 0.66088381\n",
      "Iteration 12, loss = 0.62317870\n",
      "Iteration 13, loss = 0.55223425\n",
      "Iteration 14, loss = 0.60259396\n",
      "Iteration 15, loss = 0.57253630\n",
      "Iteration 16, loss = 0.54569688\n",
      "Iteration 17, loss = 0.58692498\n",
      "Iteration 18, loss = 0.53874920\n",
      "Iteration 19, loss = 0.55897806\n",
      "Iteration 20, loss = 0.56927589\n",
      "Iteration 21, loss = 0.53528230\n",
      "Iteration 22, loss = 0.55077099\n",
      "Iteration 23, loss = 0.52947877\n",
      "Iteration 24, loss = 0.56105292\n",
      "Iteration 25, loss = 0.54233683\n",
      "Iteration 26, loss = 0.55591439\n",
      "Iteration 27, loss = 0.54267260\n",
      "Iteration 28, loss = 0.54047039\n",
      "Iteration 29, loss = 0.54678266\n",
      "Iteration 30, loss = 0.53407682\n",
      "Iteration 31, loss = 0.54193727\n",
      "Iteration 32, loss = 0.52878099\n",
      "Iteration 33, loss = 0.55809710\n",
      "Iteration 34, loss = 0.54136321\n",
      "Iteration 35, loss = 0.52454983\n",
      "Iteration 36, loss = 0.53947266\n",
      "Iteration 37, loss = 0.52946312\n",
      "Iteration 38, loss = 0.52997265\n",
      "Iteration 39, loss = 0.52742590\n",
      "Iteration 40, loss = 0.52495815\n",
      "Iteration 41, loss = 0.53297432\n",
      "Iteration 42, loss = 0.52434365\n",
      "Iteration 43, loss = 0.52913834\n",
      "Iteration 44, loss = 0.52749032\n",
      "Iteration 45, loss = 0.52434150\n",
      "Iteration 46, loss = 0.52646280\n",
      "Iteration 47, loss = 0.53299840\n",
      "Iteration 48, loss = 0.52683098\n",
      "Iteration 49, loss = 0.53726202\n",
      "Iteration 50, loss = 0.52012514\n",
      "Iteration 51, loss = 0.54213720\n",
      "Iteration 52, loss = 0.54911191\n",
      "Iteration 53, loss = 0.51933803\n",
      "Iteration 54, loss = 0.55101428\n",
      "Iteration 55, loss = 0.52581334\n",
      "Iteration 56, loss = 0.53377220\n",
      "Iteration 57, loss = 0.54868461\n",
      "Iteration 58, loss = 0.52165409\n",
      "Iteration 59, loss = 0.55844923\n",
      "Iteration 60, loss = 0.52624522\n",
      "Iteration 61, loss = 0.57208450\n",
      "Iteration 62, loss = 0.53374298\n",
      "Iteration 63, loss = 0.55436338\n",
      "Iteration 64, loss = 0.53288370\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.53884145\n",
      "Iteration 2, loss = 1.08923194\n",
      "Iteration 3, loss = 1.00438481\n",
      "Iteration 4, loss = 0.55962002\n",
      "Iteration 5, loss = 0.85471419\n",
      "Iteration 6, loss = 0.54761739\n",
      "Iteration 7, loss = 0.68679165\n",
      "Iteration 8, loss = 0.68286943\n",
      "Iteration 9, loss = 0.55320707\n",
      "Iteration 10, loss = 0.60035463\n",
      "Iteration 11, loss = 0.57598502\n",
      "Iteration 12, loss = 0.53107550\n",
      "Iteration 13, loss = 0.58278964\n",
      "Iteration 14, loss = 0.54750680\n",
      "Iteration 15, loss = 0.50863024\n",
      "Iteration 16, loss = 0.51838828\n",
      "Iteration 17, loss = 0.49535570\n",
      "Iteration 18, loss = 0.52456257\n",
      "Iteration 19, loss = 0.51601623\n",
      "Iteration 20, loss = 0.50308671\n",
      "Iteration 21, loss = 0.49714874\n",
      "Iteration 22, loss = 0.49821324\n",
      "Iteration 23, loss = 0.50195451\n",
      "Iteration 24, loss = 0.49054466\n",
      "Iteration 25, loss = 0.49311830\n",
      "Iteration 26, loss = 0.49060243\n",
      "Iteration 27, loss = 0.50695257\n",
      "Iteration 28, loss = 0.49718247\n",
      "Iteration 29, loss = 0.48812779\n",
      "Iteration 30, loss = 0.50850999\n",
      "Iteration 31, loss = 0.48400425\n",
      "Iteration 32, loss = 0.51166552\n",
      "Iteration 33, loss = 0.47886695\n",
      "Iteration 34, loss = 0.54565985\n",
      "Iteration 35, loss = 0.49460362\n",
      "Iteration 36, loss = 0.51443369\n",
      "Iteration 37, loss = 0.51747430\n",
      "Iteration 38, loss = 0.49568317\n",
      "Iteration 39, loss = 0.50904398\n",
      "Iteration 40, loss = 0.48953199\n",
      "Iteration 41, loss = 0.50441657\n",
      "Iteration 42, loss = 0.47969315\n",
      "Iteration 43, loss = 0.50787410\n",
      "Iteration 44, loss = 0.47805238\n",
      "Iteration 45, loss = 0.52758694\n",
      "Iteration 46, loss = 0.50454369\n",
      "Iteration 47, loss = 0.49095456\n",
      "Iteration 48, loss = 0.48765047\n",
      "Iteration 49, loss = 0.48120499\n",
      "Iteration 50, loss = 0.48179576\n",
      "Iteration 51, loss = 0.47873225\n",
      "Iteration 52, loss = 0.47935163\n",
      "Iteration 53, loss = 0.48109112\n",
      "Iteration 54, loss = 0.48431099\n",
      "Iteration 55, loss = 0.47850005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77811797\n",
      "Iteration 2, loss = 1.13875762\n",
      "Iteration 3, loss = 1.09253219\n",
      "Iteration 4, loss = 0.57487503\n",
      "Iteration 5, loss = 0.79725108\n",
      "Iteration 6, loss = 0.56720209\n",
      "Iteration 7, loss = 0.70513873\n",
      "Iteration 8, loss = 0.54832754\n",
      "Iteration 9, loss = 0.60230504\n",
      "Iteration 10, loss = 0.55596970\n",
      "Iteration 11, loss = 0.55445247\n",
      "Iteration 12, loss = 0.57246928\n",
      "Iteration 13, loss = 0.54160081\n",
      "Iteration 14, loss = 0.53472126\n",
      "Iteration 15, loss = 0.54988127\n",
      "Iteration 16, loss = 0.51666577\n",
      "Iteration 17, loss = 0.56112334\n",
      "Iteration 18, loss = 0.53605372\n",
      "Iteration 19, loss = 0.54215525\n",
      "Iteration 20, loss = 0.52055398\n",
      "Iteration 21, loss = 0.54689569\n",
      "Iteration 22, loss = 0.52966368\n",
      "Iteration 23, loss = 0.51903257\n",
      "Iteration 24, loss = 0.50888594\n",
      "Iteration 25, loss = 0.51476349\n",
      "Iteration 26, loss = 0.49973823\n",
      "Iteration 27, loss = 0.51373328\n",
      "Iteration 28, loss = 0.50360372\n",
      "Iteration 29, loss = 0.51110700\n",
      "Iteration 30, loss = 0.49649415\n",
      "Iteration 31, loss = 0.49846091\n",
      "Iteration 32, loss = 0.49855795\n",
      "Iteration 33, loss = 0.50828936\n",
      "Iteration 34, loss = 0.50235658\n",
      "Iteration 35, loss = 0.50032939\n",
      "Iteration 36, loss = 0.49393252\n",
      "Iteration 37, loss = 0.49296980\n",
      "Iteration 38, loss = 0.49557897\n",
      "Iteration 39, loss = 0.49346541\n",
      "Iteration 40, loss = 0.49169075\n",
      "Iteration 41, loss = 0.49322733\n",
      "Iteration 42, loss = 0.49289188\n",
      "Iteration 43, loss = 0.49545084\n",
      "Iteration 44, loss = 0.49404885\n",
      "Iteration 45, loss = 0.49135581\n",
      "Iteration 46, loss = 0.52156811\n",
      "Iteration 47, loss = 0.50003921\n",
      "Iteration 48, loss = 0.51554851\n",
      "Iteration 49, loss = 0.49444228\n",
      "Iteration 50, loss = 0.51796041\n",
      "Iteration 51, loss = 0.50020472\n",
      "Iteration 52, loss = 0.52165861\n",
      "Iteration 53, loss = 0.52596612\n",
      "Iteration 54, loss = 0.50602661\n",
      "Iteration 55, loss = 0.50460762\n",
      "Iteration 56, loss = 0.52951106\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.05847876\n",
      "Iteration 2, loss = 0.65528857\n",
      "Iteration 3, loss = 1.14267163\n",
      "Iteration 4, loss = 0.88520600\n",
      "Iteration 5, loss = 0.61978229\n",
      "Iteration 6, loss = 0.76391925\n",
      "Iteration 7, loss = 0.57170083\n",
      "Iteration 8, loss = 0.74396415\n",
      "Iteration 9, loss = 0.62570563\n",
      "Iteration 10, loss = 0.59279340\n",
      "Iteration 11, loss = 0.69677580\n",
      "Iteration 12, loss = 0.53237261\n",
      "Iteration 13, loss = 0.59051985\n",
      "Iteration 14, loss = 0.57663832\n",
      "Iteration 15, loss = 0.51663833\n",
      "Iteration 16, loss = 0.52535957\n",
      "Iteration 17, loss = 0.50354526\n",
      "Iteration 18, loss = 0.52607796\n",
      "Iteration 19, loss = 0.51232994\n",
      "Iteration 20, loss = 0.50214950\n",
      "Iteration 21, loss = 0.49570379\n",
      "Iteration 22, loss = 0.49921038\n",
      "Iteration 23, loss = 0.49245274\n",
      "Iteration 24, loss = 0.51777070\n",
      "Iteration 25, loss = 0.54413221\n",
      "Iteration 26, loss = 0.49839780\n",
      "Iteration 27, loss = 0.54332576\n",
      "Iteration 28, loss = 0.50831709\n",
      "Iteration 29, loss = 0.49280692\n",
      "Iteration 30, loss = 0.49188215\n",
      "Iteration 31, loss = 0.48702291\n",
      "Iteration 32, loss = 0.50120115\n",
      "Iteration 33, loss = 0.49209324\n",
      "Iteration 34, loss = 0.50077180\n",
      "Iteration 35, loss = 0.48639140\n",
      "Iteration 36, loss = 0.48221190\n",
      "Iteration 37, loss = 0.50547181\n",
      "Iteration 38, loss = 0.49072831\n",
      "Iteration 39, loss = 0.50264199\n",
      "Iteration 40, loss = 0.48377726\n",
      "Iteration 41, loss = 0.52933392\n",
      "Iteration 42, loss = 0.48602598\n",
      "Iteration 43, loss = 0.50671990\n",
      "Iteration 44, loss = 0.48856965\n",
      "Iteration 45, loss = 0.48296401\n",
      "Iteration 46, loss = 0.48065862\n",
      "Iteration 47, loss = 0.48892727\n",
      "Iteration 48, loss = 0.49786481\n",
      "Iteration 49, loss = 0.47932416\n",
      "Iteration 50, loss = 0.47782004\n",
      "Iteration 51, loss = 0.48205470\n",
      "Iteration 52, loss = 0.48006256\n",
      "Iteration 53, loss = 0.47862968\n",
      "Iteration 54, loss = 0.48864446\n",
      "Iteration 55, loss = 0.48962553\n",
      "Iteration 56, loss = 0.48491422\n",
      "Iteration 57, loss = 0.47696620\n",
      "Iteration 58, loss = 0.48568299\n",
      "Iteration 59, loss = 0.48469632\n",
      "Iteration 60, loss = 0.51845803\n",
      "Iteration 61, loss = 0.47506208\n",
      "Iteration 62, loss = 0.49855365\n",
      "Iteration 63, loss = 0.47311681\n",
      "Iteration 64, loss = 0.49898668\n",
      "Iteration 65, loss = 0.48229585\n",
      "Iteration 66, loss = 0.51234521\n",
      "Iteration 67, loss = 0.48605397\n",
      "Iteration 68, loss = 0.54051710\n",
      "Iteration 69, loss = 0.48491133\n",
      "Iteration 70, loss = 0.49676854\n",
      "Iteration 71, loss = 0.48531677\n",
      "Iteration 72, loss = 0.47862744\n",
      "Iteration 73, loss = 0.48362018\n",
      "Iteration 74, loss = 0.49981618\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89869940\n",
      "Iteration 2, loss = 0.87469596\n",
      "Iteration 3, loss = 0.93612379\n",
      "Iteration 4, loss = 0.57411299\n",
      "Iteration 5, loss = 0.80635552\n",
      "Iteration 6, loss = 0.68855654\n",
      "Iteration 7, loss = 0.58453265\n",
      "Iteration 8, loss = 0.67083936\n",
      "Iteration 9, loss = 0.53627680\n",
      "Iteration 10, loss = 0.66215178\n",
      "Iteration 11, loss = 0.60165932\n",
      "Iteration 12, loss = 0.56973807\n",
      "Iteration 13, loss = 0.55959557\n",
      "Iteration 14, loss = 0.55591084\n",
      "Iteration 15, loss = 0.55134701\n",
      "Iteration 16, loss = 0.54309741\n",
      "Iteration 17, loss = 0.53410328\n",
      "Iteration 18, loss = 0.56884888\n",
      "Iteration 19, loss = 0.57321306\n",
      "Iteration 20, loss = 0.57780755\n",
      "Iteration 21, loss = 0.58289940\n",
      "Iteration 22, loss = 0.56196583\n",
      "Iteration 23, loss = 0.61569054\n",
      "Iteration 24, loss = 0.52421032\n",
      "Iteration 25, loss = 0.56888631\n",
      "Iteration 26, loss = 0.52241938\n",
      "Iteration 27, loss = 0.54559813\n",
      "Iteration 28, loss = 0.53531175\n",
      "Iteration 29, loss = 0.52333367\n",
      "Iteration 30, loss = 0.53547243\n",
      "Iteration 31, loss = 0.51941659\n",
      "Iteration 32, loss = 0.51537489\n",
      "Iteration 33, loss = 0.51826572\n",
      "Iteration 34, loss = 0.51453428\n",
      "Iteration 35, loss = 0.52418996\n",
      "Iteration 36, loss = 0.54612156\n",
      "Iteration 37, loss = 0.51022770\n",
      "Iteration 38, loss = 0.53621917\n",
      "Iteration 39, loss = 0.51568907\n",
      "Iteration 40, loss = 0.52987828\n",
      "Iteration 41, loss = 0.51213427\n",
      "Iteration 42, loss = 0.53053475\n",
      "Iteration 43, loss = 0.52629348\n",
      "Iteration 44, loss = 0.51932351\n",
      "Iteration 45, loss = 0.56050659\n",
      "Iteration 46, loss = 0.56171788\n",
      "Iteration 47, loss = 0.52830443\n",
      "Iteration 48, loss = 0.52186605\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.11259527\n",
      "Iteration 2, loss = 1.29044556\n",
      "Iteration 3, loss = 1.64791992\n",
      "Iteration 4, loss = 1.04901684\n",
      "Iteration 5, loss = 0.59026126\n",
      "Iteration 6, loss = 0.98130773\n",
      "Iteration 7, loss = 0.55757555\n",
      "Iteration 8, loss = 0.73913895\n",
      "Iteration 9, loss = 0.73117587\n",
      "Iteration 10, loss = 0.56113360\n",
      "Iteration 11, loss = 0.61876337\n",
      "Iteration 12, loss = 0.62737409\n",
      "Iteration 13, loss = 0.53115932\n",
      "Iteration 14, loss = 0.53074928\n",
      "Iteration 15, loss = 0.51934501\n",
      "Iteration 16, loss = 0.50787288\n",
      "Iteration 17, loss = 0.53670307\n",
      "Iteration 18, loss = 0.50422313\n",
      "Iteration 19, loss = 0.52809006\n",
      "Iteration 20, loss = 0.52153910\n",
      "Iteration 21, loss = 0.49626902\n",
      "Iteration 22, loss = 0.50012542\n",
      "Iteration 23, loss = 0.49186465\n",
      "Iteration 24, loss = 0.49947950\n",
      "Iteration 25, loss = 0.50114345\n",
      "Iteration 26, loss = 0.49300609\n",
      "Iteration 27, loss = 0.48769113\n",
      "Iteration 28, loss = 0.51424097\n",
      "Iteration 29, loss = 0.49612388\n",
      "Iteration 30, loss = 0.50951597\n",
      "Iteration 31, loss = 0.51080078\n",
      "Iteration 32, loss = 0.48472123\n",
      "Iteration 33, loss = 0.51057341\n",
      "Iteration 34, loss = 0.49120189\n",
      "Iteration 35, loss = 0.48336061\n",
      "Iteration 36, loss = 0.49208977\n",
      "Iteration 37, loss = 0.48856657\n",
      "Iteration 38, loss = 0.48880462\n",
      "Iteration 39, loss = 0.48647185\n",
      "Iteration 40, loss = 0.48829146\n",
      "Iteration 41, loss = 0.48879957\n",
      "Iteration 42, loss = 0.48373560\n",
      "Iteration 43, loss = 0.49305415\n",
      "Iteration 44, loss = 0.49356661\n",
      "Iteration 45, loss = 0.47708984\n",
      "Iteration 46, loss = 0.49095701\n",
      "Iteration 47, loss = 0.47515421\n",
      "Iteration 48, loss = 0.49612758\n",
      "Iteration 49, loss = 0.47372953\n",
      "Iteration 50, loss = 0.51006858\n",
      "Iteration 51, loss = 0.50682156\n",
      "Iteration 52, loss = 0.48332490\n",
      "Iteration 53, loss = 0.48400078\n",
      "Iteration 54, loss = 0.47841952\n",
      "Iteration 55, loss = 0.50668039\n",
      "Iteration 56, loss = 0.47983754\n",
      "Iteration 57, loss = 0.50539575\n",
      "Iteration 58, loss = 0.47548644\n",
      "Iteration 59, loss = 0.47968058\n",
      "Iteration 60, loss = 0.47854333\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.09891281\n",
      "Iteration 2, loss = 0.58545144\n",
      "Iteration 3, loss = 0.66374128\n",
      "Iteration 4, loss = 0.58845143\n",
      "Iteration 5, loss = 0.54503522\n",
      "Iteration 6, loss = 0.62568327\n",
      "Iteration 7, loss = 0.58486346\n",
      "Iteration 8, loss = 0.55290402\n",
      "Iteration 9, loss = 0.53255171\n",
      "Iteration 10, loss = 0.55647854\n",
      "Iteration 11, loss = 0.51771211\n",
      "Iteration 12, loss = 0.51228057\n",
      "Iteration 13, loss = 0.50544159\n",
      "Iteration 14, loss = 0.51904361\n",
      "Iteration 15, loss = 0.50877571\n",
      "Iteration 16, loss = 0.50267082\n",
      "Iteration 17, loss = 0.49794917\n",
      "Iteration 18, loss = 0.50243178\n",
      "Iteration 19, loss = 0.49581913\n",
      "Iteration 20, loss = 0.51006879\n",
      "Iteration 21, loss = 0.50088847\n",
      "Iteration 22, loss = 0.49692352\n",
      "Iteration 23, loss = 0.50081693\n",
      "Iteration 24, loss = 0.49486213\n",
      "Iteration 25, loss = 0.50136362\n",
      "Iteration 26, loss = 0.50227618\n",
      "Iteration 27, loss = 0.49448823\n",
      "Iteration 28, loss = 0.50248157\n",
      "Iteration 29, loss = 0.50256173\n",
      "Iteration 30, loss = 0.49183540\n",
      "Iteration 31, loss = 0.51392354\n",
      "Iteration 32, loss = 0.49929013\n",
      "Iteration 33, loss = 0.49588398\n",
      "Iteration 34, loss = 0.49178428\n",
      "Iteration 35, loss = 0.49195597\n",
      "Iteration 36, loss = 0.49816902\n",
      "Iteration 37, loss = 0.50412452\n",
      "Iteration 38, loss = 0.53263450\n",
      "Iteration 39, loss = 0.50916089\n",
      "Iteration 40, loss = 0.50575237\n",
      "Iteration 41, loss = 0.49013924\n",
      "Iteration 42, loss = 0.53371750\n",
      "Iteration 43, loss = 0.49330505\n",
      "Iteration 44, loss = 0.52413875\n",
      "Iteration 45, loss = 0.52122787\n",
      "Iteration 46, loss = 0.49891845\n",
      "Iteration 47, loss = 0.52936003\n",
      "Iteration 48, loss = 0.49547116\n",
      "Iteration 49, loss = 0.48722450\n",
      "Iteration 50, loss = 0.51976843\n",
      "Iteration 51, loss = 0.48466129\n",
      "Iteration 52, loss = 0.52231707\n",
      "Iteration 53, loss = 0.51470745\n",
      "Iteration 54, loss = 0.51291620\n",
      "Iteration 55, loss = 0.55413802\n",
      "Iteration 56, loss = 0.51044972\n",
      "Iteration 57, loss = 0.56063379\n",
      "Iteration 58, loss = 0.51897763\n",
      "Iteration 59, loss = 0.51831373\n",
      "Iteration 60, loss = 0.51050507\n",
      "Iteration 61, loss = 0.49465227\n",
      "Iteration 62, loss = 0.48798939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.99175053\n",
      "Iteration 2, loss = 1.35562982\n",
      "Iteration 3, loss = 1.18536255\n",
      "Iteration 4, loss = 0.62080026\n",
      "Iteration 5, loss = 0.96557927\n",
      "Iteration 6, loss = 0.54616509\n",
      "Iteration 7, loss = 0.74155490\n",
      "Iteration 8, loss = 0.69168702\n",
      "Iteration 9, loss = 0.54521276\n",
      "Iteration 10, loss = 0.67166582\n",
      "Iteration 11, loss = 0.55437638\n",
      "Iteration 12, loss = 0.57477695\n",
      "Iteration 13, loss = 0.60372611\n",
      "Iteration 14, loss = 0.52145837\n",
      "Iteration 15, loss = 0.54813632\n",
      "Iteration 16, loss = 0.53868439\n",
      "Iteration 17, loss = 0.52844638\n",
      "Iteration 18, loss = 0.55754281\n",
      "Iteration 19, loss = 0.51613487\n",
      "Iteration 20, loss = 0.55031616\n",
      "Iteration 21, loss = 0.50251787\n",
      "Iteration 22, loss = 0.53254858\n",
      "Iteration 23, loss = 0.51983258\n",
      "Iteration 24, loss = 0.52076246\n",
      "Iteration 25, loss = 0.53829030\n",
      "Iteration 26, loss = 0.51260211\n",
      "Iteration 27, loss = 0.54789233\n",
      "Iteration 28, loss = 0.51460436\n",
      "Iteration 29, loss = 0.53499713\n",
      "Iteration 30, loss = 0.50186017\n",
      "Iteration 31, loss = 0.50227589\n",
      "Iteration 32, loss = 0.50123628\n",
      "Iteration 33, loss = 0.49837018\n",
      "Iteration 34, loss = 0.49775457\n",
      "Iteration 35, loss = 0.50103434\n",
      "Iteration 36, loss = 0.49935010\n",
      "Iteration 37, loss = 0.50248427\n",
      "Iteration 38, loss = 0.50362881\n",
      "Iteration 39, loss = 0.49818675\n",
      "Iteration 40, loss = 0.49625824\n",
      "Iteration 41, loss = 0.49582909\n",
      "Iteration 42, loss = 0.49720393\n",
      "Iteration 43, loss = 0.49142770\n",
      "Iteration 44, loss = 0.50439073\n",
      "Iteration 45, loss = 0.49865832\n",
      "Iteration 46, loss = 0.49123235\n",
      "Iteration 47, loss = 0.49173707\n",
      "Iteration 48, loss = 0.49605607\n",
      "Iteration 49, loss = 0.48946425\n",
      "Iteration 50, loss = 0.53425482\n",
      "Iteration 51, loss = 0.48516881\n",
      "Iteration 52, loss = 0.55109384\n",
      "Iteration 53, loss = 0.50518154\n",
      "Iteration 54, loss = 0.52573914\n",
      "Iteration 55, loss = 0.51202664\n",
      "Iteration 56, loss = 0.49639406\n",
      "Iteration 57, loss = 0.49195788\n",
      "Iteration 58, loss = 0.48922604\n",
      "Iteration 59, loss = 0.48874833\n",
      "Iteration 60, loss = 0.49049199\n",
      "Iteration 61, loss = 0.48774790\n",
      "Iteration 62, loss = 0.50033317\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.13906127\n",
      "Iteration 2, loss = 0.67462945\n",
      "Iteration 3, loss = 0.92516324\n",
      "Iteration 4, loss = 0.60303536\n",
      "Iteration 5, loss = 0.72842674\n",
      "Iteration 6, loss = 0.58863845\n",
      "Iteration 7, loss = 0.71048915\n",
      "Iteration 8, loss = 0.56007154\n",
      "Iteration 9, loss = 0.70855172\n",
      "Iteration 10, loss = 0.57852614\n",
      "Iteration 11, loss = 0.63195604\n",
      "Iteration 12, loss = 0.60271297\n",
      "Iteration 13, loss = 0.55822870\n",
      "Iteration 14, loss = 0.55340113\n",
      "Iteration 15, loss = 0.54720831\n",
      "Iteration 16, loss = 0.56133768\n",
      "Iteration 17, loss = 0.54528045\n",
      "Iteration 18, loss = 0.58563401\n",
      "Iteration 19, loss = 0.55152663\n",
      "Iteration 20, loss = 0.55357843\n",
      "Iteration 21, loss = 0.53819533\n",
      "Iteration 22, loss = 0.52385331\n",
      "Iteration 23, loss = 0.54828400\n",
      "Iteration 24, loss = 0.54285798\n",
      "Iteration 25, loss = 0.51936515\n",
      "Iteration 26, loss = 0.56643180\n",
      "Iteration 27, loss = 0.52670751\n",
      "Iteration 28, loss = 0.56935858\n",
      "Iteration 29, loss = 0.52451410\n",
      "Iteration 30, loss = 0.57382549\n",
      "Iteration 31, loss = 0.52515223\n",
      "Iteration 32, loss = 0.53422542\n",
      "Iteration 33, loss = 0.51868996\n",
      "Iteration 34, loss = 0.53213382\n",
      "Iteration 35, loss = 0.52714880\n",
      "Iteration 36, loss = 0.52961135\n",
      "Iteration 37, loss = 0.53317407\n",
      "Iteration 38, loss = 0.53900184\n",
      "Iteration 39, loss = 0.52784530\n",
      "Iteration 40, loss = 0.52311777\n",
      "Iteration 41, loss = 0.54451453\n",
      "Iteration 42, loss = 0.52333014\n",
      "Iteration 43, loss = 0.53822468\n",
      "Iteration 44, loss = 0.51904343\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.99925647\n",
      "Iteration 2, loss = 1.50111935\n",
      "Iteration 3, loss = 0.72545999\n",
      "Iteration 4, loss = 1.00626343\n",
      "Iteration 5, loss = 0.55077065\n",
      "Iteration 6, loss = 0.79518181\n",
      "Iteration 7, loss = 0.68391231\n",
      "Iteration 8, loss = 0.58658681\n",
      "Iteration 9, loss = 0.74698620\n",
      "Iteration 10, loss = 0.53575248\n",
      "Iteration 11, loss = 0.66634753\n",
      "Iteration 12, loss = 0.62971030\n",
      "Iteration 13, loss = 0.54828064\n",
      "Iteration 14, loss = 0.56863185\n",
      "Iteration 15, loss = 0.53545004\n",
      "Iteration 16, loss = 0.57710882\n",
      "Iteration 17, loss = 0.52754389\n",
      "Iteration 18, loss = 0.57320229\n",
      "Iteration 19, loss = 0.53846739\n",
      "Iteration 20, loss = 0.52115540\n",
      "Iteration 21, loss = 0.52092484\n",
      "Iteration 22, loss = 0.52110175\n",
      "Iteration 23, loss = 0.51943437\n",
      "Iteration 24, loss = 0.52178799\n",
      "Iteration 25, loss = 0.52847280\n",
      "Iteration 26, loss = 0.51442522\n",
      "Iteration 27, loss = 0.54847717\n",
      "Iteration 28, loss = 0.53702698\n",
      "Iteration 29, loss = 0.52290468\n",
      "Iteration 30, loss = 0.51839857\n",
      "Iteration 31, loss = 0.51967423\n",
      "Iteration 32, loss = 0.51198354\n",
      "Iteration 33, loss = 0.51839646\n",
      "Iteration 34, loss = 0.51065247\n",
      "Iteration 35, loss = 0.52739215\n",
      "Iteration 36, loss = 0.51100233\n",
      "Iteration 37, loss = 0.53843298\n",
      "Iteration 38, loss = 0.53074385\n",
      "Iteration 39, loss = 0.54064934\n",
      "Iteration 40, loss = 0.55107629\n",
      "Iteration 41, loss = 0.53076934\n",
      "Iteration 42, loss = 0.54489293\n",
      "Iteration 43, loss = 0.51098717\n",
      "Iteration 44, loss = 0.55166061\n",
      "Iteration 45, loss = 0.52127777\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.25350385\n",
      "Iteration 2, loss = 1.42307986\n",
      "Iteration 3, loss = 0.89988959\n",
      "Iteration 4, loss = 0.75091753\n",
      "Iteration 5, loss = 0.64437548\n",
      "Iteration 6, loss = 0.64429598\n",
      "Iteration 7, loss = 0.69167125\n",
      "Iteration 8, loss = 0.53792474\n",
      "Iteration 9, loss = 0.66159014\n",
      "Iteration 10, loss = 0.54628287\n",
      "Iteration 11, loss = 0.58553248\n",
      "Iteration 12, loss = 0.60028809\n",
      "Iteration 13, loss = 0.51707540\n",
      "Iteration 14, loss = 0.58224903\n",
      "Iteration 15, loss = 0.52750851\n",
      "Iteration 16, loss = 0.52842621\n",
      "Iteration 17, loss = 0.53959607\n",
      "Iteration 18, loss = 0.49797321\n",
      "Iteration 19, loss = 0.54085796\n",
      "Iteration 20, loss = 0.50870786\n",
      "Iteration 21, loss = 0.51533768\n",
      "Iteration 22, loss = 0.50020390\n",
      "Iteration 23, loss = 0.55282285\n",
      "Iteration 24, loss = 0.51579695\n",
      "Iteration 25, loss = 0.52516088\n",
      "Iteration 26, loss = 0.49807749\n",
      "Iteration 27, loss = 0.51345544\n",
      "Iteration 28, loss = 0.51263618\n",
      "Iteration 29, loss = 0.51373823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for sizes in layer_sizes:\n",
    "    accs = []\n",
    "    for i in range(10):\n",
    "        seed = np.random.randint(999999999)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(sizes,), max_iter=1000, verbose=1)\n",
    "        mlp.fit(X_train, y_train)\n",
    "        accs.append(mlp.score(X_test, y_test))\n",
    "    \n",
    "    scores.append(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(save_folder + 'scores.txt', scores)\n",
    "\n",
    "medians = []\n",
    "for i in range(len(scores)):\n",
    "    medians.append(np.median(scores[i]))\n",
    "\n",
    "_medians = list(np.copy(medians))\n",
    "max_list = sorted(_medians)\n",
    "max1 = _medians.index(max_list[-1])\n",
    "del(_medians[max1])\n",
    "max2 = _medians.index(max_list[-2]) + 1\n",
    "\n",
    "np.savetxt(save_folder + 'medians.txt', medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(layer_sizes, medians)\n",
    "plt.plot(layer_sizes[max1], medians[max1], 'o', label='max1', markersize=8)\n",
    "plt.plot(layer_sizes[max2], medians[max2], 'o', label='max2', markersize=8)\n",
    "plt.xlim(0, 2000)\n",
    "plt.ylim(min(medians) - 0.1, max(medians) + 0.1)\n",
    "plt.annotate('K = %i'%(layer_sizes[max1]), (layer_sizes[max1] - 4,  medians[max1] + 0.015))\n",
    "plt.annotate('K = %i'%(layer_sizes[max2]), (layer_sizes[max2] - 4,  medians[max2] + 0.015))\n",
    "plt.title('Mediana das Acuracias')\n",
    "plt.ylabel('Acuracia')\n",
    "plt.xlabel('Layer Sizes')\n",
    "plt.legend(['Teste'])\n",
    "plt.savefig(save_folder + 'P1_outs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scores).T\n",
    "\n",
    "df.to_excel(save_folder + 'scores_tudo.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scores[max1])\n",
    "\n",
    "df.to_excel(save_folder + 'best_MLP.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor Score: 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Melhor Score: \" + str(layer_sizes[max1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP com PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0             1\n",
      "count  3.060000e+02  3.060000e+02\n",
      "mean   1.857628e-16 -2.539726e-17\n",
      "std    1.055942e+00  9.998604e-01\n",
      "min   -2.503637e+00 -1.336894e+00\n",
      "25%   -6.631953e-01 -7.743523e-01\n",
      "50%    9.907102e-03 -1.348652e-01\n",
      "75%    6.548567e-01  4.159343e-01\n",
      "max    4.268923e+00  5.517063e+00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "save_folder = './com_pca/'\n",
    "\n",
    "n_comps = 2\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components = n_comps)\n",
    "\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "df_X = pd.DataFrame(X)\n",
    "print(df_X.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "layer_sizes = [5, 10, 50, 100, 200, 400, 500, 1000, 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.64592482\n",
      "Iteration 2, loss = 0.64430003\n",
      "Iteration 3, loss = 0.64263187\n",
      "Iteration 4, loss = 0.64112643\n",
      "Iteration 5, loss = 0.63948669\n",
      "Iteration 6, loss = 0.63803838\n",
      "Iteration 7, loss = 0.63650514\n",
      "Iteration 8, loss = 0.63508042\n",
      "Iteration 9, loss = 0.63364371\n",
      "Iteration 10, loss = 0.63216865\n",
      "Iteration 11, loss = 0.63083034\n",
      "Iteration 12, loss = 0.62946271\n",
      "Iteration 13, loss = 0.62808854\n",
      "Iteration 14, loss = 0.62667682\n",
      "Iteration 15, loss = 0.62529872\n",
      "Iteration 16, loss = 0.62400190\n",
      "Iteration 17, loss = 0.62258381\n",
      "Iteration 18, loss = 0.62124274\n",
      "Iteration 19, loss = 0.61985360\n",
      "Iteration 20, loss = 0.61852966\n",
      "Iteration 21, loss = 0.61720940\n",
      "Iteration 22, loss = 0.61592202\n",
      "Iteration 23, loss = 0.61457980\n",
      "Iteration 24, loss = 0.61335510\n",
      "Iteration 25, loss = 0.61207957\n",
      "Iteration 26, loss = 0.61083238\n",
      "Iteration 27, loss = 0.60969179\n",
      "Iteration 28, loss = 0.60845377\n",
      "Iteration 29, loss = 0.60727270\n",
      "Iteration 30, loss = 0.60616299\n",
      "Iteration 31, loss = 0.60507951\n",
      "Iteration 32, loss = 0.60397165\n",
      "Iteration 33, loss = 0.60290270\n",
      "Iteration 34, loss = 0.60188684\n",
      "Iteration 35, loss = 0.60090304\n",
      "Iteration 36, loss = 0.59988806\n",
      "Iteration 37, loss = 0.59892894\n",
      "Iteration 38, loss = 0.59800980\n",
      "Iteration 39, loss = 0.59700152\n",
      "Iteration 40, loss = 0.59607284\n",
      "Iteration 41, loss = 0.59512068\n",
      "Iteration 42, loss = 0.59425444\n",
      "Iteration 43, loss = 0.59322674\n",
      "Iteration 44, loss = 0.59237089\n",
      "Iteration 45, loss = 0.59141919\n",
      "Iteration 46, loss = 0.59053561\n",
      "Iteration 47, loss = 0.58963981\n",
      "Iteration 48, loss = 0.58875003\n",
      "Iteration 49, loss = 0.58780922\n",
      "Iteration 50, loss = 0.58708691\n",
      "Iteration 51, loss = 0.58615630\n",
      "Iteration 52, loss = 0.58535414\n",
      "Iteration 53, loss = 0.58453434\n",
      "Iteration 54, loss = 0.58376987\n",
      "Iteration 55, loss = 0.58297010\n",
      "Iteration 56, loss = 0.58215494\n",
      "Iteration 57, loss = 0.58139146\n",
      "Iteration 58, loss = 0.58068654\n",
      "Iteration 59, loss = 0.57986973\n",
      "Iteration 60, loss = 0.57920901\n",
      "Iteration 61, loss = 0.57841019\n",
      "Iteration 62, loss = 0.57770125\n",
      "Iteration 63, loss = 0.57705045\n",
      "Iteration 64, loss = 0.57630021\n",
      "Iteration 65, loss = 0.57565195\n",
      "Iteration 66, loss = 0.57496744\n",
      "Iteration 67, loss = 0.57432307\n",
      "Iteration 68, loss = 0.57362245\n",
      "Iteration 69, loss = 0.57296031\n",
      "Iteration 70, loss = 0.57227693\n",
      "Iteration 71, loss = 0.57158316\n",
      "Iteration 72, loss = 0.57091409\n",
      "Iteration 73, loss = 0.57028094\n",
      "Iteration 74, loss = 0.56960977\n",
      "Iteration 75, loss = 0.56900532\n",
      "Iteration 76, loss = 0.56837418\n",
      "Iteration 77, loss = 0.56774371\n",
      "Iteration 78, loss = 0.56721704\n",
      "Iteration 79, loss = 0.56655137\n",
      "Iteration 80, loss = 0.56605622\n",
      "Iteration 81, loss = 0.56541781\n",
      "Iteration 82, loss = 0.56486097\n",
      "Iteration 83, loss = 0.56433667\n",
      "Iteration 84, loss = 0.56378468\n",
      "Iteration 85, loss = 0.56327009\n",
      "Iteration 86, loss = 0.56267407\n",
      "Iteration 87, loss = 0.56216771\n",
      "Iteration 88, loss = 0.56160046\n",
      "Iteration 89, loss = 0.56102479\n",
      "Iteration 90, loss = 0.56049431\n",
      "Iteration 91, loss = 0.55991329\n",
      "Iteration 92, loss = 0.55932334\n",
      "Iteration 93, loss = 0.55880752\n",
      "Iteration 94, loss = 0.55826824\n",
      "Iteration 95, loss = 0.55776604\n",
      "Iteration 96, loss = 0.55724751\n",
      "Iteration 97, loss = 0.55680274\n",
      "Iteration 98, loss = 0.55634786\n",
      "Iteration 99, loss = 0.55594328\n",
      "Iteration 100, loss = 0.55547708\n",
      "Iteration 101, loss = 0.55505748\n",
      "Iteration 102, loss = 0.55462399\n",
      "Iteration 103, loss = 0.55426512\n",
      "Iteration 104, loss = 0.55381444\n",
      "Iteration 105, loss = 0.55336211\n",
      "Iteration 106, loss = 0.55303386\n",
      "Iteration 107, loss = 0.55257053\n",
      "Iteration 108, loss = 0.55220619\n",
      "Iteration 109, loss = 0.55180394\n",
      "Iteration 110, loss = 0.55143937\n",
      "Iteration 111, loss = 0.55108392\n",
      "Iteration 112, loss = 0.55067815\n",
      "Iteration 113, loss = 0.55031785\n",
      "Iteration 114, loss = 0.54996781\n",
      "Iteration 115, loss = 0.54960778\n",
      "Iteration 116, loss = 0.54926806\n",
      "Iteration 117, loss = 0.54885957\n",
      "Iteration 118, loss = 0.54853507\n",
      "Iteration 119, loss = 0.54817784\n",
      "Iteration 120, loss = 0.54782825\n",
      "Iteration 121, loss = 0.54750495\n",
      "Iteration 122, loss = 0.54713818\n",
      "Iteration 123, loss = 0.54685516\n",
      "Iteration 124, loss = 0.54649453\n",
      "Iteration 125, loss = 0.54618139\n",
      "Iteration 126, loss = 0.54585662\n",
      "Iteration 127, loss = 0.54552049\n",
      "Iteration 128, loss = 0.54521724\n",
      "Iteration 129, loss = 0.54489792\n",
      "Iteration 130, loss = 0.54455500\n",
      "Iteration 131, loss = 0.54423888\n",
      "Iteration 132, loss = 0.54392770\n",
      "Iteration 133, loss = 0.54365456\n",
      "Iteration 134, loss = 0.54333767\n",
      "Iteration 135, loss = 0.54302906\n",
      "Iteration 136, loss = 0.54274778\n",
      "Iteration 137, loss = 0.54250802\n",
      "Iteration 138, loss = 0.54216330\n",
      "Iteration 139, loss = 0.54191844\n",
      "Iteration 140, loss = 0.54167372\n",
      "Iteration 141, loss = 0.54142170\n",
      "Iteration 142, loss = 0.54116591\n",
      "Iteration 143, loss = 0.54096496\n",
      "Iteration 144, loss = 0.54078179\n",
      "Iteration 145, loss = 0.54054455\n",
      "Iteration 146, loss = 0.54037304\n",
      "Iteration 147, loss = 0.54015001\n",
      "Iteration 148, loss = 0.53997032\n",
      "Iteration 149, loss = 0.53973624\n",
      "Iteration 150, loss = 0.53952212\n",
      "Iteration 151, loss = 0.53931566\n",
      "Iteration 152, loss = 0.53911010\n",
      "Iteration 153, loss = 0.53889328\n",
      "Iteration 154, loss = 0.53866552\n",
      "Iteration 155, loss = 0.53848506\n",
      "Iteration 156, loss = 0.53830253\n",
      "Iteration 157, loss = 0.53810214\n",
      "Iteration 158, loss = 0.53794891\n",
      "Iteration 159, loss = 0.53774965\n",
      "Iteration 160, loss = 0.53762029\n",
      "Iteration 161, loss = 0.53742002\n",
      "Iteration 162, loss = 0.53729232\n",
      "Iteration 163, loss = 0.53712921\n",
      "Iteration 164, loss = 0.53699386\n",
      "Iteration 165, loss = 0.53683769\n",
      "Iteration 166, loss = 0.53671157\n",
      "Iteration 167, loss = 0.53660393\n",
      "Iteration 168, loss = 0.53647534\n",
      "Iteration 169, loss = 0.53633741\n",
      "Iteration 170, loss = 0.53624233\n",
      "Iteration 171, loss = 0.53611538\n",
      "Iteration 172, loss = 0.53600430\n",
      "Iteration 173, loss = 0.53585719\n",
      "Iteration 174, loss = 0.53576120\n",
      "Iteration 175, loss = 0.53562806\n",
      "Iteration 176, loss = 0.53549931\n",
      "Iteration 177, loss = 0.53538190\n",
      "Iteration 178, loss = 0.53524012\n",
      "Iteration 179, loss = 0.53514730\n",
      "Iteration 180, loss = 0.53499827\n",
      "Iteration 181, loss = 0.53489526\n",
      "Iteration 182, loss = 0.53477616\n",
      "Iteration 183, loss = 0.53467425\n",
      "Iteration 184, loss = 0.53454634\n",
      "Iteration 185, loss = 0.53444495\n",
      "Iteration 186, loss = 0.53433464\n",
      "Iteration 187, loss = 0.53421644\n",
      "Iteration 188, loss = 0.53414737\n",
      "Iteration 189, loss = 0.53399803\n",
      "Iteration 190, loss = 0.53391993\n",
      "Iteration 191, loss = 0.53383676\n",
      "Iteration 192, loss = 0.53372861\n",
      "Iteration 193, loss = 0.53365293\n",
      "Iteration 194, loss = 0.53359276\n",
      "Iteration 195, loss = 0.53349679\n",
      "Iteration 196, loss = 0.53342463\n",
      "Iteration 197, loss = 0.53333997\n",
      "Iteration 198, loss = 0.53325474\n",
      "Iteration 199, loss = 0.53318319\n",
      "Iteration 200, loss = 0.53311524\n",
      "Iteration 201, loss = 0.53299810\n",
      "Iteration 202, loss = 0.53291482\n",
      "Iteration 203, loss = 0.53283677\n",
      "Iteration 204, loss = 0.53274445\n",
      "Iteration 205, loss = 0.53266064\n",
      "Iteration 206, loss = 0.53257993\n",
      "Iteration 207, loss = 0.53249387\n",
      "Iteration 208, loss = 0.53244852\n",
      "Iteration 209, loss = 0.53236507\n",
      "Iteration 210, loss = 0.53228697\n",
      "Iteration 211, loss = 0.53223653\n",
      "Iteration 212, loss = 0.53217119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76148564\n",
      "Iteration 2, loss = 0.75968936\n",
      "Iteration 3, loss = 0.75791956\n",
      "Iteration 4, loss = 0.75630923\n",
      "Iteration 5, loss = 0.75464450\n",
      "Iteration 6, loss = 0.75304770\n",
      "Iteration 7, loss = 0.75141781\n",
      "Iteration 8, loss = 0.74984972\n",
      "Iteration 9, loss = 0.74832073\n",
      "Iteration 10, loss = 0.74663838\n",
      "Iteration 11, loss = 0.74519066\n",
      "Iteration 12, loss = 0.74360742\n",
      "Iteration 13, loss = 0.74203480\n",
      "Iteration 14, loss = 0.74057740\n",
      "Iteration 15, loss = 0.73887527\n",
      "Iteration 16, loss = 0.73736192\n",
      "Iteration 17, loss = 0.73592065\n",
      "Iteration 18, loss = 0.73434996\n",
      "Iteration 19, loss = 0.73296557\n",
      "Iteration 20, loss = 0.73147845\n",
      "Iteration 21, loss = 0.73008314\n",
      "Iteration 22, loss = 0.72866518\n",
      "Iteration 23, loss = 0.72730463\n",
      "Iteration 24, loss = 0.72583839\n",
      "Iteration 25, loss = 0.72462850\n",
      "Iteration 26, loss = 0.72313505\n",
      "Iteration 27, loss = 0.72182183\n",
      "Iteration 28, loss = 0.72027612\n",
      "Iteration 29, loss = 0.71904089\n",
      "Iteration 30, loss = 0.71761707\n",
      "Iteration 31, loss = 0.71624579\n",
      "Iteration 32, loss = 0.71492055\n",
      "Iteration 33, loss = 0.71334388\n",
      "Iteration 34, loss = 0.71221400\n",
      "Iteration 35, loss = 0.71086035\n",
      "Iteration 36, loss = 0.70952525\n",
      "Iteration 37, loss = 0.70830460\n",
      "Iteration 38, loss = 0.70695752\n",
      "Iteration 39, loss = 0.70580610\n",
      "Iteration 40, loss = 0.70452603\n",
      "Iteration 41, loss = 0.70334932\n",
      "Iteration 42, loss = 0.70198942\n",
      "Iteration 43, loss = 0.70075170\n",
      "Iteration 44, loss = 0.69955802\n",
      "Iteration 45, loss = 0.69816499\n",
      "Iteration 46, loss = 0.69703256\n",
      "Iteration 47, loss = 0.69567396\n",
      "Iteration 48, loss = 0.69455990\n",
      "Iteration 49, loss = 0.69316019\n",
      "Iteration 50, loss = 0.69197305\n",
      "Iteration 51, loss = 0.69072485\n",
      "Iteration 52, loss = 0.68931849\n",
      "Iteration 53, loss = 0.68815428\n",
      "Iteration 54, loss = 0.68679114\n",
      "Iteration 55, loss = 0.68557395\n",
      "Iteration 56, loss = 0.68421758\n",
      "Iteration 57, loss = 0.68300261\n",
      "Iteration 58, loss = 0.68178760\n",
      "Iteration 59, loss = 0.68042978\n",
      "Iteration 60, loss = 0.67939078\n",
      "Iteration 61, loss = 0.67810634\n",
      "Iteration 62, loss = 0.67698391\n",
      "Iteration 63, loss = 0.67578408\n",
      "Iteration 64, loss = 0.67461832\n",
      "Iteration 65, loss = 0.67358733\n",
      "Iteration 66, loss = 0.67239450\n",
      "Iteration 67, loss = 0.67126019\n",
      "Iteration 68, loss = 0.67019172\n",
      "Iteration 69, loss = 0.66906082\n",
      "Iteration 70, loss = 0.66798848\n",
      "Iteration 71, loss = 0.66694930\n",
      "Iteration 72, loss = 0.66578357\n",
      "Iteration 73, loss = 0.66474077\n",
      "Iteration 74, loss = 0.66361808\n",
      "Iteration 75, loss = 0.66249548\n",
      "Iteration 76, loss = 0.66135227\n",
      "Iteration 77, loss = 0.66015926\n",
      "Iteration 78, loss = 0.65899687\n",
      "Iteration 79, loss = 0.65784754\n",
      "Iteration 80, loss = 0.65676343\n",
      "Iteration 81, loss = 0.65557009\n",
      "Iteration 82, loss = 0.65450407\n",
      "Iteration 83, loss = 0.65346034\n",
      "Iteration 84, loss = 0.65238516\n",
      "Iteration 85, loss = 0.65128931\n",
      "Iteration 86, loss = 0.65030903\n",
      "Iteration 87, loss = 0.64917817\n",
      "Iteration 88, loss = 0.64817767\n",
      "Iteration 89, loss = 0.64703902\n",
      "Iteration 90, loss = 0.64616123\n",
      "Iteration 91, loss = 0.64500017\n",
      "Iteration 92, loss = 0.64403887\n",
      "Iteration 93, loss = 0.64306674\n",
      "Iteration 94, loss = 0.64206897\n",
      "Iteration 95, loss = 0.64110705\n",
      "Iteration 96, loss = 0.64019280\n",
      "Iteration 97, loss = 0.63922100\n",
      "Iteration 98, loss = 0.63827979\n",
      "Iteration 99, loss = 0.63735127\n",
      "Iteration 100, loss = 0.63645465\n",
      "Iteration 101, loss = 0.63554362\n",
      "Iteration 102, loss = 0.63466978\n",
      "Iteration 103, loss = 0.63380757\n",
      "Iteration 104, loss = 0.63297130\n",
      "Iteration 105, loss = 0.63217995\n",
      "Iteration 106, loss = 0.63129915\n",
      "Iteration 107, loss = 0.63049855\n",
      "Iteration 108, loss = 0.62971758\n",
      "Iteration 109, loss = 0.62891642\n",
      "Iteration 110, loss = 0.62814815\n",
      "Iteration 111, loss = 0.62735119\n",
      "Iteration 112, loss = 0.62656047\n",
      "Iteration 113, loss = 0.62577755\n",
      "Iteration 114, loss = 0.62497013\n",
      "Iteration 115, loss = 0.62418435\n",
      "Iteration 116, loss = 0.62327190\n",
      "Iteration 117, loss = 0.62248913\n",
      "Iteration 118, loss = 0.62157332\n",
      "Iteration 119, loss = 0.62069336\n",
      "Iteration 120, loss = 0.61989039\n",
      "Iteration 121, loss = 0.61903937\n",
      "Iteration 122, loss = 0.61816032\n",
      "Iteration 123, loss = 0.61740205\n",
      "Iteration 124, loss = 0.61653127\n",
      "Iteration 125, loss = 0.61587572\n",
      "Iteration 126, loss = 0.61504712\n",
      "Iteration 127, loss = 0.61431053\n",
      "Iteration 128, loss = 0.61348896\n",
      "Iteration 129, loss = 0.61283134\n",
      "Iteration 130, loss = 0.61206261\n",
      "Iteration 131, loss = 0.61131295\n",
      "Iteration 132, loss = 0.61060435\n",
      "Iteration 133, loss = 0.60991320\n",
      "Iteration 134, loss = 0.60919995\n",
      "Iteration 135, loss = 0.60855868\n",
      "Iteration 136, loss = 0.60788511\n",
      "Iteration 137, loss = 0.60726984\n",
      "Iteration 138, loss = 0.60659351\n",
      "Iteration 139, loss = 0.60596632\n",
      "Iteration 140, loss = 0.60540294\n",
      "Iteration 141, loss = 0.60473904\n",
      "Iteration 142, loss = 0.60412343\n",
      "Iteration 143, loss = 0.60355634\n",
      "Iteration 144, loss = 0.60294098\n",
      "Iteration 145, loss = 0.60229992\n",
      "Iteration 146, loss = 0.60179733\n",
      "Iteration 147, loss = 0.60123508\n",
      "Iteration 148, loss = 0.60059952\n",
      "Iteration 149, loss = 0.60012332\n",
      "Iteration 150, loss = 0.59950755\n",
      "Iteration 151, loss = 0.59897197\n",
      "Iteration 152, loss = 0.59845518\n",
      "Iteration 153, loss = 0.59788051\n",
      "Iteration 154, loss = 0.59736404\n",
      "Iteration 155, loss = 0.59681716\n",
      "Iteration 156, loss = 0.59630631\n",
      "Iteration 157, loss = 0.59577829\n",
      "Iteration 158, loss = 0.59522028\n",
      "Iteration 159, loss = 0.59461794\n",
      "Iteration 160, loss = 0.59412604\n",
      "Iteration 161, loss = 0.59359267\n",
      "Iteration 162, loss = 0.59302392\n",
      "Iteration 163, loss = 0.59255028\n",
      "Iteration 164, loss = 0.59202892\n",
      "Iteration 165, loss = 0.59152665\n",
      "Iteration 166, loss = 0.59109178\n",
      "Iteration 167, loss = 0.59057999\n",
      "Iteration 168, loss = 0.59013593\n",
      "Iteration 169, loss = 0.58964154\n",
      "Iteration 170, loss = 0.58922367\n",
      "Iteration 171, loss = 0.58873716\n",
      "Iteration 172, loss = 0.58831669\n",
      "Iteration 173, loss = 0.58783410\n",
      "Iteration 174, loss = 0.58742668\n",
      "Iteration 175, loss = 0.58699673\n",
      "Iteration 176, loss = 0.58654055\n",
      "Iteration 177, loss = 0.58615638\n",
      "Iteration 178, loss = 0.58574158\n",
      "Iteration 179, loss = 0.58537214\n",
      "Iteration 180, loss = 0.58495794\n",
      "Iteration 181, loss = 0.58460581\n",
      "Iteration 182, loss = 0.58426721\n",
      "Iteration 183, loss = 0.58383421\n",
      "Iteration 184, loss = 0.58352173\n",
      "Iteration 185, loss = 0.58312116\n",
      "Iteration 186, loss = 0.58277223\n",
      "Iteration 187, loss = 0.58244326\n",
      "Iteration 188, loss = 0.58206660\n",
      "Iteration 189, loss = 0.58180581\n",
      "Iteration 190, loss = 0.58144365\n",
      "Iteration 191, loss = 0.58115151\n",
      "Iteration 192, loss = 0.58085977\n",
      "Iteration 193, loss = 0.58056335\n",
      "Iteration 194, loss = 0.58028051\n",
      "Iteration 195, loss = 0.57997731\n",
      "Iteration 196, loss = 0.57967471\n",
      "Iteration 197, loss = 0.57934643\n",
      "Iteration 198, loss = 0.57901485\n",
      "Iteration 199, loss = 0.57873509\n",
      "Iteration 200, loss = 0.57842426\n",
      "Iteration 201, loss = 0.57810176\n",
      "Iteration 202, loss = 0.57777055\n",
      "Iteration 203, loss = 0.57751622\n",
      "Iteration 204, loss = 0.57715350\n",
      "Iteration 205, loss = 0.57682926\n",
      "Iteration 206, loss = 0.57653472\n",
      "Iteration 207, loss = 0.57614914\n",
      "Iteration 208, loss = 0.57587108\n",
      "Iteration 209, loss = 0.57548721\n",
      "Iteration 210, loss = 0.57522119\n",
      "Iteration 211, loss = 0.57487062\n",
      "Iteration 212, loss = 0.57455693\n",
      "Iteration 213, loss = 0.57426615\n",
      "Iteration 214, loss = 0.57397210\n",
      "Iteration 215, loss = 0.57368841\n",
      "Iteration 216, loss = 0.57344660\n",
      "Iteration 217, loss = 0.57321380\n",
      "Iteration 218, loss = 0.57294813\n",
      "Iteration 219, loss = 0.57273847\n",
      "Iteration 220, loss = 0.57246842\n",
      "Iteration 221, loss = 0.57228408\n",
      "Iteration 222, loss = 0.57205536\n",
      "Iteration 223, loss = 0.57182428\n",
      "Iteration 224, loss = 0.57161622\n",
      "Iteration 225, loss = 0.57139629\n",
      "Iteration 226, loss = 0.57121116\n",
      "Iteration 227, loss = 0.57098320\n",
      "Iteration 228, loss = 0.57079403\n",
      "Iteration 229, loss = 0.57053617\n",
      "Iteration 230, loss = 0.57035798\n",
      "Iteration 231, loss = 0.57012868\n",
      "Iteration 232, loss = 0.56991529\n",
      "Iteration 233, loss = 0.56974351\n",
      "Iteration 234, loss = 0.56955795\n",
      "Iteration 235, loss = 0.56939013\n",
      "Iteration 236, loss = 0.56921773\n",
      "Iteration 237, loss = 0.56906666\n",
      "Iteration 238, loss = 0.56889365\n",
      "Iteration 239, loss = 0.56876498\n",
      "Iteration 240, loss = 0.56861909\n",
      "Iteration 241, loss = 0.56845125\n",
      "Iteration 242, loss = 0.56833994\n",
      "Iteration 243, loss = 0.56813841\n",
      "Iteration 244, loss = 0.56798590\n",
      "Iteration 245, loss = 0.56782419\n",
      "Iteration 246, loss = 0.56764524\n",
      "Iteration 247, loss = 0.56750061\n",
      "Iteration 248, loss = 0.56732264\n",
      "Iteration 249, loss = 0.56714746\n",
      "Iteration 250, loss = 0.56699110\n",
      "Iteration 251, loss = 0.56684002\n",
      "Iteration 252, loss = 0.56665912\n",
      "Iteration 253, loss = 0.56647775\n",
      "Iteration 254, loss = 0.56631737\n",
      "Iteration 255, loss = 0.56619178\n",
      "Iteration 256, loss = 0.56604054\n",
      "Iteration 257, loss = 0.56586548\n",
      "Iteration 258, loss = 0.56568470\n",
      "Iteration 259, loss = 0.56555041\n",
      "Iteration 260, loss = 0.56533932\n",
      "Iteration 261, loss = 0.56522175\n",
      "Iteration 262, loss = 0.56498860\n",
      "Iteration 263, loss = 0.56485137\n",
      "Iteration 264, loss = 0.56466782\n",
      "Iteration 265, loss = 0.56452551\n",
      "Iteration 266, loss = 0.56436718\n",
      "Iteration 267, loss = 0.56421140\n",
      "Iteration 268, loss = 0.56407435\n",
      "Iteration 269, loss = 0.56396261\n",
      "Iteration 270, loss = 0.56381948\n",
      "Iteration 271, loss = 0.56370571\n",
      "Iteration 272, loss = 0.56357158\n",
      "Iteration 273, loss = 0.56345406\n",
      "Iteration 274, loss = 0.56331120\n",
      "Iteration 275, loss = 0.56318255\n",
      "Iteration 276, loss = 0.56304658\n",
      "Iteration 277, loss = 0.56291117\n",
      "Iteration 278, loss = 0.56276124\n",
      "Iteration 279, loss = 0.56262191\n",
      "Iteration 280, loss = 0.56250079\n",
      "Iteration 281, loss = 0.56235504\n",
      "Iteration 282, loss = 0.56222233\n",
      "Iteration 283, loss = 0.56211164\n",
      "Iteration 284, loss = 0.56197506\n",
      "Iteration 285, loss = 0.56185623\n",
      "Iteration 286, loss = 0.56173490\n",
      "Iteration 287, loss = 0.56164065\n",
      "Iteration 288, loss = 0.56154521\n",
      "Iteration 289, loss = 0.56142019\n",
      "Iteration 290, loss = 0.56131492\n",
      "Iteration 291, loss = 0.56122103\n",
      "Iteration 292, loss = 0.56109657\n",
      "Iteration 293, loss = 0.56096662\n",
      "Iteration 294, loss = 0.56084949\n",
      "Iteration 295, loss = 0.56073617\n",
      "Iteration 296, loss = 0.56064011\n",
      "Iteration 297, loss = 0.56052388\n",
      "Iteration 298, loss = 0.56044046\n",
      "Iteration 299, loss = 0.56032124\n",
      "Iteration 300, loss = 0.56024922\n",
      "Iteration 301, loss = 0.56013221\n",
      "Iteration 302, loss = 0.56003776\n",
      "Iteration 303, loss = 0.55994600\n",
      "Iteration 304, loss = 0.55985875\n",
      "Iteration 305, loss = 0.55978141\n",
      "Iteration 306, loss = 0.55967266\n",
      "Iteration 307, loss = 0.55959333\n",
      "Iteration 308, loss = 0.55950942\n",
      "Iteration 309, loss = 0.55939604\n",
      "Iteration 310, loss = 0.55930459\n",
      "Iteration 311, loss = 0.55920822\n",
      "Iteration 312, loss = 0.55915882\n",
      "Iteration 313, loss = 0.55906384\n",
      "Iteration 314, loss = 0.55897073\n",
      "Iteration 315, loss = 0.55889766\n",
      "Iteration 316, loss = 0.55877883\n",
      "Iteration 317, loss = 0.55870713\n",
      "Iteration 318, loss = 0.55862285\n",
      "Iteration 319, loss = 0.55854691\n",
      "Iteration 320, loss = 0.55844063\n",
      "Iteration 321, loss = 0.55832474\n",
      "Iteration 322, loss = 0.55824035\n",
      "Iteration 323, loss = 0.55816598\n",
      "Iteration 324, loss = 0.55805569\n",
      "Iteration 325, loss = 0.55797026\n",
      "Iteration 326, loss = 0.55789147\n",
      "Iteration 327, loss = 0.55778001\n",
      "Iteration 328, loss = 0.55770208\n",
      "Iteration 329, loss = 0.55761819\n",
      "Iteration 330, loss = 0.55752033\n",
      "Iteration 331, loss = 0.55743302\n",
      "Iteration 332, loss = 0.55733912\n",
      "Iteration 333, loss = 0.55722635\n",
      "Iteration 334, loss = 0.55714195\n",
      "Iteration 335, loss = 0.55702989\n",
      "Iteration 336, loss = 0.55691306\n",
      "Iteration 337, loss = 0.55683239\n",
      "Iteration 338, loss = 0.55670613\n",
      "Iteration 339, loss = 0.55659209\n",
      "Iteration 340, loss = 0.55651460\n",
      "Iteration 341, loss = 0.55637957\n",
      "Iteration 342, loss = 0.55630024\n",
      "Iteration 343, loss = 0.55618123\n",
      "Iteration 344, loss = 0.55606685\n",
      "Iteration 345, loss = 0.55596820\n",
      "Iteration 346, loss = 0.55591723\n",
      "Iteration 347, loss = 0.55581185\n",
      "Iteration 348, loss = 0.55573567\n",
      "Iteration 349, loss = 0.55565185\n",
      "Iteration 350, loss = 0.55557454\n",
      "Iteration 351, loss = 0.55551244\n",
      "Iteration 352, loss = 0.55543018\n",
      "Iteration 353, loss = 0.55537144\n",
      "Iteration 354, loss = 0.55527533\n",
      "Iteration 355, loss = 0.55522077\n",
      "Iteration 356, loss = 0.55514149\n",
      "Iteration 357, loss = 0.55506816\n",
      "Iteration 358, loss = 0.55500883\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06123082\n",
      "Iteration 2, loss = 1.05438949\n",
      "Iteration 3, loss = 1.04767275\n",
      "Iteration 4, loss = 1.04125142\n",
      "Iteration 5, loss = 1.03471706\n",
      "Iteration 6, loss = 1.02803843\n",
      "Iteration 7, loss = 1.02171250\n",
      "Iteration 8, loss = 1.01531744\n",
      "Iteration 9, loss = 1.00882807\n",
      "Iteration 10, loss = 1.00276461\n",
      "Iteration 11, loss = 0.99635750\n",
      "Iteration 12, loss = 0.99030823\n",
      "Iteration 13, loss = 0.98410786\n",
      "Iteration 14, loss = 0.97806619\n",
      "Iteration 15, loss = 0.97216589\n",
      "Iteration 16, loss = 0.96616928\n",
      "Iteration 17, loss = 0.96021039\n",
      "Iteration 18, loss = 0.95442566\n",
      "Iteration 19, loss = 0.94847554\n",
      "Iteration 20, loss = 0.94279283\n",
      "Iteration 21, loss = 0.93689485\n",
      "Iteration 22, loss = 0.93132076\n",
      "Iteration 23, loss = 0.92587023\n",
      "Iteration 24, loss = 0.92029583\n",
      "Iteration 25, loss = 0.91487265\n",
      "Iteration 26, loss = 0.90984612\n",
      "Iteration 27, loss = 0.90465179\n",
      "Iteration 28, loss = 0.89957362\n",
      "Iteration 29, loss = 0.89463978\n",
      "Iteration 30, loss = 0.88975297\n",
      "Iteration 31, loss = 0.88471525\n",
      "Iteration 32, loss = 0.88005216\n",
      "Iteration 33, loss = 0.87509974\n",
      "Iteration 34, loss = 0.87032945\n",
      "Iteration 35, loss = 0.86570033\n",
      "Iteration 36, loss = 0.86108759\n",
      "Iteration 37, loss = 0.85640927\n",
      "Iteration 38, loss = 0.85209124\n",
      "Iteration 39, loss = 0.84756173\n",
      "Iteration 40, loss = 0.84317397\n",
      "Iteration 41, loss = 0.83889891\n",
      "Iteration 42, loss = 0.83451411\n",
      "Iteration 43, loss = 0.83036446\n",
      "Iteration 44, loss = 0.82612191\n",
      "Iteration 45, loss = 0.82199431\n",
      "Iteration 46, loss = 0.81788242\n",
      "Iteration 47, loss = 0.81390372\n",
      "Iteration 48, loss = 0.80983298\n",
      "Iteration 49, loss = 0.80580755\n",
      "Iteration 50, loss = 0.80190314\n",
      "Iteration 51, loss = 0.79805396\n",
      "Iteration 52, loss = 0.79420831\n",
      "Iteration 53, loss = 0.79032201\n",
      "Iteration 54, loss = 0.78666445\n",
      "Iteration 55, loss = 0.78297327\n",
      "Iteration 56, loss = 0.77923026\n",
      "Iteration 57, loss = 0.77569977\n",
      "Iteration 58, loss = 0.77216873\n",
      "Iteration 59, loss = 0.76869146\n",
      "Iteration 60, loss = 0.76529630\n",
      "Iteration 61, loss = 0.76189551\n",
      "Iteration 62, loss = 0.75867533\n",
      "Iteration 63, loss = 0.75545921\n",
      "Iteration 64, loss = 0.75222984\n",
      "Iteration 65, loss = 0.74912431\n",
      "Iteration 66, loss = 0.74593402\n",
      "Iteration 67, loss = 0.74291663\n",
      "Iteration 68, loss = 0.73993709\n",
      "Iteration 69, loss = 0.73691629\n",
      "Iteration 70, loss = 0.73390775\n",
      "Iteration 71, loss = 0.73108125\n",
      "Iteration 72, loss = 0.72818786\n",
      "Iteration 73, loss = 0.72542086\n",
      "Iteration 74, loss = 0.72268026\n",
      "Iteration 75, loss = 0.71992676\n",
      "Iteration 76, loss = 0.71719118\n",
      "Iteration 77, loss = 0.71451476\n",
      "Iteration 78, loss = 0.71181479\n",
      "Iteration 79, loss = 0.70911389\n",
      "Iteration 80, loss = 0.70640849\n",
      "Iteration 81, loss = 0.70379768\n",
      "Iteration 82, loss = 0.70115324\n",
      "Iteration 83, loss = 0.69861515\n",
      "Iteration 84, loss = 0.69595721\n",
      "Iteration 85, loss = 0.69355184\n",
      "Iteration 86, loss = 0.69105747\n",
      "Iteration 87, loss = 0.68864658\n",
      "Iteration 88, loss = 0.68633668\n",
      "Iteration 89, loss = 0.68392218\n",
      "Iteration 90, loss = 0.68173328\n",
      "Iteration 91, loss = 0.67946432\n",
      "Iteration 92, loss = 0.67729760\n",
      "Iteration 93, loss = 0.67527424\n",
      "Iteration 94, loss = 0.67311562\n",
      "Iteration 95, loss = 0.67114419\n",
      "Iteration 96, loss = 0.66910281\n",
      "Iteration 97, loss = 0.66710743\n",
      "Iteration 98, loss = 0.66506920\n",
      "Iteration 99, loss = 0.66320625\n",
      "Iteration 100, loss = 0.66127778\n",
      "Iteration 101, loss = 0.65940910\n",
      "Iteration 102, loss = 0.65754085\n",
      "Iteration 103, loss = 0.65576650\n",
      "Iteration 104, loss = 0.65388150\n",
      "Iteration 105, loss = 0.65213624\n",
      "Iteration 106, loss = 0.65036352\n",
      "Iteration 107, loss = 0.64865442\n",
      "Iteration 108, loss = 0.64699095\n",
      "Iteration 109, loss = 0.64529564\n",
      "Iteration 110, loss = 0.64363768\n",
      "Iteration 111, loss = 0.64204858\n",
      "Iteration 112, loss = 0.64047229\n",
      "Iteration 113, loss = 0.63888988\n",
      "Iteration 114, loss = 0.63729431\n",
      "Iteration 115, loss = 0.63583399\n",
      "Iteration 116, loss = 0.63437305\n",
      "Iteration 117, loss = 0.63287958\n",
      "Iteration 118, loss = 0.63141732\n",
      "Iteration 119, loss = 0.63004839\n",
      "Iteration 120, loss = 0.62863088\n",
      "Iteration 121, loss = 0.62725436\n",
      "Iteration 122, loss = 0.62586016\n",
      "Iteration 123, loss = 0.62447043\n",
      "Iteration 124, loss = 0.62335014\n",
      "Iteration 125, loss = 0.62193818\n",
      "Iteration 126, loss = 0.62071173\n",
      "Iteration 127, loss = 0.61947182\n",
      "Iteration 128, loss = 0.61817633\n",
      "Iteration 129, loss = 0.61698107\n",
      "Iteration 130, loss = 0.61572518\n",
      "Iteration 131, loss = 0.61445146\n",
      "Iteration 132, loss = 0.61323534\n",
      "Iteration 133, loss = 0.61207768\n",
      "Iteration 134, loss = 0.61085174\n",
      "Iteration 135, loss = 0.60967245\n",
      "Iteration 136, loss = 0.60853458\n",
      "Iteration 137, loss = 0.60748309\n",
      "Iteration 138, loss = 0.60635885\n",
      "Iteration 139, loss = 0.60525932\n",
      "Iteration 140, loss = 0.60415802\n",
      "Iteration 141, loss = 0.60311079\n",
      "Iteration 142, loss = 0.60198368\n",
      "Iteration 143, loss = 0.60084908\n",
      "Iteration 144, loss = 0.59973685\n",
      "Iteration 145, loss = 0.59856618\n",
      "Iteration 146, loss = 0.59752593\n",
      "Iteration 147, loss = 0.59638290\n",
      "Iteration 148, loss = 0.59533616\n",
      "Iteration 149, loss = 0.59433364\n",
      "Iteration 150, loss = 0.59333727\n",
      "Iteration 151, loss = 0.59231960\n",
      "Iteration 152, loss = 0.59148801\n",
      "Iteration 153, loss = 0.59051670\n",
      "Iteration 154, loss = 0.58970519\n",
      "Iteration 155, loss = 0.58890046\n",
      "Iteration 156, loss = 0.58808945\n",
      "Iteration 157, loss = 0.58725467\n",
      "Iteration 158, loss = 0.58649810\n",
      "Iteration 159, loss = 0.58565461\n",
      "Iteration 160, loss = 0.58489322\n",
      "Iteration 161, loss = 0.58418685\n",
      "Iteration 162, loss = 0.58335535\n",
      "Iteration 163, loss = 0.58266120\n",
      "Iteration 164, loss = 0.58204071\n",
      "Iteration 165, loss = 0.58125934\n",
      "Iteration 166, loss = 0.58057505\n",
      "Iteration 167, loss = 0.57992548\n",
      "Iteration 168, loss = 0.57920560\n",
      "Iteration 169, loss = 0.57847313\n",
      "Iteration 170, loss = 0.57779136\n",
      "Iteration 171, loss = 0.57706607\n",
      "Iteration 172, loss = 0.57635400\n",
      "Iteration 173, loss = 0.57565452\n",
      "Iteration 174, loss = 0.57497066\n",
      "Iteration 175, loss = 0.57427299\n",
      "Iteration 176, loss = 0.57369065\n",
      "Iteration 177, loss = 0.57299612\n",
      "Iteration 178, loss = 0.57240181\n",
      "Iteration 179, loss = 0.57183068\n",
      "Iteration 180, loss = 0.57132698\n",
      "Iteration 181, loss = 0.57070277\n",
      "Iteration 182, loss = 0.57021203\n",
      "Iteration 183, loss = 0.56971308\n",
      "Iteration 184, loss = 0.56916509\n",
      "Iteration 185, loss = 0.56871914\n",
      "Iteration 186, loss = 0.56819364\n",
      "Iteration 187, loss = 0.56773106\n",
      "Iteration 188, loss = 0.56727232\n",
      "Iteration 189, loss = 0.56681620\n",
      "Iteration 190, loss = 0.56632675\n",
      "Iteration 191, loss = 0.56594287\n",
      "Iteration 192, loss = 0.56545627\n",
      "Iteration 193, loss = 0.56500601\n",
      "Iteration 194, loss = 0.56463762\n",
      "Iteration 195, loss = 0.56418702\n",
      "Iteration 196, loss = 0.56379594\n",
      "Iteration 197, loss = 0.56343018\n",
      "Iteration 198, loss = 0.56302276\n",
      "Iteration 199, loss = 0.56272118\n",
      "Iteration 200, loss = 0.56234474\n",
      "Iteration 201, loss = 0.56199552\n",
      "Iteration 202, loss = 0.56171836\n",
      "Iteration 203, loss = 0.56134103\n",
      "Iteration 204, loss = 0.56106542\n",
      "Iteration 205, loss = 0.56080368\n",
      "Iteration 206, loss = 0.56051430\n",
      "Iteration 207, loss = 0.56023566\n",
      "Iteration 208, loss = 0.55993786\n",
      "Iteration 209, loss = 0.55967327\n",
      "Iteration 210, loss = 0.55941647\n",
      "Iteration 211, loss = 0.55912157\n",
      "Iteration 212, loss = 0.55881179\n",
      "Iteration 213, loss = 0.55857496\n",
      "Iteration 214, loss = 0.55832909\n",
      "Iteration 215, loss = 0.55809140\n",
      "Iteration 216, loss = 0.55783928\n",
      "Iteration 217, loss = 0.55762590\n",
      "Iteration 218, loss = 0.55736836\n",
      "Iteration 219, loss = 0.55716828\n",
      "Iteration 220, loss = 0.55686853\n",
      "Iteration 221, loss = 0.55669608\n",
      "Iteration 222, loss = 0.55643220\n",
      "Iteration 223, loss = 0.55618916\n",
      "Iteration 224, loss = 0.55599229\n",
      "Iteration 225, loss = 0.55576285\n",
      "Iteration 226, loss = 0.55552262\n",
      "Iteration 227, loss = 0.55530196\n",
      "Iteration 228, loss = 0.55504637\n",
      "Iteration 229, loss = 0.55482357\n",
      "Iteration 230, loss = 0.55456611\n",
      "Iteration 231, loss = 0.55431482\n",
      "Iteration 232, loss = 0.55407860\n",
      "Iteration 233, loss = 0.55387070\n",
      "Iteration 234, loss = 0.55362938\n",
      "Iteration 235, loss = 0.55340406\n",
      "Iteration 236, loss = 0.55319939\n",
      "Iteration 237, loss = 0.55302362\n",
      "Iteration 238, loss = 0.55280076\n",
      "Iteration 239, loss = 0.55263317\n",
      "Iteration 240, loss = 0.55246192\n",
      "Iteration 241, loss = 0.55224284\n",
      "Iteration 242, loss = 0.55210782\n",
      "Iteration 243, loss = 0.55192935\n",
      "Iteration 244, loss = 0.55173339\n",
      "Iteration 245, loss = 0.55158086\n",
      "Iteration 246, loss = 0.55144517\n",
      "Iteration 247, loss = 0.55129793\n",
      "Iteration 248, loss = 0.55115641\n",
      "Iteration 249, loss = 0.55102726\n",
      "Iteration 250, loss = 0.55092907\n",
      "Iteration 251, loss = 0.55077109\n",
      "Iteration 252, loss = 0.55063817\n",
      "Iteration 253, loss = 0.55050113\n",
      "Iteration 254, loss = 0.55035961\n",
      "Iteration 255, loss = 0.55023823\n",
      "Iteration 256, loss = 0.55012003\n",
      "Iteration 257, loss = 0.55000454\n",
      "Iteration 258, loss = 0.54988289\n",
      "Iteration 259, loss = 0.54980375\n",
      "Iteration 260, loss = 0.54970934\n",
      "Iteration 261, loss = 0.54961585\n",
      "Iteration 262, loss = 0.54950562\n",
      "Iteration 263, loss = 0.54942892\n",
      "Iteration 264, loss = 0.54934643\n",
      "Iteration 265, loss = 0.54926907\n",
      "Iteration 266, loss = 0.54917638\n",
      "Iteration 267, loss = 0.54909137\n",
      "Iteration 268, loss = 0.54901188\n",
      "Iteration 269, loss = 0.54890319\n",
      "Iteration 270, loss = 0.54881416\n",
      "Iteration 271, loss = 0.54873751\n",
      "Iteration 272, loss = 0.54865205\n",
      "Iteration 273, loss = 0.54854689\n",
      "Iteration 274, loss = 0.54848531\n",
      "Iteration 275, loss = 0.54838345\n",
      "Iteration 276, loss = 0.54830643\n",
      "Iteration 277, loss = 0.54822864\n",
      "Iteration 278, loss = 0.54813249\n",
      "Iteration 279, loss = 0.54806091\n",
      "Iteration 280, loss = 0.54796435\n",
      "Iteration 281, loss = 0.54789605\n",
      "Iteration 282, loss = 0.54780965\n",
      "Iteration 283, loss = 0.54773797\n",
      "Iteration 284, loss = 0.54767694\n",
      "Iteration 285, loss = 0.54761901\n",
      "Iteration 286, loss = 0.54755161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.56248959\n",
      "Iteration 2, loss = 0.56183015\n",
      "Iteration 3, loss = 0.56116156\n",
      "Iteration 4, loss = 0.56064054\n",
      "Iteration 5, loss = 0.56007315\n",
      "Iteration 6, loss = 0.55957699\n",
      "Iteration 7, loss = 0.55909455\n",
      "Iteration 8, loss = 0.55863227\n",
      "Iteration 9, loss = 0.55821889\n",
      "Iteration 10, loss = 0.55782229\n",
      "Iteration 11, loss = 0.55735912\n",
      "Iteration 12, loss = 0.55692285\n",
      "Iteration 13, loss = 0.55650661\n",
      "Iteration 14, loss = 0.55604970\n",
      "Iteration 15, loss = 0.55567024\n",
      "Iteration 16, loss = 0.55524155\n",
      "Iteration 17, loss = 0.55484023\n",
      "Iteration 18, loss = 0.55447828\n",
      "Iteration 19, loss = 0.55408500\n",
      "Iteration 20, loss = 0.55371276\n",
      "Iteration 21, loss = 0.55332037\n",
      "Iteration 22, loss = 0.55296432\n",
      "Iteration 23, loss = 0.55263062\n",
      "Iteration 24, loss = 0.55225889\n",
      "Iteration 25, loss = 0.55196007\n",
      "Iteration 26, loss = 0.55157646\n",
      "Iteration 27, loss = 0.55132783\n",
      "Iteration 28, loss = 0.55093729\n",
      "Iteration 29, loss = 0.55061451\n",
      "Iteration 30, loss = 0.55035959\n",
      "Iteration 31, loss = 0.54999642\n",
      "Iteration 32, loss = 0.54971933\n",
      "Iteration 33, loss = 0.54941706\n",
      "Iteration 34, loss = 0.54913429\n",
      "Iteration 35, loss = 0.54886206\n",
      "Iteration 36, loss = 0.54856175\n",
      "Iteration 37, loss = 0.54828523\n",
      "Iteration 38, loss = 0.54800864\n",
      "Iteration 39, loss = 0.54778850\n",
      "Iteration 40, loss = 0.54755075\n",
      "Iteration 41, loss = 0.54733483\n",
      "Iteration 42, loss = 0.54707769\n",
      "Iteration 43, loss = 0.54685714\n",
      "Iteration 44, loss = 0.54665862\n",
      "Iteration 45, loss = 0.54642479\n",
      "Iteration 46, loss = 0.54622810\n",
      "Iteration 47, loss = 0.54599775\n",
      "Iteration 48, loss = 0.54582477\n",
      "Iteration 49, loss = 0.54561905\n",
      "Iteration 50, loss = 0.54541841\n",
      "Iteration 51, loss = 0.54523409\n",
      "Iteration 52, loss = 0.54505926\n",
      "Iteration 53, loss = 0.54486055\n",
      "Iteration 54, loss = 0.54466025\n",
      "Iteration 55, loss = 0.54446541\n",
      "Iteration 56, loss = 0.54424739\n",
      "Iteration 57, loss = 0.54408473\n",
      "Iteration 58, loss = 0.54386866\n",
      "Iteration 59, loss = 0.54366575\n",
      "Iteration 60, loss = 0.54347495\n",
      "Iteration 61, loss = 0.54328142\n",
      "Iteration 62, loss = 0.54307437\n",
      "Iteration 63, loss = 0.54289155\n",
      "Iteration 64, loss = 0.54268701\n",
      "Iteration 65, loss = 0.54251460\n",
      "Iteration 66, loss = 0.54234262\n",
      "Iteration 67, loss = 0.54215815\n",
      "Iteration 68, loss = 0.54197630\n",
      "Iteration 69, loss = 0.54182615\n",
      "Iteration 70, loss = 0.54169565\n",
      "Iteration 71, loss = 0.54152455\n",
      "Iteration 72, loss = 0.54137107\n",
      "Iteration 73, loss = 0.54124775\n",
      "Iteration 74, loss = 0.54108475\n",
      "Iteration 75, loss = 0.54094626\n",
      "Iteration 76, loss = 0.54083811\n",
      "Iteration 77, loss = 0.54070451\n",
      "Iteration 78, loss = 0.54055165\n",
      "Iteration 79, loss = 0.54042194\n",
      "Iteration 80, loss = 0.54029825\n",
      "Iteration 81, loss = 0.54016684\n",
      "Iteration 82, loss = 0.54005024\n",
      "Iteration 83, loss = 0.53991244\n",
      "Iteration 84, loss = 0.53978645\n",
      "Iteration 85, loss = 0.53968834\n",
      "Iteration 86, loss = 0.53957339\n",
      "Iteration 87, loss = 0.53945893\n",
      "Iteration 88, loss = 0.53935639\n",
      "Iteration 89, loss = 0.53924049\n",
      "Iteration 90, loss = 0.53911913\n",
      "Iteration 91, loss = 0.53899978\n",
      "Iteration 92, loss = 0.53888401\n",
      "Iteration 93, loss = 0.53884174\n",
      "Iteration 94, loss = 0.53868054\n",
      "Iteration 95, loss = 0.53860453\n",
      "Iteration 96, loss = 0.53853238\n",
      "Iteration 97, loss = 0.53843820\n",
      "Iteration 98, loss = 0.53834952\n",
      "Iteration 99, loss = 0.53827167\n",
      "Iteration 100, loss = 0.53818352\n",
      "Iteration 101, loss = 0.53810212\n",
      "Iteration 102, loss = 0.53803158\n",
      "Iteration 103, loss = 0.53795768\n",
      "Iteration 104, loss = 0.53787331\n",
      "Iteration 105, loss = 0.53779568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66167525\n",
      "Iteration 2, loss = 0.66021626\n",
      "Iteration 3, loss = 0.65890126\n",
      "Iteration 4, loss = 0.65756284\n",
      "Iteration 5, loss = 0.65634843\n",
      "Iteration 6, loss = 0.65504956\n",
      "Iteration 7, loss = 0.65384576\n",
      "Iteration 8, loss = 0.65276611\n",
      "Iteration 9, loss = 0.65157168\n",
      "Iteration 10, loss = 0.65045661\n",
      "Iteration 11, loss = 0.64935856\n",
      "Iteration 12, loss = 0.64822886\n",
      "Iteration 13, loss = 0.64715909\n",
      "Iteration 14, loss = 0.64606336\n",
      "Iteration 15, loss = 0.64497732\n",
      "Iteration 16, loss = 0.64397040\n",
      "Iteration 17, loss = 0.64286272\n",
      "Iteration 18, loss = 0.64188854\n",
      "Iteration 19, loss = 0.64083427\n",
      "Iteration 20, loss = 0.63993042\n",
      "Iteration 21, loss = 0.63889101\n",
      "Iteration 22, loss = 0.63792737\n",
      "Iteration 23, loss = 0.63704864\n",
      "Iteration 24, loss = 0.63610632\n",
      "Iteration 25, loss = 0.63520629\n",
      "Iteration 26, loss = 0.63430157\n",
      "Iteration 27, loss = 0.63340451\n",
      "Iteration 28, loss = 0.63256407\n",
      "Iteration 29, loss = 0.63166534\n",
      "Iteration 30, loss = 0.63084468\n",
      "Iteration 31, loss = 0.62999940\n",
      "Iteration 32, loss = 0.62915476\n",
      "Iteration 33, loss = 0.62833928\n",
      "Iteration 34, loss = 0.62753230\n",
      "Iteration 35, loss = 0.62676700\n",
      "Iteration 36, loss = 0.62602930\n",
      "Iteration 37, loss = 0.62528407\n",
      "Iteration 38, loss = 0.62460629\n",
      "Iteration 39, loss = 0.62396108\n",
      "Iteration 40, loss = 0.62331852\n",
      "Iteration 41, loss = 0.62270815\n",
      "Iteration 42, loss = 0.62211129\n",
      "Iteration 43, loss = 0.62147279\n",
      "Iteration 44, loss = 0.62087715\n",
      "Iteration 45, loss = 0.62029304\n",
      "Iteration 46, loss = 0.61967430\n",
      "Iteration 47, loss = 0.61910865\n",
      "Iteration 48, loss = 0.61852970\n",
      "Iteration 49, loss = 0.61796514\n",
      "Iteration 50, loss = 0.61737416\n",
      "Iteration 51, loss = 0.61683305\n",
      "Iteration 52, loss = 0.61626972\n",
      "Iteration 53, loss = 0.61571492\n",
      "Iteration 54, loss = 0.61519433\n",
      "Iteration 55, loss = 0.61461832\n",
      "Iteration 56, loss = 0.61416040\n",
      "Iteration 57, loss = 0.61361309\n",
      "Iteration 58, loss = 0.61311032\n",
      "Iteration 59, loss = 0.61261802\n",
      "Iteration 60, loss = 0.61213213\n",
      "Iteration 61, loss = 0.61164182\n",
      "Iteration 62, loss = 0.61112592\n",
      "Iteration 63, loss = 0.61063582\n",
      "Iteration 64, loss = 0.61016996\n",
      "Iteration 65, loss = 0.60969693\n",
      "Iteration 66, loss = 0.60923790\n",
      "Iteration 67, loss = 0.60877608\n",
      "Iteration 68, loss = 0.60834185\n",
      "Iteration 69, loss = 0.60787315\n",
      "Iteration 70, loss = 0.60739974\n",
      "Iteration 71, loss = 0.60694027\n",
      "Iteration 72, loss = 0.60649818\n",
      "Iteration 73, loss = 0.60606673\n",
      "Iteration 74, loss = 0.60558513\n",
      "Iteration 75, loss = 0.60517603\n",
      "Iteration 76, loss = 0.60471253\n",
      "Iteration 77, loss = 0.60431285\n",
      "Iteration 78, loss = 0.60392084\n",
      "Iteration 79, loss = 0.60351662\n",
      "Iteration 80, loss = 0.60310422\n",
      "Iteration 81, loss = 0.60273564\n",
      "Iteration 82, loss = 0.60234361\n",
      "Iteration 83, loss = 0.60196269\n",
      "Iteration 84, loss = 0.60155397\n",
      "Iteration 85, loss = 0.60120141\n",
      "Iteration 86, loss = 0.60076014\n",
      "Iteration 87, loss = 0.60037867\n",
      "Iteration 88, loss = 0.59998294\n",
      "Iteration 89, loss = 0.59953839\n",
      "Iteration 90, loss = 0.59918860\n",
      "Iteration 91, loss = 0.59873499\n",
      "Iteration 92, loss = 0.59834331\n",
      "Iteration 93, loss = 0.59794663\n",
      "Iteration 94, loss = 0.59758780\n",
      "Iteration 95, loss = 0.59719045\n",
      "Iteration 96, loss = 0.59683399\n",
      "Iteration 97, loss = 0.59649157\n",
      "Iteration 98, loss = 0.59615896\n",
      "Iteration 99, loss = 0.59581229\n",
      "Iteration 100, loss = 0.59550518\n",
      "Iteration 101, loss = 0.59516961\n",
      "Iteration 102, loss = 0.59483192\n",
      "Iteration 103, loss = 0.59452052\n",
      "Iteration 104, loss = 0.59423504\n",
      "Iteration 105, loss = 0.59390625\n",
      "Iteration 106, loss = 0.59360124\n",
      "Iteration 107, loss = 0.59330092\n",
      "Iteration 108, loss = 0.59302450\n",
      "Iteration 109, loss = 0.59271514\n",
      "Iteration 110, loss = 0.59242077\n",
      "Iteration 111, loss = 0.59214500\n",
      "Iteration 112, loss = 0.59183207\n",
      "Iteration 113, loss = 0.59157244\n",
      "Iteration 114, loss = 0.59129477\n",
      "Iteration 115, loss = 0.59100902\n",
      "Iteration 116, loss = 0.59076366\n",
      "Iteration 117, loss = 0.59047851\n",
      "Iteration 118, loss = 0.59019698\n",
      "Iteration 119, loss = 0.58995120\n",
      "Iteration 120, loss = 0.58964830\n",
      "Iteration 121, loss = 0.58942706\n",
      "Iteration 122, loss = 0.58912508\n",
      "Iteration 123, loss = 0.58887578\n",
      "Iteration 124, loss = 0.58864073\n",
      "Iteration 125, loss = 0.58838212\n",
      "Iteration 126, loss = 0.58814691\n",
      "Iteration 127, loss = 0.58792644\n",
      "Iteration 128, loss = 0.58771960\n",
      "Iteration 129, loss = 0.58747379\n",
      "Iteration 130, loss = 0.58727713\n",
      "Iteration 131, loss = 0.58706229\n",
      "Iteration 132, loss = 0.58683590\n",
      "Iteration 133, loss = 0.58662499\n",
      "Iteration 134, loss = 0.58641773\n",
      "Iteration 135, loss = 0.58620501\n",
      "Iteration 136, loss = 0.58599789\n",
      "Iteration 137, loss = 0.58581984\n",
      "Iteration 138, loss = 0.58559533\n",
      "Iteration 139, loss = 0.58540459\n",
      "Iteration 140, loss = 0.58521382\n",
      "Iteration 141, loss = 0.58502593\n",
      "Iteration 142, loss = 0.58484522\n",
      "Iteration 143, loss = 0.58465445\n",
      "Iteration 144, loss = 0.58448609\n",
      "Iteration 145, loss = 0.58432565\n",
      "Iteration 146, loss = 0.58413675\n",
      "Iteration 147, loss = 0.58397283\n",
      "Iteration 148, loss = 0.58380463\n",
      "Iteration 149, loss = 0.58363245\n",
      "Iteration 150, loss = 0.58346474\n",
      "Iteration 151, loss = 0.58328269\n",
      "Iteration 152, loss = 0.58313138\n",
      "Iteration 153, loss = 0.58296090\n",
      "Iteration 154, loss = 0.58280859\n",
      "Iteration 155, loss = 0.58262907\n",
      "Iteration 156, loss = 0.58248624\n",
      "Iteration 157, loss = 0.58231370\n",
      "Iteration 158, loss = 0.58213752\n",
      "Iteration 159, loss = 0.58199263\n",
      "Iteration 160, loss = 0.58183237\n",
      "Iteration 161, loss = 0.58165998\n",
      "Iteration 162, loss = 0.58149737\n",
      "Iteration 163, loss = 0.58137099\n",
      "Iteration 164, loss = 0.58119581\n",
      "Iteration 165, loss = 0.58104809\n",
      "Iteration 166, loss = 0.58088682\n",
      "Iteration 167, loss = 0.58073777\n",
      "Iteration 168, loss = 0.58059160\n",
      "Iteration 169, loss = 0.58042199\n",
      "Iteration 170, loss = 0.58026514\n",
      "Iteration 171, loss = 0.58011910\n",
      "Iteration 172, loss = 0.57995163\n",
      "Iteration 173, loss = 0.57979299\n",
      "Iteration 174, loss = 0.57964221\n",
      "Iteration 175, loss = 0.57948516\n",
      "Iteration 176, loss = 0.57932521\n",
      "Iteration 177, loss = 0.57919000\n",
      "Iteration 178, loss = 0.57904135\n",
      "Iteration 179, loss = 0.57886725\n",
      "Iteration 180, loss = 0.57872936\n",
      "Iteration 181, loss = 0.57856018\n",
      "Iteration 182, loss = 0.57841129\n",
      "Iteration 183, loss = 0.57824944\n",
      "Iteration 184, loss = 0.57808832\n",
      "Iteration 185, loss = 0.57795462\n",
      "Iteration 186, loss = 0.57777489\n",
      "Iteration 187, loss = 0.57764861\n",
      "Iteration 188, loss = 0.57747446\n",
      "Iteration 189, loss = 0.57733016\n",
      "Iteration 190, loss = 0.57717786\n",
      "Iteration 191, loss = 0.57703790\n",
      "Iteration 192, loss = 0.57688613\n",
      "Iteration 193, loss = 0.57673135\n",
      "Iteration 194, loss = 0.57658313\n",
      "Iteration 195, loss = 0.57643586\n",
      "Iteration 196, loss = 0.57628772\n",
      "Iteration 197, loss = 0.57613220\n",
      "Iteration 198, loss = 0.57597165\n",
      "Iteration 199, loss = 0.57585049\n",
      "Iteration 200, loss = 0.57566961\n",
      "Iteration 201, loss = 0.57550181\n",
      "Iteration 202, loss = 0.57540330\n",
      "Iteration 203, loss = 0.57519658\n",
      "Iteration 204, loss = 0.57505017\n",
      "Iteration 205, loss = 0.57492016\n",
      "Iteration 206, loss = 0.57474701\n",
      "Iteration 207, loss = 0.57462597\n",
      "Iteration 208, loss = 0.57447814\n",
      "Iteration 209, loss = 0.57432667\n",
      "Iteration 210, loss = 0.57421271\n",
      "Iteration 211, loss = 0.57407865\n",
      "Iteration 212, loss = 0.57395556\n",
      "Iteration 213, loss = 0.57383654\n",
      "Iteration 214, loss = 0.57373179\n",
      "Iteration 215, loss = 0.57360544\n",
      "Iteration 216, loss = 0.57348410\n",
      "Iteration 217, loss = 0.57339375\n",
      "Iteration 218, loss = 0.57326412\n",
      "Iteration 219, loss = 0.57313383\n",
      "Iteration 220, loss = 0.57301868\n",
      "Iteration 221, loss = 0.57290399\n",
      "Iteration 222, loss = 0.57277755\n",
      "Iteration 223, loss = 0.57265498\n",
      "Iteration 224, loss = 0.57253099\n",
      "Iteration 225, loss = 0.57242396\n",
      "Iteration 226, loss = 0.57228558\n",
      "Iteration 227, loss = 0.57217680\n",
      "Iteration 228, loss = 0.57206671\n",
      "Iteration 229, loss = 0.57193142\n",
      "Iteration 230, loss = 0.57181829\n",
      "Iteration 231, loss = 0.57169425\n",
      "Iteration 232, loss = 0.57158636\n",
      "Iteration 233, loss = 0.57145561\n",
      "Iteration 234, loss = 0.57133294\n",
      "Iteration 235, loss = 0.57121886\n",
      "Iteration 236, loss = 0.57110222\n",
      "Iteration 237, loss = 0.57099481\n",
      "Iteration 238, loss = 0.57086951\n",
      "Iteration 239, loss = 0.57078190\n",
      "Iteration 240, loss = 0.57067708\n",
      "Iteration 241, loss = 0.57056358\n",
      "Iteration 242, loss = 0.57048104\n",
      "Iteration 243, loss = 0.57035486\n",
      "Iteration 244, loss = 0.57028543\n",
      "Iteration 245, loss = 0.57019385\n",
      "Iteration 246, loss = 0.57008374\n",
      "Iteration 247, loss = 0.56998775\n",
      "Iteration 248, loss = 0.56989414\n",
      "Iteration 249, loss = 0.56980084\n",
      "Iteration 250, loss = 0.56971833\n",
      "Iteration 251, loss = 0.56960155\n",
      "Iteration 252, loss = 0.56950378\n",
      "Iteration 253, loss = 0.56940916\n",
      "Iteration 254, loss = 0.56931051\n",
      "Iteration 255, loss = 0.56922545\n",
      "Iteration 256, loss = 0.56911913\n",
      "Iteration 257, loss = 0.56903384\n",
      "Iteration 258, loss = 0.56894195\n",
      "Iteration 259, loss = 0.56884628\n",
      "Iteration 260, loss = 0.56874314\n",
      "Iteration 261, loss = 0.56864693\n",
      "Iteration 262, loss = 0.56852077\n",
      "Iteration 263, loss = 0.56841507\n",
      "Iteration 264, loss = 0.56830499\n",
      "Iteration 265, loss = 0.56817891\n",
      "Iteration 266, loss = 0.56807411\n",
      "Iteration 267, loss = 0.56796681\n",
      "Iteration 268, loss = 0.56786307\n",
      "Iteration 269, loss = 0.56774803\n",
      "Iteration 270, loss = 0.56763738\n",
      "Iteration 271, loss = 0.56753642\n",
      "Iteration 272, loss = 0.56744777\n",
      "Iteration 273, loss = 0.56734457\n",
      "Iteration 274, loss = 0.56723184\n",
      "Iteration 275, loss = 0.56713062\n",
      "Iteration 276, loss = 0.56703229\n",
      "Iteration 277, loss = 0.56692756\n",
      "Iteration 278, loss = 0.56682094\n",
      "Iteration 279, loss = 0.56672662\n",
      "Iteration 280, loss = 0.56663074\n",
      "Iteration 281, loss = 0.56652765\n",
      "Iteration 282, loss = 0.56643804\n",
      "Iteration 283, loss = 0.56634911\n",
      "Iteration 284, loss = 0.56625820\n",
      "Iteration 285, loss = 0.56615744\n",
      "Iteration 286, loss = 0.56608251\n",
      "Iteration 287, loss = 0.56598349\n",
      "Iteration 288, loss = 0.56589211\n",
      "Iteration 289, loss = 0.56581541\n",
      "Iteration 290, loss = 0.56570528\n",
      "Iteration 291, loss = 0.56561340\n",
      "Iteration 292, loss = 0.56551609\n",
      "Iteration 293, loss = 0.56542233\n",
      "Iteration 294, loss = 0.56532339\n",
      "Iteration 295, loss = 0.56523706\n",
      "Iteration 296, loss = 0.56513232\n",
      "Iteration 297, loss = 0.56503000\n",
      "Iteration 298, loss = 0.56493664\n",
      "Iteration 299, loss = 0.56483886\n",
      "Iteration 300, loss = 0.56475047\n",
      "Iteration 301, loss = 0.56464118\n",
      "Iteration 302, loss = 0.56453847\n",
      "Iteration 303, loss = 0.56444654\n",
      "Iteration 304, loss = 0.56434490\n",
      "Iteration 305, loss = 0.56425288\n",
      "Iteration 306, loss = 0.56414632\n",
      "Iteration 307, loss = 0.56405515\n",
      "Iteration 308, loss = 0.56395319\n",
      "Iteration 309, loss = 0.56386386\n",
      "Iteration 310, loss = 0.56377155\n",
      "Iteration 311, loss = 0.56367359\n",
      "Iteration 312, loss = 0.56355292\n",
      "Iteration 313, loss = 0.56345224\n",
      "Iteration 314, loss = 0.56334461\n",
      "Iteration 315, loss = 0.56322723\n",
      "Iteration 316, loss = 0.56311470\n",
      "Iteration 317, loss = 0.56299321\n",
      "Iteration 318, loss = 0.56288937\n",
      "Iteration 319, loss = 0.56276663\n",
      "Iteration 320, loss = 0.56265443\n",
      "Iteration 321, loss = 0.56253777\n",
      "Iteration 322, loss = 0.56240233\n",
      "Iteration 323, loss = 0.56227305\n",
      "Iteration 324, loss = 0.56215163\n",
      "Iteration 325, loss = 0.56200439\n",
      "Iteration 326, loss = 0.56188476\n",
      "Iteration 327, loss = 0.56174442\n",
      "Iteration 328, loss = 0.56161465\n",
      "Iteration 329, loss = 0.56149426\n",
      "Iteration 330, loss = 0.56140342\n",
      "Iteration 331, loss = 0.56126650\n",
      "Iteration 332, loss = 0.56116099\n",
      "Iteration 333, loss = 0.56104651\n",
      "Iteration 334, loss = 0.56092564\n",
      "Iteration 335, loss = 0.56082187\n",
      "Iteration 336, loss = 0.56071024\n",
      "Iteration 337, loss = 0.56060576\n",
      "Iteration 338, loss = 0.56047564\n",
      "Iteration 339, loss = 0.56038008\n",
      "Iteration 340, loss = 0.56027597\n",
      "Iteration 341, loss = 0.56014696\n",
      "Iteration 342, loss = 0.56004045\n",
      "Iteration 343, loss = 0.55994754\n",
      "Iteration 344, loss = 0.55980166\n",
      "Iteration 345, loss = 0.55970912\n",
      "Iteration 346, loss = 0.55959014\n",
      "Iteration 347, loss = 0.55948228\n",
      "Iteration 348, loss = 0.55936701\n",
      "Iteration 349, loss = 0.55926474\n",
      "Iteration 350, loss = 0.55915687\n",
      "Iteration 351, loss = 0.55904311\n",
      "Iteration 352, loss = 0.55893117\n",
      "Iteration 353, loss = 0.55879962\n",
      "Iteration 354, loss = 0.55869876\n",
      "Iteration 355, loss = 0.55858237\n",
      "Iteration 356, loss = 0.55845265\n",
      "Iteration 357, loss = 0.55833948\n",
      "Iteration 358, loss = 0.55823536\n",
      "Iteration 359, loss = 0.55810950\n",
      "Iteration 360, loss = 0.55799568\n",
      "Iteration 361, loss = 0.55788071\n",
      "Iteration 362, loss = 0.55776041\n",
      "Iteration 363, loss = 0.55762600\n",
      "Iteration 364, loss = 0.55752387\n",
      "Iteration 365, loss = 0.55738668\n",
      "Iteration 366, loss = 0.55728310\n",
      "Iteration 367, loss = 0.55715948\n",
      "Iteration 368, loss = 0.55704710\n",
      "Iteration 369, loss = 0.55694270\n",
      "Iteration 370, loss = 0.55682982\n",
      "Iteration 371, loss = 0.55671489\n",
      "Iteration 372, loss = 0.55658276\n",
      "Iteration 373, loss = 0.55645344\n",
      "Iteration 374, loss = 0.55632249\n",
      "Iteration 375, loss = 0.55620345\n",
      "Iteration 376, loss = 0.55606538\n",
      "Iteration 377, loss = 0.55595836\n",
      "Iteration 378, loss = 0.55581219\n",
      "Iteration 379, loss = 0.55570040\n",
      "Iteration 380, loss = 0.55559366\n",
      "Iteration 381, loss = 0.55549186\n",
      "Iteration 382, loss = 0.55538268\n",
      "Iteration 383, loss = 0.55527201\n",
      "Iteration 384, loss = 0.55517355\n",
      "Iteration 385, loss = 0.55505787\n",
      "Iteration 386, loss = 0.55493707\n",
      "Iteration 387, loss = 0.55482936\n",
      "Iteration 388, loss = 0.55471681\n",
      "Iteration 389, loss = 0.55461549\n",
      "Iteration 390, loss = 0.55447458\n",
      "Iteration 391, loss = 0.55437418\n",
      "Iteration 392, loss = 0.55426535\n",
      "Iteration 393, loss = 0.55416159\n",
      "Iteration 394, loss = 0.55406468\n",
      "Iteration 395, loss = 0.55395574\n",
      "Iteration 396, loss = 0.55384330\n",
      "Iteration 397, loss = 0.55372748\n",
      "Iteration 398, loss = 0.55358224\n",
      "Iteration 399, loss = 0.55347348\n",
      "Iteration 400, loss = 0.55338031\n",
      "Iteration 401, loss = 0.55323318\n",
      "Iteration 402, loss = 0.55312369\n",
      "Iteration 403, loss = 0.55301289\n",
      "Iteration 404, loss = 0.55289365\n",
      "Iteration 405, loss = 0.55278772\n",
      "Iteration 406, loss = 0.55267147\n",
      "Iteration 407, loss = 0.55256789\n",
      "Iteration 408, loss = 0.55245287\n",
      "Iteration 409, loss = 0.55234486\n",
      "Iteration 410, loss = 0.55224605\n",
      "Iteration 411, loss = 0.55212535\n",
      "Iteration 412, loss = 0.55203207\n",
      "Iteration 413, loss = 0.55194009\n",
      "Iteration 414, loss = 0.55182503\n",
      "Iteration 415, loss = 0.55173679\n",
      "Iteration 416, loss = 0.55163839\n",
      "Iteration 417, loss = 0.55154146\n",
      "Iteration 418, loss = 0.55143931\n",
      "Iteration 419, loss = 0.55134343\n",
      "Iteration 420, loss = 0.55123462\n",
      "Iteration 421, loss = 0.55112468\n",
      "Iteration 422, loss = 0.55102663\n",
      "Iteration 423, loss = 0.55095433\n",
      "Iteration 424, loss = 0.55083175\n",
      "Iteration 425, loss = 0.55075563\n",
      "Iteration 426, loss = 0.55063877\n",
      "Iteration 427, loss = 0.55054821\n",
      "Iteration 428, loss = 0.55046929\n",
      "Iteration 429, loss = 0.55036044\n",
      "Iteration 430, loss = 0.55027345\n",
      "Iteration 431, loss = 0.55018758\n",
      "Iteration 432, loss = 0.55010979\n",
      "Iteration 433, loss = 0.54999818\n",
      "Iteration 434, loss = 0.54989187\n",
      "Iteration 435, loss = 0.54979667\n",
      "Iteration 436, loss = 0.54968304\n",
      "Iteration 437, loss = 0.54956434\n",
      "Iteration 438, loss = 0.54945852\n",
      "Iteration 439, loss = 0.54934567\n",
      "Iteration 440, loss = 0.54923342\n",
      "Iteration 441, loss = 0.54912432\n",
      "Iteration 442, loss = 0.54904588\n",
      "Iteration 443, loss = 0.54893024\n",
      "Iteration 444, loss = 0.54883390\n",
      "Iteration 445, loss = 0.54873115\n",
      "Iteration 446, loss = 0.54864298\n",
      "Iteration 447, loss = 0.54854112\n",
      "Iteration 448, loss = 0.54844772\n",
      "Iteration 449, loss = 0.54834741\n",
      "Iteration 450, loss = 0.54825326\n",
      "Iteration 451, loss = 0.54815415\n",
      "Iteration 452, loss = 0.54806936\n",
      "Iteration 453, loss = 0.54798626\n",
      "Iteration 454, loss = 0.54789939\n",
      "Iteration 455, loss = 0.54781932\n",
      "Iteration 456, loss = 0.54772533\n",
      "Iteration 457, loss = 0.54765169\n",
      "Iteration 458, loss = 0.54757376\n",
      "Iteration 459, loss = 0.54749410\n",
      "Iteration 460, loss = 0.54742876\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.60644285\n",
      "Iteration 2, loss = 1.59935010\n",
      "Iteration 3, loss = 1.59206098\n",
      "Iteration 4, loss = 1.58492268\n",
      "Iteration 5, loss = 1.57792054\n",
      "Iteration 6, loss = 1.57083810\n",
      "Iteration 7, loss = 1.56383450\n",
      "Iteration 8, loss = 1.55680536\n",
      "Iteration 9, loss = 1.54994778\n",
      "Iteration 10, loss = 1.54296328\n",
      "Iteration 11, loss = 1.53627151\n",
      "Iteration 12, loss = 1.52954488\n",
      "Iteration 13, loss = 1.52265156\n",
      "Iteration 14, loss = 1.51619807\n",
      "Iteration 15, loss = 1.50973387\n",
      "Iteration 16, loss = 1.50306239\n",
      "Iteration 17, loss = 1.49671283\n",
      "Iteration 18, loss = 1.49022872\n",
      "Iteration 19, loss = 1.48392891\n",
      "Iteration 20, loss = 1.47752942\n",
      "Iteration 21, loss = 1.47118724\n",
      "Iteration 22, loss = 1.46477259\n",
      "Iteration 23, loss = 1.45854916\n",
      "Iteration 24, loss = 1.45216441\n",
      "Iteration 25, loss = 1.44604579\n",
      "Iteration 26, loss = 1.43973218\n",
      "Iteration 27, loss = 1.43365852\n",
      "Iteration 28, loss = 1.42754037\n",
      "Iteration 29, loss = 1.42152940\n",
      "Iteration 30, loss = 1.41547516\n",
      "Iteration 31, loss = 1.40950400\n",
      "Iteration 32, loss = 1.40356577\n",
      "Iteration 33, loss = 1.39764130\n",
      "Iteration 34, loss = 1.39183949\n",
      "Iteration 35, loss = 1.38576838\n",
      "Iteration 36, loss = 1.38021541\n",
      "Iteration 37, loss = 1.37438405\n",
      "Iteration 38, loss = 1.36872896\n",
      "Iteration 39, loss = 1.36303531\n",
      "Iteration 40, loss = 1.35763689\n",
      "Iteration 41, loss = 1.35208993\n",
      "Iteration 42, loss = 1.34651093\n",
      "Iteration 43, loss = 1.34113262\n",
      "Iteration 44, loss = 1.33571493\n",
      "Iteration 45, loss = 1.33022849\n",
      "Iteration 46, loss = 1.32477130\n",
      "Iteration 47, loss = 1.31948906\n",
      "Iteration 48, loss = 1.31394292\n",
      "Iteration 49, loss = 1.30862787\n",
      "Iteration 50, loss = 1.30326119\n",
      "Iteration 51, loss = 1.29791746\n",
      "Iteration 52, loss = 1.29263685\n",
      "Iteration 53, loss = 1.28729888\n",
      "Iteration 54, loss = 1.28207161\n",
      "Iteration 55, loss = 1.27682841\n",
      "Iteration 56, loss = 1.27173013\n",
      "Iteration 57, loss = 1.26630017\n",
      "Iteration 58, loss = 1.26121727\n",
      "Iteration 59, loss = 1.25604074\n",
      "Iteration 60, loss = 1.25087482\n",
      "Iteration 61, loss = 1.24582669\n",
      "Iteration 62, loss = 1.24073967\n",
      "Iteration 63, loss = 1.23560389\n",
      "Iteration 64, loss = 1.23077176\n",
      "Iteration 65, loss = 1.22573348\n",
      "Iteration 66, loss = 1.22077855\n",
      "Iteration 67, loss = 1.21597486\n",
      "Iteration 68, loss = 1.21113828\n",
      "Iteration 69, loss = 1.20629095\n",
      "Iteration 70, loss = 1.20166992\n",
      "Iteration 71, loss = 1.19689144\n",
      "Iteration 72, loss = 1.19225392\n",
      "Iteration 73, loss = 1.18767387\n",
      "Iteration 74, loss = 1.18316634\n",
      "Iteration 75, loss = 1.17860065\n",
      "Iteration 76, loss = 1.17407497\n",
      "Iteration 77, loss = 1.16968260\n",
      "Iteration 78, loss = 1.16535922\n",
      "Iteration 79, loss = 1.16098934\n",
      "Iteration 80, loss = 1.15664980\n",
      "Iteration 81, loss = 1.15248161\n",
      "Iteration 82, loss = 1.14818112\n",
      "Iteration 83, loss = 1.14401390\n",
      "Iteration 84, loss = 1.13970638\n",
      "Iteration 85, loss = 1.13553321\n",
      "Iteration 86, loss = 1.13126374\n",
      "Iteration 87, loss = 1.12706341\n",
      "Iteration 88, loss = 1.12284288\n",
      "Iteration 89, loss = 1.11874155\n",
      "Iteration 90, loss = 1.11441598\n",
      "Iteration 91, loss = 1.11026644\n",
      "Iteration 92, loss = 1.10621232\n",
      "Iteration 93, loss = 1.10186790\n",
      "Iteration 94, loss = 1.09782707\n",
      "Iteration 95, loss = 1.09358103\n",
      "Iteration 96, loss = 1.08943085\n",
      "Iteration 97, loss = 1.08531300\n",
      "Iteration 98, loss = 1.08114610\n",
      "Iteration 99, loss = 1.07704504\n",
      "Iteration 100, loss = 1.07298904\n",
      "Iteration 101, loss = 1.06900449\n",
      "Iteration 102, loss = 1.06501767\n",
      "Iteration 103, loss = 1.06108133\n",
      "Iteration 104, loss = 1.05714742\n",
      "Iteration 105, loss = 1.05336545\n",
      "Iteration 106, loss = 1.04944274\n",
      "Iteration 107, loss = 1.04561564\n",
      "Iteration 108, loss = 1.04183173\n",
      "Iteration 109, loss = 1.03795999\n",
      "Iteration 110, loss = 1.03416718\n",
      "Iteration 111, loss = 1.03034994\n",
      "Iteration 112, loss = 1.02659438\n",
      "Iteration 113, loss = 1.02285349\n",
      "Iteration 114, loss = 1.01907663\n",
      "Iteration 115, loss = 1.01542176\n",
      "Iteration 116, loss = 1.01172022\n",
      "Iteration 117, loss = 1.00813556\n",
      "Iteration 118, loss = 1.00448769\n",
      "Iteration 119, loss = 1.00099366\n",
      "Iteration 120, loss = 0.99735103\n",
      "Iteration 121, loss = 0.99391411\n",
      "Iteration 122, loss = 0.99035270\n",
      "Iteration 123, loss = 0.98687647\n",
      "Iteration 124, loss = 0.98346675\n",
      "Iteration 125, loss = 0.97997670\n",
      "Iteration 126, loss = 0.97669078\n",
      "Iteration 127, loss = 0.97333859\n",
      "Iteration 128, loss = 0.96994400\n",
      "Iteration 129, loss = 0.96667676\n",
      "Iteration 130, loss = 0.96333245\n",
      "Iteration 131, loss = 0.96005703\n",
      "Iteration 132, loss = 0.95665784\n",
      "Iteration 133, loss = 0.95336961\n",
      "Iteration 134, loss = 0.95005614\n",
      "Iteration 135, loss = 0.94680726\n",
      "Iteration 136, loss = 0.94349269\n",
      "Iteration 137, loss = 0.94031178\n",
      "Iteration 138, loss = 0.93714359\n",
      "Iteration 139, loss = 0.93389758\n",
      "Iteration 140, loss = 0.93059384\n",
      "Iteration 141, loss = 0.92750561\n",
      "Iteration 142, loss = 0.92431922\n",
      "Iteration 143, loss = 0.92113870\n",
      "Iteration 144, loss = 0.91792409\n",
      "Iteration 145, loss = 0.91480131\n",
      "Iteration 146, loss = 0.91173450\n",
      "Iteration 147, loss = 0.90860262\n",
      "Iteration 148, loss = 0.90553401\n",
      "Iteration 149, loss = 0.90246045\n",
      "Iteration 150, loss = 0.89945254\n",
      "Iteration 151, loss = 0.89645469\n",
      "Iteration 152, loss = 0.89350137\n",
      "Iteration 153, loss = 0.89044747\n",
      "Iteration 154, loss = 0.88757021\n",
      "Iteration 155, loss = 0.88455193\n",
      "Iteration 156, loss = 0.88172885\n",
      "Iteration 157, loss = 0.87867378\n",
      "Iteration 158, loss = 0.87596216\n",
      "Iteration 159, loss = 0.87303547\n",
      "Iteration 160, loss = 0.87019006\n",
      "Iteration 161, loss = 0.86738158\n",
      "Iteration 162, loss = 0.86466538\n",
      "Iteration 163, loss = 0.86185117\n",
      "Iteration 164, loss = 0.85909828\n",
      "Iteration 165, loss = 0.85633071\n",
      "Iteration 166, loss = 0.85362424\n",
      "Iteration 167, loss = 0.85094897\n",
      "Iteration 168, loss = 0.84826225\n",
      "Iteration 169, loss = 0.84556235\n",
      "Iteration 170, loss = 0.84295438\n",
      "Iteration 171, loss = 0.84022312\n",
      "Iteration 172, loss = 0.83767998\n",
      "Iteration 173, loss = 0.83498976\n",
      "Iteration 174, loss = 0.83240412\n",
      "Iteration 175, loss = 0.82977357\n",
      "Iteration 176, loss = 0.82725400\n",
      "Iteration 177, loss = 0.82473830\n",
      "Iteration 178, loss = 0.82216288\n",
      "Iteration 179, loss = 0.81981057\n",
      "Iteration 180, loss = 0.81727239\n",
      "Iteration 181, loss = 0.81488185\n",
      "Iteration 182, loss = 0.81239247\n",
      "Iteration 183, loss = 0.81004778\n",
      "Iteration 184, loss = 0.80766047\n",
      "Iteration 185, loss = 0.80513945\n",
      "Iteration 186, loss = 0.80287338\n",
      "Iteration 187, loss = 0.80044302\n",
      "Iteration 188, loss = 0.79814668\n",
      "Iteration 189, loss = 0.79580298\n",
      "Iteration 190, loss = 0.79346238\n",
      "Iteration 191, loss = 0.79111053\n",
      "Iteration 192, loss = 0.78888817\n",
      "Iteration 193, loss = 0.78651249\n",
      "Iteration 194, loss = 0.78425408\n",
      "Iteration 195, loss = 0.78196331\n",
      "Iteration 196, loss = 0.77975890\n",
      "Iteration 197, loss = 0.77745109\n",
      "Iteration 198, loss = 0.77533106\n",
      "Iteration 199, loss = 0.77310832\n",
      "Iteration 200, loss = 0.77090931\n",
      "Iteration 201, loss = 0.76882786\n",
      "Iteration 202, loss = 0.76662214\n",
      "Iteration 203, loss = 0.76463248\n",
      "Iteration 204, loss = 0.76250376\n",
      "Iteration 205, loss = 0.76048603\n",
      "Iteration 206, loss = 0.75845585\n",
      "Iteration 207, loss = 0.75646751\n",
      "Iteration 208, loss = 0.75451463\n",
      "Iteration 209, loss = 0.75255175\n",
      "Iteration 210, loss = 0.75050406\n",
      "Iteration 211, loss = 0.74861802\n",
      "Iteration 212, loss = 0.74665037\n",
      "Iteration 213, loss = 0.74468345\n",
      "Iteration 214, loss = 0.74278632\n",
      "Iteration 215, loss = 0.74079148\n",
      "Iteration 216, loss = 0.73886792\n",
      "Iteration 217, loss = 0.73693493\n",
      "Iteration 218, loss = 0.73495536\n",
      "Iteration 219, loss = 0.73303231\n",
      "Iteration 220, loss = 0.73118185\n",
      "Iteration 221, loss = 0.72924335\n",
      "Iteration 222, loss = 0.72750503\n",
      "Iteration 223, loss = 0.72564373\n",
      "Iteration 224, loss = 0.72384513\n",
      "Iteration 225, loss = 0.72201744\n",
      "Iteration 226, loss = 0.72029506\n",
      "Iteration 227, loss = 0.71850192\n",
      "Iteration 228, loss = 0.71672141\n",
      "Iteration 229, loss = 0.71491912\n",
      "Iteration 230, loss = 0.71328205\n",
      "Iteration 231, loss = 0.71148938\n",
      "Iteration 232, loss = 0.70982170\n",
      "Iteration 233, loss = 0.70806622\n",
      "Iteration 234, loss = 0.70636139\n",
      "Iteration 235, loss = 0.70462661\n",
      "Iteration 236, loss = 0.70294757\n",
      "Iteration 237, loss = 0.70126560\n",
      "Iteration 238, loss = 0.69960990\n",
      "Iteration 239, loss = 0.69790053\n",
      "Iteration 240, loss = 0.69631961\n",
      "Iteration 241, loss = 0.69479933\n",
      "Iteration 242, loss = 0.69318196\n",
      "Iteration 243, loss = 0.69161368\n",
      "Iteration 244, loss = 0.69004297\n",
      "Iteration 245, loss = 0.68851623\n",
      "Iteration 246, loss = 0.68694851\n",
      "Iteration 247, loss = 0.68543127\n",
      "Iteration 248, loss = 0.68393451\n",
      "Iteration 249, loss = 0.68234671\n",
      "Iteration 250, loss = 0.68089657\n",
      "Iteration 251, loss = 0.67932611\n",
      "Iteration 252, loss = 0.67782495\n",
      "Iteration 253, loss = 0.67637062\n",
      "Iteration 254, loss = 0.67487601\n",
      "Iteration 255, loss = 0.67332720\n",
      "Iteration 256, loss = 0.67194673\n",
      "Iteration 257, loss = 0.67053997\n",
      "Iteration 258, loss = 0.66912060\n",
      "Iteration 259, loss = 0.66779533\n",
      "Iteration 260, loss = 0.66647593\n",
      "Iteration 261, loss = 0.66521003\n",
      "Iteration 262, loss = 0.66393464\n",
      "Iteration 263, loss = 0.66265620\n",
      "Iteration 264, loss = 0.66146083\n",
      "Iteration 265, loss = 0.66019081\n",
      "Iteration 266, loss = 0.65893792\n",
      "Iteration 267, loss = 0.65770093\n",
      "Iteration 268, loss = 0.65644504\n",
      "Iteration 269, loss = 0.65522269\n",
      "Iteration 270, loss = 0.65398755\n",
      "Iteration 271, loss = 0.65269473\n",
      "Iteration 272, loss = 0.65153872\n",
      "Iteration 273, loss = 0.65027473\n",
      "Iteration 274, loss = 0.64899464\n",
      "Iteration 275, loss = 0.64782838\n",
      "Iteration 276, loss = 0.64658069\n",
      "Iteration 277, loss = 0.64546895\n",
      "Iteration 278, loss = 0.64421982\n",
      "Iteration 279, loss = 0.64312560\n",
      "Iteration 280, loss = 0.64206034\n",
      "Iteration 281, loss = 0.64092936\n",
      "Iteration 282, loss = 0.63990608\n",
      "Iteration 283, loss = 0.63882316\n",
      "Iteration 284, loss = 0.63787192\n",
      "Iteration 285, loss = 0.63676331\n",
      "Iteration 286, loss = 0.63575964\n",
      "Iteration 287, loss = 0.63471431\n",
      "Iteration 288, loss = 0.63373627\n",
      "Iteration 289, loss = 0.63268456\n",
      "Iteration 290, loss = 0.63166254\n",
      "Iteration 291, loss = 0.63071826\n",
      "Iteration 292, loss = 0.62970708\n",
      "Iteration 293, loss = 0.62878628\n",
      "Iteration 294, loss = 0.62781281\n",
      "Iteration 295, loss = 0.62690821\n",
      "Iteration 296, loss = 0.62597189\n",
      "Iteration 297, loss = 0.62507265\n",
      "Iteration 298, loss = 0.62419410\n",
      "Iteration 299, loss = 0.62331424\n",
      "Iteration 300, loss = 0.62247824\n",
      "Iteration 301, loss = 0.62157748\n",
      "Iteration 302, loss = 0.62075057\n",
      "Iteration 303, loss = 0.62003723\n",
      "Iteration 304, loss = 0.61916729\n",
      "Iteration 305, loss = 0.61845321\n",
      "Iteration 306, loss = 0.61769944\n",
      "Iteration 307, loss = 0.61694725\n",
      "Iteration 308, loss = 0.61619555\n",
      "Iteration 309, loss = 0.61547696\n",
      "Iteration 310, loss = 0.61476293\n",
      "Iteration 311, loss = 0.61404336\n",
      "Iteration 312, loss = 0.61336394\n",
      "Iteration 313, loss = 0.61260009\n",
      "Iteration 314, loss = 0.61194309\n",
      "Iteration 315, loss = 0.61120461\n",
      "Iteration 316, loss = 0.61051203\n",
      "Iteration 317, loss = 0.60986230\n",
      "Iteration 318, loss = 0.60918565\n",
      "Iteration 319, loss = 0.60847225\n",
      "Iteration 320, loss = 0.60791815\n",
      "Iteration 321, loss = 0.60730516\n",
      "Iteration 322, loss = 0.60668252\n",
      "Iteration 323, loss = 0.60609202\n",
      "Iteration 324, loss = 0.60548208\n",
      "Iteration 325, loss = 0.60489488\n",
      "Iteration 326, loss = 0.60430164\n",
      "Iteration 327, loss = 0.60367216\n",
      "Iteration 328, loss = 0.60306112\n",
      "Iteration 329, loss = 0.60247731\n",
      "Iteration 330, loss = 0.60188072\n",
      "Iteration 331, loss = 0.60126566\n",
      "Iteration 332, loss = 0.60069081\n",
      "Iteration 333, loss = 0.60007484\n",
      "Iteration 334, loss = 0.59949441\n",
      "Iteration 335, loss = 0.59894356\n",
      "Iteration 336, loss = 0.59836639\n",
      "Iteration 337, loss = 0.59780173\n",
      "Iteration 338, loss = 0.59722102\n",
      "Iteration 339, loss = 0.59670528\n",
      "Iteration 340, loss = 0.59621439\n",
      "Iteration 341, loss = 0.59562428\n",
      "Iteration 342, loss = 0.59509732\n",
      "Iteration 343, loss = 0.59453006\n",
      "Iteration 344, loss = 0.59405777\n",
      "Iteration 345, loss = 0.59354031\n",
      "Iteration 346, loss = 0.59299177\n",
      "Iteration 347, loss = 0.59248128\n",
      "Iteration 348, loss = 0.59202853\n",
      "Iteration 349, loss = 0.59157536\n",
      "Iteration 350, loss = 0.59110362\n",
      "Iteration 351, loss = 0.59063601\n",
      "Iteration 352, loss = 0.59024102\n",
      "Iteration 353, loss = 0.58985569\n",
      "Iteration 354, loss = 0.58940031\n",
      "Iteration 355, loss = 0.58902367\n",
      "Iteration 356, loss = 0.58861654\n",
      "Iteration 357, loss = 0.58821103\n",
      "Iteration 358, loss = 0.58784394\n",
      "Iteration 359, loss = 0.58745049\n",
      "Iteration 360, loss = 0.58708704\n",
      "Iteration 361, loss = 0.58674086\n",
      "Iteration 362, loss = 0.58638288\n",
      "Iteration 363, loss = 0.58602346\n",
      "Iteration 364, loss = 0.58570029\n",
      "Iteration 365, loss = 0.58536151\n",
      "Iteration 366, loss = 0.58500216\n",
      "Iteration 367, loss = 0.58466195\n",
      "Iteration 368, loss = 0.58433917\n",
      "Iteration 369, loss = 0.58397700\n",
      "Iteration 370, loss = 0.58360193\n",
      "Iteration 371, loss = 0.58335064\n",
      "Iteration 372, loss = 0.58298170\n",
      "Iteration 373, loss = 0.58263899\n",
      "Iteration 374, loss = 0.58232797\n",
      "Iteration 375, loss = 0.58208235\n",
      "Iteration 376, loss = 0.58173801\n",
      "Iteration 377, loss = 0.58142961\n",
      "Iteration 378, loss = 0.58115800\n",
      "Iteration 379, loss = 0.58088656\n",
      "Iteration 380, loss = 0.58060482\n",
      "Iteration 381, loss = 0.58034904\n",
      "Iteration 382, loss = 0.58008994\n",
      "Iteration 383, loss = 0.57984860\n",
      "Iteration 384, loss = 0.57962246\n",
      "Iteration 385, loss = 0.57939832\n",
      "Iteration 386, loss = 0.57919170\n",
      "Iteration 387, loss = 0.57893806\n",
      "Iteration 388, loss = 0.57872193\n",
      "Iteration 389, loss = 0.57852550\n",
      "Iteration 390, loss = 0.57832935\n",
      "Iteration 391, loss = 0.57812150\n",
      "Iteration 392, loss = 0.57793568\n",
      "Iteration 393, loss = 0.57772745\n",
      "Iteration 394, loss = 0.57758456\n",
      "Iteration 395, loss = 0.57738756\n",
      "Iteration 396, loss = 0.57720206\n",
      "Iteration 397, loss = 0.57702836\n",
      "Iteration 398, loss = 0.57685608\n",
      "Iteration 399, loss = 0.57670952\n",
      "Iteration 400, loss = 0.57651055\n",
      "Iteration 401, loss = 0.57639333\n",
      "Iteration 402, loss = 0.57624690\n",
      "Iteration 403, loss = 0.57609772\n",
      "Iteration 404, loss = 0.57594826\n",
      "Iteration 405, loss = 0.57583472\n",
      "Iteration 406, loss = 0.57567667\n",
      "Iteration 407, loss = 0.57556087\n",
      "Iteration 408, loss = 0.57542883\n",
      "Iteration 409, loss = 0.57529333\n",
      "Iteration 410, loss = 0.57515309\n",
      "Iteration 411, loss = 0.57504299\n",
      "Iteration 412, loss = 0.57490036\n",
      "Iteration 413, loss = 0.57475796\n",
      "Iteration 414, loss = 0.57462316\n",
      "Iteration 415, loss = 0.57447656\n",
      "Iteration 416, loss = 0.57434942\n",
      "Iteration 417, loss = 0.57421034\n",
      "Iteration 418, loss = 0.57403103\n",
      "Iteration 419, loss = 0.57388537\n",
      "Iteration 420, loss = 0.57375043\n",
      "Iteration 421, loss = 0.57357755\n",
      "Iteration 422, loss = 0.57341251\n",
      "Iteration 423, loss = 0.57328743\n",
      "Iteration 424, loss = 0.57309600\n",
      "Iteration 425, loss = 0.57296232\n",
      "Iteration 426, loss = 0.57283333\n",
      "Iteration 427, loss = 0.57267678\n",
      "Iteration 428, loss = 0.57252660\n",
      "Iteration 429, loss = 0.57241047\n",
      "Iteration 430, loss = 0.57230645\n",
      "Iteration 431, loss = 0.57217243\n",
      "Iteration 432, loss = 0.57203315\n",
      "Iteration 433, loss = 0.57192071\n",
      "Iteration 434, loss = 0.57181168\n",
      "Iteration 435, loss = 0.57167574\n",
      "Iteration 436, loss = 0.57155989\n",
      "Iteration 437, loss = 0.57144983\n",
      "Iteration 438, loss = 0.57134580\n",
      "Iteration 439, loss = 0.57123368\n",
      "Iteration 440, loss = 0.57112340\n",
      "Iteration 441, loss = 0.57101906\n",
      "Iteration 442, loss = 0.57094558\n",
      "Iteration 443, loss = 0.57079975\n",
      "Iteration 444, loss = 0.57070120\n",
      "Iteration 445, loss = 0.57062671\n",
      "Iteration 446, loss = 0.57048522\n",
      "Iteration 447, loss = 0.57041659\n",
      "Iteration 448, loss = 0.57032837\n",
      "Iteration 449, loss = 0.57023181\n",
      "Iteration 450, loss = 0.57013806\n",
      "Iteration 451, loss = 0.57006189\n",
      "Iteration 452, loss = 0.56997604\n",
      "Iteration 453, loss = 0.56990650\n",
      "Iteration 454, loss = 0.56983796\n",
      "Iteration 455, loss = 0.56976749\n",
      "Iteration 456, loss = 0.56970148\n",
      "Iteration 457, loss = 0.56963548\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.92097666\n",
      "Iteration 2, loss = 0.91860141\n",
      "Iteration 3, loss = 0.91613423\n",
      "Iteration 4, loss = 0.91377279\n",
      "Iteration 5, loss = 0.91136977\n",
      "Iteration 6, loss = 0.90905033\n",
      "Iteration 7, loss = 0.90675890\n",
      "Iteration 8, loss = 0.90446265\n",
      "Iteration 9, loss = 0.90220543\n",
      "Iteration 10, loss = 0.89997327\n",
      "Iteration 11, loss = 0.89775076\n",
      "Iteration 12, loss = 0.89548153\n",
      "Iteration 13, loss = 0.89336268\n",
      "Iteration 14, loss = 0.89114597\n",
      "Iteration 15, loss = 0.88898342\n",
      "Iteration 16, loss = 0.88679035\n",
      "Iteration 17, loss = 0.88453411\n",
      "Iteration 18, loss = 0.88247349\n",
      "Iteration 19, loss = 0.88016766\n",
      "Iteration 20, loss = 0.87803612\n",
      "Iteration 21, loss = 0.87583803\n",
      "Iteration 22, loss = 0.87373413\n",
      "Iteration 23, loss = 0.87157350\n",
      "Iteration 24, loss = 0.86950118\n",
      "Iteration 25, loss = 0.86740474\n",
      "Iteration 26, loss = 0.86537732\n",
      "Iteration 27, loss = 0.86325226\n",
      "Iteration 28, loss = 0.86118638\n",
      "Iteration 29, loss = 0.85912808\n",
      "Iteration 30, loss = 0.85710010\n",
      "Iteration 31, loss = 0.85506083\n",
      "Iteration 32, loss = 0.85305978\n",
      "Iteration 33, loss = 0.85101536\n",
      "Iteration 34, loss = 0.84908553\n",
      "Iteration 35, loss = 0.84699778\n",
      "Iteration 36, loss = 0.84511326\n",
      "Iteration 37, loss = 0.84310246\n",
      "Iteration 38, loss = 0.84122455\n",
      "Iteration 39, loss = 0.83927852\n",
      "Iteration 40, loss = 0.83725777\n",
      "Iteration 41, loss = 0.83542698\n",
      "Iteration 42, loss = 0.83359920\n",
      "Iteration 43, loss = 0.83168179\n",
      "Iteration 44, loss = 0.82982879\n",
      "Iteration 45, loss = 0.82803149\n",
      "Iteration 46, loss = 0.82625012\n",
      "Iteration 47, loss = 0.82445257\n",
      "Iteration 48, loss = 0.82270401\n",
      "Iteration 49, loss = 0.82091107\n",
      "Iteration 50, loss = 0.81921438\n",
      "Iteration 51, loss = 0.81749546\n",
      "Iteration 52, loss = 0.81568708\n",
      "Iteration 53, loss = 0.81396081\n",
      "Iteration 54, loss = 0.81217106\n",
      "Iteration 55, loss = 0.81042204\n",
      "Iteration 56, loss = 0.80868343\n",
      "Iteration 57, loss = 0.80693312\n",
      "Iteration 58, loss = 0.80516192\n",
      "Iteration 59, loss = 0.80349627\n",
      "Iteration 60, loss = 0.80179049\n",
      "Iteration 61, loss = 0.80013126\n",
      "Iteration 62, loss = 0.79843175\n",
      "Iteration 63, loss = 0.79683458\n",
      "Iteration 64, loss = 0.79508625\n",
      "Iteration 65, loss = 0.79342783\n",
      "Iteration 66, loss = 0.79175771\n",
      "Iteration 67, loss = 0.79009065\n",
      "Iteration 68, loss = 0.78839620\n",
      "Iteration 69, loss = 0.78676093\n",
      "Iteration 70, loss = 0.78509230\n",
      "Iteration 71, loss = 0.78352775\n",
      "Iteration 72, loss = 0.78193558\n",
      "Iteration 73, loss = 0.78032106\n",
      "Iteration 74, loss = 0.77875762\n",
      "Iteration 75, loss = 0.77722409\n",
      "Iteration 76, loss = 0.77564837\n",
      "Iteration 77, loss = 0.77416225\n",
      "Iteration 78, loss = 0.77253110\n",
      "Iteration 79, loss = 0.77103201\n",
      "Iteration 80, loss = 0.76947245\n",
      "Iteration 81, loss = 0.76795445\n",
      "Iteration 82, loss = 0.76644970\n",
      "Iteration 83, loss = 0.76493068\n",
      "Iteration 84, loss = 0.76343181\n",
      "Iteration 85, loss = 0.76193120\n",
      "Iteration 86, loss = 0.76049568\n",
      "Iteration 87, loss = 0.75905153\n",
      "Iteration 88, loss = 0.75754754\n",
      "Iteration 89, loss = 0.75613321\n",
      "Iteration 90, loss = 0.75467591\n",
      "Iteration 91, loss = 0.75330817\n",
      "Iteration 92, loss = 0.75186494\n",
      "Iteration 93, loss = 0.75049880\n",
      "Iteration 94, loss = 0.74907312\n",
      "Iteration 95, loss = 0.74768966\n",
      "Iteration 96, loss = 0.74633189\n",
      "Iteration 97, loss = 0.74503700\n",
      "Iteration 98, loss = 0.74362094\n",
      "Iteration 99, loss = 0.74232926\n",
      "Iteration 100, loss = 0.74100568\n",
      "Iteration 101, loss = 0.73973262\n",
      "Iteration 102, loss = 0.73829908\n",
      "Iteration 103, loss = 0.73698560\n",
      "Iteration 104, loss = 0.73563897\n",
      "Iteration 105, loss = 0.73428761\n",
      "Iteration 106, loss = 0.73292746\n",
      "Iteration 107, loss = 0.73158095\n",
      "Iteration 108, loss = 0.73022474\n",
      "Iteration 109, loss = 0.72886927\n",
      "Iteration 110, loss = 0.72752066\n",
      "Iteration 111, loss = 0.72619389\n",
      "Iteration 112, loss = 0.72484719\n",
      "Iteration 113, loss = 0.72345176\n",
      "Iteration 114, loss = 0.72216425\n",
      "Iteration 115, loss = 0.72077958\n",
      "Iteration 116, loss = 0.71957082\n",
      "Iteration 117, loss = 0.71823247\n",
      "Iteration 118, loss = 0.71698654\n",
      "Iteration 119, loss = 0.71573429\n",
      "Iteration 120, loss = 0.71452246\n",
      "Iteration 121, loss = 0.71329283\n",
      "Iteration 122, loss = 0.71203953\n",
      "Iteration 123, loss = 0.71083534\n",
      "Iteration 124, loss = 0.70959421\n",
      "Iteration 125, loss = 0.70835020\n",
      "Iteration 126, loss = 0.70706216\n",
      "Iteration 127, loss = 0.70588219\n",
      "Iteration 128, loss = 0.70461211\n",
      "Iteration 129, loss = 0.70333032\n",
      "Iteration 130, loss = 0.70200854\n",
      "Iteration 131, loss = 0.70087182\n",
      "Iteration 132, loss = 0.69963076\n",
      "Iteration 133, loss = 0.69839002\n",
      "Iteration 134, loss = 0.69715155\n",
      "Iteration 135, loss = 0.69599533\n",
      "Iteration 136, loss = 0.69484137\n",
      "Iteration 137, loss = 0.69366547\n",
      "Iteration 138, loss = 0.69249924\n",
      "Iteration 139, loss = 0.69124235\n",
      "Iteration 140, loss = 0.69017324\n",
      "Iteration 141, loss = 0.68898852\n",
      "Iteration 142, loss = 0.68781756\n",
      "Iteration 143, loss = 0.68669415\n",
      "Iteration 144, loss = 0.68558703\n",
      "Iteration 145, loss = 0.68451040\n",
      "Iteration 146, loss = 0.68340723\n",
      "Iteration 147, loss = 0.68233045\n",
      "Iteration 148, loss = 0.68123789\n",
      "Iteration 149, loss = 0.68015909\n",
      "Iteration 150, loss = 0.67903567\n",
      "Iteration 151, loss = 0.67784538\n",
      "Iteration 152, loss = 0.67673415\n",
      "Iteration 153, loss = 0.67551621\n",
      "Iteration 154, loss = 0.67430182\n",
      "Iteration 155, loss = 0.67311775\n",
      "Iteration 156, loss = 0.67193396\n",
      "Iteration 157, loss = 0.67068390\n",
      "Iteration 158, loss = 0.66951499\n",
      "Iteration 159, loss = 0.66825586\n",
      "Iteration 160, loss = 0.66700361\n",
      "Iteration 161, loss = 0.66579322\n",
      "Iteration 162, loss = 0.66446069\n",
      "Iteration 163, loss = 0.66325951\n",
      "Iteration 164, loss = 0.66193302\n",
      "Iteration 165, loss = 0.66070015\n",
      "Iteration 166, loss = 0.65942267\n",
      "Iteration 167, loss = 0.65824355\n",
      "Iteration 168, loss = 0.65695841\n",
      "Iteration 169, loss = 0.65578308\n",
      "Iteration 170, loss = 0.65457966\n",
      "Iteration 171, loss = 0.65343518\n",
      "Iteration 172, loss = 0.65222258\n",
      "Iteration 173, loss = 0.65106823\n",
      "Iteration 174, loss = 0.64998575\n",
      "Iteration 175, loss = 0.64884606\n",
      "Iteration 176, loss = 0.64789453\n",
      "Iteration 177, loss = 0.64679164\n",
      "Iteration 178, loss = 0.64581279\n",
      "Iteration 179, loss = 0.64484574\n",
      "Iteration 180, loss = 0.64379143\n",
      "Iteration 181, loss = 0.64291943\n",
      "Iteration 182, loss = 0.64188124\n",
      "Iteration 183, loss = 0.64097855\n",
      "Iteration 184, loss = 0.64004959\n",
      "Iteration 185, loss = 0.63911624\n",
      "Iteration 186, loss = 0.63820520\n",
      "Iteration 187, loss = 0.63731340\n",
      "Iteration 188, loss = 0.63646445\n",
      "Iteration 189, loss = 0.63558402\n",
      "Iteration 190, loss = 0.63477274\n",
      "Iteration 191, loss = 0.63390137\n",
      "Iteration 192, loss = 0.63315740\n",
      "Iteration 193, loss = 0.63231927\n",
      "Iteration 194, loss = 0.63155813\n",
      "Iteration 195, loss = 0.63072757\n",
      "Iteration 196, loss = 0.63002119\n",
      "Iteration 197, loss = 0.62921256\n",
      "Iteration 198, loss = 0.62842314\n",
      "Iteration 199, loss = 0.62762695\n",
      "Iteration 200, loss = 0.62676640\n",
      "Iteration 201, loss = 0.62595119\n",
      "Iteration 202, loss = 0.62515076\n",
      "Iteration 203, loss = 0.62422489\n",
      "Iteration 204, loss = 0.62339887\n",
      "Iteration 205, loss = 0.62255028\n",
      "Iteration 206, loss = 0.62164758\n",
      "Iteration 207, loss = 0.62081015\n",
      "Iteration 208, loss = 0.62002411\n",
      "Iteration 209, loss = 0.61922106\n",
      "Iteration 210, loss = 0.61850045\n",
      "Iteration 211, loss = 0.61772726\n",
      "Iteration 212, loss = 0.61698004\n",
      "Iteration 213, loss = 0.61637891\n",
      "Iteration 214, loss = 0.61560019\n",
      "Iteration 215, loss = 0.61493709\n",
      "Iteration 216, loss = 0.61414672\n",
      "Iteration 217, loss = 0.61347906\n",
      "Iteration 218, loss = 0.61277041\n",
      "Iteration 219, loss = 0.61207232\n",
      "Iteration 220, loss = 0.61134436\n",
      "Iteration 221, loss = 0.61071768\n",
      "Iteration 222, loss = 0.61003986\n",
      "Iteration 223, loss = 0.60939882\n",
      "Iteration 224, loss = 0.60879109\n",
      "Iteration 225, loss = 0.60807754\n",
      "Iteration 226, loss = 0.60750162\n",
      "Iteration 227, loss = 0.60684049\n",
      "Iteration 228, loss = 0.60628524\n",
      "Iteration 229, loss = 0.60560806\n",
      "Iteration 230, loss = 0.60507543\n",
      "Iteration 231, loss = 0.60442866\n",
      "Iteration 232, loss = 0.60385897\n",
      "Iteration 233, loss = 0.60327911\n",
      "Iteration 234, loss = 0.60273884\n",
      "Iteration 235, loss = 0.60208812\n",
      "Iteration 236, loss = 0.60158212\n",
      "Iteration 237, loss = 0.60097897\n",
      "Iteration 238, loss = 0.60044678\n",
      "Iteration 239, loss = 0.59993936\n",
      "Iteration 240, loss = 0.59948598\n",
      "Iteration 241, loss = 0.59897824\n",
      "Iteration 242, loss = 0.59850004\n",
      "Iteration 243, loss = 0.59809887\n",
      "Iteration 244, loss = 0.59763173\n",
      "Iteration 245, loss = 0.59724139\n",
      "Iteration 246, loss = 0.59680588\n",
      "Iteration 247, loss = 0.59635842\n",
      "Iteration 248, loss = 0.59594267\n",
      "Iteration 249, loss = 0.59555645\n",
      "Iteration 250, loss = 0.59512330\n",
      "Iteration 251, loss = 0.59478173\n",
      "Iteration 252, loss = 0.59440265\n",
      "Iteration 253, loss = 0.59399152\n",
      "Iteration 254, loss = 0.59371301\n",
      "Iteration 255, loss = 0.59335094\n",
      "Iteration 256, loss = 0.59300392\n",
      "Iteration 257, loss = 0.59271915\n",
      "Iteration 258, loss = 0.59241095\n",
      "Iteration 259, loss = 0.59208234\n",
      "Iteration 260, loss = 0.59177683\n",
      "Iteration 261, loss = 0.59152115\n",
      "Iteration 262, loss = 0.59120600\n",
      "Iteration 263, loss = 0.59091340\n",
      "Iteration 264, loss = 0.59066408\n",
      "Iteration 265, loss = 0.59042106\n",
      "Iteration 266, loss = 0.59016488\n",
      "Iteration 267, loss = 0.58995179\n",
      "Iteration 268, loss = 0.58973297\n",
      "Iteration 269, loss = 0.58948662\n",
      "Iteration 270, loss = 0.58927719\n",
      "Iteration 271, loss = 0.58906862\n",
      "Iteration 272, loss = 0.58889074\n",
      "Iteration 273, loss = 0.58864175\n",
      "Iteration 274, loss = 0.58848186\n",
      "Iteration 275, loss = 0.58832297\n",
      "Iteration 276, loss = 0.58810594\n",
      "Iteration 277, loss = 0.58798168\n",
      "Iteration 278, loss = 0.58779844\n",
      "Iteration 279, loss = 0.58762529\n",
      "Iteration 280, loss = 0.58746024\n",
      "Iteration 281, loss = 0.58730134\n",
      "Iteration 282, loss = 0.58715155\n",
      "Iteration 283, loss = 0.58694871\n",
      "Iteration 284, loss = 0.58678580\n",
      "Iteration 285, loss = 0.58663823\n",
      "Iteration 286, loss = 0.58650506\n",
      "Iteration 287, loss = 0.58630691\n",
      "Iteration 288, loss = 0.58616998\n",
      "Iteration 289, loss = 0.58605116\n",
      "Iteration 290, loss = 0.58592567\n",
      "Iteration 291, loss = 0.58578830\n",
      "Iteration 292, loss = 0.58573300\n",
      "Iteration 293, loss = 0.58558618\n",
      "Iteration 294, loss = 0.58549565\n",
      "Iteration 295, loss = 0.58538794\n",
      "Iteration 296, loss = 0.58528822\n",
      "Iteration 297, loss = 0.58517743\n",
      "Iteration 298, loss = 0.58506005\n",
      "Iteration 299, loss = 0.58494692\n",
      "Iteration 300, loss = 0.58486223\n",
      "Iteration 301, loss = 0.58473774\n",
      "Iteration 302, loss = 0.58462156\n",
      "Iteration 303, loss = 0.58450759\n",
      "Iteration 304, loss = 0.58442679\n",
      "Iteration 305, loss = 0.58429590\n",
      "Iteration 306, loss = 0.58418068\n",
      "Iteration 307, loss = 0.58408976\n",
      "Iteration 308, loss = 0.58398584\n",
      "Iteration 309, loss = 0.58386035\n",
      "Iteration 310, loss = 0.58375055\n",
      "Iteration 311, loss = 0.58366510\n",
      "Iteration 312, loss = 0.58356415\n",
      "Iteration 313, loss = 0.58345379\n",
      "Iteration 314, loss = 0.58335478\n",
      "Iteration 315, loss = 0.58326853\n",
      "Iteration 316, loss = 0.58321600\n",
      "Iteration 317, loss = 0.58311743\n",
      "Iteration 318, loss = 0.58306797\n",
      "Iteration 319, loss = 0.58298854\n",
      "Iteration 320, loss = 0.58294838\n",
      "Iteration 321, loss = 0.58284615\n",
      "Iteration 322, loss = 0.58278918\n",
      "Iteration 323, loss = 0.58271497\n",
      "Iteration 324, loss = 0.58262783\n",
      "Iteration 325, loss = 0.58254521\n",
      "Iteration 326, loss = 0.58248168\n",
      "Iteration 327, loss = 0.58242725\n",
      "Iteration 328, loss = 0.58234386\n",
      "Iteration 329, loss = 0.58228136\n",
      "Iteration 330, loss = 0.58223969\n",
      "Iteration 331, loss = 0.58218521\n",
      "Iteration 332, loss = 0.58213204\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.64007234\n",
      "Iteration 2, loss = 0.63835406\n",
      "Iteration 3, loss = 0.63663952\n",
      "Iteration 4, loss = 0.63501484\n",
      "Iteration 5, loss = 0.63340950\n",
      "Iteration 6, loss = 0.63173480\n",
      "Iteration 7, loss = 0.63010807\n",
      "Iteration 8, loss = 0.62853682\n",
      "Iteration 9, loss = 0.62699749\n",
      "Iteration 10, loss = 0.62543579\n",
      "Iteration 11, loss = 0.62389013\n",
      "Iteration 12, loss = 0.62240821\n",
      "Iteration 13, loss = 0.62093299\n",
      "Iteration 14, loss = 0.61949250\n",
      "Iteration 15, loss = 0.61807884\n",
      "Iteration 16, loss = 0.61664585\n",
      "Iteration 17, loss = 0.61517631\n",
      "Iteration 18, loss = 0.61388298\n",
      "Iteration 19, loss = 0.61248562\n",
      "Iteration 20, loss = 0.61116120\n",
      "Iteration 21, loss = 0.60978600\n",
      "Iteration 22, loss = 0.60853415\n",
      "Iteration 23, loss = 0.60725650\n",
      "Iteration 24, loss = 0.60594811\n",
      "Iteration 25, loss = 0.60482320\n",
      "Iteration 26, loss = 0.60358188\n",
      "Iteration 27, loss = 0.60231636\n",
      "Iteration 28, loss = 0.60127767\n",
      "Iteration 29, loss = 0.60006434\n",
      "Iteration 30, loss = 0.59893931\n",
      "Iteration 31, loss = 0.59784018\n",
      "Iteration 32, loss = 0.59679389\n",
      "Iteration 33, loss = 0.59572209\n",
      "Iteration 34, loss = 0.59466607\n",
      "Iteration 35, loss = 0.59371074\n",
      "Iteration 36, loss = 0.59275839\n",
      "Iteration 37, loss = 0.59177926\n",
      "Iteration 38, loss = 0.59089683\n",
      "Iteration 39, loss = 0.58990226\n",
      "Iteration 40, loss = 0.58912535\n",
      "Iteration 41, loss = 0.58819679\n",
      "Iteration 42, loss = 0.58733191\n",
      "Iteration 43, loss = 0.58655090\n",
      "Iteration 44, loss = 0.58564694\n",
      "Iteration 45, loss = 0.58480663\n",
      "Iteration 46, loss = 0.58403438\n",
      "Iteration 47, loss = 0.58320336\n",
      "Iteration 48, loss = 0.58247725\n",
      "Iteration 49, loss = 0.58167581\n",
      "Iteration 50, loss = 0.58087926\n",
      "Iteration 51, loss = 0.58015086\n",
      "Iteration 52, loss = 0.57940637\n",
      "Iteration 53, loss = 0.57861766\n",
      "Iteration 54, loss = 0.57787893\n",
      "Iteration 55, loss = 0.57707967\n",
      "Iteration 56, loss = 0.57637237\n",
      "Iteration 57, loss = 0.57561590\n",
      "Iteration 58, loss = 0.57491664\n",
      "Iteration 59, loss = 0.57413391\n",
      "Iteration 60, loss = 0.57344490\n",
      "Iteration 61, loss = 0.57279463\n",
      "Iteration 62, loss = 0.57211406\n",
      "Iteration 63, loss = 0.57144361\n",
      "Iteration 64, loss = 0.57084383\n",
      "Iteration 65, loss = 0.57024145\n",
      "Iteration 66, loss = 0.56967184\n",
      "Iteration 67, loss = 0.56905383\n",
      "Iteration 68, loss = 0.56850991\n",
      "Iteration 69, loss = 0.56790325\n",
      "Iteration 70, loss = 0.56742609\n",
      "Iteration 71, loss = 0.56685846\n",
      "Iteration 72, loss = 0.56632547\n",
      "Iteration 73, loss = 0.56584809\n",
      "Iteration 74, loss = 0.56532492\n",
      "Iteration 75, loss = 0.56488084\n",
      "Iteration 76, loss = 0.56436622\n",
      "Iteration 77, loss = 0.56389844\n",
      "Iteration 78, loss = 0.56337670\n",
      "Iteration 79, loss = 0.56292232\n",
      "Iteration 80, loss = 0.56245321\n",
      "Iteration 81, loss = 0.56192443\n",
      "Iteration 82, loss = 0.56146830\n",
      "Iteration 83, loss = 0.56095862\n",
      "Iteration 84, loss = 0.56045129\n",
      "Iteration 85, loss = 0.55997618\n",
      "Iteration 86, loss = 0.55953168\n",
      "Iteration 87, loss = 0.55902652\n",
      "Iteration 88, loss = 0.55862210\n",
      "Iteration 89, loss = 0.55812635\n",
      "Iteration 90, loss = 0.55770919\n",
      "Iteration 91, loss = 0.55727244\n",
      "Iteration 92, loss = 0.55687880\n",
      "Iteration 93, loss = 0.55650723\n",
      "Iteration 94, loss = 0.55608207\n",
      "Iteration 95, loss = 0.55572071\n",
      "Iteration 96, loss = 0.55534594\n",
      "Iteration 97, loss = 0.55498386\n",
      "Iteration 98, loss = 0.55463348\n",
      "Iteration 99, loss = 0.55427313\n",
      "Iteration 100, loss = 0.55384799\n",
      "Iteration 101, loss = 0.55355909\n",
      "Iteration 102, loss = 0.55319812\n",
      "Iteration 103, loss = 0.55282589\n",
      "Iteration 104, loss = 0.55257380\n",
      "Iteration 105, loss = 0.55223270\n",
      "Iteration 106, loss = 0.55196656\n",
      "Iteration 107, loss = 0.55172902\n",
      "Iteration 108, loss = 0.55141748\n",
      "Iteration 109, loss = 0.55115992\n",
      "Iteration 110, loss = 0.55091414\n",
      "Iteration 111, loss = 0.55066229\n",
      "Iteration 112, loss = 0.55035961\n",
      "Iteration 113, loss = 0.55014545\n",
      "Iteration 114, loss = 0.54991406\n",
      "Iteration 115, loss = 0.54969521\n",
      "Iteration 116, loss = 0.54945990\n",
      "Iteration 117, loss = 0.54922963\n",
      "Iteration 118, loss = 0.54901016\n",
      "Iteration 119, loss = 0.54880932\n",
      "Iteration 120, loss = 0.54863405\n",
      "Iteration 121, loss = 0.54837154\n",
      "Iteration 122, loss = 0.54820845\n",
      "Iteration 123, loss = 0.54799716\n",
      "Iteration 124, loss = 0.54783594\n",
      "Iteration 125, loss = 0.54762072\n",
      "Iteration 126, loss = 0.54743940\n",
      "Iteration 127, loss = 0.54724950\n",
      "Iteration 128, loss = 0.54708630\n",
      "Iteration 129, loss = 0.54689857\n",
      "Iteration 130, loss = 0.54671182\n",
      "Iteration 131, loss = 0.54650484\n",
      "Iteration 132, loss = 0.54635300\n",
      "Iteration 133, loss = 0.54613182\n",
      "Iteration 134, loss = 0.54598754\n",
      "Iteration 135, loss = 0.54577625\n",
      "Iteration 136, loss = 0.54560043\n",
      "Iteration 137, loss = 0.54542200\n",
      "Iteration 138, loss = 0.54523150\n",
      "Iteration 139, loss = 0.54508266\n",
      "Iteration 140, loss = 0.54490795\n",
      "Iteration 141, loss = 0.54474189\n",
      "Iteration 142, loss = 0.54457565\n",
      "Iteration 143, loss = 0.54440575\n",
      "Iteration 144, loss = 0.54430682\n",
      "Iteration 145, loss = 0.54412677\n",
      "Iteration 146, loss = 0.54398387\n",
      "Iteration 147, loss = 0.54383430\n",
      "Iteration 148, loss = 0.54369281\n",
      "Iteration 149, loss = 0.54356654\n",
      "Iteration 150, loss = 0.54342189\n",
      "Iteration 151, loss = 0.54332699\n",
      "Iteration 152, loss = 0.54319079\n",
      "Iteration 153, loss = 0.54310525\n",
      "Iteration 154, loss = 0.54299782\n",
      "Iteration 155, loss = 0.54290283\n",
      "Iteration 156, loss = 0.54283724\n",
      "Iteration 157, loss = 0.54273935\n",
      "Iteration 158, loss = 0.54266629\n",
      "Iteration 159, loss = 0.54259348\n",
      "Iteration 160, loss = 0.54251743\n",
      "Iteration 161, loss = 0.54245055\n",
      "Iteration 162, loss = 0.54238274\n",
      "Iteration 163, loss = 0.54232097\n",
      "Iteration 164, loss = 0.54223598\n",
      "Iteration 165, loss = 0.54218518\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66540597\n",
      "Iteration 2, loss = 0.66363812\n",
      "Iteration 3, loss = 0.66195546\n",
      "Iteration 4, loss = 0.66025615\n",
      "Iteration 5, loss = 0.65854534\n",
      "Iteration 6, loss = 0.65687908\n",
      "Iteration 7, loss = 0.65516280\n",
      "Iteration 8, loss = 0.65335323\n",
      "Iteration 9, loss = 0.65175957\n",
      "Iteration 10, loss = 0.65013358\n",
      "Iteration 11, loss = 0.64846398\n",
      "Iteration 12, loss = 0.64682853\n",
      "Iteration 13, loss = 0.64513133\n",
      "Iteration 14, loss = 0.64360359\n",
      "Iteration 15, loss = 0.64200332\n",
      "Iteration 16, loss = 0.64031957\n",
      "Iteration 17, loss = 0.63876695\n",
      "Iteration 18, loss = 0.63722207\n",
      "Iteration 19, loss = 0.63564644\n",
      "Iteration 20, loss = 0.63413627\n",
      "Iteration 21, loss = 0.63262291\n",
      "Iteration 22, loss = 0.63106972\n",
      "Iteration 23, loss = 0.62965787\n",
      "Iteration 24, loss = 0.62827148\n",
      "Iteration 25, loss = 0.62676285\n",
      "Iteration 26, loss = 0.62548498\n",
      "Iteration 27, loss = 0.62411329\n",
      "Iteration 28, loss = 0.62274779\n",
      "Iteration 29, loss = 0.62147684\n",
      "Iteration 30, loss = 0.62015754\n",
      "Iteration 31, loss = 0.61889650\n",
      "Iteration 32, loss = 0.61764318\n",
      "Iteration 33, loss = 0.61651599\n",
      "Iteration 34, loss = 0.61514983\n",
      "Iteration 35, loss = 0.61402163\n",
      "Iteration 36, loss = 0.61282085\n",
      "Iteration 37, loss = 0.61169660\n",
      "Iteration 38, loss = 0.61049435\n",
      "Iteration 39, loss = 0.60936860\n",
      "Iteration 40, loss = 0.60836942\n",
      "Iteration 41, loss = 0.60720515\n",
      "Iteration 42, loss = 0.60618903\n",
      "Iteration 43, loss = 0.60521928\n",
      "Iteration 44, loss = 0.60416383\n",
      "Iteration 45, loss = 0.60324145\n",
      "Iteration 46, loss = 0.60224299\n",
      "Iteration 47, loss = 0.60130620\n",
      "Iteration 48, loss = 0.60034248\n",
      "Iteration 49, loss = 0.59935496\n",
      "Iteration 50, loss = 0.59844999\n",
      "Iteration 51, loss = 0.59746980\n",
      "Iteration 52, loss = 0.59657537\n",
      "Iteration 53, loss = 0.59558318\n",
      "Iteration 54, loss = 0.59479976\n",
      "Iteration 55, loss = 0.59383800\n",
      "Iteration 56, loss = 0.59296640\n",
      "Iteration 57, loss = 0.59213632\n",
      "Iteration 58, loss = 0.59131097\n",
      "Iteration 59, loss = 0.59043373\n",
      "Iteration 60, loss = 0.58961414\n",
      "Iteration 61, loss = 0.58879586\n",
      "Iteration 62, loss = 0.58795285\n",
      "Iteration 63, loss = 0.58711749\n",
      "Iteration 64, loss = 0.58630790\n",
      "Iteration 65, loss = 0.58550866\n",
      "Iteration 66, loss = 0.58465046\n",
      "Iteration 67, loss = 0.58389196\n",
      "Iteration 68, loss = 0.58308708\n",
      "Iteration 69, loss = 0.58235358\n",
      "Iteration 70, loss = 0.58160571\n",
      "Iteration 71, loss = 0.58080229\n",
      "Iteration 72, loss = 0.58015621\n",
      "Iteration 73, loss = 0.57947021\n",
      "Iteration 74, loss = 0.57874288\n",
      "Iteration 75, loss = 0.57811861\n",
      "Iteration 76, loss = 0.57746019\n",
      "Iteration 77, loss = 0.57684602\n",
      "Iteration 78, loss = 0.57628720\n",
      "Iteration 79, loss = 0.57569986\n",
      "Iteration 80, loss = 0.57514468\n",
      "Iteration 81, loss = 0.57461334\n",
      "Iteration 82, loss = 0.57406270\n",
      "Iteration 83, loss = 0.57355942\n",
      "Iteration 84, loss = 0.57302586\n",
      "Iteration 85, loss = 0.57244792\n",
      "Iteration 86, loss = 0.57199829\n",
      "Iteration 87, loss = 0.57148142\n",
      "Iteration 88, loss = 0.57099883\n",
      "Iteration 89, loss = 0.57054817\n",
      "Iteration 90, loss = 0.57005731\n",
      "Iteration 91, loss = 0.56962706\n",
      "Iteration 92, loss = 0.56913943\n",
      "Iteration 93, loss = 0.56866566\n",
      "Iteration 94, loss = 0.56825392\n",
      "Iteration 95, loss = 0.56775469\n",
      "Iteration 96, loss = 0.56730885\n",
      "Iteration 97, loss = 0.56685112\n",
      "Iteration 98, loss = 0.56641976\n",
      "Iteration 99, loss = 0.56599243\n",
      "Iteration 100, loss = 0.56553201\n",
      "Iteration 101, loss = 0.56511585\n",
      "Iteration 102, loss = 0.56467818\n",
      "Iteration 103, loss = 0.56426770\n",
      "Iteration 104, loss = 0.56383951\n",
      "Iteration 105, loss = 0.56348668\n",
      "Iteration 106, loss = 0.56305686\n",
      "Iteration 107, loss = 0.56272137\n",
      "Iteration 108, loss = 0.56232188\n",
      "Iteration 109, loss = 0.56196098\n",
      "Iteration 110, loss = 0.56163147\n",
      "Iteration 111, loss = 0.56123619\n",
      "Iteration 112, loss = 0.56092244\n",
      "Iteration 113, loss = 0.56053473\n",
      "Iteration 114, loss = 0.56024662\n",
      "Iteration 115, loss = 0.55989416\n",
      "Iteration 116, loss = 0.55956013\n",
      "Iteration 117, loss = 0.55922499\n",
      "Iteration 118, loss = 0.55889097\n",
      "Iteration 119, loss = 0.55859994\n",
      "Iteration 120, loss = 0.55824852\n",
      "Iteration 121, loss = 0.55796648\n",
      "Iteration 122, loss = 0.55767059\n",
      "Iteration 123, loss = 0.55736824\n",
      "Iteration 124, loss = 0.55709959\n",
      "Iteration 125, loss = 0.55676974\n",
      "Iteration 126, loss = 0.55648670\n",
      "Iteration 127, loss = 0.55621348\n",
      "Iteration 128, loss = 0.55591634\n",
      "Iteration 129, loss = 0.55562529\n",
      "Iteration 130, loss = 0.55532000\n",
      "Iteration 131, loss = 0.55503857\n",
      "Iteration 132, loss = 0.55471997\n",
      "Iteration 133, loss = 0.55441178\n",
      "Iteration 134, loss = 0.55413311\n",
      "Iteration 135, loss = 0.55385099\n",
      "Iteration 136, loss = 0.55355027\n",
      "Iteration 137, loss = 0.55331205\n",
      "Iteration 138, loss = 0.55303625\n",
      "Iteration 139, loss = 0.55274032\n",
      "Iteration 140, loss = 0.55249362\n",
      "Iteration 141, loss = 0.55224913\n",
      "Iteration 142, loss = 0.55195115\n",
      "Iteration 143, loss = 0.55173629\n",
      "Iteration 144, loss = 0.55147710\n",
      "Iteration 145, loss = 0.55122476\n",
      "Iteration 146, loss = 0.55094638\n",
      "Iteration 147, loss = 0.55071081\n",
      "Iteration 148, loss = 0.55046068\n",
      "Iteration 149, loss = 0.55026276\n",
      "Iteration 150, loss = 0.55000738\n",
      "Iteration 151, loss = 0.54978422\n",
      "Iteration 152, loss = 0.54958657\n",
      "Iteration 153, loss = 0.54939638\n",
      "Iteration 154, loss = 0.54919484\n",
      "Iteration 155, loss = 0.54899225\n",
      "Iteration 156, loss = 0.54880881\n",
      "Iteration 157, loss = 0.54859363\n",
      "Iteration 158, loss = 0.54841347\n",
      "Iteration 159, loss = 0.54820474\n",
      "Iteration 160, loss = 0.54801888\n",
      "Iteration 161, loss = 0.54786486\n",
      "Iteration 162, loss = 0.54763604\n",
      "Iteration 163, loss = 0.54747688\n",
      "Iteration 164, loss = 0.54730011\n",
      "Iteration 165, loss = 0.54712571\n",
      "Iteration 166, loss = 0.54696179\n",
      "Iteration 167, loss = 0.54678330\n",
      "Iteration 168, loss = 0.54661745\n",
      "Iteration 169, loss = 0.54643547\n",
      "Iteration 170, loss = 0.54625670\n",
      "Iteration 171, loss = 0.54607303\n",
      "Iteration 172, loss = 0.54591772\n",
      "Iteration 173, loss = 0.54572193\n",
      "Iteration 174, loss = 0.54553413\n",
      "Iteration 175, loss = 0.54540097\n",
      "Iteration 176, loss = 0.54523256\n",
      "Iteration 177, loss = 0.54506637\n",
      "Iteration 178, loss = 0.54492522\n",
      "Iteration 179, loss = 0.54479362\n",
      "Iteration 180, loss = 0.54464313\n",
      "Iteration 181, loss = 0.54451919\n",
      "Iteration 182, loss = 0.54439895\n",
      "Iteration 183, loss = 0.54428503\n",
      "Iteration 184, loss = 0.54414043\n",
      "Iteration 185, loss = 0.54403007\n",
      "Iteration 186, loss = 0.54390783\n",
      "Iteration 187, loss = 0.54379132\n",
      "Iteration 188, loss = 0.54368035\n",
      "Iteration 189, loss = 0.54361387\n",
      "Iteration 190, loss = 0.54347699\n",
      "Iteration 191, loss = 0.54338057\n",
      "Iteration 192, loss = 0.54328662\n",
      "Iteration 193, loss = 0.54318685\n",
      "Iteration 194, loss = 0.54308379\n",
      "Iteration 195, loss = 0.54298660\n",
      "Iteration 196, loss = 0.54288788\n",
      "Iteration 197, loss = 0.54278022\n",
      "Iteration 198, loss = 0.54269596\n",
      "Iteration 199, loss = 0.54256789\n",
      "Iteration 200, loss = 0.54246626\n",
      "Iteration 201, loss = 0.54234977\n",
      "Iteration 202, loss = 0.54225994\n",
      "Iteration 203, loss = 0.54215229\n",
      "Iteration 204, loss = 0.54204244\n",
      "Iteration 205, loss = 0.54193727\n",
      "Iteration 206, loss = 0.54185662\n",
      "Iteration 207, loss = 0.54176928\n",
      "Iteration 208, loss = 0.54169222\n",
      "Iteration 209, loss = 0.54158966\n",
      "Iteration 210, loss = 0.54150045\n",
      "Iteration 211, loss = 0.54141351\n",
      "Iteration 212, loss = 0.54132503\n",
      "Iteration 213, loss = 0.54123793\n",
      "Iteration 214, loss = 0.54114059\n",
      "Iteration 215, loss = 0.54107132\n",
      "Iteration 216, loss = 0.54095965\n",
      "Iteration 217, loss = 0.54089996\n",
      "Iteration 218, loss = 0.54079478\n",
      "Iteration 219, loss = 0.54074786\n",
      "Iteration 220, loss = 0.54066920\n",
      "Iteration 221, loss = 0.54060205\n",
      "Iteration 222, loss = 0.54054669\n",
      "Iteration 223, loss = 0.54050266\n",
      "Iteration 224, loss = 0.54042472\n",
      "Iteration 225, loss = 0.54034527\n",
      "Iteration 226, loss = 0.54029889\n",
      "Iteration 227, loss = 0.54020399\n",
      "Iteration 228, loss = 0.54012174\n",
      "Iteration 229, loss = 0.54003982\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.63449908\n",
      "Iteration 2, loss = 0.63307689\n",
      "Iteration 3, loss = 0.63149364\n",
      "Iteration 4, loss = 0.63027888\n",
      "Iteration 5, loss = 0.62895821\n",
      "Iteration 6, loss = 0.62776272\n",
      "Iteration 7, loss = 0.62647395\n",
      "Iteration 8, loss = 0.62535223\n",
      "Iteration 9, loss = 0.62416687\n",
      "Iteration 10, loss = 0.62298294\n",
      "Iteration 11, loss = 0.62186553\n",
      "Iteration 12, loss = 0.62086958\n",
      "Iteration 13, loss = 0.61976426\n",
      "Iteration 14, loss = 0.61873262\n",
      "Iteration 15, loss = 0.61770355\n",
      "Iteration 16, loss = 0.61671199\n",
      "Iteration 17, loss = 0.61567463\n",
      "Iteration 18, loss = 0.61471156\n",
      "Iteration 19, loss = 0.61375011\n",
      "Iteration 20, loss = 0.61279748\n",
      "Iteration 21, loss = 0.61183905\n",
      "Iteration 22, loss = 0.61092522\n",
      "Iteration 23, loss = 0.61011897\n",
      "Iteration 24, loss = 0.60928231\n",
      "Iteration 25, loss = 0.60836077\n",
      "Iteration 26, loss = 0.60756614\n",
      "Iteration 27, loss = 0.60681300\n",
      "Iteration 28, loss = 0.60604707\n",
      "Iteration 29, loss = 0.60527454\n",
      "Iteration 30, loss = 0.60455094\n",
      "Iteration 31, loss = 0.60385985\n",
      "Iteration 32, loss = 0.60315430\n",
      "Iteration 33, loss = 0.60247788\n",
      "Iteration 34, loss = 0.60178594\n",
      "Iteration 35, loss = 0.60114812\n",
      "Iteration 36, loss = 0.60052825\n",
      "Iteration 37, loss = 0.59990249\n",
      "Iteration 38, loss = 0.59927544\n",
      "Iteration 39, loss = 0.59868861\n",
      "Iteration 40, loss = 0.59814108\n",
      "Iteration 41, loss = 0.59761418\n",
      "Iteration 42, loss = 0.59704993\n",
      "Iteration 43, loss = 0.59650029\n",
      "Iteration 44, loss = 0.59597056\n",
      "Iteration 45, loss = 0.59547133\n",
      "Iteration 46, loss = 0.59491175\n",
      "Iteration 47, loss = 0.59448707\n",
      "Iteration 48, loss = 0.59396578\n",
      "Iteration 49, loss = 0.59346541\n",
      "Iteration 50, loss = 0.59303293\n",
      "Iteration 51, loss = 0.59260161\n",
      "Iteration 52, loss = 0.59222160\n",
      "Iteration 53, loss = 0.59181784\n",
      "Iteration 54, loss = 0.59138754\n",
      "Iteration 55, loss = 0.59108322\n",
      "Iteration 56, loss = 0.59069019\n",
      "Iteration 57, loss = 0.59032880\n",
      "Iteration 58, loss = 0.58995425\n",
      "Iteration 59, loss = 0.58958551\n",
      "Iteration 60, loss = 0.58918684\n",
      "Iteration 61, loss = 0.58881198\n",
      "Iteration 62, loss = 0.58843621\n",
      "Iteration 63, loss = 0.58806553\n",
      "Iteration 64, loss = 0.58770223\n",
      "Iteration 65, loss = 0.58734854\n",
      "Iteration 66, loss = 0.58694894\n",
      "Iteration 67, loss = 0.58665107\n",
      "Iteration 68, loss = 0.58624188\n",
      "Iteration 69, loss = 0.58592907\n",
      "Iteration 70, loss = 0.58562635\n",
      "Iteration 71, loss = 0.58522614\n",
      "Iteration 72, loss = 0.58492559\n",
      "Iteration 73, loss = 0.58462866\n",
      "Iteration 74, loss = 0.58429946\n",
      "Iteration 75, loss = 0.58396562\n",
      "Iteration 76, loss = 0.58371396\n",
      "Iteration 77, loss = 0.58339282\n",
      "Iteration 78, loss = 0.58314399\n",
      "Iteration 79, loss = 0.58286839\n",
      "Iteration 80, loss = 0.58263911\n",
      "Iteration 81, loss = 0.58237117\n",
      "Iteration 82, loss = 0.58215940\n",
      "Iteration 83, loss = 0.58190207\n",
      "Iteration 84, loss = 0.58163902\n",
      "Iteration 85, loss = 0.58135580\n",
      "Iteration 86, loss = 0.58108693\n",
      "Iteration 87, loss = 0.58080182\n",
      "Iteration 88, loss = 0.58052257\n",
      "Iteration 89, loss = 0.58020275\n",
      "Iteration 90, loss = 0.58000873\n",
      "Iteration 91, loss = 0.57969345\n",
      "Iteration 92, loss = 0.57941439\n",
      "Iteration 93, loss = 0.57921211\n",
      "Iteration 94, loss = 0.57899280\n",
      "Iteration 95, loss = 0.57874951\n",
      "Iteration 96, loss = 0.57853564\n",
      "Iteration 97, loss = 0.57834017\n",
      "Iteration 98, loss = 0.57816997\n",
      "Iteration 99, loss = 0.57796849\n",
      "Iteration 100, loss = 0.57780354\n",
      "Iteration 101, loss = 0.57761220\n",
      "Iteration 102, loss = 0.57745270\n",
      "Iteration 103, loss = 0.57725278\n",
      "Iteration 104, loss = 0.57708871\n",
      "Iteration 105, loss = 0.57690158\n",
      "Iteration 106, loss = 0.57667166\n",
      "Iteration 107, loss = 0.57649882\n",
      "Iteration 108, loss = 0.57628244\n",
      "Iteration 109, loss = 0.57611057\n",
      "Iteration 110, loss = 0.57593036\n",
      "Iteration 111, loss = 0.57572480\n",
      "Iteration 112, loss = 0.57552509\n",
      "Iteration 113, loss = 0.57536642\n",
      "Iteration 114, loss = 0.57521293\n",
      "Iteration 115, loss = 0.57500066\n",
      "Iteration 116, loss = 0.57482295\n",
      "Iteration 117, loss = 0.57466450\n",
      "Iteration 118, loss = 0.57445712\n",
      "Iteration 119, loss = 0.57428133\n",
      "Iteration 120, loss = 0.57412410\n",
      "Iteration 121, loss = 0.57391248\n",
      "Iteration 122, loss = 0.57375978\n",
      "Iteration 123, loss = 0.57357941\n",
      "Iteration 124, loss = 0.57342445\n",
      "Iteration 125, loss = 0.57325453\n",
      "Iteration 126, loss = 0.57311956\n",
      "Iteration 127, loss = 0.57298108\n",
      "Iteration 128, loss = 0.57288077\n",
      "Iteration 129, loss = 0.57275226\n",
      "Iteration 130, loss = 0.57261996\n",
      "Iteration 131, loss = 0.57249884\n",
      "Iteration 132, loss = 0.57239513\n",
      "Iteration 133, loss = 0.57226242\n",
      "Iteration 134, loss = 0.57215423\n",
      "Iteration 135, loss = 0.57203355\n",
      "Iteration 136, loss = 0.57187234\n",
      "Iteration 137, loss = 0.57176386\n",
      "Iteration 138, loss = 0.57162344\n",
      "Iteration 139, loss = 0.57151190\n",
      "Iteration 140, loss = 0.57135484\n",
      "Iteration 141, loss = 0.57122620\n",
      "Iteration 142, loss = 0.57105744\n",
      "Iteration 143, loss = 0.57094214\n",
      "Iteration 144, loss = 0.57078414\n",
      "Iteration 145, loss = 0.57070809\n",
      "Iteration 146, loss = 0.57056060\n",
      "Iteration 147, loss = 0.57046534\n",
      "Iteration 148, loss = 0.57035618\n",
      "Iteration 149, loss = 0.57028015\n",
      "Iteration 150, loss = 0.57014430\n",
      "Iteration 151, loss = 0.57005425\n",
      "Iteration 152, loss = 0.56992306\n",
      "Iteration 153, loss = 0.56979435\n",
      "Iteration 154, loss = 0.56970173\n",
      "Iteration 155, loss = 0.56955400\n",
      "Iteration 156, loss = 0.56944985\n",
      "Iteration 157, loss = 0.56931088\n",
      "Iteration 158, loss = 0.56920819\n",
      "Iteration 159, loss = 0.56909747\n",
      "Iteration 160, loss = 0.56898223\n",
      "Iteration 161, loss = 0.56885823\n",
      "Iteration 162, loss = 0.56874172\n",
      "Iteration 163, loss = 0.56862151\n",
      "Iteration 164, loss = 0.56852642\n",
      "Iteration 165, loss = 0.56845819\n",
      "Iteration 166, loss = 0.56832437\n",
      "Iteration 167, loss = 0.56823682\n",
      "Iteration 168, loss = 0.56814634\n",
      "Iteration 169, loss = 0.56804546\n",
      "Iteration 170, loss = 0.56795647\n",
      "Iteration 171, loss = 0.56788692\n",
      "Iteration 172, loss = 0.56776739\n",
      "Iteration 173, loss = 0.56768875\n",
      "Iteration 174, loss = 0.56757660\n",
      "Iteration 175, loss = 0.56748506\n",
      "Iteration 176, loss = 0.56737846\n",
      "Iteration 177, loss = 0.56726199\n",
      "Iteration 178, loss = 0.56717240\n",
      "Iteration 179, loss = 0.56705480\n",
      "Iteration 180, loss = 0.56695145\n",
      "Iteration 181, loss = 0.56683372\n",
      "Iteration 182, loss = 0.56674414\n",
      "Iteration 183, loss = 0.56664989\n",
      "Iteration 184, loss = 0.56655780\n",
      "Iteration 185, loss = 0.56645723\n",
      "Iteration 186, loss = 0.56640523\n",
      "Iteration 187, loss = 0.56631494\n",
      "Iteration 188, loss = 0.56627555\n",
      "Iteration 189, loss = 0.56624604\n",
      "Iteration 190, loss = 0.56615007\n",
      "Iteration 191, loss = 0.56606627\n",
      "Iteration 192, loss = 0.56596069\n",
      "Iteration 193, loss = 0.56587647\n",
      "Iteration 194, loss = 0.56579751\n",
      "Iteration 195, loss = 0.56570189\n",
      "Iteration 196, loss = 0.56561059\n",
      "Iteration 197, loss = 0.56551476\n",
      "Iteration 198, loss = 0.56541509\n",
      "Iteration 199, loss = 0.56535591\n",
      "Iteration 200, loss = 0.56523399\n",
      "Iteration 201, loss = 0.56514319\n",
      "Iteration 202, loss = 0.56506299\n",
      "Iteration 203, loss = 0.56499830\n",
      "Iteration 204, loss = 0.56490442\n",
      "Iteration 205, loss = 0.56483317\n",
      "Iteration 206, loss = 0.56474441\n",
      "Iteration 207, loss = 0.56469105\n",
      "Iteration 208, loss = 0.56461873\n",
      "Iteration 209, loss = 0.56453402\n",
      "Iteration 210, loss = 0.56448006\n",
      "Iteration 211, loss = 0.56442315\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68819326\n",
      "Iteration 2, loss = 0.68545950\n",
      "Iteration 3, loss = 0.68274745\n",
      "Iteration 4, loss = 0.67994834\n",
      "Iteration 5, loss = 0.67757527\n",
      "Iteration 6, loss = 0.67500610\n",
      "Iteration 7, loss = 0.67254814\n",
      "Iteration 8, loss = 0.67020664\n",
      "Iteration 9, loss = 0.66759367\n",
      "Iteration 10, loss = 0.66526059\n",
      "Iteration 11, loss = 0.66282759\n",
      "Iteration 12, loss = 0.66060258\n",
      "Iteration 13, loss = 0.65805104\n",
      "Iteration 14, loss = 0.65575789\n",
      "Iteration 15, loss = 0.65348010\n",
      "Iteration 16, loss = 0.65114792\n",
      "Iteration 17, loss = 0.64894015\n",
      "Iteration 18, loss = 0.64658156\n",
      "Iteration 19, loss = 0.64449635\n",
      "Iteration 20, loss = 0.64241397\n",
      "Iteration 21, loss = 0.64032103\n",
      "Iteration 22, loss = 0.63821464\n",
      "Iteration 23, loss = 0.63629592\n",
      "Iteration 24, loss = 0.63436904\n",
      "Iteration 25, loss = 0.63246539\n",
      "Iteration 26, loss = 0.63072456\n",
      "Iteration 27, loss = 0.62877264\n",
      "Iteration 28, loss = 0.62705846\n",
      "Iteration 29, loss = 0.62513431\n",
      "Iteration 30, loss = 0.62341706\n",
      "Iteration 31, loss = 0.62154399\n",
      "Iteration 32, loss = 0.61965093\n",
      "Iteration 33, loss = 0.61781835\n",
      "Iteration 34, loss = 0.61597587\n",
      "Iteration 35, loss = 0.61411018\n",
      "Iteration 36, loss = 0.61231827\n",
      "Iteration 37, loss = 0.61049506\n",
      "Iteration 38, loss = 0.60867305\n",
      "Iteration 39, loss = 0.60697898\n",
      "Iteration 40, loss = 0.60530626\n",
      "Iteration 41, loss = 0.60358379\n",
      "Iteration 42, loss = 0.60191467\n",
      "Iteration 43, loss = 0.60023321\n",
      "Iteration 44, loss = 0.59858668\n",
      "Iteration 45, loss = 0.59682898\n",
      "Iteration 46, loss = 0.59513579\n",
      "Iteration 47, loss = 0.59338601\n",
      "Iteration 48, loss = 0.59177851\n",
      "Iteration 49, loss = 0.58996108\n",
      "Iteration 50, loss = 0.58825612\n",
      "Iteration 51, loss = 0.58675732\n",
      "Iteration 52, loss = 0.58505773\n",
      "Iteration 53, loss = 0.58356805\n",
      "Iteration 54, loss = 0.58193782\n",
      "Iteration 55, loss = 0.58057510\n",
      "Iteration 56, loss = 0.57915623\n",
      "Iteration 57, loss = 0.57771072\n",
      "Iteration 58, loss = 0.57630519\n",
      "Iteration 59, loss = 0.57505197\n",
      "Iteration 60, loss = 0.57368102\n",
      "Iteration 61, loss = 0.57237405\n",
      "Iteration 62, loss = 0.57113457\n",
      "Iteration 63, loss = 0.56990327\n",
      "Iteration 64, loss = 0.56861228\n",
      "Iteration 65, loss = 0.56742106\n",
      "Iteration 66, loss = 0.56628457\n",
      "Iteration 67, loss = 0.56514931\n",
      "Iteration 68, loss = 0.56397856\n",
      "Iteration 69, loss = 0.56286940\n",
      "Iteration 70, loss = 0.56177572\n",
      "Iteration 71, loss = 0.56076907\n",
      "Iteration 72, loss = 0.55956356\n",
      "Iteration 73, loss = 0.55862470\n",
      "Iteration 74, loss = 0.55758777\n",
      "Iteration 75, loss = 0.55651417\n",
      "Iteration 76, loss = 0.55545966\n",
      "Iteration 77, loss = 0.55453767\n",
      "Iteration 78, loss = 0.55350461\n",
      "Iteration 79, loss = 0.55254102\n",
      "Iteration 80, loss = 0.55161890\n",
      "Iteration 81, loss = 0.55063901\n",
      "Iteration 82, loss = 0.54979378\n",
      "Iteration 83, loss = 0.54887659\n",
      "Iteration 84, loss = 0.54791281\n",
      "Iteration 85, loss = 0.54721258\n",
      "Iteration 86, loss = 0.54619017\n",
      "Iteration 87, loss = 0.54541437\n",
      "Iteration 88, loss = 0.54458422\n",
      "Iteration 89, loss = 0.54385830\n",
      "Iteration 90, loss = 0.54303560\n",
      "Iteration 91, loss = 0.54249583\n",
      "Iteration 92, loss = 0.54173984\n",
      "Iteration 93, loss = 0.54110617\n",
      "Iteration 94, loss = 0.54052626\n",
      "Iteration 95, loss = 0.53987035\n",
      "Iteration 96, loss = 0.53931559\n",
      "Iteration 97, loss = 0.53867875\n",
      "Iteration 98, loss = 0.53811415\n",
      "Iteration 99, loss = 0.53753005\n",
      "Iteration 100, loss = 0.53699047\n",
      "Iteration 101, loss = 0.53645604\n",
      "Iteration 102, loss = 0.53590772\n",
      "Iteration 103, loss = 0.53544095\n",
      "Iteration 104, loss = 0.53493442\n",
      "Iteration 105, loss = 0.53459845\n",
      "Iteration 106, loss = 0.53412369\n",
      "Iteration 107, loss = 0.53369828\n",
      "Iteration 108, loss = 0.53332792\n",
      "Iteration 109, loss = 0.53296661\n",
      "Iteration 110, loss = 0.53263664\n",
      "Iteration 111, loss = 0.53231139\n",
      "Iteration 112, loss = 0.53193047\n",
      "Iteration 113, loss = 0.53163668\n",
      "Iteration 114, loss = 0.53133452\n",
      "Iteration 115, loss = 0.53096484\n",
      "Iteration 116, loss = 0.53063075\n",
      "Iteration 117, loss = 0.53037936\n",
      "Iteration 118, loss = 0.53003519\n",
      "Iteration 119, loss = 0.52975410\n",
      "Iteration 120, loss = 0.52944041\n",
      "Iteration 121, loss = 0.52918247\n",
      "Iteration 122, loss = 0.52884420\n",
      "Iteration 123, loss = 0.52861453\n",
      "Iteration 124, loss = 0.52832286\n",
      "Iteration 125, loss = 0.52798467\n",
      "Iteration 126, loss = 0.52771676\n",
      "Iteration 127, loss = 0.52750869\n",
      "Iteration 128, loss = 0.52719982\n",
      "Iteration 129, loss = 0.52697919\n",
      "Iteration 130, loss = 0.52673691\n",
      "Iteration 131, loss = 0.52651494\n",
      "Iteration 132, loss = 0.52629382\n",
      "Iteration 133, loss = 0.52602837\n",
      "Iteration 134, loss = 0.52585153\n",
      "Iteration 135, loss = 0.52562313\n",
      "Iteration 136, loss = 0.52541570\n",
      "Iteration 137, loss = 0.52528230\n",
      "Iteration 138, loss = 0.52510826\n",
      "Iteration 139, loss = 0.52495338\n",
      "Iteration 140, loss = 0.52483948\n",
      "Iteration 141, loss = 0.52466757\n",
      "Iteration 142, loss = 0.52457915\n",
      "Iteration 143, loss = 0.52444609\n",
      "Iteration 144, loss = 0.52432486\n",
      "Iteration 145, loss = 0.52425937\n",
      "Iteration 146, loss = 0.52412243\n",
      "Iteration 147, loss = 0.52399142\n",
      "Iteration 148, loss = 0.52397505\n",
      "Iteration 149, loss = 0.52381706\n",
      "Iteration 150, loss = 0.52372766\n",
      "Iteration 151, loss = 0.52364216\n",
      "Iteration 152, loss = 0.52354730\n",
      "Iteration 153, loss = 0.52343877\n",
      "Iteration 154, loss = 0.52332877\n",
      "Iteration 155, loss = 0.52320001\n",
      "Iteration 156, loss = 0.52313068\n",
      "Iteration 157, loss = 0.52297512\n",
      "Iteration 158, loss = 0.52285982\n",
      "Iteration 159, loss = 0.52275923\n",
      "Iteration 160, loss = 0.52265400\n",
      "Iteration 161, loss = 0.52256905\n",
      "Iteration 162, loss = 0.52244178\n",
      "Iteration 163, loss = 0.52237826\n",
      "Iteration 164, loss = 0.52228410\n",
      "Iteration 165, loss = 0.52220048\n",
      "Iteration 166, loss = 0.52211328\n",
      "Iteration 167, loss = 0.52202655\n",
      "Iteration 168, loss = 0.52194713\n",
      "Iteration 169, loss = 0.52185164\n",
      "Iteration 170, loss = 0.52179762\n",
      "Iteration 171, loss = 0.52170013\n",
      "Iteration 172, loss = 0.52165046\n",
      "Iteration 173, loss = 0.52157922\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.06072258\n",
      "Iteration 2, loss = 1.05387721\n",
      "Iteration 3, loss = 1.04701963\n",
      "Iteration 4, loss = 1.04045983\n",
      "Iteration 5, loss = 1.03379818\n",
      "Iteration 6, loss = 1.02723142\n",
      "Iteration 7, loss = 1.02083134\n",
      "Iteration 8, loss = 1.01428355\n",
      "Iteration 9, loss = 1.00807371\n",
      "Iteration 10, loss = 1.00190405\n",
      "Iteration 11, loss = 0.99550307\n",
      "Iteration 12, loss = 0.98954473\n",
      "Iteration 13, loss = 0.98353367\n",
      "Iteration 14, loss = 0.97758360\n",
      "Iteration 15, loss = 0.97175613\n",
      "Iteration 16, loss = 0.96589626\n",
      "Iteration 17, loss = 0.96023831\n",
      "Iteration 18, loss = 0.95452969\n",
      "Iteration 19, loss = 0.94895032\n",
      "Iteration 20, loss = 0.94344531\n",
      "Iteration 21, loss = 0.93784234\n",
      "Iteration 22, loss = 0.93257923\n",
      "Iteration 23, loss = 0.92719720\n",
      "Iteration 24, loss = 0.92191913\n",
      "Iteration 25, loss = 0.91690109\n",
      "Iteration 26, loss = 0.91169611\n",
      "Iteration 27, loss = 0.90672219\n",
      "Iteration 28, loss = 0.90182630\n",
      "Iteration 29, loss = 0.89698359\n",
      "Iteration 30, loss = 0.89203164\n",
      "Iteration 31, loss = 0.88726354\n",
      "Iteration 32, loss = 0.88258191\n",
      "Iteration 33, loss = 0.87775330\n",
      "Iteration 34, loss = 0.87305616\n",
      "Iteration 35, loss = 0.86845632\n",
      "Iteration 36, loss = 0.86402306\n",
      "Iteration 37, loss = 0.85935961\n",
      "Iteration 38, loss = 0.85502415\n",
      "Iteration 39, loss = 0.85062366\n",
      "Iteration 40, loss = 0.84642718\n",
      "Iteration 41, loss = 0.84221131\n",
      "Iteration 42, loss = 0.83813949\n",
      "Iteration 43, loss = 0.83409874\n",
      "Iteration 44, loss = 0.83000691\n",
      "Iteration 45, loss = 0.82609685\n",
      "Iteration 46, loss = 0.82219878\n",
      "Iteration 47, loss = 0.81840978\n",
      "Iteration 48, loss = 0.81452258\n",
      "Iteration 49, loss = 0.81081663\n",
      "Iteration 50, loss = 0.80711329\n",
      "Iteration 51, loss = 0.80334711\n",
      "Iteration 52, loss = 0.79971646\n",
      "Iteration 53, loss = 0.79614908\n",
      "Iteration 54, loss = 0.79244258\n",
      "Iteration 55, loss = 0.78906457\n",
      "Iteration 56, loss = 0.78534683\n",
      "Iteration 57, loss = 0.78196722\n",
      "Iteration 58, loss = 0.77852614\n",
      "Iteration 59, loss = 0.77508789\n",
      "Iteration 60, loss = 0.77174444\n",
      "Iteration 61, loss = 0.76832879\n",
      "Iteration 62, loss = 0.76518506\n",
      "Iteration 63, loss = 0.76180854\n",
      "Iteration 64, loss = 0.75863216\n",
      "Iteration 65, loss = 0.75543948\n",
      "Iteration 66, loss = 0.75239272\n",
      "Iteration 67, loss = 0.74927813\n",
      "Iteration 68, loss = 0.74640544\n",
      "Iteration 69, loss = 0.74332407\n",
      "Iteration 70, loss = 0.74043352\n",
      "Iteration 71, loss = 0.73767313\n",
      "Iteration 72, loss = 0.73483012\n",
      "Iteration 73, loss = 0.73205361\n",
      "Iteration 74, loss = 0.72932366\n",
      "Iteration 75, loss = 0.72662582\n",
      "Iteration 76, loss = 0.72388701\n",
      "Iteration 77, loss = 0.72117677\n",
      "Iteration 78, loss = 0.71861420\n",
      "Iteration 79, loss = 0.71598934\n",
      "Iteration 80, loss = 0.71339084\n",
      "Iteration 81, loss = 0.71085560\n",
      "Iteration 82, loss = 0.70845105\n",
      "Iteration 83, loss = 0.70591739\n",
      "Iteration 84, loss = 0.70347686\n",
      "Iteration 85, loss = 0.70105880\n",
      "Iteration 86, loss = 0.69873002\n",
      "Iteration 87, loss = 0.69624815\n",
      "Iteration 88, loss = 0.69387673\n",
      "Iteration 89, loss = 0.69161466\n",
      "Iteration 90, loss = 0.68924359\n",
      "Iteration 91, loss = 0.68699800\n",
      "Iteration 92, loss = 0.68471041\n",
      "Iteration 93, loss = 0.68244258\n",
      "Iteration 94, loss = 0.68023886\n",
      "Iteration 95, loss = 0.67803921\n",
      "Iteration 96, loss = 0.67579641\n",
      "Iteration 97, loss = 0.67379002\n",
      "Iteration 98, loss = 0.67160005\n",
      "Iteration 99, loss = 0.66966511\n",
      "Iteration 100, loss = 0.66750190\n",
      "Iteration 101, loss = 0.66565492\n",
      "Iteration 102, loss = 0.66366001\n",
      "Iteration 103, loss = 0.66167548\n",
      "Iteration 104, loss = 0.65986403\n",
      "Iteration 105, loss = 0.65782310\n",
      "Iteration 106, loss = 0.65594012\n",
      "Iteration 107, loss = 0.65403385\n",
      "Iteration 108, loss = 0.65213713\n",
      "Iteration 109, loss = 0.65025516\n",
      "Iteration 110, loss = 0.64832691\n",
      "Iteration 111, loss = 0.64649230\n",
      "Iteration 112, loss = 0.64463540\n",
      "Iteration 113, loss = 0.64279714\n",
      "Iteration 114, loss = 0.64102709\n",
      "Iteration 115, loss = 0.63932249\n",
      "Iteration 116, loss = 0.63746018\n",
      "Iteration 117, loss = 0.63579939\n",
      "Iteration 118, loss = 0.63408471\n",
      "Iteration 119, loss = 0.63240383\n",
      "Iteration 120, loss = 0.63064743\n",
      "Iteration 121, loss = 0.62901034\n",
      "Iteration 122, loss = 0.62733878\n",
      "Iteration 123, loss = 0.62556973\n",
      "Iteration 124, loss = 0.62401669\n",
      "Iteration 125, loss = 0.62243948\n",
      "Iteration 126, loss = 0.62079618\n",
      "Iteration 127, loss = 0.61922774\n",
      "Iteration 128, loss = 0.61777673\n",
      "Iteration 129, loss = 0.61620427\n",
      "Iteration 130, loss = 0.61473534\n",
      "Iteration 131, loss = 0.61331738\n",
      "Iteration 132, loss = 0.61183413\n",
      "Iteration 133, loss = 0.61037121\n",
      "Iteration 134, loss = 0.60897745\n",
      "Iteration 135, loss = 0.60747990\n",
      "Iteration 136, loss = 0.60609867\n",
      "Iteration 137, loss = 0.60466817\n",
      "Iteration 138, loss = 0.60326739\n",
      "Iteration 139, loss = 0.60183232\n",
      "Iteration 140, loss = 0.60041862\n",
      "Iteration 141, loss = 0.59902602\n",
      "Iteration 142, loss = 0.59764903\n",
      "Iteration 143, loss = 0.59628878\n",
      "Iteration 144, loss = 0.59495833\n",
      "Iteration 145, loss = 0.59363233\n",
      "Iteration 146, loss = 0.59242481\n",
      "Iteration 147, loss = 0.59110875\n",
      "Iteration 148, loss = 0.58995186\n",
      "Iteration 149, loss = 0.58869337\n",
      "Iteration 150, loss = 0.58754677\n",
      "Iteration 151, loss = 0.58634351\n",
      "Iteration 152, loss = 0.58514129\n",
      "Iteration 153, loss = 0.58400711\n",
      "Iteration 154, loss = 0.58291729\n",
      "Iteration 155, loss = 0.58176728\n",
      "Iteration 156, loss = 0.58065869\n",
      "Iteration 157, loss = 0.57964306\n",
      "Iteration 158, loss = 0.57855982\n",
      "Iteration 159, loss = 0.57753724\n",
      "Iteration 160, loss = 0.57650982\n",
      "Iteration 161, loss = 0.57551191\n",
      "Iteration 162, loss = 0.57453393\n",
      "Iteration 163, loss = 0.57354178\n",
      "Iteration 164, loss = 0.57254396\n",
      "Iteration 165, loss = 0.57159383\n",
      "Iteration 166, loss = 0.57065190\n",
      "Iteration 167, loss = 0.56964198\n",
      "Iteration 168, loss = 0.56870770\n",
      "Iteration 169, loss = 0.56785664\n",
      "Iteration 170, loss = 0.56688512\n",
      "Iteration 171, loss = 0.56601123\n",
      "Iteration 172, loss = 0.56518331\n",
      "Iteration 173, loss = 0.56421137\n",
      "Iteration 174, loss = 0.56338273\n",
      "Iteration 175, loss = 0.56258689\n",
      "Iteration 176, loss = 0.56168508\n",
      "Iteration 177, loss = 0.56080921\n",
      "Iteration 178, loss = 0.56005064\n",
      "Iteration 179, loss = 0.55914812\n",
      "Iteration 180, loss = 0.55839815\n",
      "Iteration 181, loss = 0.55756761\n",
      "Iteration 182, loss = 0.55678652\n",
      "Iteration 183, loss = 0.55601148\n",
      "Iteration 184, loss = 0.55522674\n",
      "Iteration 185, loss = 0.55456710\n",
      "Iteration 186, loss = 0.55380235\n",
      "Iteration 187, loss = 0.55304870\n",
      "Iteration 188, loss = 0.55240006\n",
      "Iteration 189, loss = 0.55174564\n",
      "Iteration 190, loss = 0.55103149\n",
      "Iteration 191, loss = 0.55040636\n",
      "Iteration 192, loss = 0.54974045\n",
      "Iteration 193, loss = 0.54915048\n",
      "Iteration 194, loss = 0.54843293\n",
      "Iteration 195, loss = 0.54793032\n",
      "Iteration 196, loss = 0.54726787\n",
      "Iteration 197, loss = 0.54669407\n",
      "Iteration 198, loss = 0.54615975\n",
      "Iteration 199, loss = 0.54551276\n",
      "Iteration 200, loss = 0.54497374\n",
      "Iteration 201, loss = 0.54434960\n",
      "Iteration 202, loss = 0.54374922\n",
      "Iteration 203, loss = 0.54313820\n",
      "Iteration 204, loss = 0.54252772\n",
      "Iteration 205, loss = 0.54187861\n",
      "Iteration 206, loss = 0.54130663\n",
      "Iteration 207, loss = 0.54066805\n",
      "Iteration 208, loss = 0.54013582\n",
      "Iteration 209, loss = 0.53952045\n",
      "Iteration 210, loss = 0.53894623\n",
      "Iteration 211, loss = 0.53842450\n",
      "Iteration 212, loss = 0.53787381\n",
      "Iteration 213, loss = 0.53737196\n",
      "Iteration 214, loss = 0.53682926\n",
      "Iteration 215, loss = 0.53631300\n",
      "Iteration 216, loss = 0.53586139\n",
      "Iteration 217, loss = 0.53535936\n",
      "Iteration 218, loss = 0.53487795\n",
      "Iteration 219, loss = 0.53441243\n",
      "Iteration 220, loss = 0.53397043\n",
      "Iteration 221, loss = 0.53352133\n",
      "Iteration 222, loss = 0.53310032\n",
      "Iteration 223, loss = 0.53263512\n",
      "Iteration 224, loss = 0.53221182\n",
      "Iteration 225, loss = 0.53185011\n",
      "Iteration 226, loss = 0.53144168\n",
      "Iteration 227, loss = 0.53103654\n",
      "Iteration 228, loss = 0.53065489\n",
      "Iteration 229, loss = 0.53036981\n",
      "Iteration 230, loss = 0.52987362\n",
      "Iteration 231, loss = 0.52951251\n",
      "Iteration 232, loss = 0.52915845\n",
      "Iteration 233, loss = 0.52875321\n",
      "Iteration 234, loss = 0.52837195\n",
      "Iteration 235, loss = 0.52796256\n",
      "Iteration 236, loss = 0.52756006\n",
      "Iteration 237, loss = 0.52713080\n",
      "Iteration 238, loss = 0.52672249\n",
      "Iteration 239, loss = 0.52627739\n",
      "Iteration 240, loss = 0.52586169\n",
      "Iteration 241, loss = 0.52546678\n",
      "Iteration 242, loss = 0.52503678\n",
      "Iteration 243, loss = 0.52465685\n",
      "Iteration 244, loss = 0.52425530\n",
      "Iteration 245, loss = 0.52386741\n",
      "Iteration 246, loss = 0.52354839\n",
      "Iteration 247, loss = 0.52322417\n",
      "Iteration 248, loss = 0.52287060\n",
      "Iteration 249, loss = 0.52260341\n",
      "Iteration 250, loss = 0.52230344\n",
      "Iteration 251, loss = 0.52203543\n",
      "Iteration 252, loss = 0.52178347\n",
      "Iteration 253, loss = 0.52152703\n",
      "Iteration 254, loss = 0.52128089\n",
      "Iteration 255, loss = 0.52099759\n",
      "Iteration 256, loss = 0.52075037\n",
      "Iteration 257, loss = 0.52050305\n",
      "Iteration 258, loss = 0.52023707\n",
      "Iteration 259, loss = 0.52002081\n",
      "Iteration 260, loss = 0.51975973\n",
      "Iteration 261, loss = 0.51953971\n",
      "Iteration 262, loss = 0.51933887\n",
      "Iteration 263, loss = 0.51911138\n",
      "Iteration 264, loss = 0.51887345\n",
      "Iteration 265, loss = 0.51867115\n",
      "Iteration 266, loss = 0.51847649\n",
      "Iteration 267, loss = 0.51825013\n",
      "Iteration 268, loss = 0.51799718\n",
      "Iteration 269, loss = 0.51783507\n",
      "Iteration 270, loss = 0.51757768\n",
      "Iteration 271, loss = 0.51733777\n",
      "Iteration 272, loss = 0.51715203\n",
      "Iteration 273, loss = 0.51696341\n",
      "Iteration 274, loss = 0.51673605\n",
      "Iteration 275, loss = 0.51654973\n",
      "Iteration 276, loss = 0.51638058\n",
      "Iteration 277, loss = 0.51616913\n",
      "Iteration 278, loss = 0.51600626\n",
      "Iteration 279, loss = 0.51580968\n",
      "Iteration 280, loss = 0.51564138\n",
      "Iteration 281, loss = 0.51547803\n",
      "Iteration 282, loss = 0.51530478\n",
      "Iteration 283, loss = 0.51514515\n",
      "Iteration 284, loss = 0.51498972\n",
      "Iteration 285, loss = 0.51481103\n",
      "Iteration 286, loss = 0.51468319\n",
      "Iteration 287, loss = 0.51453503\n",
      "Iteration 288, loss = 0.51437178\n",
      "Iteration 289, loss = 0.51423815\n",
      "Iteration 290, loss = 0.51409739\n",
      "Iteration 291, loss = 0.51400815\n",
      "Iteration 292, loss = 0.51385647\n",
      "Iteration 293, loss = 0.51375419\n",
      "Iteration 294, loss = 0.51362594\n",
      "Iteration 295, loss = 0.51349409\n",
      "Iteration 296, loss = 0.51339181\n",
      "Iteration 297, loss = 0.51328103\n",
      "Iteration 298, loss = 0.51313949\n",
      "Iteration 299, loss = 0.51303946\n",
      "Iteration 300, loss = 0.51291856\n",
      "Iteration 301, loss = 0.51279363\n",
      "Iteration 302, loss = 0.51268854\n",
      "Iteration 303, loss = 0.51255678\n",
      "Iteration 304, loss = 0.51248353\n",
      "Iteration 305, loss = 0.51236692\n",
      "Iteration 306, loss = 0.51225327\n",
      "Iteration 307, loss = 0.51215267\n",
      "Iteration 308, loss = 0.51204756\n",
      "Iteration 309, loss = 0.51194507\n",
      "Iteration 310, loss = 0.51185503\n",
      "Iteration 311, loss = 0.51171830\n",
      "Iteration 312, loss = 0.51164641\n",
      "Iteration 313, loss = 0.51153635\n",
      "Iteration 314, loss = 0.51144683\n",
      "Iteration 315, loss = 0.51134786\n",
      "Iteration 316, loss = 0.51125063\n",
      "Iteration 317, loss = 0.51115953\n",
      "Iteration 318, loss = 0.51105545\n",
      "Iteration 319, loss = 0.51098884\n",
      "Iteration 320, loss = 0.51090130\n",
      "Iteration 321, loss = 0.51081213\n",
      "Iteration 322, loss = 0.51074823\n",
      "Iteration 323, loss = 0.51067693\n",
      "Iteration 324, loss = 0.51059847\n",
      "Iteration 325, loss = 0.51052674\n",
      "Iteration 326, loss = 0.51045823\n",
      "Iteration 327, loss = 0.51039295\n",
      "Iteration 328, loss = 0.51030027\n",
      "Iteration 329, loss = 0.51024067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62667196\n",
      "Iteration 2, loss = 0.62471719\n",
      "Iteration 3, loss = 0.62306634\n",
      "Iteration 4, loss = 0.62113983\n",
      "Iteration 5, loss = 0.61971429\n",
      "Iteration 6, loss = 0.61807102\n",
      "Iteration 7, loss = 0.61654225\n",
      "Iteration 8, loss = 0.61505691\n",
      "Iteration 9, loss = 0.61367084\n",
      "Iteration 10, loss = 0.61231562\n",
      "Iteration 11, loss = 0.61088086\n",
      "Iteration 12, loss = 0.60962906\n",
      "Iteration 13, loss = 0.60838250\n",
      "Iteration 14, loss = 0.60709231\n",
      "Iteration 15, loss = 0.60584078\n",
      "Iteration 16, loss = 0.60481662\n",
      "Iteration 17, loss = 0.60350243\n",
      "Iteration 18, loss = 0.60253176\n",
      "Iteration 19, loss = 0.60131602\n",
      "Iteration 20, loss = 0.60047806\n",
      "Iteration 21, loss = 0.59931672\n",
      "Iteration 22, loss = 0.59840595\n",
      "Iteration 23, loss = 0.59742698\n",
      "Iteration 24, loss = 0.59661659\n",
      "Iteration 25, loss = 0.59563110\n",
      "Iteration 26, loss = 0.59478842\n",
      "Iteration 27, loss = 0.59395593\n",
      "Iteration 28, loss = 0.59314212\n",
      "Iteration 29, loss = 0.59233355\n",
      "Iteration 30, loss = 0.59158019\n",
      "Iteration 31, loss = 0.59078749\n",
      "Iteration 32, loss = 0.59019391\n",
      "Iteration 33, loss = 0.58941224\n",
      "Iteration 34, loss = 0.58879463\n",
      "Iteration 35, loss = 0.58812528\n",
      "Iteration 36, loss = 0.58748130\n",
      "Iteration 37, loss = 0.58683286\n",
      "Iteration 38, loss = 0.58620343\n",
      "Iteration 39, loss = 0.58558936\n",
      "Iteration 40, loss = 0.58492618\n",
      "Iteration 41, loss = 0.58442782\n",
      "Iteration 42, loss = 0.58381218\n",
      "Iteration 43, loss = 0.58324541\n",
      "Iteration 44, loss = 0.58269035\n",
      "Iteration 45, loss = 0.58220562\n",
      "Iteration 46, loss = 0.58170295\n",
      "Iteration 47, loss = 0.58117660\n",
      "Iteration 48, loss = 0.58070870\n",
      "Iteration 49, loss = 0.58024289\n",
      "Iteration 50, loss = 0.57986639\n",
      "Iteration 51, loss = 0.57946504\n",
      "Iteration 52, loss = 0.57909331\n",
      "Iteration 53, loss = 0.57875405\n",
      "Iteration 54, loss = 0.57839334\n",
      "Iteration 55, loss = 0.57803815\n",
      "Iteration 56, loss = 0.57763546\n",
      "Iteration 57, loss = 0.57729230\n",
      "Iteration 58, loss = 0.57695200\n",
      "Iteration 59, loss = 0.57655732\n",
      "Iteration 60, loss = 0.57620736\n",
      "Iteration 61, loss = 0.57587043\n",
      "Iteration 62, loss = 0.57548349\n",
      "Iteration 63, loss = 0.57511315\n",
      "Iteration 64, loss = 0.57479300\n",
      "Iteration 65, loss = 0.57441628\n",
      "Iteration 66, loss = 0.57405863\n",
      "Iteration 67, loss = 0.57379736\n",
      "Iteration 68, loss = 0.57340030\n",
      "Iteration 69, loss = 0.57314776\n",
      "Iteration 70, loss = 0.57285503\n",
      "Iteration 71, loss = 0.57254593\n",
      "Iteration 72, loss = 0.57231892\n",
      "Iteration 73, loss = 0.57207217\n",
      "Iteration 74, loss = 0.57181868\n",
      "Iteration 75, loss = 0.57159919\n",
      "Iteration 76, loss = 0.57138320\n",
      "Iteration 77, loss = 0.57113561\n",
      "Iteration 78, loss = 0.57094005\n",
      "Iteration 79, loss = 0.57070277\n",
      "Iteration 80, loss = 0.57053024\n",
      "Iteration 81, loss = 0.57030355\n",
      "Iteration 82, loss = 0.57013098\n",
      "Iteration 83, loss = 0.56991361\n",
      "Iteration 84, loss = 0.56980094\n",
      "Iteration 85, loss = 0.56962286\n",
      "Iteration 86, loss = 0.56949940\n",
      "Iteration 87, loss = 0.56936136\n",
      "Iteration 88, loss = 0.56923381\n",
      "Iteration 89, loss = 0.56910870\n",
      "Iteration 90, loss = 0.56895691\n",
      "Iteration 91, loss = 0.56881106\n",
      "Iteration 92, loss = 0.56867073\n",
      "Iteration 93, loss = 0.56850850\n",
      "Iteration 94, loss = 0.56837273\n",
      "Iteration 95, loss = 0.56823279\n",
      "Iteration 96, loss = 0.56809138\n",
      "Iteration 97, loss = 0.56794887\n",
      "Iteration 98, loss = 0.56781855\n",
      "Iteration 99, loss = 0.56775286\n",
      "Iteration 100, loss = 0.56758782\n",
      "Iteration 101, loss = 0.56751320\n",
      "Iteration 102, loss = 0.56741941\n",
      "Iteration 103, loss = 0.56736039\n",
      "Iteration 104, loss = 0.56726389\n",
      "Iteration 105, loss = 0.56719078\n",
      "Iteration 106, loss = 0.56710242\n",
      "Iteration 107, loss = 0.56710474\n",
      "Iteration 108, loss = 0.56696238\n",
      "Iteration 109, loss = 0.56691081\n",
      "Iteration 110, loss = 0.56682399\n",
      "Iteration 111, loss = 0.56674805\n",
      "Iteration 112, loss = 0.56669377\n",
      "Iteration 113, loss = 0.56662044\n",
      "Iteration 114, loss = 0.56658075\n",
      "Iteration 115, loss = 0.56649196\n",
      "Iteration 116, loss = 0.56642527\n",
      "Iteration 117, loss = 0.56640572\n",
      "Iteration 118, loss = 0.56630647\n",
      "Iteration 119, loss = 0.56622954\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76343039\n",
      "Iteration 2, loss = 0.75840253\n",
      "Iteration 3, loss = 0.75362917\n",
      "Iteration 4, loss = 0.74902215\n",
      "Iteration 5, loss = 0.74428578\n",
      "Iteration 6, loss = 0.73978366\n",
      "Iteration 7, loss = 0.73538921\n",
      "Iteration 8, loss = 0.73101949\n",
      "Iteration 9, loss = 0.72682441\n",
      "Iteration 10, loss = 0.72256678\n",
      "Iteration 11, loss = 0.71850954\n",
      "Iteration 12, loss = 0.71438511\n",
      "Iteration 13, loss = 0.71058021\n",
      "Iteration 14, loss = 0.70669550\n",
      "Iteration 15, loss = 0.70288154\n",
      "Iteration 16, loss = 0.69928183\n",
      "Iteration 17, loss = 0.69566668\n",
      "Iteration 18, loss = 0.69228080\n",
      "Iteration 19, loss = 0.68892049\n",
      "Iteration 20, loss = 0.68566603\n",
      "Iteration 21, loss = 0.68232456\n",
      "Iteration 22, loss = 0.67940403\n",
      "Iteration 23, loss = 0.67631620\n",
      "Iteration 24, loss = 0.67350127\n",
      "Iteration 25, loss = 0.67055080\n",
      "Iteration 26, loss = 0.66786948\n",
      "Iteration 27, loss = 0.66513400\n",
      "Iteration 28, loss = 0.66250923\n",
      "Iteration 29, loss = 0.66001563\n",
      "Iteration 30, loss = 0.65737780\n",
      "Iteration 31, loss = 0.65500802\n",
      "Iteration 32, loss = 0.65248451\n",
      "Iteration 33, loss = 0.65010462\n",
      "Iteration 34, loss = 0.64776306\n",
      "Iteration 35, loss = 0.64546766\n",
      "Iteration 36, loss = 0.64319305\n",
      "Iteration 37, loss = 0.64098466\n",
      "Iteration 38, loss = 0.63883695\n",
      "Iteration 39, loss = 0.63662765\n",
      "Iteration 40, loss = 0.63450422\n",
      "Iteration 41, loss = 0.63237800\n",
      "Iteration 42, loss = 0.63047187\n",
      "Iteration 43, loss = 0.62832881\n",
      "Iteration 44, loss = 0.62640706\n",
      "Iteration 45, loss = 0.62455459\n",
      "Iteration 46, loss = 0.62258173\n",
      "Iteration 47, loss = 0.62087417\n",
      "Iteration 48, loss = 0.61907847\n",
      "Iteration 49, loss = 0.61739161\n",
      "Iteration 50, loss = 0.61577901\n",
      "Iteration 51, loss = 0.61419084\n",
      "Iteration 52, loss = 0.61278153\n",
      "Iteration 53, loss = 0.61116488\n",
      "Iteration 54, loss = 0.60980618\n",
      "Iteration 55, loss = 0.60840732\n",
      "Iteration 56, loss = 0.60700964\n",
      "Iteration 57, loss = 0.60569720\n",
      "Iteration 58, loss = 0.60446059\n",
      "Iteration 59, loss = 0.60312060\n",
      "Iteration 60, loss = 0.60194758\n",
      "Iteration 61, loss = 0.60085243\n",
      "Iteration 62, loss = 0.59965129\n",
      "Iteration 63, loss = 0.59855618\n",
      "Iteration 64, loss = 0.59753164\n",
      "Iteration 65, loss = 0.59643187\n",
      "Iteration 66, loss = 0.59545971\n",
      "Iteration 67, loss = 0.59448471\n",
      "Iteration 68, loss = 0.59346620\n",
      "Iteration 69, loss = 0.59253175\n",
      "Iteration 70, loss = 0.59160352\n",
      "Iteration 71, loss = 0.59069620\n",
      "Iteration 72, loss = 0.58983896\n",
      "Iteration 73, loss = 0.58896721\n",
      "Iteration 74, loss = 0.58817413\n",
      "Iteration 75, loss = 0.58740359\n",
      "Iteration 76, loss = 0.58662356\n",
      "Iteration 77, loss = 0.58577728\n",
      "Iteration 78, loss = 0.58503636\n",
      "Iteration 79, loss = 0.58429673\n",
      "Iteration 80, loss = 0.58354375\n",
      "Iteration 81, loss = 0.58275008\n",
      "Iteration 82, loss = 0.58205387\n",
      "Iteration 83, loss = 0.58138284\n",
      "Iteration 84, loss = 0.58069734\n",
      "Iteration 85, loss = 0.58002273\n",
      "Iteration 86, loss = 0.57940553\n",
      "Iteration 87, loss = 0.57873499\n",
      "Iteration 88, loss = 0.57814340\n",
      "Iteration 89, loss = 0.57754792\n",
      "Iteration 90, loss = 0.57690932\n",
      "Iteration 91, loss = 0.57632654\n",
      "Iteration 92, loss = 0.57577544\n",
      "Iteration 93, loss = 0.57513338\n",
      "Iteration 94, loss = 0.57457847\n",
      "Iteration 95, loss = 0.57405205\n",
      "Iteration 96, loss = 0.57352827\n",
      "Iteration 97, loss = 0.57297667\n",
      "Iteration 98, loss = 0.57253677\n",
      "Iteration 99, loss = 0.57203700\n",
      "Iteration 100, loss = 0.57156238\n",
      "Iteration 101, loss = 0.57113118\n",
      "Iteration 102, loss = 0.57071699\n",
      "Iteration 103, loss = 0.57027274\n",
      "Iteration 104, loss = 0.56982625\n",
      "Iteration 105, loss = 0.56948280\n",
      "Iteration 106, loss = 0.56908257\n",
      "Iteration 107, loss = 0.56869975\n",
      "Iteration 108, loss = 0.56831967\n",
      "Iteration 109, loss = 0.56798059\n",
      "Iteration 110, loss = 0.56763487\n",
      "Iteration 111, loss = 0.56730833\n",
      "Iteration 112, loss = 0.56693157\n",
      "Iteration 113, loss = 0.56659358\n",
      "Iteration 114, loss = 0.56627028\n",
      "Iteration 115, loss = 0.56596320\n",
      "Iteration 116, loss = 0.56564277\n",
      "Iteration 117, loss = 0.56529056\n",
      "Iteration 118, loss = 0.56506151\n",
      "Iteration 119, loss = 0.56474794\n",
      "Iteration 120, loss = 0.56444114\n",
      "Iteration 121, loss = 0.56422281\n",
      "Iteration 122, loss = 0.56389530\n",
      "Iteration 123, loss = 0.56365214\n",
      "Iteration 124, loss = 0.56337391\n",
      "Iteration 125, loss = 0.56313646\n",
      "Iteration 126, loss = 0.56286105\n",
      "Iteration 127, loss = 0.56263857\n",
      "Iteration 128, loss = 0.56240371\n",
      "Iteration 129, loss = 0.56213338\n",
      "Iteration 130, loss = 0.56194784\n",
      "Iteration 131, loss = 0.56169686\n",
      "Iteration 132, loss = 0.56146748\n",
      "Iteration 133, loss = 0.56125236\n",
      "Iteration 134, loss = 0.56105420\n",
      "Iteration 135, loss = 0.56084645\n",
      "Iteration 136, loss = 0.56062276\n",
      "Iteration 137, loss = 0.56047915\n",
      "Iteration 138, loss = 0.56026412\n",
      "Iteration 139, loss = 0.56011291\n",
      "Iteration 140, loss = 0.55990478\n",
      "Iteration 141, loss = 0.55974949\n",
      "Iteration 142, loss = 0.55958461\n",
      "Iteration 143, loss = 0.55944476\n",
      "Iteration 144, loss = 0.55928384\n",
      "Iteration 145, loss = 0.55917370\n",
      "Iteration 146, loss = 0.55900399\n",
      "Iteration 147, loss = 0.55887970\n",
      "Iteration 148, loss = 0.55878537\n",
      "Iteration 149, loss = 0.55864878\n",
      "Iteration 150, loss = 0.55853817\n",
      "Iteration 151, loss = 0.55843548\n",
      "Iteration 152, loss = 0.55831975\n",
      "Iteration 153, loss = 0.55818827\n",
      "Iteration 154, loss = 0.55807692\n",
      "Iteration 155, loss = 0.55797266\n",
      "Iteration 156, loss = 0.55787772\n",
      "Iteration 157, loss = 0.55778037\n",
      "Iteration 158, loss = 0.55769042\n",
      "Iteration 159, loss = 0.55761128\n",
      "Iteration 160, loss = 0.55753782\n",
      "Iteration 161, loss = 0.55745942\n",
      "Iteration 162, loss = 0.55740708\n",
      "Iteration 163, loss = 0.55732452\n",
      "Iteration 164, loss = 0.55725598\n",
      "Iteration 165, loss = 0.55720318\n",
      "Iteration 166, loss = 0.55713102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.89224942\n",
      "Iteration 2, loss = 0.88771991\n",
      "Iteration 3, loss = 0.88347055\n",
      "Iteration 4, loss = 0.87907502\n",
      "Iteration 5, loss = 0.87482659\n",
      "Iteration 6, loss = 0.87059950\n",
      "Iteration 7, loss = 0.86635947\n",
      "Iteration 8, loss = 0.86220390\n",
      "Iteration 9, loss = 0.85801350\n",
      "Iteration 10, loss = 0.85393273\n",
      "Iteration 11, loss = 0.84986722\n",
      "Iteration 12, loss = 0.84590871\n",
      "Iteration 13, loss = 0.84181961\n",
      "Iteration 14, loss = 0.83794387\n",
      "Iteration 15, loss = 0.83415781\n",
      "Iteration 16, loss = 0.83033255\n",
      "Iteration 17, loss = 0.82659812\n",
      "Iteration 18, loss = 0.82288310\n",
      "Iteration 19, loss = 0.81924712\n",
      "Iteration 20, loss = 0.81570082\n",
      "Iteration 21, loss = 0.81216915\n",
      "Iteration 22, loss = 0.80854082\n",
      "Iteration 23, loss = 0.80519218\n",
      "Iteration 24, loss = 0.80159101\n",
      "Iteration 25, loss = 0.79821446\n",
      "Iteration 26, loss = 0.79480953\n",
      "Iteration 27, loss = 0.79140158\n",
      "Iteration 28, loss = 0.78802642\n",
      "Iteration 29, loss = 0.78476978\n",
      "Iteration 30, loss = 0.78145947\n",
      "Iteration 31, loss = 0.77830587\n",
      "Iteration 32, loss = 0.77511028\n",
      "Iteration 33, loss = 0.77197390\n",
      "Iteration 34, loss = 0.76888652\n",
      "Iteration 35, loss = 0.76583539\n",
      "Iteration 36, loss = 0.76271297\n",
      "Iteration 37, loss = 0.75971204\n",
      "Iteration 38, loss = 0.75668936\n",
      "Iteration 39, loss = 0.75372363\n",
      "Iteration 40, loss = 0.75078531\n",
      "Iteration 41, loss = 0.74777199\n",
      "Iteration 42, loss = 0.74489246\n",
      "Iteration 43, loss = 0.74207774\n",
      "Iteration 44, loss = 0.73924055\n",
      "Iteration 45, loss = 0.73644326\n",
      "Iteration 46, loss = 0.73367027\n",
      "Iteration 47, loss = 0.73097724\n",
      "Iteration 48, loss = 0.72824872\n",
      "Iteration 49, loss = 0.72553744\n",
      "Iteration 50, loss = 0.72284951\n",
      "Iteration 51, loss = 0.72023858\n",
      "Iteration 52, loss = 0.71770901\n",
      "Iteration 53, loss = 0.71519630\n",
      "Iteration 54, loss = 0.71253689\n",
      "Iteration 55, loss = 0.71023126\n",
      "Iteration 56, loss = 0.70768784\n",
      "Iteration 57, loss = 0.70531936\n",
      "Iteration 58, loss = 0.70289479\n",
      "Iteration 59, loss = 0.70052946\n",
      "Iteration 60, loss = 0.69809106\n",
      "Iteration 61, loss = 0.69583878\n",
      "Iteration 62, loss = 0.69345521\n",
      "Iteration 63, loss = 0.69124614\n",
      "Iteration 64, loss = 0.68901429\n",
      "Iteration 65, loss = 0.68678493\n",
      "Iteration 66, loss = 0.68455909\n",
      "Iteration 67, loss = 0.68237080\n",
      "Iteration 68, loss = 0.68017445\n",
      "Iteration 69, loss = 0.67798966\n",
      "Iteration 70, loss = 0.67585481\n",
      "Iteration 71, loss = 0.67364167\n",
      "Iteration 72, loss = 0.67148142\n",
      "Iteration 73, loss = 0.66947051\n",
      "Iteration 74, loss = 0.66727384\n",
      "Iteration 75, loss = 0.66511324\n",
      "Iteration 76, loss = 0.66306422\n",
      "Iteration 77, loss = 0.66109326\n",
      "Iteration 78, loss = 0.65907062\n",
      "Iteration 79, loss = 0.65706322\n",
      "Iteration 80, loss = 0.65498721\n",
      "Iteration 81, loss = 0.65307456\n",
      "Iteration 82, loss = 0.65118710\n",
      "Iteration 83, loss = 0.64910867\n",
      "Iteration 84, loss = 0.64729538\n",
      "Iteration 85, loss = 0.64538407\n",
      "Iteration 86, loss = 0.64353042\n",
      "Iteration 87, loss = 0.64165347\n",
      "Iteration 88, loss = 0.63982873\n",
      "Iteration 89, loss = 0.63802670\n",
      "Iteration 90, loss = 0.63621466\n",
      "Iteration 91, loss = 0.63447956\n",
      "Iteration 92, loss = 0.63271441\n",
      "Iteration 93, loss = 0.63096429\n",
      "Iteration 94, loss = 0.62926296\n",
      "Iteration 95, loss = 0.62758780\n",
      "Iteration 96, loss = 0.62583072\n",
      "Iteration 97, loss = 0.62421663\n",
      "Iteration 98, loss = 0.62260214\n",
      "Iteration 99, loss = 0.62094177\n",
      "Iteration 100, loss = 0.61934130\n",
      "Iteration 101, loss = 0.61789115\n",
      "Iteration 102, loss = 0.61627685\n",
      "Iteration 103, loss = 0.61476092\n",
      "Iteration 104, loss = 0.61333992\n",
      "Iteration 105, loss = 0.61178686\n",
      "Iteration 106, loss = 0.61038299\n",
      "Iteration 107, loss = 0.60899043\n",
      "Iteration 108, loss = 0.60753792\n",
      "Iteration 109, loss = 0.60624902\n",
      "Iteration 110, loss = 0.60498884\n",
      "Iteration 111, loss = 0.60372897\n",
      "Iteration 112, loss = 0.60252298\n",
      "Iteration 113, loss = 0.60131715\n",
      "Iteration 114, loss = 0.60015348\n",
      "Iteration 115, loss = 0.59901061\n",
      "Iteration 116, loss = 0.59782336\n",
      "Iteration 117, loss = 0.59672727\n",
      "Iteration 118, loss = 0.59553141\n",
      "Iteration 119, loss = 0.59441407\n",
      "Iteration 120, loss = 0.59337772\n",
      "Iteration 121, loss = 0.59227764\n",
      "Iteration 122, loss = 0.59120346\n",
      "Iteration 123, loss = 0.59015400\n",
      "Iteration 124, loss = 0.58911841\n",
      "Iteration 125, loss = 0.58808025\n",
      "Iteration 126, loss = 0.58703435\n",
      "Iteration 127, loss = 0.58601477\n",
      "Iteration 128, loss = 0.58492315\n",
      "Iteration 129, loss = 0.58395746\n",
      "Iteration 130, loss = 0.58289869\n",
      "Iteration 131, loss = 0.58198678\n",
      "Iteration 132, loss = 0.58095281\n",
      "Iteration 133, loss = 0.57996185\n",
      "Iteration 134, loss = 0.57904398\n",
      "Iteration 135, loss = 0.57809951\n",
      "Iteration 136, loss = 0.57718710\n",
      "Iteration 137, loss = 0.57622753\n",
      "Iteration 138, loss = 0.57535947\n",
      "Iteration 139, loss = 0.57441590\n",
      "Iteration 140, loss = 0.57356595\n",
      "Iteration 141, loss = 0.57277161\n",
      "Iteration 142, loss = 0.57182680\n",
      "Iteration 143, loss = 0.57102944\n",
      "Iteration 144, loss = 0.57028171\n",
      "Iteration 145, loss = 0.56948296\n",
      "Iteration 146, loss = 0.56870658\n",
      "Iteration 147, loss = 0.56793281\n",
      "Iteration 148, loss = 0.56714610\n",
      "Iteration 149, loss = 0.56639577\n",
      "Iteration 150, loss = 0.56568388\n",
      "Iteration 151, loss = 0.56488030\n",
      "Iteration 152, loss = 0.56417527\n",
      "Iteration 153, loss = 0.56349402\n",
      "Iteration 154, loss = 0.56280165\n",
      "Iteration 155, loss = 0.56214351\n",
      "Iteration 156, loss = 0.56150326\n",
      "Iteration 157, loss = 0.56082409\n",
      "Iteration 158, loss = 0.56018219\n",
      "Iteration 159, loss = 0.55957035\n",
      "Iteration 160, loss = 0.55895915\n",
      "Iteration 161, loss = 0.55832202\n",
      "Iteration 162, loss = 0.55769202\n",
      "Iteration 163, loss = 0.55713756\n",
      "Iteration 164, loss = 0.55650702\n",
      "Iteration 165, loss = 0.55598945\n",
      "Iteration 166, loss = 0.55541668\n",
      "Iteration 167, loss = 0.55483243\n",
      "Iteration 168, loss = 0.55435869\n",
      "Iteration 169, loss = 0.55379001\n",
      "Iteration 170, loss = 0.55325191\n",
      "Iteration 171, loss = 0.55267734\n",
      "Iteration 172, loss = 0.55211927\n",
      "Iteration 173, loss = 0.55159069\n",
      "Iteration 174, loss = 0.55103931\n",
      "Iteration 175, loss = 0.55052337\n",
      "Iteration 176, loss = 0.54997972\n",
      "Iteration 177, loss = 0.54949665\n",
      "Iteration 178, loss = 0.54897463\n",
      "Iteration 179, loss = 0.54850548\n",
      "Iteration 180, loss = 0.54809103\n",
      "Iteration 181, loss = 0.54764953\n",
      "Iteration 182, loss = 0.54720600\n",
      "Iteration 183, loss = 0.54681153\n",
      "Iteration 184, loss = 0.54637437\n",
      "Iteration 185, loss = 0.54596456\n",
      "Iteration 186, loss = 0.54556436\n",
      "Iteration 187, loss = 0.54512249\n",
      "Iteration 188, loss = 0.54473621\n",
      "Iteration 189, loss = 0.54430461\n",
      "Iteration 190, loss = 0.54394752\n",
      "Iteration 191, loss = 0.54357903\n",
      "Iteration 192, loss = 0.54330384\n",
      "Iteration 193, loss = 0.54289040\n",
      "Iteration 194, loss = 0.54259301\n",
      "Iteration 195, loss = 0.54225302\n",
      "Iteration 196, loss = 0.54194249\n",
      "Iteration 197, loss = 0.54158880\n",
      "Iteration 198, loss = 0.54132944\n",
      "Iteration 199, loss = 0.54094115\n",
      "Iteration 200, loss = 0.54064121\n",
      "Iteration 201, loss = 0.54041386\n",
      "Iteration 202, loss = 0.54001610\n",
      "Iteration 203, loss = 0.53979694\n",
      "Iteration 204, loss = 0.53950361\n",
      "Iteration 205, loss = 0.53921889\n",
      "Iteration 206, loss = 0.53897180\n",
      "Iteration 207, loss = 0.53869968\n",
      "Iteration 208, loss = 0.53849977\n",
      "Iteration 209, loss = 0.53822262\n",
      "Iteration 210, loss = 0.53803391\n",
      "Iteration 211, loss = 0.53778766\n",
      "Iteration 212, loss = 0.53760289\n",
      "Iteration 213, loss = 0.53740144\n",
      "Iteration 214, loss = 0.53719938\n",
      "Iteration 215, loss = 0.53700403\n",
      "Iteration 216, loss = 0.53680977\n",
      "Iteration 217, loss = 0.53661822\n",
      "Iteration 218, loss = 0.53646654\n",
      "Iteration 219, loss = 0.53628859\n",
      "Iteration 220, loss = 0.53608455\n",
      "Iteration 221, loss = 0.53592737\n",
      "Iteration 222, loss = 0.53573629\n",
      "Iteration 223, loss = 0.53559076\n",
      "Iteration 224, loss = 0.53538622\n",
      "Iteration 225, loss = 0.53525474\n",
      "Iteration 226, loss = 0.53504874\n",
      "Iteration 227, loss = 0.53490862\n",
      "Iteration 228, loss = 0.53473346\n",
      "Iteration 229, loss = 0.53462630\n",
      "Iteration 230, loss = 0.53445176\n",
      "Iteration 231, loss = 0.53434361\n",
      "Iteration 232, loss = 0.53424249\n",
      "Iteration 233, loss = 0.53412301\n",
      "Iteration 234, loss = 0.53402904\n",
      "Iteration 235, loss = 0.53393915\n",
      "Iteration 236, loss = 0.53383459\n",
      "Iteration 237, loss = 0.53374511\n",
      "Iteration 238, loss = 0.53362352\n",
      "Iteration 239, loss = 0.53353820\n",
      "Iteration 240, loss = 0.53341647\n",
      "Iteration 241, loss = 0.53330278\n",
      "Iteration 242, loss = 0.53319344\n",
      "Iteration 243, loss = 0.53306180\n",
      "Iteration 244, loss = 0.53299870\n",
      "Iteration 245, loss = 0.53285413\n",
      "Iteration 246, loss = 0.53278138\n",
      "Iteration 247, loss = 0.53266034\n",
      "Iteration 248, loss = 0.53261536\n",
      "Iteration 249, loss = 0.53249256\n",
      "Iteration 250, loss = 0.53242613\n",
      "Iteration 251, loss = 0.53235712\n",
      "Iteration 252, loss = 0.53227640\n",
      "Iteration 253, loss = 0.53219058\n",
      "Iteration 254, loss = 0.53213413\n",
      "Iteration 255, loss = 0.53202307\n",
      "Iteration 256, loss = 0.53193620\n",
      "Iteration 257, loss = 0.53183754\n",
      "Iteration 258, loss = 0.53177122\n",
      "Iteration 259, loss = 0.53164527\n",
      "Iteration 260, loss = 0.53157821\n",
      "Iteration 261, loss = 0.53147041\n",
      "Iteration 262, loss = 0.53138633\n",
      "Iteration 263, loss = 0.53130750\n",
      "Iteration 264, loss = 0.53120705\n",
      "Iteration 265, loss = 0.53113187\n",
      "Iteration 266, loss = 0.53105359\n",
      "Iteration 267, loss = 0.53093441\n",
      "Iteration 268, loss = 0.53083110\n",
      "Iteration 269, loss = 0.53074846\n",
      "Iteration 270, loss = 0.53070837\n",
      "Iteration 271, loss = 0.53058374\n",
      "Iteration 272, loss = 0.53050352\n",
      "Iteration 273, loss = 0.53040778\n",
      "Iteration 274, loss = 0.53033135\n",
      "Iteration 275, loss = 0.53023519\n",
      "Iteration 276, loss = 0.53017615\n",
      "Iteration 277, loss = 0.53007122\n",
      "Iteration 278, loss = 0.53001025\n",
      "Iteration 279, loss = 0.52992933\n",
      "Iteration 280, loss = 0.52986278\n",
      "Iteration 281, loss = 0.52980701\n",
      "Iteration 282, loss = 0.52973148\n",
      "Iteration 283, loss = 0.52965978\n",
      "Iteration 284, loss = 0.52962041\n",
      "Iteration 285, loss = 0.52954472\n",
      "Iteration 286, loss = 0.52951640\n",
      "Iteration 287, loss = 0.52944394\n",
      "Iteration 288, loss = 0.52937388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61613256\n",
      "Iteration 2, loss = 0.61457153\n",
      "Iteration 3, loss = 0.61303148\n",
      "Iteration 4, loss = 0.61154583\n",
      "Iteration 5, loss = 0.61012984\n",
      "Iteration 6, loss = 0.60862917\n",
      "Iteration 7, loss = 0.60722424\n",
      "Iteration 8, loss = 0.60587142\n",
      "Iteration 9, loss = 0.60459847\n",
      "Iteration 10, loss = 0.60331029\n",
      "Iteration 11, loss = 0.60184121\n",
      "Iteration 12, loss = 0.60063144\n",
      "Iteration 13, loss = 0.59933849\n",
      "Iteration 14, loss = 0.59819034\n",
      "Iteration 15, loss = 0.59691728\n",
      "Iteration 16, loss = 0.59581727\n",
      "Iteration 17, loss = 0.59458296\n",
      "Iteration 18, loss = 0.59358279\n",
      "Iteration 19, loss = 0.59253166\n",
      "Iteration 20, loss = 0.59138973\n",
      "Iteration 21, loss = 0.59047765\n",
      "Iteration 22, loss = 0.58928059\n",
      "Iteration 23, loss = 0.58836668\n",
      "Iteration 24, loss = 0.58723641\n",
      "Iteration 25, loss = 0.58625940\n",
      "Iteration 26, loss = 0.58520042\n",
      "Iteration 27, loss = 0.58425353\n",
      "Iteration 28, loss = 0.58336264\n",
      "Iteration 29, loss = 0.58235339\n",
      "Iteration 30, loss = 0.58143502\n",
      "Iteration 31, loss = 0.58060665\n",
      "Iteration 32, loss = 0.57975096\n",
      "Iteration 33, loss = 0.57880970\n",
      "Iteration 34, loss = 0.57804579\n",
      "Iteration 35, loss = 0.57733563\n",
      "Iteration 36, loss = 0.57641190\n",
      "Iteration 37, loss = 0.57576627\n",
      "Iteration 38, loss = 0.57497213\n",
      "Iteration 39, loss = 0.57417817\n",
      "Iteration 40, loss = 0.57352946\n",
      "Iteration 41, loss = 0.57285764\n",
      "Iteration 42, loss = 0.57215058\n",
      "Iteration 43, loss = 0.57145001\n",
      "Iteration 44, loss = 0.57081036\n",
      "Iteration 45, loss = 0.57018948\n",
      "Iteration 46, loss = 0.56954062\n",
      "Iteration 47, loss = 0.56894622\n",
      "Iteration 48, loss = 0.56836733\n",
      "Iteration 49, loss = 0.56776366\n",
      "Iteration 50, loss = 0.56727887\n",
      "Iteration 51, loss = 0.56667702\n",
      "Iteration 52, loss = 0.56618912\n",
      "Iteration 53, loss = 0.56569410\n",
      "Iteration 54, loss = 0.56516208\n",
      "Iteration 55, loss = 0.56466330\n",
      "Iteration 56, loss = 0.56420025\n",
      "Iteration 57, loss = 0.56369785\n",
      "Iteration 58, loss = 0.56323160\n",
      "Iteration 59, loss = 0.56280257\n",
      "Iteration 60, loss = 0.56231887\n",
      "Iteration 61, loss = 0.56193729\n",
      "Iteration 62, loss = 0.56144242\n",
      "Iteration 63, loss = 0.56105372\n",
      "Iteration 64, loss = 0.56061113\n",
      "Iteration 65, loss = 0.56022378\n",
      "Iteration 66, loss = 0.55975293\n",
      "Iteration 67, loss = 0.55935936\n",
      "Iteration 68, loss = 0.55894311\n",
      "Iteration 69, loss = 0.55851832\n",
      "Iteration 70, loss = 0.55815857\n",
      "Iteration 71, loss = 0.55772326\n",
      "Iteration 72, loss = 0.55745548\n",
      "Iteration 73, loss = 0.55702457\n",
      "Iteration 74, loss = 0.55673683\n",
      "Iteration 75, loss = 0.55642097\n",
      "Iteration 76, loss = 0.55614528\n",
      "Iteration 77, loss = 0.55586845\n",
      "Iteration 78, loss = 0.55555831\n",
      "Iteration 79, loss = 0.55528549\n",
      "Iteration 80, loss = 0.55500449\n",
      "Iteration 81, loss = 0.55466738\n",
      "Iteration 82, loss = 0.55439242\n",
      "Iteration 83, loss = 0.55408491\n",
      "Iteration 84, loss = 0.55383882\n",
      "Iteration 85, loss = 0.55349986\n",
      "Iteration 86, loss = 0.55322409\n",
      "Iteration 87, loss = 0.55296690\n",
      "Iteration 88, loss = 0.55268495\n",
      "Iteration 89, loss = 0.55242859\n",
      "Iteration 90, loss = 0.55220607\n",
      "Iteration 91, loss = 0.55193890\n",
      "Iteration 92, loss = 0.55170708\n",
      "Iteration 93, loss = 0.55150140\n",
      "Iteration 94, loss = 0.55124597\n",
      "Iteration 95, loss = 0.55103412\n",
      "Iteration 96, loss = 0.55084574\n",
      "Iteration 97, loss = 0.55063657\n",
      "Iteration 98, loss = 0.55043172\n",
      "Iteration 99, loss = 0.55025565\n",
      "Iteration 100, loss = 0.55005865\n",
      "Iteration 101, loss = 0.54988496\n",
      "Iteration 102, loss = 0.54971598\n",
      "Iteration 103, loss = 0.54953335\n",
      "Iteration 104, loss = 0.54932748\n",
      "Iteration 105, loss = 0.54916971\n",
      "Iteration 106, loss = 0.54896985\n",
      "Iteration 107, loss = 0.54879187\n",
      "Iteration 108, loss = 0.54862100\n",
      "Iteration 109, loss = 0.54842308\n",
      "Iteration 110, loss = 0.54825267\n",
      "Iteration 111, loss = 0.54808438\n",
      "Iteration 112, loss = 0.54789094\n",
      "Iteration 113, loss = 0.54770960\n",
      "Iteration 114, loss = 0.54752345\n",
      "Iteration 115, loss = 0.54739207\n",
      "Iteration 116, loss = 0.54723241\n",
      "Iteration 117, loss = 0.54704352\n",
      "Iteration 118, loss = 0.54691884\n",
      "Iteration 119, loss = 0.54677112\n",
      "Iteration 120, loss = 0.54663516\n",
      "Iteration 121, loss = 0.54649950\n",
      "Iteration 122, loss = 0.54636437\n",
      "Iteration 123, loss = 0.54627501\n",
      "Iteration 124, loss = 0.54614177\n",
      "Iteration 125, loss = 0.54605440\n",
      "Iteration 126, loss = 0.54592720\n",
      "Iteration 127, loss = 0.54582013\n",
      "Iteration 128, loss = 0.54571807\n",
      "Iteration 129, loss = 0.54562599\n",
      "Iteration 130, loss = 0.54551765\n",
      "Iteration 131, loss = 0.54542603\n",
      "Iteration 132, loss = 0.54534343\n",
      "Iteration 133, loss = 0.54522905\n",
      "Iteration 134, loss = 0.54508338\n",
      "Iteration 135, loss = 0.54500504\n",
      "Iteration 136, loss = 0.54487303\n",
      "Iteration 137, loss = 0.54476175\n",
      "Iteration 138, loss = 0.54465589\n",
      "Iteration 139, loss = 0.54454622\n",
      "Iteration 140, loss = 0.54444355\n",
      "Iteration 141, loss = 0.54436352\n",
      "Iteration 142, loss = 0.54430834\n",
      "Iteration 143, loss = 0.54417677\n",
      "Iteration 144, loss = 0.54409339\n",
      "Iteration 145, loss = 0.54401649\n",
      "Iteration 146, loss = 0.54394340\n",
      "Iteration 147, loss = 0.54385107\n",
      "Iteration 148, loss = 0.54378219\n",
      "Iteration 149, loss = 0.54370090\n",
      "Iteration 150, loss = 0.54358656\n",
      "Iteration 151, loss = 0.54352453\n",
      "Iteration 152, loss = 0.54343323\n",
      "Iteration 153, loss = 0.54336829\n",
      "Iteration 154, loss = 0.54325236\n",
      "Iteration 155, loss = 0.54319567\n",
      "Iteration 156, loss = 0.54310617\n",
      "Iteration 157, loss = 0.54303813\n",
      "Iteration 158, loss = 0.54293578\n",
      "Iteration 159, loss = 0.54286721\n",
      "Iteration 160, loss = 0.54279596\n",
      "Iteration 161, loss = 0.54272695\n",
      "Iteration 162, loss = 0.54265252\n",
      "Iteration 163, loss = 0.54261377\n",
      "Iteration 164, loss = 0.54254754\n",
      "Iteration 165, loss = 0.54252153\n",
      "Iteration 166, loss = 0.54245158\n",
      "Iteration 167, loss = 0.54239994\n",
      "Iteration 168, loss = 0.54235373\n",
      "Iteration 169, loss = 0.54228762\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67424247\n",
      "Iteration 2, loss = 0.67152795\n",
      "Iteration 3, loss = 0.66880606\n",
      "Iteration 4, loss = 0.66613270\n",
      "Iteration 5, loss = 0.66355813\n",
      "Iteration 6, loss = 0.66095795\n",
      "Iteration 7, loss = 0.65841125\n",
      "Iteration 8, loss = 0.65608502\n",
      "Iteration 9, loss = 0.65357024\n",
      "Iteration 10, loss = 0.65126156\n",
      "Iteration 11, loss = 0.64891809\n",
      "Iteration 12, loss = 0.64668784\n",
      "Iteration 13, loss = 0.64428131\n",
      "Iteration 14, loss = 0.64219492\n",
      "Iteration 15, loss = 0.64000787\n",
      "Iteration 16, loss = 0.63781105\n",
      "Iteration 17, loss = 0.63576290\n",
      "Iteration 18, loss = 0.63364011\n",
      "Iteration 19, loss = 0.63174325\n",
      "Iteration 20, loss = 0.62969095\n",
      "Iteration 21, loss = 0.62774116\n",
      "Iteration 22, loss = 0.62586976\n",
      "Iteration 23, loss = 0.62396141\n",
      "Iteration 24, loss = 0.62218153\n",
      "Iteration 25, loss = 0.62020368\n",
      "Iteration 26, loss = 0.61844435\n",
      "Iteration 27, loss = 0.61663161\n",
      "Iteration 28, loss = 0.61484783\n",
      "Iteration 29, loss = 0.61312864\n",
      "Iteration 30, loss = 0.61141630\n",
      "Iteration 31, loss = 0.60971903\n",
      "Iteration 32, loss = 0.60807779\n",
      "Iteration 33, loss = 0.60657729\n",
      "Iteration 34, loss = 0.60499040\n",
      "Iteration 35, loss = 0.60346418\n",
      "Iteration 36, loss = 0.60198054\n",
      "Iteration 37, loss = 0.60062450\n",
      "Iteration 38, loss = 0.59913979\n",
      "Iteration 39, loss = 0.59763525\n",
      "Iteration 40, loss = 0.59621161\n",
      "Iteration 41, loss = 0.59482747\n",
      "Iteration 42, loss = 0.59344385\n",
      "Iteration 43, loss = 0.59200984\n",
      "Iteration 44, loss = 0.59068850\n",
      "Iteration 45, loss = 0.58942354\n",
      "Iteration 46, loss = 0.58811309\n",
      "Iteration 47, loss = 0.58685619\n",
      "Iteration 48, loss = 0.58560031\n",
      "Iteration 49, loss = 0.58446224\n",
      "Iteration 50, loss = 0.58321816\n",
      "Iteration 51, loss = 0.58213668\n",
      "Iteration 52, loss = 0.58103945\n",
      "Iteration 53, loss = 0.57991198\n",
      "Iteration 54, loss = 0.57889646\n",
      "Iteration 55, loss = 0.57788096\n",
      "Iteration 56, loss = 0.57689055\n",
      "Iteration 57, loss = 0.57593700\n",
      "Iteration 58, loss = 0.57500760\n",
      "Iteration 59, loss = 0.57404663\n",
      "Iteration 60, loss = 0.57311347\n",
      "Iteration 61, loss = 0.57222282\n",
      "Iteration 62, loss = 0.57129079\n",
      "Iteration 63, loss = 0.57038477\n",
      "Iteration 64, loss = 0.56949632\n",
      "Iteration 65, loss = 0.56865161\n",
      "Iteration 66, loss = 0.56774530\n",
      "Iteration 67, loss = 0.56695287\n",
      "Iteration 68, loss = 0.56609916\n",
      "Iteration 69, loss = 0.56533624\n",
      "Iteration 70, loss = 0.56453631\n",
      "Iteration 71, loss = 0.56378723\n",
      "Iteration 72, loss = 0.56302689\n",
      "Iteration 73, loss = 0.56224386\n",
      "Iteration 74, loss = 0.56160653\n",
      "Iteration 75, loss = 0.56077773\n",
      "Iteration 76, loss = 0.56008276\n",
      "Iteration 77, loss = 0.55934151\n",
      "Iteration 78, loss = 0.55866263\n",
      "Iteration 79, loss = 0.55799287\n",
      "Iteration 80, loss = 0.55731694\n",
      "Iteration 81, loss = 0.55658303\n",
      "Iteration 82, loss = 0.55595055\n",
      "Iteration 83, loss = 0.55527991\n",
      "Iteration 84, loss = 0.55464136\n",
      "Iteration 85, loss = 0.55395397\n",
      "Iteration 86, loss = 0.55342280\n",
      "Iteration 87, loss = 0.55280321\n",
      "Iteration 88, loss = 0.55224175\n",
      "Iteration 89, loss = 0.55168784\n",
      "Iteration 90, loss = 0.55113961\n",
      "Iteration 91, loss = 0.55059751\n",
      "Iteration 92, loss = 0.55008597\n",
      "Iteration 93, loss = 0.54950361\n",
      "Iteration 94, loss = 0.54898499\n",
      "Iteration 95, loss = 0.54847230\n",
      "Iteration 96, loss = 0.54789181\n",
      "Iteration 97, loss = 0.54734070\n",
      "Iteration 98, loss = 0.54686334\n",
      "Iteration 99, loss = 0.54628376\n",
      "Iteration 100, loss = 0.54577757\n",
      "Iteration 101, loss = 0.54525888\n",
      "Iteration 102, loss = 0.54471708\n",
      "Iteration 103, loss = 0.54418085\n",
      "Iteration 104, loss = 0.54370607\n",
      "Iteration 105, loss = 0.54315631\n",
      "Iteration 106, loss = 0.54267747\n",
      "Iteration 107, loss = 0.54218322\n",
      "Iteration 108, loss = 0.54171002\n",
      "Iteration 109, loss = 0.54123785\n",
      "Iteration 110, loss = 0.54073617\n",
      "Iteration 111, loss = 0.54028183\n",
      "Iteration 112, loss = 0.53987947\n",
      "Iteration 113, loss = 0.53946133\n",
      "Iteration 114, loss = 0.53898237\n",
      "Iteration 115, loss = 0.53866714\n",
      "Iteration 116, loss = 0.53821541\n",
      "Iteration 117, loss = 0.53772643\n",
      "Iteration 118, loss = 0.53741655\n",
      "Iteration 119, loss = 0.53695717\n",
      "Iteration 120, loss = 0.53657437\n",
      "Iteration 121, loss = 0.53619264\n",
      "Iteration 122, loss = 0.53580360\n",
      "Iteration 123, loss = 0.53546901\n",
      "Iteration 124, loss = 0.53509011\n",
      "Iteration 125, loss = 0.53479093\n",
      "Iteration 126, loss = 0.53442922\n",
      "Iteration 127, loss = 0.53416177\n",
      "Iteration 128, loss = 0.53384522\n",
      "Iteration 129, loss = 0.53354696\n",
      "Iteration 130, loss = 0.53330810\n",
      "Iteration 131, loss = 0.53299042\n",
      "Iteration 132, loss = 0.53270329\n",
      "Iteration 133, loss = 0.53242784\n",
      "Iteration 134, loss = 0.53216930\n",
      "Iteration 135, loss = 0.53187323\n",
      "Iteration 136, loss = 0.53162054\n",
      "Iteration 137, loss = 0.53133257\n",
      "Iteration 138, loss = 0.53108460\n",
      "Iteration 139, loss = 0.53088809\n",
      "Iteration 140, loss = 0.53065619\n",
      "Iteration 141, loss = 0.53041080\n",
      "Iteration 142, loss = 0.53023846\n",
      "Iteration 143, loss = 0.53000371\n",
      "Iteration 144, loss = 0.52978348\n",
      "Iteration 145, loss = 0.52956686\n",
      "Iteration 146, loss = 0.52933256\n",
      "Iteration 147, loss = 0.52913799\n",
      "Iteration 148, loss = 0.52896537\n",
      "Iteration 149, loss = 0.52872337\n",
      "Iteration 150, loss = 0.52853547\n",
      "Iteration 151, loss = 0.52839986\n",
      "Iteration 152, loss = 0.52814924\n",
      "Iteration 153, loss = 0.52795437\n",
      "Iteration 154, loss = 0.52776603\n",
      "Iteration 155, loss = 0.52761785\n",
      "Iteration 156, loss = 0.52734679\n",
      "Iteration 157, loss = 0.52717617\n",
      "Iteration 158, loss = 0.52699109\n",
      "Iteration 159, loss = 0.52675969\n",
      "Iteration 160, loss = 0.52663793\n",
      "Iteration 161, loss = 0.52643276\n",
      "Iteration 162, loss = 0.52621462\n",
      "Iteration 163, loss = 0.52606515\n",
      "Iteration 164, loss = 0.52583514\n",
      "Iteration 165, loss = 0.52566228\n",
      "Iteration 166, loss = 0.52547795\n",
      "Iteration 167, loss = 0.52527057\n",
      "Iteration 168, loss = 0.52511799\n",
      "Iteration 169, loss = 0.52493172\n",
      "Iteration 170, loss = 0.52476280\n",
      "Iteration 171, loss = 0.52459083\n",
      "Iteration 172, loss = 0.52442502\n",
      "Iteration 173, loss = 0.52427247\n",
      "Iteration 174, loss = 0.52409047\n",
      "Iteration 175, loss = 0.52393687\n",
      "Iteration 176, loss = 0.52378739\n",
      "Iteration 177, loss = 0.52365608\n",
      "Iteration 178, loss = 0.52348332\n",
      "Iteration 179, loss = 0.52335815\n",
      "Iteration 180, loss = 0.52321298\n",
      "Iteration 181, loss = 0.52307321\n",
      "Iteration 182, loss = 0.52293741\n",
      "Iteration 183, loss = 0.52279293\n",
      "Iteration 184, loss = 0.52266800\n",
      "Iteration 185, loss = 0.52253054\n",
      "Iteration 186, loss = 0.52237304\n",
      "Iteration 187, loss = 0.52223693\n",
      "Iteration 188, loss = 0.52207541\n",
      "Iteration 189, loss = 0.52193678\n",
      "Iteration 190, loss = 0.52179143\n",
      "Iteration 191, loss = 0.52163323\n",
      "Iteration 192, loss = 0.52148525\n",
      "Iteration 193, loss = 0.52134848\n",
      "Iteration 194, loss = 0.52122295\n",
      "Iteration 195, loss = 0.52110256\n",
      "Iteration 196, loss = 0.52097718\n",
      "Iteration 197, loss = 0.52085676\n",
      "Iteration 198, loss = 0.52074499\n",
      "Iteration 199, loss = 0.52063644\n",
      "Iteration 200, loss = 0.52050584\n",
      "Iteration 201, loss = 0.52042442\n",
      "Iteration 202, loss = 0.52030825\n",
      "Iteration 203, loss = 0.52020614\n",
      "Iteration 204, loss = 0.52015188\n",
      "Iteration 205, loss = 0.52001667\n",
      "Iteration 206, loss = 0.51993686\n",
      "Iteration 207, loss = 0.51985903\n",
      "Iteration 208, loss = 0.51979027\n",
      "Iteration 209, loss = 0.51969445\n",
      "Iteration 210, loss = 0.51962216\n",
      "Iteration 211, loss = 0.51955010\n",
      "Iteration 212, loss = 0.51949446\n",
      "Iteration 213, loss = 0.51944371\n",
      "Iteration 214, loss = 0.51937662\n",
      "Iteration 215, loss = 0.51929013\n",
      "Iteration 216, loss = 0.51922857\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.01987409\n",
      "Iteration 2, loss = 1.01503879\n",
      "Iteration 3, loss = 1.01023922\n",
      "Iteration 4, loss = 1.00559232\n",
      "Iteration 5, loss = 1.00090288\n",
      "Iteration 6, loss = 0.99636537\n",
      "Iteration 7, loss = 0.99182769\n",
      "Iteration 8, loss = 0.98718774\n",
      "Iteration 9, loss = 0.98277096\n",
      "Iteration 10, loss = 0.97831925\n",
      "Iteration 11, loss = 0.97399023\n",
      "Iteration 12, loss = 0.96958695\n",
      "Iteration 13, loss = 0.96530192\n",
      "Iteration 14, loss = 0.96105833\n",
      "Iteration 15, loss = 0.95684178\n",
      "Iteration 16, loss = 0.95265854\n",
      "Iteration 17, loss = 0.94865868\n",
      "Iteration 18, loss = 0.94456609\n",
      "Iteration 19, loss = 0.94040933\n",
      "Iteration 20, loss = 0.93647965\n",
      "Iteration 21, loss = 0.93250349\n",
      "Iteration 22, loss = 0.92851282\n",
      "Iteration 23, loss = 0.92466435\n",
      "Iteration 24, loss = 0.92079721\n",
      "Iteration 25, loss = 0.91700808\n",
      "Iteration 26, loss = 0.91321124\n",
      "Iteration 27, loss = 0.90951299\n",
      "Iteration 28, loss = 0.90587948\n",
      "Iteration 29, loss = 0.90214679\n",
      "Iteration 30, loss = 0.89863347\n",
      "Iteration 31, loss = 0.89515042\n",
      "Iteration 32, loss = 0.89149402\n",
      "Iteration 33, loss = 0.88818561\n",
      "Iteration 34, loss = 0.88465015\n",
      "Iteration 35, loss = 0.88123028\n",
      "Iteration 36, loss = 0.87797770\n",
      "Iteration 37, loss = 0.87453184\n",
      "Iteration 38, loss = 0.87129147\n",
      "Iteration 39, loss = 0.86797217\n",
      "Iteration 40, loss = 0.86474901\n",
      "Iteration 41, loss = 0.86153565\n",
      "Iteration 42, loss = 0.85845445\n",
      "Iteration 43, loss = 0.85533124\n",
      "Iteration 44, loss = 0.85233436\n",
      "Iteration 45, loss = 0.84931754\n",
      "Iteration 46, loss = 0.84641813\n",
      "Iteration 47, loss = 0.84353402\n",
      "Iteration 48, loss = 0.84068045\n",
      "Iteration 49, loss = 0.83785200\n",
      "Iteration 50, loss = 0.83512913\n",
      "Iteration 51, loss = 0.83230781\n",
      "Iteration 52, loss = 0.82951783\n",
      "Iteration 53, loss = 0.82686792\n",
      "Iteration 54, loss = 0.82407593\n",
      "Iteration 55, loss = 0.82139860\n",
      "Iteration 56, loss = 0.81878996\n",
      "Iteration 57, loss = 0.81608484\n",
      "Iteration 58, loss = 0.81351756\n",
      "Iteration 59, loss = 0.81085013\n",
      "Iteration 60, loss = 0.80827819\n",
      "Iteration 61, loss = 0.80571333\n",
      "Iteration 62, loss = 0.80316368\n",
      "Iteration 63, loss = 0.80055589\n",
      "Iteration 64, loss = 0.79803729\n",
      "Iteration 65, loss = 0.79554657\n",
      "Iteration 66, loss = 0.79306085\n",
      "Iteration 67, loss = 0.79059749\n",
      "Iteration 68, loss = 0.78810370\n",
      "Iteration 69, loss = 0.78574674\n",
      "Iteration 70, loss = 0.78330679\n",
      "Iteration 71, loss = 0.78096016\n",
      "Iteration 72, loss = 0.77856528\n",
      "Iteration 73, loss = 0.77626608\n",
      "Iteration 74, loss = 0.77393009\n",
      "Iteration 75, loss = 0.77161422\n",
      "Iteration 76, loss = 0.76934427\n",
      "Iteration 77, loss = 0.76710828\n",
      "Iteration 78, loss = 0.76479439\n",
      "Iteration 79, loss = 0.76251517\n",
      "Iteration 80, loss = 0.76030716\n",
      "Iteration 81, loss = 0.75810484\n",
      "Iteration 82, loss = 0.75589657\n",
      "Iteration 83, loss = 0.75377432\n",
      "Iteration 84, loss = 0.75164278\n",
      "Iteration 85, loss = 0.74950505\n",
      "Iteration 86, loss = 0.74745913\n",
      "Iteration 87, loss = 0.74534431\n",
      "Iteration 88, loss = 0.74338881\n",
      "Iteration 89, loss = 0.74128026\n",
      "Iteration 90, loss = 0.73919531\n",
      "Iteration 91, loss = 0.73723712\n",
      "Iteration 92, loss = 0.73513052\n",
      "Iteration 93, loss = 0.73318608\n",
      "Iteration 94, loss = 0.73113403\n",
      "Iteration 95, loss = 0.72909304\n",
      "Iteration 96, loss = 0.72706320\n",
      "Iteration 97, loss = 0.72508290\n",
      "Iteration 98, loss = 0.72309172\n",
      "Iteration 99, loss = 0.72100341\n",
      "Iteration 100, loss = 0.71902559\n",
      "Iteration 101, loss = 0.71705265\n",
      "Iteration 102, loss = 0.71502306\n",
      "Iteration 103, loss = 0.71302628\n",
      "Iteration 104, loss = 0.71108034\n",
      "Iteration 105, loss = 0.70914151\n",
      "Iteration 106, loss = 0.70721795\n",
      "Iteration 107, loss = 0.70520504\n",
      "Iteration 108, loss = 0.70337369\n",
      "Iteration 109, loss = 0.70142698\n",
      "Iteration 110, loss = 0.69959019\n",
      "Iteration 111, loss = 0.69764880\n",
      "Iteration 112, loss = 0.69577390\n",
      "Iteration 113, loss = 0.69406564\n",
      "Iteration 114, loss = 0.69216918\n",
      "Iteration 115, loss = 0.69036740\n",
      "Iteration 116, loss = 0.68858478\n",
      "Iteration 117, loss = 0.68690197\n",
      "Iteration 118, loss = 0.68513859\n",
      "Iteration 119, loss = 0.68334622\n",
      "Iteration 120, loss = 0.68168763\n",
      "Iteration 121, loss = 0.68002408\n",
      "Iteration 122, loss = 0.67835484\n",
      "Iteration 123, loss = 0.67669446\n",
      "Iteration 124, loss = 0.67497527\n",
      "Iteration 125, loss = 0.67333576\n",
      "Iteration 126, loss = 0.67174045\n",
      "Iteration 127, loss = 0.67000744\n",
      "Iteration 128, loss = 0.66840641\n",
      "Iteration 129, loss = 0.66675893\n",
      "Iteration 130, loss = 0.66515014\n",
      "Iteration 131, loss = 0.66349705\n",
      "Iteration 132, loss = 0.66186057\n",
      "Iteration 133, loss = 0.66028052\n",
      "Iteration 134, loss = 0.65865974\n",
      "Iteration 135, loss = 0.65707923\n",
      "Iteration 136, loss = 0.65558693\n",
      "Iteration 137, loss = 0.65402897\n",
      "Iteration 138, loss = 0.65260644\n",
      "Iteration 139, loss = 0.65110661\n",
      "Iteration 140, loss = 0.64977511\n",
      "Iteration 141, loss = 0.64834967\n",
      "Iteration 142, loss = 0.64699236\n",
      "Iteration 143, loss = 0.64551107\n",
      "Iteration 144, loss = 0.64416247\n",
      "Iteration 145, loss = 0.64282606\n",
      "Iteration 146, loss = 0.64138593\n",
      "Iteration 147, loss = 0.64006524\n",
      "Iteration 148, loss = 0.63876025\n",
      "Iteration 149, loss = 0.63746934\n",
      "Iteration 150, loss = 0.63614112\n",
      "Iteration 151, loss = 0.63489854\n",
      "Iteration 152, loss = 0.63355818\n",
      "Iteration 153, loss = 0.63233416\n",
      "Iteration 154, loss = 0.63105309\n",
      "Iteration 155, loss = 0.62977581\n",
      "Iteration 156, loss = 0.62854644\n",
      "Iteration 157, loss = 0.62728504\n",
      "Iteration 158, loss = 0.62613714\n",
      "Iteration 159, loss = 0.62492934\n",
      "Iteration 160, loss = 0.62375179\n",
      "Iteration 161, loss = 0.62262794\n",
      "Iteration 162, loss = 0.62149404\n",
      "Iteration 163, loss = 0.62035292\n",
      "Iteration 164, loss = 0.61928968\n",
      "Iteration 165, loss = 0.61814310\n",
      "Iteration 166, loss = 0.61705187\n",
      "Iteration 167, loss = 0.61599653\n",
      "Iteration 168, loss = 0.61494831\n",
      "Iteration 169, loss = 0.61390794\n",
      "Iteration 170, loss = 0.61286314\n",
      "Iteration 171, loss = 0.61191107\n",
      "Iteration 172, loss = 0.61092480\n",
      "Iteration 173, loss = 0.60991372\n",
      "Iteration 174, loss = 0.60906320\n",
      "Iteration 175, loss = 0.60805823\n",
      "Iteration 176, loss = 0.60718013\n",
      "Iteration 177, loss = 0.60625704\n",
      "Iteration 178, loss = 0.60534412\n",
      "Iteration 179, loss = 0.60452539\n",
      "Iteration 180, loss = 0.60359467\n",
      "Iteration 181, loss = 0.60280859\n",
      "Iteration 182, loss = 0.60188693\n",
      "Iteration 183, loss = 0.60107408\n",
      "Iteration 184, loss = 0.60024305\n",
      "Iteration 185, loss = 0.59946271\n",
      "Iteration 186, loss = 0.59858446\n",
      "Iteration 187, loss = 0.59788555\n",
      "Iteration 188, loss = 0.59704549\n",
      "Iteration 189, loss = 0.59632821\n",
      "Iteration 190, loss = 0.59550021\n",
      "Iteration 191, loss = 0.59482816\n",
      "Iteration 192, loss = 0.59409076\n",
      "Iteration 193, loss = 0.59328182\n",
      "Iteration 194, loss = 0.59259205\n",
      "Iteration 195, loss = 0.59188111\n",
      "Iteration 196, loss = 0.59122764\n",
      "Iteration 197, loss = 0.59050141\n",
      "Iteration 198, loss = 0.58980662\n",
      "Iteration 199, loss = 0.58921611\n",
      "Iteration 200, loss = 0.58854162\n",
      "Iteration 201, loss = 0.58791834\n",
      "Iteration 202, loss = 0.58727760\n",
      "Iteration 203, loss = 0.58665124\n",
      "Iteration 204, loss = 0.58599161\n",
      "Iteration 205, loss = 0.58536479\n",
      "Iteration 206, loss = 0.58471795\n",
      "Iteration 207, loss = 0.58402109\n",
      "Iteration 208, loss = 0.58338743\n",
      "Iteration 209, loss = 0.58273744\n",
      "Iteration 210, loss = 0.58206298\n",
      "Iteration 211, loss = 0.58141304\n",
      "Iteration 212, loss = 0.58076748\n",
      "Iteration 213, loss = 0.58017120\n",
      "Iteration 214, loss = 0.57960683\n",
      "Iteration 215, loss = 0.57895235\n",
      "Iteration 216, loss = 0.57841469\n",
      "Iteration 217, loss = 0.57792887\n",
      "Iteration 218, loss = 0.57734573\n",
      "Iteration 219, loss = 0.57685850\n",
      "Iteration 220, loss = 0.57633460\n",
      "Iteration 221, loss = 0.57585437\n",
      "Iteration 222, loss = 0.57540663\n",
      "Iteration 223, loss = 0.57487497\n",
      "Iteration 224, loss = 0.57448155\n",
      "Iteration 225, loss = 0.57395515\n",
      "Iteration 226, loss = 0.57354763\n",
      "Iteration 227, loss = 0.57306955\n",
      "Iteration 228, loss = 0.57261861\n",
      "Iteration 229, loss = 0.57218005\n",
      "Iteration 230, loss = 0.57171340\n",
      "Iteration 231, loss = 0.57130075\n",
      "Iteration 232, loss = 0.57082796\n",
      "Iteration 233, loss = 0.57045124\n",
      "Iteration 234, loss = 0.57001532\n",
      "Iteration 235, loss = 0.56964862\n",
      "Iteration 236, loss = 0.56924679\n",
      "Iteration 237, loss = 0.56885649\n",
      "Iteration 238, loss = 0.56849419\n",
      "Iteration 239, loss = 0.56816471\n",
      "Iteration 240, loss = 0.56779098\n",
      "Iteration 241, loss = 0.56748525\n",
      "Iteration 242, loss = 0.56715084\n",
      "Iteration 243, loss = 0.56681702\n",
      "Iteration 244, loss = 0.56649651\n",
      "Iteration 245, loss = 0.56616655\n",
      "Iteration 246, loss = 0.56587414\n",
      "Iteration 247, loss = 0.56557709\n",
      "Iteration 248, loss = 0.56522592\n",
      "Iteration 249, loss = 0.56496868\n",
      "Iteration 250, loss = 0.56465665\n",
      "Iteration 251, loss = 0.56430290\n",
      "Iteration 252, loss = 0.56409451\n",
      "Iteration 253, loss = 0.56380904\n",
      "Iteration 254, loss = 0.56349577\n",
      "Iteration 255, loss = 0.56324274\n",
      "Iteration 256, loss = 0.56298636\n",
      "Iteration 257, loss = 0.56272604\n",
      "Iteration 258, loss = 0.56248534\n",
      "Iteration 259, loss = 0.56221776\n",
      "Iteration 260, loss = 0.56197662\n",
      "Iteration 261, loss = 0.56175209\n",
      "Iteration 262, loss = 0.56148938\n",
      "Iteration 263, loss = 0.56130389\n",
      "Iteration 264, loss = 0.56105628\n",
      "Iteration 265, loss = 0.56086370\n",
      "Iteration 266, loss = 0.56064171\n",
      "Iteration 267, loss = 0.56043379\n",
      "Iteration 268, loss = 0.56026077\n",
      "Iteration 269, loss = 0.56006668\n",
      "Iteration 270, loss = 0.55985335\n",
      "Iteration 271, loss = 0.55965837\n",
      "Iteration 272, loss = 0.55947808\n",
      "Iteration 273, loss = 0.55925702\n",
      "Iteration 274, loss = 0.55913553\n",
      "Iteration 275, loss = 0.55888244\n",
      "Iteration 276, loss = 0.55873512\n",
      "Iteration 277, loss = 0.55857443\n",
      "Iteration 278, loss = 0.55839773\n",
      "Iteration 279, loss = 0.55821705\n",
      "Iteration 280, loss = 0.55803211\n",
      "Iteration 281, loss = 0.55783816\n",
      "Iteration 282, loss = 0.55767289\n",
      "Iteration 283, loss = 0.55746712\n",
      "Iteration 284, loss = 0.55732554\n",
      "Iteration 285, loss = 0.55715069\n",
      "Iteration 286, loss = 0.55697962\n",
      "Iteration 287, loss = 0.55680716\n",
      "Iteration 288, loss = 0.55666467\n",
      "Iteration 289, loss = 0.55653979\n",
      "Iteration 290, loss = 0.55638351\n",
      "Iteration 291, loss = 0.55626111\n",
      "Iteration 292, loss = 0.55613248\n",
      "Iteration 293, loss = 0.55599734\n",
      "Iteration 294, loss = 0.55588476\n",
      "Iteration 295, loss = 0.55574465\n",
      "Iteration 296, loss = 0.55563039\n",
      "Iteration 297, loss = 0.55549041\n",
      "Iteration 298, loss = 0.55538782\n",
      "Iteration 299, loss = 0.55526483\n",
      "Iteration 300, loss = 0.55515236\n",
      "Iteration 301, loss = 0.55501886\n",
      "Iteration 302, loss = 0.55492765\n",
      "Iteration 303, loss = 0.55479511\n",
      "Iteration 304, loss = 0.55467988\n",
      "Iteration 305, loss = 0.55458484\n",
      "Iteration 306, loss = 0.55447752\n",
      "Iteration 307, loss = 0.55439742\n",
      "Iteration 308, loss = 0.55429969\n",
      "Iteration 309, loss = 0.55422148\n",
      "Iteration 310, loss = 0.55410685\n",
      "Iteration 311, loss = 0.55401148\n",
      "Iteration 312, loss = 0.55391656\n",
      "Iteration 313, loss = 0.55381659\n",
      "Iteration 314, loss = 0.55371151\n",
      "Iteration 315, loss = 0.55363259\n",
      "Iteration 316, loss = 0.55352938\n",
      "Iteration 317, loss = 0.55343837\n",
      "Iteration 318, loss = 0.55335792\n",
      "Iteration 319, loss = 0.55327585\n",
      "Iteration 320, loss = 0.55317647\n",
      "Iteration 321, loss = 0.55308762\n",
      "Iteration 322, loss = 0.55299033\n",
      "Iteration 323, loss = 0.55287886\n",
      "Iteration 324, loss = 0.55282073\n",
      "Iteration 325, loss = 0.55268719\n",
      "Iteration 326, loss = 0.55260233\n",
      "Iteration 327, loss = 0.55251649\n",
      "Iteration 328, loss = 0.55244142\n",
      "Iteration 329, loss = 0.55233675\n",
      "Iteration 330, loss = 0.55226139\n",
      "Iteration 331, loss = 0.55217192\n",
      "Iteration 332, loss = 0.55208235\n",
      "Iteration 333, loss = 0.55200797\n",
      "Iteration 334, loss = 0.55190634\n",
      "Iteration 335, loss = 0.55185765\n",
      "Iteration 336, loss = 0.55172892\n",
      "Iteration 337, loss = 0.55167777\n",
      "Iteration 338, loss = 0.55159644\n",
      "Iteration 339, loss = 0.55153925\n",
      "Iteration 340, loss = 0.55145497\n",
      "Iteration 341, loss = 0.55142388\n",
      "Iteration 342, loss = 0.55135410\n",
      "Iteration 343, loss = 0.55131506\n",
      "Iteration 344, loss = 0.55126737\n",
      "Iteration 345, loss = 0.55123755\n",
      "Iteration 346, loss = 0.55119152\n",
      "Iteration 347, loss = 0.55116874\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69064701\n",
      "Iteration 2, loss = 0.68826217\n",
      "Iteration 3, loss = 0.68604747\n",
      "Iteration 4, loss = 0.68380447\n",
      "Iteration 5, loss = 0.68166376\n",
      "Iteration 6, loss = 0.67959643\n",
      "Iteration 7, loss = 0.67749273\n",
      "Iteration 8, loss = 0.67548456\n",
      "Iteration 9, loss = 0.67340183\n",
      "Iteration 10, loss = 0.67140722\n",
      "Iteration 11, loss = 0.66948702\n",
      "Iteration 12, loss = 0.66747044\n",
      "Iteration 13, loss = 0.66561562\n",
      "Iteration 14, loss = 0.66365221\n",
      "Iteration 15, loss = 0.66184731\n",
      "Iteration 16, loss = 0.66005545\n",
      "Iteration 17, loss = 0.65821128\n",
      "Iteration 18, loss = 0.65637912\n",
      "Iteration 19, loss = 0.65469549\n",
      "Iteration 20, loss = 0.65293217\n",
      "Iteration 21, loss = 0.65113718\n",
      "Iteration 22, loss = 0.64949191\n",
      "Iteration 23, loss = 0.64768692\n",
      "Iteration 24, loss = 0.64617355\n",
      "Iteration 25, loss = 0.64444205\n",
      "Iteration 26, loss = 0.64280915\n",
      "Iteration 27, loss = 0.64125231\n",
      "Iteration 28, loss = 0.63964929\n",
      "Iteration 29, loss = 0.63807073\n",
      "Iteration 30, loss = 0.63649280\n",
      "Iteration 31, loss = 0.63493604\n",
      "Iteration 32, loss = 0.63341083\n",
      "Iteration 33, loss = 0.63188670\n",
      "Iteration 34, loss = 0.63031735\n",
      "Iteration 35, loss = 0.62894647\n",
      "Iteration 36, loss = 0.62743044\n",
      "Iteration 37, loss = 0.62600327\n",
      "Iteration 38, loss = 0.62457867\n",
      "Iteration 39, loss = 0.62315721\n",
      "Iteration 40, loss = 0.62189728\n",
      "Iteration 41, loss = 0.62054760\n",
      "Iteration 42, loss = 0.61925971\n",
      "Iteration 43, loss = 0.61809675\n",
      "Iteration 44, loss = 0.61686521\n",
      "Iteration 45, loss = 0.61569816\n",
      "Iteration 46, loss = 0.61450946\n",
      "Iteration 47, loss = 0.61334981\n",
      "Iteration 48, loss = 0.61222477\n",
      "Iteration 49, loss = 0.61096293\n",
      "Iteration 50, loss = 0.60984336\n",
      "Iteration 51, loss = 0.60867760\n",
      "Iteration 52, loss = 0.60758284\n",
      "Iteration 53, loss = 0.60634079\n",
      "Iteration 54, loss = 0.60536200\n",
      "Iteration 55, loss = 0.60429168\n",
      "Iteration 56, loss = 0.60317707\n",
      "Iteration 57, loss = 0.60220331\n",
      "Iteration 58, loss = 0.60112582\n",
      "Iteration 59, loss = 0.60011710\n",
      "Iteration 60, loss = 0.59914987\n",
      "Iteration 61, loss = 0.59816780\n",
      "Iteration 62, loss = 0.59724489\n",
      "Iteration 63, loss = 0.59628192\n",
      "Iteration 64, loss = 0.59539707\n",
      "Iteration 65, loss = 0.59457716\n",
      "Iteration 66, loss = 0.59374446\n",
      "Iteration 67, loss = 0.59291711\n",
      "Iteration 68, loss = 0.59207861\n",
      "Iteration 69, loss = 0.59134085\n",
      "Iteration 70, loss = 0.59051041\n",
      "Iteration 71, loss = 0.58969262\n",
      "Iteration 72, loss = 0.58895259\n",
      "Iteration 73, loss = 0.58811881\n",
      "Iteration 74, loss = 0.58724442\n",
      "Iteration 75, loss = 0.58649118\n",
      "Iteration 76, loss = 0.58569009\n",
      "Iteration 77, loss = 0.58491910\n",
      "Iteration 78, loss = 0.58417016\n",
      "Iteration 79, loss = 0.58339671\n",
      "Iteration 80, loss = 0.58262887\n",
      "Iteration 81, loss = 0.58195414\n",
      "Iteration 82, loss = 0.58118582\n",
      "Iteration 83, loss = 0.58052201\n",
      "Iteration 84, loss = 0.57982989\n",
      "Iteration 85, loss = 0.57910625\n",
      "Iteration 86, loss = 0.57841268\n",
      "Iteration 87, loss = 0.57776203\n",
      "Iteration 88, loss = 0.57704378\n",
      "Iteration 89, loss = 0.57634600\n",
      "Iteration 90, loss = 0.57573037\n",
      "Iteration 91, loss = 0.57508212\n",
      "Iteration 92, loss = 0.57443245\n",
      "Iteration 93, loss = 0.57386954\n",
      "Iteration 94, loss = 0.57329000\n",
      "Iteration 95, loss = 0.57274377\n",
      "Iteration 96, loss = 0.57217348\n",
      "Iteration 97, loss = 0.57168066\n",
      "Iteration 98, loss = 0.57117591\n",
      "Iteration 99, loss = 0.57068043\n",
      "Iteration 100, loss = 0.57023005\n",
      "Iteration 101, loss = 0.56974357\n",
      "Iteration 102, loss = 0.56925995\n",
      "Iteration 103, loss = 0.56874593\n",
      "Iteration 104, loss = 0.56830859\n",
      "Iteration 105, loss = 0.56777148\n",
      "Iteration 106, loss = 0.56732616\n",
      "Iteration 107, loss = 0.56679295\n",
      "Iteration 108, loss = 0.56630719\n",
      "Iteration 109, loss = 0.56590186\n",
      "Iteration 110, loss = 0.56538845\n",
      "Iteration 111, loss = 0.56497293\n",
      "Iteration 112, loss = 0.56452947\n",
      "Iteration 113, loss = 0.56411625\n",
      "Iteration 114, loss = 0.56374710\n",
      "Iteration 115, loss = 0.56337468\n",
      "Iteration 116, loss = 0.56297913\n",
      "Iteration 117, loss = 0.56260428\n",
      "Iteration 118, loss = 0.56228458\n",
      "Iteration 119, loss = 0.56193484\n",
      "Iteration 120, loss = 0.56156174\n",
      "Iteration 121, loss = 0.56122611\n",
      "Iteration 122, loss = 0.56093186\n",
      "Iteration 123, loss = 0.56060336\n",
      "Iteration 124, loss = 0.56026812\n",
      "Iteration 125, loss = 0.55997595\n",
      "Iteration 126, loss = 0.55965843\n",
      "Iteration 127, loss = 0.55935977\n",
      "Iteration 128, loss = 0.55904998\n",
      "Iteration 129, loss = 0.55881482\n",
      "Iteration 130, loss = 0.55850172\n",
      "Iteration 131, loss = 0.55826396\n",
      "Iteration 132, loss = 0.55797479\n",
      "Iteration 133, loss = 0.55775078\n",
      "Iteration 134, loss = 0.55751147\n",
      "Iteration 135, loss = 0.55726331\n",
      "Iteration 136, loss = 0.55701739\n",
      "Iteration 137, loss = 0.55676633\n",
      "Iteration 138, loss = 0.55656249\n",
      "Iteration 139, loss = 0.55628110\n",
      "Iteration 140, loss = 0.55605675\n",
      "Iteration 141, loss = 0.55585676\n",
      "Iteration 142, loss = 0.55561412\n",
      "Iteration 143, loss = 0.55538490\n",
      "Iteration 144, loss = 0.55521149\n",
      "Iteration 145, loss = 0.55499299\n",
      "Iteration 146, loss = 0.55480393\n",
      "Iteration 147, loss = 0.55464683\n",
      "Iteration 148, loss = 0.55445942\n",
      "Iteration 149, loss = 0.55430526\n",
      "Iteration 150, loss = 0.55411615\n",
      "Iteration 151, loss = 0.55395048\n",
      "Iteration 152, loss = 0.55378165\n",
      "Iteration 153, loss = 0.55362222\n",
      "Iteration 154, loss = 0.55342083\n",
      "Iteration 155, loss = 0.55333069\n",
      "Iteration 156, loss = 0.55312093\n",
      "Iteration 157, loss = 0.55300971\n",
      "Iteration 158, loss = 0.55289126\n",
      "Iteration 159, loss = 0.55273118\n",
      "Iteration 160, loss = 0.55259855\n",
      "Iteration 161, loss = 0.55245188\n",
      "Iteration 162, loss = 0.55236321\n",
      "Iteration 163, loss = 0.55222308\n",
      "Iteration 164, loss = 0.55207790\n",
      "Iteration 165, loss = 0.55200809\n",
      "Iteration 166, loss = 0.55185152\n",
      "Iteration 167, loss = 0.55175470\n",
      "Iteration 168, loss = 0.55169000\n",
      "Iteration 169, loss = 0.55156369\n",
      "Iteration 170, loss = 0.55146015\n",
      "Iteration 171, loss = 0.55137720\n",
      "Iteration 172, loss = 0.55129906\n",
      "Iteration 173, loss = 0.55119142\n",
      "Iteration 174, loss = 0.55111756\n",
      "Iteration 175, loss = 0.55102797\n",
      "Iteration 176, loss = 0.55094625\n",
      "Iteration 177, loss = 0.55087009\n",
      "Iteration 178, loss = 0.55078481\n",
      "Iteration 179, loss = 0.55070050\n",
      "Iteration 180, loss = 0.55066256\n",
      "Iteration 181, loss = 0.55055829\n",
      "Iteration 182, loss = 0.55049154\n",
      "Iteration 183, loss = 0.55044316\n",
      "Iteration 184, loss = 0.55038257\n",
      "Iteration 185, loss = 0.55032923\n",
      "Iteration 186, loss = 0.55028691\n",
      "Iteration 187, loss = 0.55022935\n",
      "Iteration 188, loss = 0.55022097\n",
      "Iteration 189, loss = 0.55013200\n",
      "Iteration 190, loss = 0.55006000\n",
      "Iteration 191, loss = 0.55000091\n",
      "Iteration 192, loss = 0.54996361\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66536613\n",
      "Iteration 2, loss = 0.66091467\n",
      "Iteration 3, loss = 0.65671489\n",
      "Iteration 4, loss = 0.65248437\n",
      "Iteration 5, loss = 0.64874299\n",
      "Iteration 6, loss = 0.64487387\n",
      "Iteration 7, loss = 0.64122895\n",
      "Iteration 8, loss = 0.63770648\n",
      "Iteration 9, loss = 0.63414408\n",
      "Iteration 10, loss = 0.63115483\n",
      "Iteration 11, loss = 0.62781461\n",
      "Iteration 12, loss = 0.62480525\n",
      "Iteration 13, loss = 0.62175396\n",
      "Iteration 14, loss = 0.61899508\n",
      "Iteration 15, loss = 0.61618966\n",
      "Iteration 16, loss = 0.61368645\n",
      "Iteration 17, loss = 0.61113707\n",
      "Iteration 18, loss = 0.60866142\n",
      "Iteration 19, loss = 0.60638049\n",
      "Iteration 20, loss = 0.60430959\n",
      "Iteration 21, loss = 0.60213870\n",
      "Iteration 22, loss = 0.60034194\n",
      "Iteration 23, loss = 0.59830201\n",
      "Iteration 24, loss = 0.59658019\n",
      "Iteration 25, loss = 0.59481417\n",
      "Iteration 26, loss = 0.59315497\n",
      "Iteration 27, loss = 0.59160110\n",
      "Iteration 28, loss = 0.58995229\n",
      "Iteration 29, loss = 0.58831746\n",
      "Iteration 30, loss = 0.58686911\n",
      "Iteration 31, loss = 0.58535214\n",
      "Iteration 32, loss = 0.58404612\n",
      "Iteration 33, loss = 0.58262705\n",
      "Iteration 34, loss = 0.58131024\n",
      "Iteration 35, loss = 0.58021104\n",
      "Iteration 36, loss = 0.57903524\n",
      "Iteration 37, loss = 0.57794871\n",
      "Iteration 38, loss = 0.57700663\n",
      "Iteration 39, loss = 0.57597532\n",
      "Iteration 40, loss = 0.57511997\n",
      "Iteration 41, loss = 0.57435354\n",
      "Iteration 42, loss = 0.57343755\n",
      "Iteration 43, loss = 0.57270743\n",
      "Iteration 44, loss = 0.57194706\n",
      "Iteration 45, loss = 0.57126161\n",
      "Iteration 46, loss = 0.57051240\n",
      "Iteration 47, loss = 0.56990353\n",
      "Iteration 48, loss = 0.56917465\n",
      "Iteration 49, loss = 0.56850616\n",
      "Iteration 50, loss = 0.56792475\n",
      "Iteration 51, loss = 0.56731016\n",
      "Iteration 52, loss = 0.56669729\n",
      "Iteration 53, loss = 0.56619885\n",
      "Iteration 54, loss = 0.56562282\n",
      "Iteration 55, loss = 0.56519636\n",
      "Iteration 56, loss = 0.56464971\n",
      "Iteration 57, loss = 0.56423137\n",
      "Iteration 58, loss = 0.56390679\n",
      "Iteration 59, loss = 0.56340578\n",
      "Iteration 60, loss = 0.56311424\n",
      "Iteration 61, loss = 0.56271033\n",
      "Iteration 62, loss = 0.56242128\n",
      "Iteration 63, loss = 0.56210350\n",
      "Iteration 64, loss = 0.56176414\n",
      "Iteration 65, loss = 0.56149212\n",
      "Iteration 66, loss = 0.56119841\n",
      "Iteration 67, loss = 0.56091210\n",
      "Iteration 68, loss = 0.56062340\n",
      "Iteration 69, loss = 0.56031363\n",
      "Iteration 70, loss = 0.56004388\n",
      "Iteration 71, loss = 0.55975971\n",
      "Iteration 72, loss = 0.55946773\n",
      "Iteration 73, loss = 0.55918876\n",
      "Iteration 74, loss = 0.55895819\n",
      "Iteration 75, loss = 0.55871000\n",
      "Iteration 76, loss = 0.55850776\n",
      "Iteration 77, loss = 0.55828192\n",
      "Iteration 78, loss = 0.55803772\n",
      "Iteration 79, loss = 0.55785015\n",
      "Iteration 80, loss = 0.55764114\n",
      "Iteration 81, loss = 0.55740889\n",
      "Iteration 82, loss = 0.55723888\n",
      "Iteration 83, loss = 0.55704707\n",
      "Iteration 84, loss = 0.55683316\n",
      "Iteration 85, loss = 0.55664273\n",
      "Iteration 86, loss = 0.55646323\n",
      "Iteration 87, loss = 0.55628098\n",
      "Iteration 88, loss = 0.55608938\n",
      "Iteration 89, loss = 0.55594127\n",
      "Iteration 90, loss = 0.55576224\n",
      "Iteration 91, loss = 0.55561754\n",
      "Iteration 92, loss = 0.55549497\n",
      "Iteration 93, loss = 0.55536389\n",
      "Iteration 94, loss = 0.55525996\n",
      "Iteration 95, loss = 0.55515407\n",
      "Iteration 96, loss = 0.55506592\n",
      "Iteration 97, loss = 0.55497893\n",
      "Iteration 98, loss = 0.55485826\n",
      "Iteration 99, loss = 0.55475605\n",
      "Iteration 100, loss = 0.55464674\n",
      "Iteration 101, loss = 0.55452542\n",
      "Iteration 102, loss = 0.55441520\n",
      "Iteration 103, loss = 0.55432318\n",
      "Iteration 104, loss = 0.55417315\n",
      "Iteration 105, loss = 0.55404409\n",
      "Iteration 106, loss = 0.55392642\n",
      "Iteration 107, loss = 0.55379277\n",
      "Iteration 108, loss = 0.55369674\n",
      "Iteration 109, loss = 0.55355410\n",
      "Iteration 110, loss = 0.55340457\n",
      "Iteration 111, loss = 0.55328302\n",
      "Iteration 112, loss = 0.55314038\n",
      "Iteration 113, loss = 0.55301715\n",
      "Iteration 114, loss = 0.55285271\n",
      "Iteration 115, loss = 0.55273791\n",
      "Iteration 116, loss = 0.55264554\n",
      "Iteration 117, loss = 0.55251216\n",
      "Iteration 118, loss = 0.55237846\n",
      "Iteration 119, loss = 0.55228850\n",
      "Iteration 120, loss = 0.55221414\n",
      "Iteration 121, loss = 0.55210323\n",
      "Iteration 122, loss = 0.55201309\n",
      "Iteration 123, loss = 0.55195905\n",
      "Iteration 124, loss = 0.55190207\n",
      "Iteration 125, loss = 0.55185180\n",
      "Iteration 126, loss = 0.55178602\n",
      "Iteration 127, loss = 0.55172370\n",
      "Iteration 128, loss = 0.55164124\n",
      "Iteration 129, loss = 0.55159428\n",
      "Iteration 130, loss = 0.55151303\n",
      "Iteration 131, loss = 0.55143328\n",
      "Iteration 132, loss = 0.55135630\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.82254731\n",
      "Iteration 2, loss = 0.81209155\n",
      "Iteration 3, loss = 0.80216305\n",
      "Iteration 4, loss = 0.79189107\n",
      "Iteration 5, loss = 0.78270212\n",
      "Iteration 6, loss = 0.77321045\n",
      "Iteration 7, loss = 0.76415953\n",
      "Iteration 8, loss = 0.75545018\n",
      "Iteration 9, loss = 0.74725993\n",
      "Iteration 10, loss = 0.73871248\n",
      "Iteration 11, loss = 0.73100284\n",
      "Iteration 12, loss = 0.72336456\n",
      "Iteration 13, loss = 0.71599028\n",
      "Iteration 14, loss = 0.70883099\n",
      "Iteration 15, loss = 0.70181169\n",
      "Iteration 16, loss = 0.69504978\n",
      "Iteration 17, loss = 0.68861953\n",
      "Iteration 18, loss = 0.68209012\n",
      "Iteration 19, loss = 0.67619655\n",
      "Iteration 20, loss = 0.66994437\n",
      "Iteration 21, loss = 0.66457945\n",
      "Iteration 22, loss = 0.65883064\n",
      "Iteration 23, loss = 0.65353288\n",
      "Iteration 24, loss = 0.64851682\n",
      "Iteration 25, loss = 0.64341798\n",
      "Iteration 26, loss = 0.63861335\n",
      "Iteration 27, loss = 0.63411600\n",
      "Iteration 28, loss = 0.62981572\n",
      "Iteration 29, loss = 0.62540321\n",
      "Iteration 30, loss = 0.62165162\n",
      "Iteration 31, loss = 0.61759785\n",
      "Iteration 32, loss = 0.61396490\n",
      "Iteration 33, loss = 0.61061335\n",
      "Iteration 34, loss = 0.60725035\n",
      "Iteration 35, loss = 0.60378086\n",
      "Iteration 36, loss = 0.60083956\n",
      "Iteration 37, loss = 0.59801721\n",
      "Iteration 38, loss = 0.59493770\n",
      "Iteration 39, loss = 0.59214186\n",
      "Iteration 40, loss = 0.58965774\n",
      "Iteration 41, loss = 0.58681674\n",
      "Iteration 42, loss = 0.58455200\n",
      "Iteration 43, loss = 0.58187714\n",
      "Iteration 44, loss = 0.57965317\n",
      "Iteration 45, loss = 0.57720091\n",
      "Iteration 46, loss = 0.57502593\n",
      "Iteration 47, loss = 0.57286752\n",
      "Iteration 48, loss = 0.57100465\n",
      "Iteration 49, loss = 0.56891239\n",
      "Iteration 50, loss = 0.56706142\n",
      "Iteration 51, loss = 0.56544822\n",
      "Iteration 52, loss = 0.56385855\n",
      "Iteration 53, loss = 0.56235770\n",
      "Iteration 54, loss = 0.56100157\n",
      "Iteration 55, loss = 0.55961822\n",
      "Iteration 56, loss = 0.55849415\n",
      "Iteration 57, loss = 0.55741500\n",
      "Iteration 58, loss = 0.55637072\n",
      "Iteration 59, loss = 0.55551370\n",
      "Iteration 60, loss = 0.55460574\n",
      "Iteration 61, loss = 0.55379494\n",
      "Iteration 62, loss = 0.55317047\n",
      "Iteration 63, loss = 0.55240779\n",
      "Iteration 64, loss = 0.55187668\n",
      "Iteration 65, loss = 0.55120190\n",
      "Iteration 66, loss = 0.55054214\n",
      "Iteration 67, loss = 0.55002594\n",
      "Iteration 68, loss = 0.54954436\n",
      "Iteration 69, loss = 0.54890361\n",
      "Iteration 70, loss = 0.54846504\n",
      "Iteration 71, loss = 0.54803502\n",
      "Iteration 72, loss = 0.54763471\n",
      "Iteration 73, loss = 0.54718607\n",
      "Iteration 74, loss = 0.54692428\n",
      "Iteration 75, loss = 0.54652962\n",
      "Iteration 76, loss = 0.54621680\n",
      "Iteration 77, loss = 0.54595613\n",
      "Iteration 78, loss = 0.54573310\n",
      "Iteration 79, loss = 0.54542371\n",
      "Iteration 80, loss = 0.54534508\n",
      "Iteration 81, loss = 0.54505297\n",
      "Iteration 82, loss = 0.54486640\n",
      "Iteration 83, loss = 0.54470607\n",
      "Iteration 84, loss = 0.54458946\n",
      "Iteration 85, loss = 0.54436919\n",
      "Iteration 86, loss = 0.54425699\n",
      "Iteration 87, loss = 0.54410475\n",
      "Iteration 88, loss = 0.54398693\n",
      "Iteration 89, loss = 0.54384839\n",
      "Iteration 90, loss = 0.54375875\n",
      "Iteration 91, loss = 0.54358866\n",
      "Iteration 92, loss = 0.54346913\n",
      "Iteration 93, loss = 0.54339925\n",
      "Iteration 94, loss = 0.54327308\n",
      "Iteration 95, loss = 0.54313465\n",
      "Iteration 96, loss = 0.54306598\n",
      "Iteration 97, loss = 0.54303176\n",
      "Iteration 98, loss = 0.54294527\n",
      "Iteration 99, loss = 0.54285265\n",
      "Iteration 100, loss = 0.54281001\n",
      "Iteration 101, loss = 0.54277413\n",
      "Iteration 102, loss = 0.54268049\n",
      "Iteration 103, loss = 0.54264790\n",
      "Iteration 104, loss = 0.54264813\n",
      "Iteration 105, loss = 0.54258482\n",
      "Iteration 106, loss = 0.54255744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61381601\n",
      "Iteration 2, loss = 0.60916725\n",
      "Iteration 3, loss = 0.60462297\n",
      "Iteration 4, loss = 0.60037088\n",
      "Iteration 5, loss = 0.59658198\n",
      "Iteration 6, loss = 0.59305289\n",
      "Iteration 7, loss = 0.58949018\n",
      "Iteration 8, loss = 0.58624489\n",
      "Iteration 9, loss = 0.58307246\n",
      "Iteration 10, loss = 0.58013331\n",
      "Iteration 11, loss = 0.57756271\n",
      "Iteration 12, loss = 0.57488060\n",
      "Iteration 13, loss = 0.57244583\n",
      "Iteration 14, loss = 0.56996011\n",
      "Iteration 15, loss = 0.56774949\n",
      "Iteration 16, loss = 0.56582336\n",
      "Iteration 17, loss = 0.56366371\n",
      "Iteration 18, loss = 0.56191284\n",
      "Iteration 19, loss = 0.56030730\n",
      "Iteration 20, loss = 0.55867460\n",
      "Iteration 21, loss = 0.55732923\n",
      "Iteration 22, loss = 0.55582808\n",
      "Iteration 23, loss = 0.55477288\n",
      "Iteration 24, loss = 0.55361648\n",
      "Iteration 25, loss = 0.55244364\n",
      "Iteration 26, loss = 0.55148977\n",
      "Iteration 27, loss = 0.55058833\n",
      "Iteration 28, loss = 0.54957885\n",
      "Iteration 29, loss = 0.54877239\n",
      "Iteration 30, loss = 0.54795259\n",
      "Iteration 31, loss = 0.54715500\n",
      "Iteration 32, loss = 0.54645540\n",
      "Iteration 33, loss = 0.54583208\n",
      "Iteration 34, loss = 0.54539261\n",
      "Iteration 35, loss = 0.54475994\n",
      "Iteration 36, loss = 0.54451719\n",
      "Iteration 37, loss = 0.54395675\n",
      "Iteration 38, loss = 0.54349733\n",
      "Iteration 39, loss = 0.54319119\n",
      "Iteration 40, loss = 0.54277258\n",
      "Iteration 41, loss = 0.54228572\n",
      "Iteration 42, loss = 0.54200293\n",
      "Iteration 43, loss = 0.54164401\n",
      "Iteration 44, loss = 0.54127135\n",
      "Iteration 45, loss = 0.54105466\n",
      "Iteration 46, loss = 0.54072404\n",
      "Iteration 47, loss = 0.54053282\n",
      "Iteration 48, loss = 0.54024522\n",
      "Iteration 49, loss = 0.53996428\n",
      "Iteration 50, loss = 0.53979411\n",
      "Iteration 51, loss = 0.53952121\n",
      "Iteration 52, loss = 0.53933332\n",
      "Iteration 53, loss = 0.53921576\n",
      "Iteration 54, loss = 0.53914186\n",
      "Iteration 55, loss = 0.53889952\n",
      "Iteration 56, loss = 0.53875429\n",
      "Iteration 57, loss = 0.53857214\n",
      "Iteration 58, loss = 0.53849123\n",
      "Iteration 59, loss = 0.53831418\n",
      "Iteration 60, loss = 0.53818922\n",
      "Iteration 61, loss = 0.53808858\n",
      "Iteration 62, loss = 0.53803997\n",
      "Iteration 63, loss = 0.53795281\n",
      "Iteration 64, loss = 0.53780886\n",
      "Iteration 65, loss = 0.53770144\n",
      "Iteration 66, loss = 0.53763434\n",
      "Iteration 67, loss = 0.53746024\n",
      "Iteration 68, loss = 0.53740829\n",
      "Iteration 69, loss = 0.53729044\n",
      "Iteration 70, loss = 0.53714439\n",
      "Iteration 71, loss = 0.53705125\n",
      "Iteration 72, loss = 0.53698992\n",
      "Iteration 73, loss = 0.53687502\n",
      "Iteration 74, loss = 0.53673378\n",
      "Iteration 75, loss = 0.53663510\n",
      "Iteration 76, loss = 0.53657597\n",
      "Iteration 77, loss = 0.53648823\n",
      "Iteration 78, loss = 0.53638063\n",
      "Iteration 79, loss = 0.53631374\n",
      "Iteration 80, loss = 0.53621383\n",
      "Iteration 81, loss = 0.53609878\n",
      "Iteration 82, loss = 0.53604629\n",
      "Iteration 83, loss = 0.53611818\n",
      "Iteration 84, loss = 0.53595888\n",
      "Iteration 85, loss = 0.53588369\n",
      "Iteration 86, loss = 0.53588050\n",
      "Iteration 87, loss = 0.53576498\n",
      "Iteration 88, loss = 0.53570296\n",
      "Iteration 89, loss = 0.53564402\n",
      "Iteration 90, loss = 0.53559874\n",
      "Iteration 91, loss = 0.53551987\n",
      "Iteration 92, loss = 0.53541878\n",
      "Iteration 93, loss = 0.53530720\n",
      "Iteration 94, loss = 0.53521375\n",
      "Iteration 95, loss = 0.53513625\n",
      "Iteration 96, loss = 0.53502131\n",
      "Iteration 97, loss = 0.53496549\n",
      "Iteration 98, loss = 0.53481956\n",
      "Iteration 99, loss = 0.53481647\n",
      "Iteration 100, loss = 0.53476662\n",
      "Iteration 101, loss = 0.53468876\n",
      "Iteration 102, loss = 0.53464299\n",
      "Iteration 103, loss = 0.53464575\n",
      "Iteration 104, loss = 0.53479869\n",
      "Iteration 105, loss = 0.53464383\n",
      "Iteration 106, loss = 0.53460179\n",
      "Iteration 107, loss = 0.53451162\n",
      "Iteration 108, loss = 0.53447694\n",
      "Iteration 109, loss = 0.53457595\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70017796\n",
      "Iteration 2, loss = 0.69072356\n",
      "Iteration 3, loss = 0.68245939\n",
      "Iteration 4, loss = 0.67393242\n",
      "Iteration 5, loss = 0.66609544\n",
      "Iteration 6, loss = 0.65809067\n",
      "Iteration 7, loss = 0.65071331\n",
      "Iteration 8, loss = 0.64328275\n",
      "Iteration 9, loss = 0.63627978\n",
      "Iteration 10, loss = 0.62989049\n",
      "Iteration 11, loss = 0.62327246\n",
      "Iteration 12, loss = 0.61738118\n",
      "Iteration 13, loss = 0.61167536\n",
      "Iteration 14, loss = 0.60612398\n",
      "Iteration 15, loss = 0.60085296\n",
      "Iteration 16, loss = 0.59604228\n",
      "Iteration 17, loss = 0.59132695\n",
      "Iteration 18, loss = 0.58732500\n",
      "Iteration 19, loss = 0.58285883\n",
      "Iteration 20, loss = 0.57920248\n",
      "Iteration 21, loss = 0.57559936\n",
      "Iteration 22, loss = 0.57218367\n",
      "Iteration 23, loss = 0.56889556\n",
      "Iteration 24, loss = 0.56601579\n",
      "Iteration 25, loss = 0.56298043\n",
      "Iteration 26, loss = 0.56014506\n",
      "Iteration 27, loss = 0.55753908\n",
      "Iteration 28, loss = 0.55515979\n",
      "Iteration 29, loss = 0.55269647\n",
      "Iteration 30, loss = 0.55047980\n",
      "Iteration 31, loss = 0.54849286\n",
      "Iteration 32, loss = 0.54649163\n",
      "Iteration 33, loss = 0.54478074\n",
      "Iteration 34, loss = 0.54308371\n",
      "Iteration 35, loss = 0.54162211\n",
      "Iteration 36, loss = 0.53997029\n",
      "Iteration 37, loss = 0.53870626\n",
      "Iteration 38, loss = 0.53765310\n",
      "Iteration 39, loss = 0.53650533\n",
      "Iteration 40, loss = 0.53538660\n",
      "Iteration 41, loss = 0.53456083\n",
      "Iteration 42, loss = 0.53379182\n",
      "Iteration 43, loss = 0.53298661\n",
      "Iteration 44, loss = 0.53238061\n",
      "Iteration 45, loss = 0.53165167\n",
      "Iteration 46, loss = 0.53115300\n",
      "Iteration 47, loss = 0.53078183\n",
      "Iteration 48, loss = 0.53005738\n",
      "Iteration 49, loss = 0.52965648\n",
      "Iteration 50, loss = 0.52918469\n",
      "Iteration 51, loss = 0.52875623\n",
      "Iteration 52, loss = 0.52823934\n",
      "Iteration 53, loss = 0.52781845\n",
      "Iteration 54, loss = 0.52746248\n",
      "Iteration 55, loss = 0.52719116\n",
      "Iteration 56, loss = 0.52701363\n",
      "Iteration 57, loss = 0.52669864\n",
      "Iteration 58, loss = 0.52646615\n",
      "Iteration 59, loss = 0.52633792\n",
      "Iteration 60, loss = 0.52626069\n",
      "Iteration 61, loss = 0.52609976\n",
      "Iteration 62, loss = 0.52593355\n",
      "Iteration 63, loss = 0.52582753\n",
      "Iteration 64, loss = 0.52562914\n",
      "Iteration 65, loss = 0.52539786\n",
      "Iteration 66, loss = 0.52528620\n",
      "Iteration 67, loss = 0.52505785\n",
      "Iteration 68, loss = 0.52490381\n",
      "Iteration 69, loss = 0.52474740\n",
      "Iteration 70, loss = 0.52460158\n",
      "Iteration 71, loss = 0.52440773\n",
      "Iteration 72, loss = 0.52427645\n",
      "Iteration 73, loss = 0.52434043\n",
      "Iteration 74, loss = 0.52408337\n",
      "Iteration 75, loss = 0.52398899\n",
      "Iteration 76, loss = 0.52391856\n",
      "Iteration 77, loss = 0.52384614\n",
      "Iteration 78, loss = 0.52374876\n",
      "Iteration 79, loss = 0.52371959\n",
      "Iteration 80, loss = 0.52361304\n",
      "Iteration 81, loss = 0.52356012\n",
      "Iteration 82, loss = 0.52353018\n",
      "Iteration 83, loss = 0.52346438\n",
      "Iteration 84, loss = 0.52339836\n",
      "Iteration 85, loss = 0.52334646\n",
      "Iteration 86, loss = 0.52329503\n",
      "Iteration 87, loss = 0.52329920\n",
      "Iteration 88, loss = 0.52321886\n",
      "Iteration 89, loss = 0.52318033\n",
      "Iteration 90, loss = 0.52314608\n",
      "Iteration 91, loss = 0.52311960\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73607699\n",
      "Iteration 2, loss = 0.72758687\n",
      "Iteration 3, loss = 0.71901122\n",
      "Iteration 4, loss = 0.71128560\n",
      "Iteration 5, loss = 0.70344659\n",
      "Iteration 6, loss = 0.69582646\n",
      "Iteration 7, loss = 0.68887435\n",
      "Iteration 8, loss = 0.68201340\n",
      "Iteration 9, loss = 0.67526461\n",
      "Iteration 10, loss = 0.66896113\n",
      "Iteration 11, loss = 0.66329707\n",
      "Iteration 12, loss = 0.65739212\n",
      "Iteration 13, loss = 0.65199227\n",
      "Iteration 14, loss = 0.64694005\n",
      "Iteration 15, loss = 0.64199258\n",
      "Iteration 16, loss = 0.63717311\n",
      "Iteration 17, loss = 0.63241624\n",
      "Iteration 18, loss = 0.62821574\n",
      "Iteration 19, loss = 0.62391866\n",
      "Iteration 20, loss = 0.61959280\n",
      "Iteration 21, loss = 0.61563501\n",
      "Iteration 22, loss = 0.61170408\n",
      "Iteration 23, loss = 0.60829095\n",
      "Iteration 24, loss = 0.60460657\n",
      "Iteration 25, loss = 0.60092462\n",
      "Iteration 26, loss = 0.59768841\n",
      "Iteration 27, loss = 0.59469438\n",
      "Iteration 28, loss = 0.59149188\n",
      "Iteration 29, loss = 0.58853172\n",
      "Iteration 30, loss = 0.58586051\n",
      "Iteration 31, loss = 0.58312173\n",
      "Iteration 32, loss = 0.58044581\n",
      "Iteration 33, loss = 0.57810134\n",
      "Iteration 34, loss = 0.57569688\n",
      "Iteration 35, loss = 0.57320693\n",
      "Iteration 36, loss = 0.57102470\n",
      "Iteration 37, loss = 0.56888206\n",
      "Iteration 38, loss = 0.56667861\n",
      "Iteration 39, loss = 0.56463032\n",
      "Iteration 40, loss = 0.56269187\n",
      "Iteration 41, loss = 0.56075488\n",
      "Iteration 42, loss = 0.55900851\n",
      "Iteration 43, loss = 0.55713440\n",
      "Iteration 44, loss = 0.55558582\n",
      "Iteration 45, loss = 0.55412370\n",
      "Iteration 46, loss = 0.55261646\n",
      "Iteration 47, loss = 0.55121433\n",
      "Iteration 48, loss = 0.54981668\n",
      "Iteration 49, loss = 0.54863976\n",
      "Iteration 50, loss = 0.54739986\n",
      "Iteration 51, loss = 0.54618590\n",
      "Iteration 52, loss = 0.54512845\n",
      "Iteration 53, loss = 0.54386316\n",
      "Iteration 54, loss = 0.54284475\n",
      "Iteration 55, loss = 0.54173383\n",
      "Iteration 56, loss = 0.54088096\n",
      "Iteration 57, loss = 0.53990303\n",
      "Iteration 58, loss = 0.53902227\n",
      "Iteration 59, loss = 0.53830904\n",
      "Iteration 60, loss = 0.53745423\n",
      "Iteration 61, loss = 0.53692540\n",
      "Iteration 62, loss = 0.53621651\n",
      "Iteration 63, loss = 0.53564827\n",
      "Iteration 64, loss = 0.53515620\n",
      "Iteration 65, loss = 0.53463710\n",
      "Iteration 66, loss = 0.53422876\n",
      "Iteration 67, loss = 0.53382769\n",
      "Iteration 68, loss = 0.53334556\n",
      "Iteration 69, loss = 0.53296492\n",
      "Iteration 70, loss = 0.53270254\n",
      "Iteration 71, loss = 0.53220505\n",
      "Iteration 72, loss = 0.53181535\n",
      "Iteration 73, loss = 0.53149523\n",
      "Iteration 74, loss = 0.53115985\n",
      "Iteration 75, loss = 0.53078865\n",
      "Iteration 76, loss = 0.53049892\n",
      "Iteration 77, loss = 0.53011984\n",
      "Iteration 78, loss = 0.52990267\n",
      "Iteration 79, loss = 0.52955211\n",
      "Iteration 80, loss = 0.52942361\n",
      "Iteration 81, loss = 0.52913760\n",
      "Iteration 82, loss = 0.52892104\n",
      "Iteration 83, loss = 0.52870003\n",
      "Iteration 84, loss = 0.52844099\n",
      "Iteration 85, loss = 0.52823707\n",
      "Iteration 86, loss = 0.52804489\n",
      "Iteration 87, loss = 0.52774734\n",
      "Iteration 88, loss = 0.52754608\n",
      "Iteration 89, loss = 0.52734286\n",
      "Iteration 90, loss = 0.52707081\n",
      "Iteration 91, loss = 0.52688739\n",
      "Iteration 92, loss = 0.52660825\n",
      "Iteration 93, loss = 0.52635039\n",
      "Iteration 94, loss = 0.52618801\n",
      "Iteration 95, loss = 0.52596165\n",
      "Iteration 96, loss = 0.52579322\n",
      "Iteration 97, loss = 0.52557657\n",
      "Iteration 98, loss = 0.52544014\n",
      "Iteration 99, loss = 0.52527233\n",
      "Iteration 100, loss = 0.52512355\n",
      "Iteration 101, loss = 0.52498295\n",
      "Iteration 102, loss = 0.52488209\n",
      "Iteration 103, loss = 0.52470725\n",
      "Iteration 104, loss = 0.52465774\n",
      "Iteration 105, loss = 0.52455864\n",
      "Iteration 106, loss = 0.52462492\n",
      "Iteration 107, loss = 0.52448030\n",
      "Iteration 108, loss = 0.52436560\n",
      "Iteration 109, loss = 0.52428406\n",
      "Iteration 110, loss = 0.52421444\n",
      "Iteration 111, loss = 0.52410554\n",
      "Iteration 112, loss = 0.52400650\n",
      "Iteration 113, loss = 0.52391776\n",
      "Iteration 114, loss = 0.52383895\n",
      "Iteration 115, loss = 0.52379025\n",
      "Iteration 116, loss = 0.52367216\n",
      "Iteration 117, loss = 0.52359958\n",
      "Iteration 118, loss = 0.52351085\n",
      "Iteration 119, loss = 0.52342576\n",
      "Iteration 120, loss = 0.52333668\n",
      "Iteration 121, loss = 0.52326370\n",
      "Iteration 122, loss = 0.52316730\n",
      "Iteration 123, loss = 0.52309143\n",
      "Iteration 124, loss = 0.52300499\n",
      "Iteration 125, loss = 0.52292178\n",
      "Iteration 126, loss = 0.52285149\n",
      "Iteration 127, loss = 0.52279385\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.77203382\n",
      "Iteration 2, loss = 0.76184510\n",
      "Iteration 3, loss = 0.75146075\n",
      "Iteration 4, loss = 0.74183214\n",
      "Iteration 5, loss = 0.73255231\n",
      "Iteration 6, loss = 0.72320376\n",
      "Iteration 7, loss = 0.71440589\n",
      "Iteration 8, loss = 0.70592979\n",
      "Iteration 9, loss = 0.69764508\n",
      "Iteration 10, loss = 0.68972504\n",
      "Iteration 11, loss = 0.68226018\n",
      "Iteration 12, loss = 0.67480511\n",
      "Iteration 13, loss = 0.66787010\n",
      "Iteration 14, loss = 0.66104238\n",
      "Iteration 15, loss = 0.65499512\n",
      "Iteration 16, loss = 0.64874865\n",
      "Iteration 17, loss = 0.64316777\n",
      "Iteration 18, loss = 0.63763860\n",
      "Iteration 19, loss = 0.63237648\n",
      "Iteration 20, loss = 0.62784177\n",
      "Iteration 21, loss = 0.62318569\n",
      "Iteration 22, loss = 0.61860879\n",
      "Iteration 23, loss = 0.61465917\n",
      "Iteration 24, loss = 0.61045134\n",
      "Iteration 25, loss = 0.60682786\n",
      "Iteration 26, loss = 0.60314387\n",
      "Iteration 27, loss = 0.59980636\n",
      "Iteration 28, loss = 0.59621009\n",
      "Iteration 29, loss = 0.59312675\n",
      "Iteration 30, loss = 0.59018537\n",
      "Iteration 31, loss = 0.58741083\n",
      "Iteration 32, loss = 0.58470754\n",
      "Iteration 33, loss = 0.58213184\n",
      "Iteration 34, loss = 0.57965129\n",
      "Iteration 35, loss = 0.57752677\n",
      "Iteration 36, loss = 0.57537962\n",
      "Iteration 37, loss = 0.57336435\n",
      "Iteration 38, loss = 0.57149517\n",
      "Iteration 39, loss = 0.56964717\n",
      "Iteration 40, loss = 0.56787091\n",
      "Iteration 41, loss = 0.56639877\n",
      "Iteration 42, loss = 0.56477909\n",
      "Iteration 43, loss = 0.56322663\n",
      "Iteration 44, loss = 0.56172189\n",
      "Iteration 45, loss = 0.56031284\n",
      "Iteration 46, loss = 0.55889747\n",
      "Iteration 47, loss = 0.55753211\n",
      "Iteration 48, loss = 0.55612276\n",
      "Iteration 49, loss = 0.55492680\n",
      "Iteration 50, loss = 0.55357259\n",
      "Iteration 51, loss = 0.55261260\n",
      "Iteration 52, loss = 0.55131942\n",
      "Iteration 53, loss = 0.55053573\n",
      "Iteration 54, loss = 0.54944482\n",
      "Iteration 55, loss = 0.54864920\n",
      "Iteration 56, loss = 0.54787365\n",
      "Iteration 57, loss = 0.54708152\n",
      "Iteration 58, loss = 0.54628686\n",
      "Iteration 59, loss = 0.54555680\n",
      "Iteration 60, loss = 0.54490823\n",
      "Iteration 61, loss = 0.54423051\n",
      "Iteration 62, loss = 0.54361564\n",
      "Iteration 63, loss = 0.54298630\n",
      "Iteration 64, loss = 0.54245938\n",
      "Iteration 65, loss = 0.54188922\n",
      "Iteration 66, loss = 0.54135451\n",
      "Iteration 67, loss = 0.54092437\n",
      "Iteration 68, loss = 0.54037251\n",
      "Iteration 69, loss = 0.54008834\n",
      "Iteration 70, loss = 0.53970573\n",
      "Iteration 71, loss = 0.53927106\n",
      "Iteration 72, loss = 0.53898061\n",
      "Iteration 73, loss = 0.53871584\n",
      "Iteration 74, loss = 0.53853426\n",
      "Iteration 75, loss = 0.53822086\n",
      "Iteration 76, loss = 0.53800936\n",
      "Iteration 77, loss = 0.53787197\n",
      "Iteration 78, loss = 0.53760998\n",
      "Iteration 79, loss = 0.53742489\n",
      "Iteration 80, loss = 0.53724991\n",
      "Iteration 81, loss = 0.53710179\n",
      "Iteration 82, loss = 0.53689474\n",
      "Iteration 83, loss = 0.53666940\n",
      "Iteration 84, loss = 0.53646403\n",
      "Iteration 85, loss = 0.53633279\n",
      "Iteration 86, loss = 0.53608947\n",
      "Iteration 87, loss = 0.53594513\n",
      "Iteration 88, loss = 0.53572800\n",
      "Iteration 89, loss = 0.53558080\n",
      "Iteration 90, loss = 0.53547879\n",
      "Iteration 91, loss = 0.53528013\n",
      "Iteration 92, loss = 0.53509934\n",
      "Iteration 93, loss = 0.53496796\n",
      "Iteration 94, loss = 0.53484542\n",
      "Iteration 95, loss = 0.53468521\n",
      "Iteration 96, loss = 0.53451990\n",
      "Iteration 97, loss = 0.53440306\n",
      "Iteration 98, loss = 0.53424783\n",
      "Iteration 99, loss = 0.53418017\n",
      "Iteration 100, loss = 0.53400572\n",
      "Iteration 101, loss = 0.53390137\n",
      "Iteration 102, loss = 0.53383491\n",
      "Iteration 103, loss = 0.53375469\n",
      "Iteration 104, loss = 0.53360403\n",
      "Iteration 105, loss = 0.53353723\n",
      "Iteration 106, loss = 0.53346246\n",
      "Iteration 107, loss = 0.53337222\n",
      "Iteration 108, loss = 0.53333524\n",
      "Iteration 109, loss = 0.53322423\n",
      "Iteration 110, loss = 0.53314470\n",
      "Iteration 111, loss = 0.53309923\n",
      "Iteration 112, loss = 0.53302816\n",
      "Iteration 113, loss = 0.53299093\n",
      "Iteration 114, loss = 0.53293351\n",
      "Iteration 115, loss = 0.53284277\n",
      "Iteration 116, loss = 0.53278720\n",
      "Iteration 117, loss = 0.53272038\n",
      "Iteration 118, loss = 0.53265845\n",
      "Iteration 119, loss = 0.53261598\n",
      "Iteration 120, loss = 0.53252727\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66054288\n",
      "Iteration 2, loss = 0.65528685\n",
      "Iteration 3, loss = 0.65043998\n",
      "Iteration 4, loss = 0.64558496\n",
      "Iteration 5, loss = 0.64107919\n",
      "Iteration 6, loss = 0.63633040\n",
      "Iteration 7, loss = 0.63200155\n",
      "Iteration 8, loss = 0.62793980\n",
      "Iteration 9, loss = 0.62387592\n",
      "Iteration 10, loss = 0.61979110\n",
      "Iteration 11, loss = 0.61596669\n",
      "Iteration 12, loss = 0.61237695\n",
      "Iteration 13, loss = 0.60883432\n",
      "Iteration 14, loss = 0.60555492\n",
      "Iteration 15, loss = 0.60242366\n",
      "Iteration 16, loss = 0.59917726\n",
      "Iteration 17, loss = 0.59602224\n",
      "Iteration 18, loss = 0.59350772\n",
      "Iteration 19, loss = 0.59052553\n",
      "Iteration 20, loss = 0.58789337\n",
      "Iteration 21, loss = 0.58513592\n",
      "Iteration 22, loss = 0.58264978\n",
      "Iteration 23, loss = 0.58028979\n",
      "Iteration 24, loss = 0.57770972\n",
      "Iteration 25, loss = 0.57561095\n",
      "Iteration 26, loss = 0.57335416\n",
      "Iteration 27, loss = 0.57128356\n",
      "Iteration 28, loss = 0.56942421\n",
      "Iteration 29, loss = 0.56737541\n",
      "Iteration 30, loss = 0.56570823\n",
      "Iteration 31, loss = 0.56388834\n",
      "Iteration 32, loss = 0.56223943\n",
      "Iteration 33, loss = 0.56070266\n",
      "Iteration 34, loss = 0.55912107\n",
      "Iteration 35, loss = 0.55761295\n",
      "Iteration 36, loss = 0.55618590\n",
      "Iteration 37, loss = 0.55480175\n",
      "Iteration 38, loss = 0.55349919\n",
      "Iteration 39, loss = 0.55223756\n",
      "Iteration 40, loss = 0.55104699\n",
      "Iteration 41, loss = 0.54981733\n",
      "Iteration 42, loss = 0.54870906\n",
      "Iteration 43, loss = 0.54769256\n",
      "Iteration 44, loss = 0.54651945\n",
      "Iteration 45, loss = 0.54565392\n",
      "Iteration 46, loss = 0.54473372\n",
      "Iteration 47, loss = 0.54381694\n",
      "Iteration 48, loss = 0.54306411\n",
      "Iteration 49, loss = 0.54225097\n",
      "Iteration 50, loss = 0.54177223\n",
      "Iteration 51, loss = 0.54085457\n",
      "Iteration 52, loss = 0.54035086\n",
      "Iteration 53, loss = 0.53978613\n",
      "Iteration 54, loss = 0.53912369\n",
      "Iteration 55, loss = 0.53860272\n",
      "Iteration 56, loss = 0.53806743\n",
      "Iteration 57, loss = 0.53760950\n",
      "Iteration 58, loss = 0.53720956\n",
      "Iteration 59, loss = 0.53681954\n",
      "Iteration 60, loss = 0.53642521\n",
      "Iteration 61, loss = 0.53616281\n",
      "Iteration 62, loss = 0.53584193\n",
      "Iteration 63, loss = 0.53559200\n",
      "Iteration 64, loss = 0.53522095\n",
      "Iteration 65, loss = 0.53504789\n",
      "Iteration 66, loss = 0.53482185\n",
      "Iteration 67, loss = 0.53452778\n",
      "Iteration 68, loss = 0.53432779\n",
      "Iteration 69, loss = 0.53414684\n",
      "Iteration 70, loss = 0.53402448\n",
      "Iteration 71, loss = 0.53383816\n",
      "Iteration 72, loss = 0.53370543\n",
      "Iteration 73, loss = 0.53364998\n",
      "Iteration 74, loss = 0.53343937\n",
      "Iteration 75, loss = 0.53329911\n",
      "Iteration 76, loss = 0.53315657\n",
      "Iteration 77, loss = 0.53303406\n",
      "Iteration 78, loss = 0.53284304\n",
      "Iteration 79, loss = 0.53267981\n",
      "Iteration 80, loss = 0.53260112\n",
      "Iteration 81, loss = 0.53242814\n",
      "Iteration 82, loss = 0.53230394\n",
      "Iteration 83, loss = 0.53221321\n",
      "Iteration 84, loss = 0.53207669\n",
      "Iteration 85, loss = 0.53198531\n",
      "Iteration 86, loss = 0.53190348\n",
      "Iteration 87, loss = 0.53180485\n",
      "Iteration 88, loss = 0.53169254\n",
      "Iteration 89, loss = 0.53168921\n",
      "Iteration 90, loss = 0.53164947\n",
      "Iteration 91, loss = 0.53155936\n",
      "Iteration 92, loss = 0.53151126\n",
      "Iteration 93, loss = 0.53153264\n",
      "Iteration 94, loss = 0.53145922\n",
      "Iteration 95, loss = 0.53137180\n",
      "Iteration 96, loss = 0.53132493\n",
      "Iteration 97, loss = 0.53124184\n",
      "Iteration 98, loss = 0.53114942\n",
      "Iteration 99, loss = 0.53110440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66420878\n",
      "Iteration 2, loss = 0.65833960\n",
      "Iteration 3, loss = 0.65284115\n",
      "Iteration 4, loss = 0.64775571\n",
      "Iteration 5, loss = 0.64292024\n",
      "Iteration 6, loss = 0.63793311\n",
      "Iteration 7, loss = 0.63313816\n",
      "Iteration 8, loss = 0.62873274\n",
      "Iteration 9, loss = 0.62428839\n",
      "Iteration 10, loss = 0.62004309\n",
      "Iteration 11, loss = 0.61627155\n",
      "Iteration 12, loss = 0.61195753\n",
      "Iteration 13, loss = 0.60823157\n",
      "Iteration 14, loss = 0.60497234\n",
      "Iteration 15, loss = 0.60136449\n",
      "Iteration 16, loss = 0.59815972\n",
      "Iteration 17, loss = 0.59509830\n",
      "Iteration 18, loss = 0.59209919\n",
      "Iteration 19, loss = 0.58934045\n",
      "Iteration 20, loss = 0.58647557\n",
      "Iteration 21, loss = 0.58394386\n",
      "Iteration 22, loss = 0.58142536\n",
      "Iteration 23, loss = 0.57903406\n",
      "Iteration 24, loss = 0.57668150\n",
      "Iteration 25, loss = 0.57453972\n",
      "Iteration 26, loss = 0.57234960\n",
      "Iteration 27, loss = 0.57032907\n",
      "Iteration 28, loss = 0.56832415\n",
      "Iteration 29, loss = 0.56651095\n",
      "Iteration 30, loss = 0.56463655\n",
      "Iteration 31, loss = 0.56290188\n",
      "Iteration 32, loss = 0.56138254\n",
      "Iteration 33, loss = 0.55971528\n",
      "Iteration 34, loss = 0.55836940\n",
      "Iteration 35, loss = 0.55695326\n",
      "Iteration 36, loss = 0.55571503\n",
      "Iteration 37, loss = 0.55460462\n",
      "Iteration 38, loss = 0.55331388\n",
      "Iteration 39, loss = 0.55253488\n",
      "Iteration 40, loss = 0.55139627\n",
      "Iteration 41, loss = 0.55043950\n",
      "Iteration 42, loss = 0.54964748\n",
      "Iteration 43, loss = 0.54879859\n",
      "Iteration 44, loss = 0.54813249\n",
      "Iteration 45, loss = 0.54744124\n",
      "Iteration 46, loss = 0.54678419\n",
      "Iteration 47, loss = 0.54627630\n",
      "Iteration 48, loss = 0.54580343\n",
      "Iteration 49, loss = 0.54524649\n",
      "Iteration 50, loss = 0.54486590\n",
      "Iteration 51, loss = 0.54433125\n",
      "Iteration 52, loss = 0.54386410\n",
      "Iteration 53, loss = 0.54346777\n",
      "Iteration 54, loss = 0.54303399\n",
      "Iteration 55, loss = 0.54261714\n",
      "Iteration 56, loss = 0.54218133\n",
      "Iteration 57, loss = 0.54176934\n",
      "Iteration 58, loss = 0.54154718\n",
      "Iteration 59, loss = 0.54112189\n",
      "Iteration 60, loss = 0.54077514\n",
      "Iteration 61, loss = 0.54041246\n",
      "Iteration 62, loss = 0.54019974\n",
      "Iteration 63, loss = 0.53988900\n",
      "Iteration 64, loss = 0.53960408\n",
      "Iteration 65, loss = 0.53929706\n",
      "Iteration 66, loss = 0.53911343\n",
      "Iteration 67, loss = 0.53882128\n",
      "Iteration 68, loss = 0.53860754\n",
      "Iteration 69, loss = 0.53839414\n",
      "Iteration 70, loss = 0.53820532\n",
      "Iteration 71, loss = 0.53799599\n",
      "Iteration 72, loss = 0.53778855\n",
      "Iteration 73, loss = 0.53758507\n",
      "Iteration 74, loss = 0.53740422\n",
      "Iteration 75, loss = 0.53717345\n",
      "Iteration 76, loss = 0.53703693\n",
      "Iteration 77, loss = 0.53681776\n",
      "Iteration 78, loss = 0.53664321\n",
      "Iteration 79, loss = 0.53649800\n",
      "Iteration 80, loss = 0.53629599\n",
      "Iteration 81, loss = 0.53614100\n",
      "Iteration 82, loss = 0.53599121\n",
      "Iteration 83, loss = 0.53586827\n",
      "Iteration 84, loss = 0.53576901\n",
      "Iteration 85, loss = 0.53561994\n",
      "Iteration 86, loss = 0.53551074\n",
      "Iteration 87, loss = 0.53535182\n",
      "Iteration 88, loss = 0.53522998\n",
      "Iteration 89, loss = 0.53509161\n",
      "Iteration 90, loss = 0.53498220\n",
      "Iteration 91, loss = 0.53484530\n",
      "Iteration 92, loss = 0.53470545\n",
      "Iteration 93, loss = 0.53459545\n",
      "Iteration 94, loss = 0.53446814\n",
      "Iteration 95, loss = 0.53436751\n",
      "Iteration 96, loss = 0.53425000\n",
      "Iteration 97, loss = 0.53415864\n",
      "Iteration 98, loss = 0.53405959\n",
      "Iteration 99, loss = 0.53397607\n",
      "Iteration 100, loss = 0.53390324\n",
      "Iteration 101, loss = 0.53382307\n",
      "Iteration 102, loss = 0.53377992\n",
      "Iteration 103, loss = 0.53369050\n",
      "Iteration 104, loss = 0.53368963\n",
      "Iteration 105, loss = 0.53357142\n",
      "Iteration 106, loss = 0.53350738\n",
      "Iteration 107, loss = 0.53339388\n",
      "Iteration 108, loss = 0.53331649\n",
      "Iteration 109, loss = 0.53317823\n",
      "Iteration 110, loss = 0.53306238\n",
      "Iteration 111, loss = 0.53293407\n",
      "Iteration 112, loss = 0.53288041\n",
      "Iteration 113, loss = 0.53271983\n",
      "Iteration 114, loss = 0.53259813\n",
      "Iteration 115, loss = 0.53246756\n",
      "Iteration 116, loss = 0.53239479\n",
      "Iteration 117, loss = 0.53227311\n",
      "Iteration 118, loss = 0.53214453\n",
      "Iteration 119, loss = 0.53205079\n",
      "Iteration 120, loss = 0.53215990\n",
      "Iteration 121, loss = 0.53207346\n",
      "Iteration 122, loss = 0.53202646\n",
      "Iteration 123, loss = 0.53204385\n",
      "Iteration 124, loss = 0.53203341\n",
      "Iteration 125, loss = 0.53197693\n",
      "Iteration 126, loss = 0.53195903\n",
      "Iteration 127, loss = 0.53187903\n",
      "Iteration 128, loss = 0.53179270\n",
      "Iteration 129, loss = 0.53173463\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69774612\n",
      "Iteration 2, loss = 0.69082088\n",
      "Iteration 3, loss = 0.68435743\n",
      "Iteration 4, loss = 0.67803148\n",
      "Iteration 5, loss = 0.67227653\n",
      "Iteration 6, loss = 0.66658770\n",
      "Iteration 7, loss = 0.66121866\n",
      "Iteration 8, loss = 0.65611892\n",
      "Iteration 9, loss = 0.65108590\n",
      "Iteration 10, loss = 0.64633248\n",
      "Iteration 11, loss = 0.64148508\n",
      "Iteration 12, loss = 0.63697355\n",
      "Iteration 13, loss = 0.63270712\n",
      "Iteration 14, loss = 0.62856699\n",
      "Iteration 15, loss = 0.62464612\n",
      "Iteration 16, loss = 0.62091192\n",
      "Iteration 17, loss = 0.61756572\n",
      "Iteration 18, loss = 0.61405463\n",
      "Iteration 19, loss = 0.61085139\n",
      "Iteration 20, loss = 0.60819997\n",
      "Iteration 21, loss = 0.60489288\n",
      "Iteration 22, loss = 0.60217728\n",
      "Iteration 23, loss = 0.59959251\n",
      "Iteration 24, loss = 0.59698791\n",
      "Iteration 25, loss = 0.59442086\n",
      "Iteration 26, loss = 0.59208311\n",
      "Iteration 27, loss = 0.58975538\n",
      "Iteration 28, loss = 0.58771794\n",
      "Iteration 29, loss = 0.58563857\n",
      "Iteration 30, loss = 0.58370688\n",
      "Iteration 31, loss = 0.58192453\n",
      "Iteration 32, loss = 0.58012774\n",
      "Iteration 33, loss = 0.57844559\n",
      "Iteration 34, loss = 0.57686790\n",
      "Iteration 35, loss = 0.57551584\n",
      "Iteration 36, loss = 0.57389459\n",
      "Iteration 37, loss = 0.57266102\n",
      "Iteration 38, loss = 0.57134721\n",
      "Iteration 39, loss = 0.57014772\n",
      "Iteration 40, loss = 0.56908500\n",
      "Iteration 41, loss = 0.56793210\n",
      "Iteration 42, loss = 0.56682010\n",
      "Iteration 43, loss = 0.56586523\n",
      "Iteration 44, loss = 0.56476102\n",
      "Iteration 45, loss = 0.56389904\n",
      "Iteration 46, loss = 0.56306576\n",
      "Iteration 47, loss = 0.56224381\n",
      "Iteration 48, loss = 0.56146090\n",
      "Iteration 49, loss = 0.56091497\n",
      "Iteration 50, loss = 0.56022942\n",
      "Iteration 51, loss = 0.55956982\n",
      "Iteration 52, loss = 0.55912803\n",
      "Iteration 53, loss = 0.55848297\n",
      "Iteration 54, loss = 0.55801811\n",
      "Iteration 55, loss = 0.55755098\n",
      "Iteration 56, loss = 0.55710598\n",
      "Iteration 57, loss = 0.55668531\n",
      "Iteration 58, loss = 0.55624907\n",
      "Iteration 59, loss = 0.55582196\n",
      "Iteration 60, loss = 0.55534110\n",
      "Iteration 61, loss = 0.55495259\n",
      "Iteration 62, loss = 0.55463415\n",
      "Iteration 63, loss = 0.55426712\n",
      "Iteration 64, loss = 0.55403164\n",
      "Iteration 65, loss = 0.55369412\n",
      "Iteration 66, loss = 0.55351381\n",
      "Iteration 67, loss = 0.55319220\n",
      "Iteration 68, loss = 0.55297686\n",
      "Iteration 69, loss = 0.55280433\n",
      "Iteration 70, loss = 0.55258583\n",
      "Iteration 71, loss = 0.55241356\n",
      "Iteration 72, loss = 0.55221527\n",
      "Iteration 73, loss = 0.55210461\n",
      "Iteration 74, loss = 0.55195504\n",
      "Iteration 75, loss = 0.55183251\n",
      "Iteration 76, loss = 0.55169790\n",
      "Iteration 77, loss = 0.55154104\n",
      "Iteration 78, loss = 0.55143059\n",
      "Iteration 79, loss = 0.55134004\n",
      "Iteration 80, loss = 0.55117396\n",
      "Iteration 81, loss = 0.55108437\n",
      "Iteration 82, loss = 0.55102204\n",
      "Iteration 83, loss = 0.55087489\n",
      "Iteration 84, loss = 0.55077733\n",
      "Iteration 85, loss = 0.55071161\n",
      "Iteration 86, loss = 0.55062118\n",
      "Iteration 87, loss = 0.55062120\n",
      "Iteration 88, loss = 0.55054642\n",
      "Iteration 89, loss = 0.55050632\n",
      "Iteration 90, loss = 0.55040665\n",
      "Iteration 91, loss = 0.55032271\n",
      "Iteration 92, loss = 0.55024105\n",
      "Iteration 93, loss = 0.55024671\n",
      "Iteration 94, loss = 0.55016329\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75271914\n",
      "Iteration 2, loss = 0.74476018\n",
      "Iteration 3, loss = 0.73743757\n",
      "Iteration 4, loss = 0.73043911\n",
      "Iteration 5, loss = 0.72333534\n",
      "Iteration 6, loss = 0.71678808\n",
      "Iteration 7, loss = 0.71035500\n",
      "Iteration 8, loss = 0.70389492\n",
      "Iteration 9, loss = 0.69806352\n",
      "Iteration 10, loss = 0.69255985\n",
      "Iteration 11, loss = 0.68669346\n",
      "Iteration 12, loss = 0.68135590\n",
      "Iteration 13, loss = 0.67629778\n",
      "Iteration 14, loss = 0.67138486\n",
      "Iteration 15, loss = 0.66657778\n",
      "Iteration 16, loss = 0.66203214\n",
      "Iteration 17, loss = 0.65762973\n",
      "Iteration 18, loss = 0.65331159\n",
      "Iteration 19, loss = 0.64918099\n",
      "Iteration 20, loss = 0.64532832\n",
      "Iteration 21, loss = 0.64152864\n",
      "Iteration 22, loss = 0.63757903\n",
      "Iteration 23, loss = 0.63394724\n",
      "Iteration 24, loss = 0.63050732\n",
      "Iteration 25, loss = 0.62742394\n",
      "Iteration 26, loss = 0.62386489\n",
      "Iteration 27, loss = 0.62087723\n",
      "Iteration 28, loss = 0.61779930\n",
      "Iteration 29, loss = 0.61510016\n",
      "Iteration 30, loss = 0.61203948\n",
      "Iteration 31, loss = 0.60939449\n",
      "Iteration 32, loss = 0.60672231\n",
      "Iteration 33, loss = 0.60419686\n",
      "Iteration 34, loss = 0.60159591\n",
      "Iteration 35, loss = 0.59921516\n",
      "Iteration 36, loss = 0.59692713\n",
      "Iteration 37, loss = 0.59447326\n",
      "Iteration 38, loss = 0.59236831\n",
      "Iteration 39, loss = 0.59008041\n",
      "Iteration 40, loss = 0.58806230\n",
      "Iteration 41, loss = 0.58587113\n",
      "Iteration 42, loss = 0.58392499\n",
      "Iteration 43, loss = 0.58198276\n",
      "Iteration 44, loss = 0.58021272\n",
      "Iteration 45, loss = 0.57847953\n",
      "Iteration 46, loss = 0.57683151\n",
      "Iteration 47, loss = 0.57523199\n",
      "Iteration 48, loss = 0.57372839\n",
      "Iteration 49, loss = 0.57235083\n",
      "Iteration 50, loss = 0.57098087\n",
      "Iteration 51, loss = 0.56953408\n",
      "Iteration 52, loss = 0.56824734\n",
      "Iteration 53, loss = 0.56702095\n",
      "Iteration 54, loss = 0.56581034\n",
      "Iteration 55, loss = 0.56463913\n",
      "Iteration 56, loss = 0.56344864\n",
      "Iteration 57, loss = 0.56236808\n",
      "Iteration 58, loss = 0.56126956\n",
      "Iteration 59, loss = 0.56016760\n",
      "Iteration 60, loss = 0.55921248\n",
      "Iteration 61, loss = 0.55822229\n",
      "Iteration 62, loss = 0.55724933\n",
      "Iteration 63, loss = 0.55641640\n",
      "Iteration 64, loss = 0.55568144\n",
      "Iteration 65, loss = 0.55493422\n",
      "Iteration 66, loss = 0.55422123\n",
      "Iteration 67, loss = 0.55350457\n",
      "Iteration 68, loss = 0.55291690\n",
      "Iteration 69, loss = 0.55235650\n",
      "Iteration 70, loss = 0.55175963\n",
      "Iteration 71, loss = 0.55127846\n",
      "Iteration 72, loss = 0.55074625\n",
      "Iteration 73, loss = 0.55025433\n",
      "Iteration 74, loss = 0.54989041\n",
      "Iteration 75, loss = 0.54946893\n",
      "Iteration 76, loss = 0.54913837\n",
      "Iteration 77, loss = 0.54878371\n",
      "Iteration 78, loss = 0.54854744\n",
      "Iteration 79, loss = 0.54822167\n",
      "Iteration 80, loss = 0.54800293\n",
      "Iteration 81, loss = 0.54772681\n",
      "Iteration 82, loss = 0.54744481\n",
      "Iteration 83, loss = 0.54725102\n",
      "Iteration 84, loss = 0.54705188\n",
      "Iteration 85, loss = 0.54686036\n",
      "Iteration 86, loss = 0.54660693\n",
      "Iteration 87, loss = 0.54638558\n",
      "Iteration 88, loss = 0.54607090\n",
      "Iteration 89, loss = 0.54593713\n",
      "Iteration 90, loss = 0.54558147\n",
      "Iteration 91, loss = 0.54532697\n",
      "Iteration 92, loss = 0.54521584\n",
      "Iteration 93, loss = 0.54492997\n",
      "Iteration 94, loss = 0.54480388\n",
      "Iteration 95, loss = 0.54461776\n",
      "Iteration 96, loss = 0.54447335\n",
      "Iteration 97, loss = 0.54429006\n",
      "Iteration 98, loss = 0.54420632\n",
      "Iteration 99, loss = 0.54407835\n",
      "Iteration 100, loss = 0.54392815\n",
      "Iteration 101, loss = 0.54392457\n",
      "Iteration 102, loss = 0.54373654\n",
      "Iteration 103, loss = 0.54368974\n",
      "Iteration 104, loss = 0.54371422\n",
      "Iteration 105, loss = 0.54356594\n",
      "Iteration 106, loss = 0.54348050\n",
      "Iteration 107, loss = 0.54339940\n",
      "Iteration 108, loss = 0.54326346\n",
      "Iteration 109, loss = 0.54314934\n",
      "Iteration 110, loss = 0.54304140\n",
      "Iteration 111, loss = 0.54294474\n",
      "Iteration 112, loss = 0.54282853\n",
      "Iteration 113, loss = 0.54274572\n",
      "Iteration 114, loss = 0.54262663\n",
      "Iteration 115, loss = 0.54255515\n",
      "Iteration 116, loss = 0.54254845\n",
      "Iteration 117, loss = 0.54242113\n",
      "Iteration 118, loss = 0.54240343\n",
      "Iteration 119, loss = 0.54229401\n",
      "Iteration 120, loss = 0.54221769\n",
      "Iteration 121, loss = 0.54214547\n",
      "Iteration 122, loss = 0.54206295\n",
      "Iteration 123, loss = 0.54197134\n",
      "Iteration 124, loss = 0.54188467\n",
      "Iteration 125, loss = 0.54180782\n",
      "Iteration 126, loss = 0.54175875\n",
      "Iteration 127, loss = 0.54176152\n",
      "Iteration 128, loss = 0.54163763\n",
      "Iteration 129, loss = 0.54160090\n",
      "Iteration 130, loss = 0.54155339\n",
      "Iteration 131, loss = 0.54150650\n",
      "Iteration 132, loss = 0.54151677\n",
      "Iteration 133, loss = 0.54149526\n",
      "Iteration 134, loss = 0.54144010\n",
      "Iteration 135, loss = 0.54124889\n",
      "Iteration 136, loss = 0.54124202\n",
      "Iteration 137, loss = 0.54117111\n",
      "Iteration 138, loss = 0.54104611\n",
      "Iteration 139, loss = 0.54099240\n",
      "Iteration 140, loss = 0.54097876\n",
      "Iteration 141, loss = 0.54091502\n",
      "Iteration 142, loss = 0.54084917\n",
      "Iteration 143, loss = 0.54078132\n",
      "Iteration 144, loss = 0.54078807\n",
      "Iteration 145, loss = 0.54069741\n",
      "Iteration 146, loss = 0.54064651\n",
      "Iteration 147, loss = 0.54064250\n",
      "Iteration 148, loss = 0.54058560\n",
      "Iteration 149, loss = 0.54054289\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.61689667\n",
      "Iteration 2, loss = 0.61162745\n",
      "Iteration 3, loss = 0.60674465\n",
      "Iteration 4, loss = 0.60217688\n",
      "Iteration 5, loss = 0.59753174\n",
      "Iteration 6, loss = 0.59336471\n",
      "Iteration 7, loss = 0.58924720\n",
      "Iteration 8, loss = 0.58535559\n",
      "Iteration 9, loss = 0.58175154\n",
      "Iteration 10, loss = 0.57828001\n",
      "Iteration 11, loss = 0.57506961\n",
      "Iteration 12, loss = 0.57199898\n",
      "Iteration 13, loss = 0.56923784\n",
      "Iteration 14, loss = 0.56647892\n",
      "Iteration 15, loss = 0.56404241\n",
      "Iteration 16, loss = 0.56187921\n",
      "Iteration 17, loss = 0.55951547\n",
      "Iteration 18, loss = 0.55785167\n",
      "Iteration 19, loss = 0.55606483\n",
      "Iteration 20, loss = 0.55416328\n",
      "Iteration 21, loss = 0.55265549\n",
      "Iteration 22, loss = 0.55137080\n",
      "Iteration 23, loss = 0.54993760\n",
      "Iteration 24, loss = 0.54879571\n",
      "Iteration 25, loss = 0.54769412\n",
      "Iteration 26, loss = 0.54672091\n",
      "Iteration 27, loss = 0.54578872\n",
      "Iteration 28, loss = 0.54481420\n",
      "Iteration 29, loss = 0.54409136\n",
      "Iteration 30, loss = 0.54318997\n",
      "Iteration 31, loss = 0.54270073\n",
      "Iteration 32, loss = 0.54198618\n",
      "Iteration 33, loss = 0.54134749\n",
      "Iteration 34, loss = 0.54085645\n",
      "Iteration 35, loss = 0.54043326\n",
      "Iteration 36, loss = 0.53999973\n",
      "Iteration 37, loss = 0.53967341\n",
      "Iteration 38, loss = 0.53929422\n",
      "Iteration 39, loss = 0.53899048\n",
      "Iteration 40, loss = 0.53868813\n",
      "Iteration 41, loss = 0.53837272\n",
      "Iteration 42, loss = 0.53813217\n",
      "Iteration 43, loss = 0.53785291\n",
      "Iteration 44, loss = 0.53764061\n",
      "Iteration 45, loss = 0.53737294\n",
      "Iteration 46, loss = 0.53715739\n",
      "Iteration 47, loss = 0.53689614\n",
      "Iteration 48, loss = 0.53674396\n",
      "Iteration 49, loss = 0.53652415\n",
      "Iteration 50, loss = 0.53633287\n",
      "Iteration 51, loss = 0.53614801\n",
      "Iteration 52, loss = 0.53600428\n",
      "Iteration 53, loss = 0.53585240\n",
      "Iteration 54, loss = 0.53577502\n",
      "Iteration 55, loss = 0.53557005\n",
      "Iteration 56, loss = 0.53543710\n",
      "Iteration 57, loss = 0.53529191\n",
      "Iteration 58, loss = 0.53516239\n",
      "Iteration 59, loss = 0.53502871\n",
      "Iteration 60, loss = 0.53490261\n",
      "Iteration 61, loss = 0.53472536\n",
      "Iteration 62, loss = 0.53462199\n",
      "Iteration 63, loss = 0.53453777\n",
      "Iteration 64, loss = 0.53437106\n",
      "Iteration 65, loss = 0.53428115\n",
      "Iteration 66, loss = 0.53416993\n",
      "Iteration 67, loss = 0.53407555\n",
      "Iteration 68, loss = 0.53403596\n",
      "Iteration 69, loss = 0.53389716\n",
      "Iteration 70, loss = 0.53383133\n",
      "Iteration 71, loss = 0.53375526\n",
      "Iteration 72, loss = 0.53358849\n",
      "Iteration 73, loss = 0.53346136\n",
      "Iteration 74, loss = 0.53340870\n",
      "Iteration 75, loss = 0.53321120\n",
      "Iteration 76, loss = 0.53311971\n",
      "Iteration 77, loss = 0.53305201\n",
      "Iteration 78, loss = 0.53294849\n",
      "Iteration 79, loss = 0.53280814\n",
      "Iteration 80, loss = 0.53270434\n",
      "Iteration 81, loss = 0.53272271\n",
      "Iteration 82, loss = 0.53259051\n",
      "Iteration 83, loss = 0.53252221\n",
      "Iteration 84, loss = 0.53248950\n",
      "Iteration 85, loss = 0.53235381\n",
      "Iteration 86, loss = 0.53226089\n",
      "Iteration 87, loss = 0.53213435\n",
      "Iteration 88, loss = 0.53210788\n",
      "Iteration 89, loss = 0.53201727\n",
      "Iteration 90, loss = 0.53198775\n",
      "Iteration 91, loss = 0.53203009\n",
      "Iteration 92, loss = 0.53197461\n",
      "Iteration 93, loss = 0.53191299\n",
      "Iteration 94, loss = 0.53184396\n",
      "Iteration 95, loss = 0.53177088\n",
      "Iteration 96, loss = 0.53168958\n",
      "Iteration 97, loss = 0.53164256\n",
      "Iteration 98, loss = 0.53152674\n",
      "Iteration 99, loss = 0.53146318\n",
      "Iteration 100, loss = 0.53137822\n",
      "Iteration 101, loss = 0.53132874\n",
      "Iteration 102, loss = 0.53127576\n",
      "Iteration 103, loss = 0.53116137\n",
      "Iteration 104, loss = 0.53105988\n",
      "Iteration 105, loss = 0.53100235\n",
      "Iteration 106, loss = 0.53096121\n",
      "Iteration 107, loss = 0.53090377\n",
      "Iteration 108, loss = 0.53083600\n",
      "Iteration 109, loss = 0.53080679\n",
      "Iteration 110, loss = 0.53072468\n",
      "Iteration 111, loss = 0.53068539\n",
      "Iteration 112, loss = 0.53065051\n",
      "Iteration 113, loss = 0.53068020\n",
      "Iteration 114, loss = 0.53057905\n",
      "Iteration 115, loss = 0.53053953\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75015500\n",
      "Iteration 2, loss = 0.73886226\n",
      "Iteration 3, loss = 0.72891636\n",
      "Iteration 4, loss = 0.71881547\n",
      "Iteration 5, loss = 0.70930201\n",
      "Iteration 6, loss = 0.69997581\n",
      "Iteration 7, loss = 0.69099139\n",
      "Iteration 8, loss = 0.68272476\n",
      "Iteration 9, loss = 0.67455216\n",
      "Iteration 10, loss = 0.66666231\n",
      "Iteration 11, loss = 0.65915561\n",
      "Iteration 12, loss = 0.65174071\n",
      "Iteration 13, loss = 0.64502578\n",
      "Iteration 14, loss = 0.63829345\n",
      "Iteration 15, loss = 0.63220698\n",
      "Iteration 16, loss = 0.62588077\n",
      "Iteration 17, loss = 0.62013424\n",
      "Iteration 18, loss = 0.61454176\n",
      "Iteration 19, loss = 0.60933342\n",
      "Iteration 20, loss = 0.60405853\n",
      "Iteration 21, loss = 0.59921101\n",
      "Iteration 22, loss = 0.59458466\n",
      "Iteration 23, loss = 0.59000978\n",
      "Iteration 24, loss = 0.58597054\n",
      "Iteration 25, loss = 0.58211548\n",
      "Iteration 26, loss = 0.57828149\n",
      "Iteration 27, loss = 0.57486202\n",
      "Iteration 28, loss = 0.57149429\n",
      "Iteration 29, loss = 0.56844766\n",
      "Iteration 30, loss = 0.56547829\n",
      "Iteration 31, loss = 0.56288645\n",
      "Iteration 32, loss = 0.56021242\n",
      "Iteration 33, loss = 0.55774495\n",
      "Iteration 34, loss = 0.55570467\n",
      "Iteration 35, loss = 0.55338633\n",
      "Iteration 36, loss = 0.55162009\n",
      "Iteration 37, loss = 0.54965674\n",
      "Iteration 38, loss = 0.54820832\n",
      "Iteration 39, loss = 0.54637202\n",
      "Iteration 40, loss = 0.54498767\n",
      "Iteration 41, loss = 0.54358743\n",
      "Iteration 42, loss = 0.54239890\n",
      "Iteration 43, loss = 0.54115451\n",
      "Iteration 44, loss = 0.53996486\n",
      "Iteration 45, loss = 0.53891951\n",
      "Iteration 46, loss = 0.53804803\n",
      "Iteration 47, loss = 0.53718380\n",
      "Iteration 48, loss = 0.53630700\n",
      "Iteration 49, loss = 0.53565898\n",
      "Iteration 50, loss = 0.53491832\n",
      "Iteration 51, loss = 0.53436956\n",
      "Iteration 52, loss = 0.53393701\n",
      "Iteration 53, loss = 0.53346340\n",
      "Iteration 54, loss = 0.53314487\n",
      "Iteration 55, loss = 0.53281634\n",
      "Iteration 56, loss = 0.53265882\n",
      "Iteration 57, loss = 0.53237460\n",
      "Iteration 58, loss = 0.53229443\n",
      "Iteration 59, loss = 0.53200227\n",
      "Iteration 60, loss = 0.53177374\n",
      "Iteration 61, loss = 0.53164637\n",
      "Iteration 62, loss = 0.53139930\n",
      "Iteration 63, loss = 0.53120318\n",
      "Iteration 64, loss = 0.53087572\n",
      "Iteration 65, loss = 0.53069592\n",
      "Iteration 66, loss = 0.53054220\n",
      "Iteration 67, loss = 0.53023028\n",
      "Iteration 68, loss = 0.53009412\n",
      "Iteration 69, loss = 0.53002485\n",
      "Iteration 70, loss = 0.52996406\n",
      "Iteration 71, loss = 0.52982393\n",
      "Iteration 72, loss = 0.52961449\n",
      "Iteration 73, loss = 0.52943038\n",
      "Iteration 74, loss = 0.52927415\n",
      "Iteration 75, loss = 0.52910036\n",
      "Iteration 76, loss = 0.52894037\n",
      "Iteration 77, loss = 0.52888163\n",
      "Iteration 78, loss = 0.52872766\n",
      "Iteration 79, loss = 0.52859630\n",
      "Iteration 80, loss = 0.52851661\n",
      "Iteration 81, loss = 0.52842923\n",
      "Iteration 82, loss = 0.52832898\n",
      "Iteration 83, loss = 0.52824914\n",
      "Iteration 84, loss = 0.52813461\n",
      "Iteration 85, loss = 0.52800462\n",
      "Iteration 86, loss = 0.52792923\n",
      "Iteration 87, loss = 0.52805896\n",
      "Iteration 88, loss = 0.52780178\n",
      "Iteration 89, loss = 0.52771832\n",
      "Iteration 90, loss = 0.52763011\n",
      "Iteration 91, loss = 0.52755646\n",
      "Iteration 92, loss = 0.52748491\n",
      "Iteration 93, loss = 0.52741001\n",
      "Iteration 94, loss = 0.52735798\n",
      "Iteration 95, loss = 0.52725970\n",
      "Iteration 96, loss = 0.52719206\n",
      "Iteration 97, loss = 0.52712558\n",
      "Iteration 98, loss = 0.52701510\n",
      "Iteration 99, loss = 0.52696665\n",
      "Iteration 100, loss = 0.52688524\n",
      "Iteration 101, loss = 0.52678849\n",
      "Iteration 102, loss = 0.52683254\n",
      "Iteration 103, loss = 0.52668310\n",
      "Iteration 104, loss = 0.52659085\n",
      "Iteration 105, loss = 0.52653106\n",
      "Iteration 106, loss = 0.52646678\n",
      "Iteration 107, loss = 0.52640410\n",
      "Iteration 108, loss = 0.52636024\n",
      "Iteration 109, loss = 0.52628865\n",
      "Iteration 110, loss = 0.52622602\n",
      "Iteration 111, loss = 0.52627129\n",
      "Iteration 112, loss = 0.52613279\n",
      "Iteration 113, loss = 0.52607847\n",
      "Iteration 114, loss = 0.52604349\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69473057\n",
      "Iteration 2, loss = 0.68508485\n",
      "Iteration 3, loss = 0.67565364\n",
      "Iteration 4, loss = 0.66686697\n",
      "Iteration 5, loss = 0.65820134\n",
      "Iteration 6, loss = 0.65038830\n",
      "Iteration 7, loss = 0.64282662\n",
      "Iteration 8, loss = 0.63536374\n",
      "Iteration 9, loss = 0.62875971\n",
      "Iteration 10, loss = 0.62227714\n",
      "Iteration 11, loss = 0.61656902\n",
      "Iteration 12, loss = 0.61044478\n",
      "Iteration 13, loss = 0.60548686\n",
      "Iteration 14, loss = 0.60051810\n",
      "Iteration 15, loss = 0.59569839\n",
      "Iteration 16, loss = 0.59187120\n",
      "Iteration 17, loss = 0.58764029\n",
      "Iteration 18, loss = 0.58414534\n",
      "Iteration 19, loss = 0.58077991\n",
      "Iteration 20, loss = 0.57769702\n",
      "Iteration 21, loss = 0.57495895\n",
      "Iteration 22, loss = 0.57256260\n",
      "Iteration 23, loss = 0.57011419\n",
      "Iteration 24, loss = 0.56800450\n",
      "Iteration 25, loss = 0.56620007\n",
      "Iteration 26, loss = 0.56471646\n",
      "Iteration 27, loss = 0.56319040\n",
      "Iteration 28, loss = 0.56174233\n",
      "Iteration 29, loss = 0.56077178\n",
      "Iteration 30, loss = 0.55995312\n",
      "Iteration 31, loss = 0.55891849\n",
      "Iteration 32, loss = 0.55819349\n",
      "Iteration 33, loss = 0.55767579\n",
      "Iteration 34, loss = 0.55710009\n",
      "Iteration 35, loss = 0.55660781\n",
      "Iteration 36, loss = 0.55611124\n",
      "Iteration 37, loss = 0.55570440\n",
      "Iteration 38, loss = 0.55536158\n",
      "Iteration 39, loss = 0.55479454\n",
      "Iteration 40, loss = 0.55438099\n",
      "Iteration 41, loss = 0.55403220\n",
      "Iteration 42, loss = 0.55369376\n",
      "Iteration 43, loss = 0.55320029\n",
      "Iteration 44, loss = 0.55291458\n",
      "Iteration 45, loss = 0.55263158\n",
      "Iteration 46, loss = 0.55231504\n",
      "Iteration 47, loss = 0.55199220\n",
      "Iteration 48, loss = 0.55178602\n",
      "Iteration 49, loss = 0.55150420\n",
      "Iteration 50, loss = 0.55125701\n",
      "Iteration 51, loss = 0.55099755\n",
      "Iteration 52, loss = 0.55088864\n",
      "Iteration 53, loss = 0.55059457\n",
      "Iteration 54, loss = 0.55044554\n",
      "Iteration 55, loss = 0.55022881\n",
      "Iteration 56, loss = 0.55011081\n",
      "Iteration 57, loss = 0.54998704\n",
      "Iteration 58, loss = 0.54980829\n",
      "Iteration 59, loss = 0.54965477\n",
      "Iteration 60, loss = 0.54954151\n",
      "Iteration 61, loss = 0.54945259\n",
      "Iteration 62, loss = 0.54940162\n",
      "Iteration 63, loss = 0.54922253\n",
      "Iteration 64, loss = 0.54917376\n",
      "Iteration 65, loss = 0.54903404\n",
      "Iteration 66, loss = 0.54893064\n",
      "Iteration 67, loss = 0.54881796\n",
      "Iteration 68, loss = 0.54872583\n",
      "Iteration 69, loss = 0.54862233\n",
      "Iteration 70, loss = 0.54853549\n",
      "Iteration 71, loss = 0.54842631\n",
      "Iteration 72, loss = 0.54838936\n",
      "Iteration 73, loss = 0.54825659\n",
      "Iteration 74, loss = 0.54814033\n",
      "Iteration 75, loss = 0.54806016\n",
      "Iteration 76, loss = 0.54794948\n",
      "Iteration 77, loss = 0.54787407\n",
      "Iteration 78, loss = 0.54790320\n",
      "Iteration 79, loss = 0.54777834\n",
      "Iteration 80, loss = 0.54767292\n",
      "Iteration 81, loss = 0.54760753\n",
      "Iteration 82, loss = 0.54749761\n",
      "Iteration 83, loss = 0.54743044\n",
      "Iteration 84, loss = 0.54741487\n",
      "Iteration 85, loss = 0.54734608\n",
      "Iteration 86, loss = 0.54730678\n",
      "Iteration 87, loss = 0.54738736\n",
      "Iteration 88, loss = 0.54732604\n",
      "Iteration 89, loss = 0.54729674\n",
      "Iteration 90, loss = 0.54721034\n",
      "Iteration 91, loss = 0.54717265\n",
      "Iteration 92, loss = 0.54701950\n",
      "Iteration 93, loss = 0.54700441\n",
      "Iteration 94, loss = 0.54697417\n",
      "Iteration 95, loss = 0.54698382\n",
      "Iteration 96, loss = 0.54685477\n",
      "Iteration 97, loss = 0.54671434\n",
      "Iteration 98, loss = 0.54662431\n",
      "Iteration 99, loss = 0.54656620\n",
      "Iteration 100, loss = 0.54638009\n",
      "Iteration 101, loss = 0.54629451\n",
      "Iteration 102, loss = 0.54625544\n",
      "Iteration 103, loss = 0.54617144\n",
      "Iteration 104, loss = 0.54608546\n",
      "Iteration 105, loss = 0.54601646\n",
      "Iteration 106, loss = 0.54599371\n",
      "Iteration 107, loss = 0.54589657\n",
      "Iteration 108, loss = 0.54594143\n",
      "Iteration 109, loss = 0.54590346\n",
      "Iteration 110, loss = 0.54589570\n",
      "Iteration 111, loss = 0.54588340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65149746\n",
      "Iteration 2, loss = 0.64335624\n",
      "Iteration 3, loss = 0.63629234\n",
      "Iteration 4, loss = 0.62916863\n",
      "Iteration 5, loss = 0.62263678\n",
      "Iteration 6, loss = 0.61643677\n",
      "Iteration 7, loss = 0.61045228\n",
      "Iteration 8, loss = 0.60502467\n",
      "Iteration 9, loss = 0.60016760\n",
      "Iteration 10, loss = 0.59508068\n",
      "Iteration 11, loss = 0.59071744\n",
      "Iteration 12, loss = 0.58649082\n",
      "Iteration 13, loss = 0.58259336\n",
      "Iteration 14, loss = 0.57895180\n",
      "Iteration 15, loss = 0.57550167\n",
      "Iteration 16, loss = 0.57238550\n",
      "Iteration 17, loss = 0.56930075\n",
      "Iteration 18, loss = 0.56632414\n",
      "Iteration 19, loss = 0.56375417\n",
      "Iteration 20, loss = 0.56133337\n",
      "Iteration 21, loss = 0.55911615\n",
      "Iteration 22, loss = 0.55685632\n",
      "Iteration 23, loss = 0.55490019\n",
      "Iteration 24, loss = 0.55311113\n",
      "Iteration 25, loss = 0.55154800\n",
      "Iteration 26, loss = 0.55003389\n",
      "Iteration 27, loss = 0.54851868\n",
      "Iteration 28, loss = 0.54751415\n",
      "Iteration 29, loss = 0.54618165\n",
      "Iteration 30, loss = 0.54517619\n",
      "Iteration 31, loss = 0.54443383\n",
      "Iteration 32, loss = 0.54383246\n",
      "Iteration 33, loss = 0.54299666\n",
      "Iteration 34, loss = 0.54241836\n",
      "Iteration 35, loss = 0.54197219\n",
      "Iteration 36, loss = 0.54162665\n",
      "Iteration 37, loss = 0.54111255\n",
      "Iteration 38, loss = 0.54064175\n",
      "Iteration 39, loss = 0.54021863\n",
      "Iteration 40, loss = 0.53988800\n",
      "Iteration 41, loss = 0.53943312\n",
      "Iteration 42, loss = 0.53913847\n",
      "Iteration 43, loss = 0.53879428\n",
      "Iteration 44, loss = 0.53849842\n",
      "Iteration 45, loss = 0.53814000\n",
      "Iteration 46, loss = 0.53797205\n",
      "Iteration 47, loss = 0.53775580\n",
      "Iteration 48, loss = 0.53752746\n",
      "Iteration 49, loss = 0.53733805\n",
      "Iteration 50, loss = 0.53714156\n",
      "Iteration 51, loss = 0.53695187\n",
      "Iteration 52, loss = 0.53683525\n",
      "Iteration 53, loss = 0.53665006\n",
      "Iteration 54, loss = 0.53646166\n",
      "Iteration 55, loss = 0.53629350\n",
      "Iteration 56, loss = 0.53622440\n",
      "Iteration 57, loss = 0.53606593\n",
      "Iteration 58, loss = 0.53597834\n",
      "Iteration 59, loss = 0.53576194\n",
      "Iteration 60, loss = 0.53568975\n",
      "Iteration 61, loss = 0.53550842\n",
      "Iteration 62, loss = 0.53538772\n",
      "Iteration 63, loss = 0.53527138\n",
      "Iteration 64, loss = 0.53518571\n",
      "Iteration 65, loss = 0.53507433\n",
      "Iteration 66, loss = 0.53489067\n",
      "Iteration 67, loss = 0.53472785\n",
      "Iteration 68, loss = 0.53455575\n",
      "Iteration 69, loss = 0.53448001\n",
      "Iteration 70, loss = 0.53434450\n",
      "Iteration 71, loss = 0.53422306\n",
      "Iteration 72, loss = 0.53407624\n",
      "Iteration 73, loss = 0.53396602\n",
      "Iteration 74, loss = 0.53385167\n",
      "Iteration 75, loss = 0.53369462\n",
      "Iteration 76, loss = 0.53361385\n",
      "Iteration 77, loss = 0.53347846\n",
      "Iteration 78, loss = 0.53333488\n",
      "Iteration 79, loss = 0.53326970\n",
      "Iteration 80, loss = 0.53315286\n",
      "Iteration 81, loss = 0.53301817\n",
      "Iteration 82, loss = 0.53293122\n",
      "Iteration 83, loss = 0.53280273\n",
      "Iteration 84, loss = 0.53267214\n",
      "Iteration 85, loss = 0.53256733\n",
      "Iteration 86, loss = 0.53247163\n",
      "Iteration 87, loss = 0.53235332\n",
      "Iteration 88, loss = 0.53230935\n",
      "Iteration 89, loss = 0.53221954\n",
      "Iteration 90, loss = 0.53206138\n",
      "Iteration 91, loss = 0.53197442\n",
      "Iteration 92, loss = 0.53180330\n",
      "Iteration 93, loss = 0.53173598\n",
      "Iteration 94, loss = 0.53165124\n",
      "Iteration 95, loss = 0.53168150\n",
      "Iteration 96, loss = 0.53156916\n",
      "Iteration 97, loss = 0.53147666\n",
      "Iteration 98, loss = 0.53138673\n",
      "Iteration 99, loss = 0.53132841\n",
      "Iteration 100, loss = 0.53127848\n",
      "Iteration 101, loss = 0.53121372\n",
      "Iteration 102, loss = 0.53131292\n",
      "Iteration 103, loss = 0.53124481\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62188357\n",
      "Iteration 2, loss = 0.61347731\n",
      "Iteration 3, loss = 0.60579601\n",
      "Iteration 4, loss = 0.59885937\n",
      "Iteration 5, loss = 0.59238046\n",
      "Iteration 6, loss = 0.58648066\n",
      "Iteration 7, loss = 0.58113718\n",
      "Iteration 8, loss = 0.57608800\n",
      "Iteration 9, loss = 0.57165389\n",
      "Iteration 10, loss = 0.56734527\n",
      "Iteration 11, loss = 0.56346473\n",
      "Iteration 12, loss = 0.55990491\n",
      "Iteration 13, loss = 0.55646388\n",
      "Iteration 14, loss = 0.55330247\n",
      "Iteration 15, loss = 0.55057369\n",
      "Iteration 16, loss = 0.54770704\n",
      "Iteration 17, loss = 0.54512564\n",
      "Iteration 18, loss = 0.54290252\n",
      "Iteration 19, loss = 0.54074895\n",
      "Iteration 20, loss = 0.53880614\n",
      "Iteration 21, loss = 0.53699462\n",
      "Iteration 22, loss = 0.53554845\n",
      "Iteration 23, loss = 0.53425288\n",
      "Iteration 24, loss = 0.53296630\n",
      "Iteration 25, loss = 0.53189817\n",
      "Iteration 26, loss = 0.53089951\n",
      "Iteration 27, loss = 0.53005338\n",
      "Iteration 28, loss = 0.52919796\n",
      "Iteration 29, loss = 0.52831696\n",
      "Iteration 30, loss = 0.52771189\n",
      "Iteration 31, loss = 0.52698299\n",
      "Iteration 32, loss = 0.52649194\n",
      "Iteration 33, loss = 0.52584815\n",
      "Iteration 34, loss = 0.52528748\n",
      "Iteration 35, loss = 0.52482946\n",
      "Iteration 36, loss = 0.52434562\n",
      "Iteration 37, loss = 0.52381839\n",
      "Iteration 38, loss = 0.52337690\n",
      "Iteration 39, loss = 0.52315058\n",
      "Iteration 40, loss = 0.52264395\n",
      "Iteration 41, loss = 0.52239703\n",
      "Iteration 42, loss = 0.52204400\n",
      "Iteration 43, loss = 0.52178298\n",
      "Iteration 44, loss = 0.52152291\n",
      "Iteration 45, loss = 0.52122460\n",
      "Iteration 46, loss = 0.52093221\n",
      "Iteration 47, loss = 0.52078402\n",
      "Iteration 48, loss = 0.52055399\n",
      "Iteration 49, loss = 0.52026972\n",
      "Iteration 50, loss = 0.52018243\n",
      "Iteration 51, loss = 0.51986245\n",
      "Iteration 52, loss = 0.51968495\n",
      "Iteration 53, loss = 0.51954888\n",
      "Iteration 54, loss = 0.51940485\n",
      "Iteration 55, loss = 0.51933820\n",
      "Iteration 56, loss = 0.51914265\n",
      "Iteration 57, loss = 0.51895052\n",
      "Iteration 58, loss = 0.51878356\n",
      "Iteration 59, loss = 0.51867505\n",
      "Iteration 60, loss = 0.51855772\n",
      "Iteration 61, loss = 0.51846110\n",
      "Iteration 62, loss = 0.51836556\n",
      "Iteration 63, loss = 0.51831298\n",
      "Iteration 64, loss = 0.51817772\n",
      "Iteration 65, loss = 0.51797991\n",
      "Iteration 66, loss = 0.51786918\n",
      "Iteration 67, loss = 0.51766399\n",
      "Iteration 68, loss = 0.51757442\n",
      "Iteration 69, loss = 0.51741654\n",
      "Iteration 70, loss = 0.51743933\n",
      "Iteration 71, loss = 0.51726480\n",
      "Iteration 72, loss = 0.51718715\n",
      "Iteration 73, loss = 0.51711442\n",
      "Iteration 74, loss = 0.51700525\n",
      "Iteration 75, loss = 0.51684635\n",
      "Iteration 76, loss = 0.51668002\n",
      "Iteration 77, loss = 0.51653617\n",
      "Iteration 78, loss = 0.51631439\n",
      "Iteration 79, loss = 0.51621878\n",
      "Iteration 80, loss = 0.51602706\n",
      "Iteration 81, loss = 0.51590029\n",
      "Iteration 82, loss = 0.51575601\n",
      "Iteration 83, loss = 0.51580414\n",
      "Iteration 84, loss = 0.51564253\n",
      "Iteration 85, loss = 0.51552243\n",
      "Iteration 86, loss = 0.51541893\n",
      "Iteration 87, loss = 0.51530843\n",
      "Iteration 88, loss = 0.51515334\n",
      "Iteration 89, loss = 0.51501912\n",
      "Iteration 90, loss = 0.51489990\n",
      "Iteration 91, loss = 0.51488833\n",
      "Iteration 92, loss = 0.51486316\n",
      "Iteration 93, loss = 0.51468077\n",
      "Iteration 94, loss = 0.51457986\n",
      "Iteration 95, loss = 0.51458918\n",
      "Iteration 96, loss = 0.51441231\n",
      "Iteration 97, loss = 0.51429681\n",
      "Iteration 98, loss = 0.51426476\n",
      "Iteration 99, loss = 0.51413029\n",
      "Iteration 100, loss = 0.51399442\n",
      "Iteration 101, loss = 0.51395651\n",
      "Iteration 102, loss = 0.51390652\n",
      "Iteration 103, loss = 0.51387848\n",
      "Iteration 104, loss = 0.51379513\n",
      "Iteration 105, loss = 0.51372416\n",
      "Iteration 106, loss = 0.51362503\n",
      "Iteration 107, loss = 0.51351759\n",
      "Iteration 108, loss = 0.51344854\n",
      "Iteration 109, loss = 0.51326763\n",
      "Iteration 110, loss = 0.51318806\n",
      "Iteration 111, loss = 0.51298378\n",
      "Iteration 112, loss = 0.51293912\n",
      "Iteration 113, loss = 0.51281009\n",
      "Iteration 114, loss = 0.51274340\n",
      "Iteration 115, loss = 0.51263671\n",
      "Iteration 116, loss = 0.51255212\n",
      "Iteration 117, loss = 0.51249236\n",
      "Iteration 118, loss = 0.51235436\n",
      "Iteration 119, loss = 0.51228679\n",
      "Iteration 120, loss = 0.51223793\n",
      "Iteration 121, loss = 0.51215174\n",
      "Iteration 122, loss = 0.51211002\n",
      "Iteration 123, loss = 0.51204968\n",
      "Iteration 124, loss = 0.51193905\n",
      "Iteration 125, loss = 0.51189875\n",
      "Iteration 126, loss = 0.51183534\n",
      "Iteration 127, loss = 0.51175019\n",
      "Iteration 128, loss = 0.51175358\n",
      "Iteration 129, loss = 0.51173214\n",
      "Iteration 130, loss = 0.51166437\n",
      "Iteration 131, loss = 0.51164348\n",
      "Iteration 132, loss = 0.51153710\n",
      "Iteration 133, loss = 0.51159881\n",
      "Iteration 134, loss = 0.51148085\n",
      "Iteration 135, loss = 0.51135418\n",
      "Iteration 136, loss = 0.51125849\n",
      "Iteration 137, loss = 0.51119479\n",
      "Iteration 138, loss = 0.51105361\n",
      "Iteration 139, loss = 0.51104556\n",
      "Iteration 140, loss = 0.51092015\n",
      "Iteration 141, loss = 0.51081721\n",
      "Iteration 142, loss = 0.51069557\n",
      "Iteration 143, loss = 0.51071402\n",
      "Iteration 144, loss = 0.51058027\n",
      "Iteration 145, loss = 0.51038246\n",
      "Iteration 146, loss = 0.51029487\n",
      "Iteration 147, loss = 0.51042216\n",
      "Iteration 148, loss = 0.51022406\n",
      "Iteration 149, loss = 0.51022999\n",
      "Iteration 150, loss = 0.51016638\n",
      "Iteration 151, loss = 0.51010996\n",
      "Iteration 152, loss = 0.51000735\n",
      "Iteration 153, loss = 0.50991352\n",
      "Iteration 154, loss = 0.50981530\n",
      "Iteration 155, loss = 0.50984714\n",
      "Iteration 156, loss = 0.50981010\n",
      "Iteration 157, loss = 0.50974164\n",
      "Iteration 158, loss = 0.50969908\n",
      "Iteration 159, loss = 0.50967942\n",
      "Iteration 160, loss = 0.50959178\n",
      "Iteration 161, loss = 0.50953162\n",
      "Iteration 162, loss = 0.50968105\n",
      "Iteration 163, loss = 0.50965481\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66719284\n",
      "Iteration 2, loss = 0.65906936\n",
      "Iteration 3, loss = 0.65130795\n",
      "Iteration 4, loss = 0.64386491\n",
      "Iteration 5, loss = 0.63735701\n",
      "Iteration 6, loss = 0.63066242\n",
      "Iteration 7, loss = 0.62498462\n",
      "Iteration 8, loss = 0.61894413\n",
      "Iteration 9, loss = 0.61399299\n",
      "Iteration 10, loss = 0.60864452\n",
      "Iteration 11, loss = 0.60381403\n",
      "Iteration 12, loss = 0.59951137\n",
      "Iteration 13, loss = 0.59525885\n",
      "Iteration 14, loss = 0.59110098\n",
      "Iteration 15, loss = 0.58776421\n",
      "Iteration 16, loss = 0.58427896\n",
      "Iteration 17, loss = 0.58090828\n",
      "Iteration 18, loss = 0.57805960\n",
      "Iteration 19, loss = 0.57558911\n",
      "Iteration 20, loss = 0.57302309\n",
      "Iteration 21, loss = 0.57071617\n",
      "Iteration 22, loss = 0.56858162\n",
      "Iteration 23, loss = 0.56664575\n",
      "Iteration 24, loss = 0.56510298\n",
      "Iteration 25, loss = 0.56339201\n",
      "Iteration 26, loss = 0.56204770\n",
      "Iteration 27, loss = 0.56052260\n",
      "Iteration 28, loss = 0.55930084\n",
      "Iteration 29, loss = 0.55840314\n",
      "Iteration 30, loss = 0.55737015\n",
      "Iteration 31, loss = 0.55628892\n",
      "Iteration 32, loss = 0.55551579\n",
      "Iteration 33, loss = 0.55490009\n",
      "Iteration 34, loss = 0.55426785\n",
      "Iteration 35, loss = 0.55374458\n",
      "Iteration 36, loss = 0.55339687\n",
      "Iteration 37, loss = 0.55297976\n",
      "Iteration 38, loss = 0.55251123\n",
      "Iteration 39, loss = 0.55221379\n",
      "Iteration 40, loss = 0.55191523\n",
      "Iteration 41, loss = 0.55142683\n",
      "Iteration 42, loss = 0.55120250\n",
      "Iteration 43, loss = 0.55081768\n",
      "Iteration 44, loss = 0.55051297\n",
      "Iteration 45, loss = 0.55014846\n",
      "Iteration 46, loss = 0.54993483\n",
      "Iteration 47, loss = 0.54965698\n",
      "Iteration 48, loss = 0.54939638\n",
      "Iteration 49, loss = 0.54912499\n",
      "Iteration 50, loss = 0.54884005\n",
      "Iteration 51, loss = 0.54874241\n",
      "Iteration 52, loss = 0.54851710\n",
      "Iteration 53, loss = 0.54832493\n",
      "Iteration 54, loss = 0.54823325\n",
      "Iteration 55, loss = 0.54803728\n",
      "Iteration 56, loss = 0.54794384\n",
      "Iteration 57, loss = 0.54787723\n",
      "Iteration 58, loss = 0.54772896\n",
      "Iteration 59, loss = 0.54763765\n",
      "Iteration 60, loss = 0.54751722\n",
      "Iteration 61, loss = 0.54738660\n",
      "Iteration 62, loss = 0.54737072\n",
      "Iteration 63, loss = 0.54715331\n",
      "Iteration 64, loss = 0.54707945\n",
      "Iteration 65, loss = 0.54701182\n",
      "Iteration 66, loss = 0.54696524\n",
      "Iteration 67, loss = 0.54686074\n",
      "Iteration 68, loss = 0.54678924\n",
      "Iteration 69, loss = 0.54668352\n",
      "Iteration 70, loss = 0.54660106\n",
      "Iteration 71, loss = 0.54651691\n",
      "Iteration 72, loss = 0.54653995\n",
      "Iteration 73, loss = 0.54641814\n",
      "Iteration 74, loss = 0.54636918\n",
      "Iteration 75, loss = 0.54632314\n",
      "Iteration 76, loss = 0.54618264\n",
      "Iteration 77, loss = 0.54613869\n",
      "Iteration 78, loss = 0.54609108\n",
      "Iteration 79, loss = 0.54602064\n",
      "Iteration 80, loss = 0.54601394\n",
      "Iteration 81, loss = 0.54593778\n",
      "Iteration 82, loss = 0.54593359\n",
      "Iteration 83, loss = 0.54586898\n",
      "Iteration 84, loss = 0.54576291\n",
      "Iteration 85, loss = 0.54571050\n",
      "Iteration 86, loss = 0.54564063\n",
      "Iteration 87, loss = 0.54554480\n",
      "Iteration 88, loss = 0.54550336\n",
      "Iteration 89, loss = 0.54538256\n",
      "Iteration 90, loss = 0.54541043\n",
      "Iteration 91, loss = 0.54516388\n",
      "Iteration 92, loss = 0.54511643\n",
      "Iteration 93, loss = 0.54502375\n",
      "Iteration 94, loss = 0.54495058\n",
      "Iteration 95, loss = 0.54493680\n",
      "Iteration 96, loss = 0.54478652\n",
      "Iteration 97, loss = 0.54472232\n",
      "Iteration 98, loss = 0.54458859\n",
      "Iteration 99, loss = 0.54447168\n",
      "Iteration 100, loss = 0.54440584\n",
      "Iteration 101, loss = 0.54427513\n",
      "Iteration 102, loss = 0.54424182\n",
      "Iteration 103, loss = 0.54411009\n",
      "Iteration 104, loss = 0.54404315\n",
      "Iteration 105, loss = 0.54393350\n",
      "Iteration 106, loss = 0.54390378\n",
      "Iteration 107, loss = 0.54387099\n",
      "Iteration 108, loss = 0.54374755\n",
      "Iteration 109, loss = 0.54372594\n",
      "Iteration 110, loss = 0.54362909\n",
      "Iteration 111, loss = 0.54353284\n",
      "Iteration 112, loss = 0.54339574\n",
      "Iteration 113, loss = 0.54324318\n",
      "Iteration 114, loss = 0.54314414\n",
      "Iteration 115, loss = 0.54302267\n",
      "Iteration 116, loss = 0.54297891\n",
      "Iteration 117, loss = 0.54283957\n",
      "Iteration 118, loss = 0.54277358\n",
      "Iteration 119, loss = 0.54262997\n",
      "Iteration 120, loss = 0.54258966\n",
      "Iteration 121, loss = 0.54242938\n",
      "Iteration 122, loss = 0.54232772\n",
      "Iteration 123, loss = 0.54241113\n",
      "Iteration 124, loss = 0.54229759\n",
      "Iteration 125, loss = 0.54238351\n",
      "Iteration 126, loss = 0.54236765\n",
      "Iteration 127, loss = 0.54231553\n",
      "Iteration 128, loss = 0.54224240\n",
      "Iteration 129, loss = 0.54221323\n",
      "Iteration 130, loss = 0.54211383\n",
      "Iteration 131, loss = 0.54200303\n",
      "Iteration 132, loss = 0.54191335\n",
      "Iteration 133, loss = 0.54181722\n",
      "Iteration 134, loss = 0.54166722\n",
      "Iteration 135, loss = 0.54159523\n",
      "Iteration 136, loss = 0.54147076\n",
      "Iteration 137, loss = 0.54138358\n",
      "Iteration 138, loss = 0.54128683\n",
      "Iteration 139, loss = 0.54120617\n",
      "Iteration 140, loss = 0.54119811\n",
      "Iteration 141, loss = 0.54108698\n",
      "Iteration 142, loss = 0.54109576\n",
      "Iteration 143, loss = 0.54102467\n",
      "Iteration 144, loss = 0.54101878\n",
      "Iteration 145, loss = 0.54102092\n",
      "Iteration 146, loss = 0.54095322\n",
      "Iteration 147, loss = 0.54091677\n",
      "Iteration 148, loss = 0.54083606\n",
      "Iteration 149, loss = 0.54077117\n",
      "Iteration 150, loss = 0.54071212\n",
      "Iteration 151, loss = 0.54067242\n",
      "Iteration 152, loss = 0.54063053\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74016698\n",
      "Iteration 2, loss = 0.72631017\n",
      "Iteration 3, loss = 0.71373537\n",
      "Iteration 4, loss = 0.70155945\n",
      "Iteration 5, loss = 0.68979452\n",
      "Iteration 6, loss = 0.67881701\n",
      "Iteration 7, loss = 0.66810783\n",
      "Iteration 8, loss = 0.65876181\n",
      "Iteration 9, loss = 0.64975426\n",
      "Iteration 10, loss = 0.64127343\n",
      "Iteration 11, loss = 0.63311887\n",
      "Iteration 12, loss = 0.62634053\n",
      "Iteration 13, loss = 0.61936863\n",
      "Iteration 14, loss = 0.61280892\n",
      "Iteration 15, loss = 0.60742332\n",
      "Iteration 16, loss = 0.60200086\n",
      "Iteration 17, loss = 0.59710076\n",
      "Iteration 18, loss = 0.59264384\n",
      "Iteration 19, loss = 0.58842371\n",
      "Iteration 20, loss = 0.58456117\n",
      "Iteration 21, loss = 0.58127926\n",
      "Iteration 22, loss = 0.57816043\n",
      "Iteration 23, loss = 0.57513152\n",
      "Iteration 24, loss = 0.57274043\n",
      "Iteration 25, loss = 0.57048017\n",
      "Iteration 26, loss = 0.56824804\n",
      "Iteration 27, loss = 0.56642397\n",
      "Iteration 28, loss = 0.56468136\n",
      "Iteration 29, loss = 0.56326783\n",
      "Iteration 30, loss = 0.56202876\n",
      "Iteration 31, loss = 0.56060617\n",
      "Iteration 32, loss = 0.55953606\n",
      "Iteration 33, loss = 0.55874817\n",
      "Iteration 34, loss = 0.55786569\n",
      "Iteration 35, loss = 0.55707231\n",
      "Iteration 36, loss = 0.55643238\n",
      "Iteration 37, loss = 0.55603201\n",
      "Iteration 38, loss = 0.55533941\n",
      "Iteration 39, loss = 0.55503765\n",
      "Iteration 40, loss = 0.55462209\n",
      "Iteration 41, loss = 0.55431392\n",
      "Iteration 42, loss = 0.55403920\n",
      "Iteration 43, loss = 0.55370901\n",
      "Iteration 44, loss = 0.55353472\n",
      "Iteration 45, loss = 0.55333035\n",
      "Iteration 46, loss = 0.55313191\n",
      "Iteration 47, loss = 0.55293436\n",
      "Iteration 48, loss = 0.55277902\n",
      "Iteration 49, loss = 0.55255609\n",
      "Iteration 50, loss = 0.55242554\n",
      "Iteration 51, loss = 0.55229855\n",
      "Iteration 52, loss = 0.55209538\n",
      "Iteration 53, loss = 0.55193221\n",
      "Iteration 54, loss = 0.55181646\n",
      "Iteration 55, loss = 0.55167334\n",
      "Iteration 56, loss = 0.55154740\n",
      "Iteration 57, loss = 0.55143998\n",
      "Iteration 58, loss = 0.55139367\n",
      "Iteration 59, loss = 0.55118655\n",
      "Iteration 60, loss = 0.55116156\n",
      "Iteration 61, loss = 0.55108469\n",
      "Iteration 62, loss = 0.55103942\n",
      "Iteration 63, loss = 0.55094431\n",
      "Iteration 64, loss = 0.55085878\n",
      "Iteration 65, loss = 0.55081333\n",
      "Iteration 66, loss = 0.55072274\n",
      "Iteration 67, loss = 0.55070218\n",
      "Iteration 68, loss = 0.55060178\n",
      "Iteration 69, loss = 0.55053250\n",
      "Iteration 70, loss = 0.55045226\n",
      "Iteration 71, loss = 0.55039904\n",
      "Iteration 72, loss = 0.55030796\n",
      "Iteration 73, loss = 0.55031989\n",
      "Iteration 74, loss = 0.55024771\n",
      "Iteration 75, loss = 0.55022903\n",
      "Iteration 76, loss = 0.55018857\n",
      "Iteration 77, loss = 0.55014335\n",
      "Iteration 78, loss = 0.55007909\n",
      "Iteration 79, loss = 0.55002244\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.57333202\n",
      "Iteration 2, loss = 0.56845362\n",
      "Iteration 3, loss = 0.56470747\n",
      "Iteration 4, loss = 0.56124183\n",
      "Iteration 5, loss = 0.55793864\n",
      "Iteration 6, loss = 0.55512551\n",
      "Iteration 7, loss = 0.55245976\n",
      "Iteration 8, loss = 0.55011729\n",
      "Iteration 9, loss = 0.54791570\n",
      "Iteration 10, loss = 0.54588508\n",
      "Iteration 11, loss = 0.54426007\n",
      "Iteration 12, loss = 0.54223043\n",
      "Iteration 13, loss = 0.54061677\n",
      "Iteration 14, loss = 0.53924585\n",
      "Iteration 15, loss = 0.53757836\n",
      "Iteration 16, loss = 0.53631086\n",
      "Iteration 17, loss = 0.53525740\n",
      "Iteration 18, loss = 0.53410673\n",
      "Iteration 19, loss = 0.53338257\n",
      "Iteration 20, loss = 0.53256553\n",
      "Iteration 21, loss = 0.53171047\n",
      "Iteration 22, loss = 0.53120451\n",
      "Iteration 23, loss = 0.53075646\n",
      "Iteration 24, loss = 0.53045160\n",
      "Iteration 25, loss = 0.52990601\n",
      "Iteration 26, loss = 0.52964250\n",
      "Iteration 27, loss = 0.52933245\n",
      "Iteration 28, loss = 0.52908847\n",
      "Iteration 29, loss = 0.52887530\n",
      "Iteration 30, loss = 0.52865418\n",
      "Iteration 31, loss = 0.52842623\n",
      "Iteration 32, loss = 0.52818527\n",
      "Iteration 33, loss = 0.52793070\n",
      "Iteration 34, loss = 0.52790478\n",
      "Iteration 35, loss = 0.52753123\n",
      "Iteration 36, loss = 0.52735279\n",
      "Iteration 37, loss = 0.52717359\n",
      "Iteration 38, loss = 0.52703972\n",
      "Iteration 39, loss = 0.52687276\n",
      "Iteration 40, loss = 0.52672865\n",
      "Iteration 41, loss = 0.52660407\n",
      "Iteration 42, loss = 0.52643320\n",
      "Iteration 43, loss = 0.52624211\n",
      "Iteration 44, loss = 0.52620065\n",
      "Iteration 45, loss = 0.52604145\n",
      "Iteration 46, loss = 0.52591867\n",
      "Iteration 47, loss = 0.52579010\n",
      "Iteration 48, loss = 0.52561520\n",
      "Iteration 49, loss = 0.52546241\n",
      "Iteration 50, loss = 0.52536562\n",
      "Iteration 51, loss = 0.52520049\n",
      "Iteration 52, loss = 0.52505181\n",
      "Iteration 53, loss = 0.52494352\n",
      "Iteration 54, loss = 0.52489809\n",
      "Iteration 55, loss = 0.52480670\n",
      "Iteration 56, loss = 0.52463332\n",
      "Iteration 57, loss = 0.52456715\n",
      "Iteration 58, loss = 0.52445060\n",
      "Iteration 59, loss = 0.52440454\n",
      "Iteration 60, loss = 0.52433643\n",
      "Iteration 61, loss = 0.52434309\n",
      "Iteration 62, loss = 0.52425180\n",
      "Iteration 63, loss = 0.52418289\n",
      "Iteration 64, loss = 0.52417744\n",
      "Iteration 65, loss = 0.52406429\n",
      "Iteration 66, loss = 0.52394595\n",
      "Iteration 67, loss = 0.52384691\n",
      "Iteration 68, loss = 0.52372935\n",
      "Iteration 69, loss = 0.52359188\n",
      "Iteration 70, loss = 0.52357436\n",
      "Iteration 71, loss = 0.52345764\n",
      "Iteration 72, loss = 0.52333925\n",
      "Iteration 73, loss = 0.52322844\n",
      "Iteration 74, loss = 0.52308475\n",
      "Iteration 75, loss = 0.52291345\n",
      "Iteration 76, loss = 0.52286726\n",
      "Iteration 77, loss = 0.52282474\n",
      "Iteration 78, loss = 0.52286301\n",
      "Iteration 79, loss = 0.52277494\n",
      "Iteration 80, loss = 0.52277194\n",
      "Iteration 81, loss = 0.52268900\n",
      "Iteration 82, loss = 0.52260276\n",
      "Iteration 83, loss = 0.52250605\n",
      "Iteration 84, loss = 0.52239017\n",
      "Iteration 85, loss = 0.52225771\n",
      "Iteration 86, loss = 0.52218292\n",
      "Iteration 87, loss = 0.52203595\n",
      "Iteration 88, loss = 0.52198571\n",
      "Iteration 89, loss = 0.52195635\n",
      "Iteration 90, loss = 0.52197169\n",
      "Iteration 91, loss = 0.52187507\n",
      "Iteration 92, loss = 0.52177172\n",
      "Iteration 93, loss = 0.52175215\n",
      "Iteration 94, loss = 0.52177989\n",
      "Iteration 95, loss = 0.52171481\n",
      "Iteration 96, loss = 0.52166228\n",
      "Iteration 97, loss = 0.52153392\n",
      "Iteration 98, loss = 0.52139975\n",
      "Iteration 99, loss = 0.52127517\n",
      "Iteration 100, loss = 0.52136250\n",
      "Iteration 101, loss = 0.52116113\n",
      "Iteration 102, loss = 0.52104859\n",
      "Iteration 103, loss = 0.52114490\n",
      "Iteration 104, loss = 0.52103475\n",
      "Iteration 105, loss = 0.52099631\n",
      "Iteration 106, loss = 0.52095421\n",
      "Iteration 107, loss = 0.52091182\n",
      "Iteration 108, loss = 0.52091771\n",
      "Iteration 109, loss = 0.52095574\n",
      "Iteration 110, loss = 0.52099605\n",
      "Iteration 111, loss = 0.52109929\n",
      "Iteration 112, loss = 0.52111465\n",
      "Iteration 113, loss = 0.52112212\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73705016\n",
      "Iteration 2, loss = 0.72396100\n",
      "Iteration 3, loss = 0.71159677\n",
      "Iteration 4, loss = 0.69936847\n",
      "Iteration 5, loss = 0.68870956\n",
      "Iteration 6, loss = 0.67817676\n",
      "Iteration 7, loss = 0.66771793\n",
      "Iteration 8, loss = 0.65871351\n",
      "Iteration 9, loss = 0.65015436\n",
      "Iteration 10, loss = 0.64172093\n",
      "Iteration 11, loss = 0.63412367\n",
      "Iteration 12, loss = 0.62660151\n",
      "Iteration 13, loss = 0.62019217\n",
      "Iteration 14, loss = 0.61380611\n",
      "Iteration 15, loss = 0.60813101\n",
      "Iteration 16, loss = 0.60268299\n",
      "Iteration 17, loss = 0.59766609\n",
      "Iteration 18, loss = 0.59308160\n",
      "Iteration 19, loss = 0.58878149\n",
      "Iteration 20, loss = 0.58456934\n",
      "Iteration 21, loss = 0.58084106\n",
      "Iteration 22, loss = 0.57762299\n",
      "Iteration 23, loss = 0.57431132\n",
      "Iteration 24, loss = 0.57122105\n",
      "Iteration 25, loss = 0.56843720\n",
      "Iteration 26, loss = 0.56614857\n",
      "Iteration 27, loss = 0.56340532\n",
      "Iteration 28, loss = 0.56121050\n",
      "Iteration 29, loss = 0.55914864\n",
      "Iteration 30, loss = 0.55717870\n",
      "Iteration 31, loss = 0.55536460\n",
      "Iteration 32, loss = 0.55365183\n",
      "Iteration 33, loss = 0.55194181\n",
      "Iteration 34, loss = 0.55062361\n",
      "Iteration 35, loss = 0.54922641\n",
      "Iteration 36, loss = 0.54795618\n",
      "Iteration 37, loss = 0.54673685\n",
      "Iteration 38, loss = 0.54559109\n",
      "Iteration 39, loss = 0.54477278\n",
      "Iteration 40, loss = 0.54359535\n",
      "Iteration 41, loss = 0.54288040\n",
      "Iteration 42, loss = 0.54186550\n",
      "Iteration 43, loss = 0.54139463\n",
      "Iteration 44, loss = 0.54063017\n",
      "Iteration 45, loss = 0.53998394\n",
      "Iteration 46, loss = 0.53932585\n",
      "Iteration 47, loss = 0.53874473\n",
      "Iteration 48, loss = 0.53820823\n",
      "Iteration 49, loss = 0.53753128\n",
      "Iteration 50, loss = 0.53717977\n",
      "Iteration 51, loss = 0.53657943\n",
      "Iteration 52, loss = 0.53627033\n",
      "Iteration 53, loss = 0.53577670\n",
      "Iteration 54, loss = 0.53566445\n",
      "Iteration 55, loss = 0.53523037\n",
      "Iteration 56, loss = 0.53510543\n",
      "Iteration 57, loss = 0.53480716\n",
      "Iteration 58, loss = 0.53463675\n",
      "Iteration 59, loss = 0.53440128\n",
      "Iteration 60, loss = 0.53423793\n",
      "Iteration 61, loss = 0.53409876\n",
      "Iteration 62, loss = 0.53396598\n",
      "Iteration 63, loss = 0.53380176\n",
      "Iteration 64, loss = 0.53369730\n",
      "Iteration 65, loss = 0.53356461\n",
      "Iteration 66, loss = 0.53350169\n",
      "Iteration 67, loss = 0.53335604\n",
      "Iteration 68, loss = 0.53326312\n",
      "Iteration 69, loss = 0.53317308\n",
      "Iteration 70, loss = 0.53308866\n",
      "Iteration 71, loss = 0.53300857\n",
      "Iteration 72, loss = 0.53289819\n",
      "Iteration 73, loss = 0.53288602\n",
      "Iteration 74, loss = 0.53276748\n",
      "Iteration 75, loss = 0.53267805\n",
      "Iteration 76, loss = 0.53259385\n",
      "Iteration 77, loss = 0.53248244\n",
      "Iteration 78, loss = 0.53239576\n",
      "Iteration 79, loss = 0.53222917\n",
      "Iteration 80, loss = 0.53213716\n",
      "Iteration 81, loss = 0.53208822\n",
      "Iteration 82, loss = 0.53198349\n",
      "Iteration 83, loss = 0.53190068\n",
      "Iteration 84, loss = 0.53178799\n",
      "Iteration 85, loss = 0.53172223\n",
      "Iteration 86, loss = 0.53159847\n",
      "Iteration 87, loss = 0.53150339\n",
      "Iteration 88, loss = 0.53147726\n",
      "Iteration 89, loss = 0.53137263\n",
      "Iteration 90, loss = 0.53129963\n",
      "Iteration 91, loss = 0.53123154\n",
      "Iteration 92, loss = 0.53117212\n",
      "Iteration 93, loss = 0.53108771\n",
      "Iteration 94, loss = 0.53102169\n",
      "Iteration 95, loss = 0.53098051\n",
      "Iteration 96, loss = 0.53094344\n",
      "Iteration 97, loss = 0.53098659\n",
      "Iteration 98, loss = 0.53101290\n",
      "Iteration 99, loss = 0.53098002\n",
      "Iteration 100, loss = 0.53097710\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.62025083\n",
      "Iteration 2, loss = 0.61352672\n",
      "Iteration 3, loss = 0.60704379\n",
      "Iteration 4, loss = 0.60089580\n",
      "Iteration 5, loss = 0.59506501\n",
      "Iteration 6, loss = 0.58928513\n",
      "Iteration 7, loss = 0.58401688\n",
      "Iteration 8, loss = 0.57902648\n",
      "Iteration 9, loss = 0.57432531\n",
      "Iteration 10, loss = 0.57001860\n",
      "Iteration 11, loss = 0.56592453\n",
      "Iteration 12, loss = 0.56240125\n",
      "Iteration 13, loss = 0.55899576\n",
      "Iteration 14, loss = 0.55620818\n",
      "Iteration 15, loss = 0.55310273\n",
      "Iteration 16, loss = 0.55065697\n",
      "Iteration 17, loss = 0.54845857\n",
      "Iteration 18, loss = 0.54625916\n",
      "Iteration 19, loss = 0.54426455\n",
      "Iteration 20, loss = 0.54253182\n",
      "Iteration 21, loss = 0.54100311\n",
      "Iteration 22, loss = 0.53991128\n",
      "Iteration 23, loss = 0.53849818\n",
      "Iteration 24, loss = 0.53754647\n",
      "Iteration 25, loss = 0.53655059\n",
      "Iteration 26, loss = 0.53567807\n",
      "Iteration 27, loss = 0.53495626\n",
      "Iteration 28, loss = 0.53406081\n",
      "Iteration 29, loss = 0.53346503\n",
      "Iteration 30, loss = 0.53276050\n",
      "Iteration 31, loss = 0.53216477\n",
      "Iteration 32, loss = 0.53151369\n",
      "Iteration 33, loss = 0.53103260\n",
      "Iteration 34, loss = 0.53053532\n",
      "Iteration 35, loss = 0.53008053\n",
      "Iteration 36, loss = 0.52971198\n",
      "Iteration 37, loss = 0.52948831\n",
      "Iteration 38, loss = 0.52917731\n",
      "Iteration 39, loss = 0.52886683\n",
      "Iteration 40, loss = 0.52859916\n",
      "Iteration 41, loss = 0.52839919\n",
      "Iteration 42, loss = 0.52822270\n",
      "Iteration 43, loss = 0.52800113\n",
      "Iteration 44, loss = 0.52793927\n",
      "Iteration 45, loss = 0.52761375\n",
      "Iteration 46, loss = 0.52742685\n",
      "Iteration 47, loss = 0.52734365\n",
      "Iteration 48, loss = 0.52713872\n",
      "Iteration 49, loss = 0.52694493\n",
      "Iteration 50, loss = 0.52681864\n",
      "Iteration 51, loss = 0.52669666\n",
      "Iteration 52, loss = 0.52650527\n",
      "Iteration 53, loss = 0.52636996\n",
      "Iteration 54, loss = 0.52646797\n",
      "Iteration 55, loss = 0.52622519\n",
      "Iteration 56, loss = 0.52612547\n",
      "Iteration 57, loss = 0.52602884\n",
      "Iteration 58, loss = 0.52601393\n",
      "Iteration 59, loss = 0.52593311\n",
      "Iteration 60, loss = 0.52581894\n",
      "Iteration 61, loss = 0.52572448\n",
      "Iteration 62, loss = 0.52567884\n",
      "Iteration 63, loss = 0.52549154\n",
      "Iteration 64, loss = 0.52545989\n",
      "Iteration 65, loss = 0.52528819\n",
      "Iteration 66, loss = 0.52511937\n",
      "Iteration 67, loss = 0.52507339\n",
      "Iteration 68, loss = 0.52500478\n",
      "Iteration 69, loss = 0.52492094\n",
      "Iteration 70, loss = 0.52491182\n",
      "Iteration 71, loss = 0.52479657\n",
      "Iteration 72, loss = 0.52472283\n",
      "Iteration 73, loss = 0.52462717\n",
      "Iteration 74, loss = 0.52449352\n",
      "Iteration 75, loss = 0.52440449\n",
      "Iteration 76, loss = 0.52434677\n",
      "Iteration 77, loss = 0.52417575\n",
      "Iteration 78, loss = 0.52412474\n",
      "Iteration 79, loss = 0.52404672\n",
      "Iteration 80, loss = 0.52407854\n",
      "Iteration 81, loss = 0.52396961\n",
      "Iteration 82, loss = 0.52402759\n",
      "Iteration 83, loss = 0.52401774\n",
      "Iteration 84, loss = 0.52392515\n",
      "Iteration 85, loss = 0.52380950\n",
      "Iteration 86, loss = 0.52367644\n",
      "Iteration 87, loss = 0.52352215\n",
      "Iteration 88, loss = 0.52356715\n",
      "Iteration 89, loss = 0.52338374\n",
      "Iteration 90, loss = 0.52330033\n",
      "Iteration 91, loss = 0.52326839\n",
      "Iteration 92, loss = 0.52316316\n",
      "Iteration 93, loss = 0.52306264\n",
      "Iteration 94, loss = 0.52303571\n",
      "Iteration 95, loss = 0.52292407\n",
      "Iteration 96, loss = 0.52283214\n",
      "Iteration 97, loss = 0.52280536\n",
      "Iteration 98, loss = 0.52270685\n",
      "Iteration 99, loss = 0.52267708\n",
      "Iteration 100, loss = 0.52255717\n",
      "Iteration 101, loss = 0.52248944\n",
      "Iteration 102, loss = 0.52240083\n",
      "Iteration 103, loss = 0.52234192\n",
      "Iteration 104, loss = 0.52235938\n",
      "Iteration 105, loss = 0.52229059\n",
      "Iteration 106, loss = 0.52221238\n",
      "Iteration 107, loss = 0.52218208\n",
      "Iteration 108, loss = 0.52208058\n",
      "Iteration 109, loss = 0.52203224\n",
      "Iteration 110, loss = 0.52193225\n",
      "Iteration 111, loss = 0.52196895\n",
      "Iteration 112, loss = 0.52200492\n",
      "Iteration 113, loss = 0.52195191\n",
      "Iteration 114, loss = 0.52182113\n",
      "Iteration 115, loss = 0.52178187\n",
      "Iteration 116, loss = 0.52176324\n",
      "Iteration 117, loss = 0.52162101\n",
      "Iteration 118, loss = 0.52155208\n",
      "Iteration 119, loss = 0.52149342\n",
      "Iteration 120, loss = 0.52140806\n",
      "Iteration 121, loss = 0.52137173\n",
      "Iteration 122, loss = 0.52130151\n",
      "Iteration 123, loss = 0.52124236\n",
      "Iteration 124, loss = 0.52117236\n",
      "Iteration 125, loss = 0.52103198\n",
      "Iteration 126, loss = 0.52105085\n",
      "Iteration 127, loss = 0.52097883\n",
      "Iteration 128, loss = 0.52097824\n",
      "Iteration 129, loss = 0.52094541\n",
      "Iteration 130, loss = 0.52085684\n",
      "Iteration 131, loss = 0.52076743\n",
      "Iteration 132, loss = 0.52080631\n",
      "Iteration 133, loss = 0.52071155\n",
      "Iteration 134, loss = 0.52064299\n",
      "Iteration 135, loss = 0.52063379\n",
      "Iteration 136, loss = 0.52062962\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67460667\n",
      "Iteration 2, loss = 0.66575751\n",
      "Iteration 3, loss = 0.65763729\n",
      "Iteration 4, loss = 0.64965315\n",
      "Iteration 5, loss = 0.64207892\n",
      "Iteration 6, loss = 0.63488504\n",
      "Iteration 7, loss = 0.62820184\n",
      "Iteration 8, loss = 0.62137742\n",
      "Iteration 9, loss = 0.61534759\n",
      "Iteration 10, loss = 0.60916780\n",
      "Iteration 11, loss = 0.60326946\n",
      "Iteration 12, loss = 0.59795712\n",
      "Iteration 13, loss = 0.59271161\n",
      "Iteration 14, loss = 0.58770270\n",
      "Iteration 15, loss = 0.58309493\n",
      "Iteration 16, loss = 0.57843853\n",
      "Iteration 17, loss = 0.57418948\n",
      "Iteration 18, loss = 0.57030840\n",
      "Iteration 19, loss = 0.56662661\n",
      "Iteration 20, loss = 0.56314326\n",
      "Iteration 21, loss = 0.55961421\n",
      "Iteration 22, loss = 0.55672327\n",
      "Iteration 23, loss = 0.55410327\n",
      "Iteration 24, loss = 0.55125511\n",
      "Iteration 25, loss = 0.54905652\n",
      "Iteration 26, loss = 0.54670297\n",
      "Iteration 27, loss = 0.54470351\n",
      "Iteration 28, loss = 0.54291533\n",
      "Iteration 29, loss = 0.54121735\n",
      "Iteration 30, loss = 0.53975546\n",
      "Iteration 31, loss = 0.53851025\n",
      "Iteration 32, loss = 0.53724068\n",
      "Iteration 33, loss = 0.53611739\n",
      "Iteration 34, loss = 0.53501399\n",
      "Iteration 35, loss = 0.53421376\n",
      "Iteration 36, loss = 0.53339199\n",
      "Iteration 37, loss = 0.53264394\n",
      "Iteration 38, loss = 0.53193815\n",
      "Iteration 39, loss = 0.53138348\n",
      "Iteration 40, loss = 0.53080093\n",
      "Iteration 41, loss = 0.53030037\n",
      "Iteration 42, loss = 0.52986183\n",
      "Iteration 43, loss = 0.52939241\n",
      "Iteration 44, loss = 0.52904624\n",
      "Iteration 45, loss = 0.52859391\n",
      "Iteration 46, loss = 0.52828433\n",
      "Iteration 47, loss = 0.52813186\n",
      "Iteration 48, loss = 0.52771154\n",
      "Iteration 49, loss = 0.52752376\n",
      "Iteration 50, loss = 0.52724158\n",
      "Iteration 51, loss = 0.52709173\n",
      "Iteration 52, loss = 0.52684574\n",
      "Iteration 53, loss = 0.52668286\n",
      "Iteration 54, loss = 0.52654740\n",
      "Iteration 55, loss = 0.52646333\n",
      "Iteration 56, loss = 0.52637594\n",
      "Iteration 57, loss = 0.52627967\n",
      "Iteration 58, loss = 0.52621702\n",
      "Iteration 59, loss = 0.52611593\n",
      "Iteration 60, loss = 0.52605007\n",
      "Iteration 61, loss = 0.52598236\n",
      "Iteration 62, loss = 0.52586540\n",
      "Iteration 63, loss = 0.52579959\n",
      "Iteration 64, loss = 0.52567117\n",
      "Iteration 65, loss = 0.52566435\n",
      "Iteration 66, loss = 0.52564480\n",
      "Iteration 67, loss = 0.52562984\n",
      "Iteration 68, loss = 0.52555775\n",
      "Iteration 69, loss = 0.52591209\n",
      "Iteration 70, loss = 0.52576653\n",
      "Iteration 71, loss = 0.52587296\n",
      "Iteration 72, loss = 0.52588604\n",
      "Iteration 73, loss = 0.52585765\n",
      "Iteration 74, loss = 0.52582893\n",
      "Iteration 75, loss = 0.52575517\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72573843\n",
      "Iteration 2, loss = 0.70969891\n",
      "Iteration 3, loss = 0.69511731\n",
      "Iteration 4, loss = 0.68193267\n",
      "Iteration 5, loss = 0.66873380\n",
      "Iteration 6, loss = 0.65786061\n",
      "Iteration 7, loss = 0.64667579\n",
      "Iteration 8, loss = 0.63668624\n",
      "Iteration 9, loss = 0.62777824\n",
      "Iteration 10, loss = 0.61898560\n",
      "Iteration 11, loss = 0.61093050\n",
      "Iteration 12, loss = 0.60366943\n",
      "Iteration 13, loss = 0.59671839\n",
      "Iteration 14, loss = 0.59099651\n",
      "Iteration 15, loss = 0.58492396\n",
      "Iteration 16, loss = 0.57931166\n",
      "Iteration 17, loss = 0.57466065\n",
      "Iteration 18, loss = 0.56992966\n",
      "Iteration 19, loss = 0.56570255\n",
      "Iteration 20, loss = 0.56198203\n",
      "Iteration 21, loss = 0.55871950\n",
      "Iteration 22, loss = 0.55514107\n",
      "Iteration 23, loss = 0.55254259\n",
      "Iteration 24, loss = 0.54992332\n",
      "Iteration 25, loss = 0.54820371\n",
      "Iteration 26, loss = 0.54631048\n",
      "Iteration 27, loss = 0.54449836\n",
      "Iteration 28, loss = 0.54345944\n",
      "Iteration 29, loss = 0.54234695\n",
      "Iteration 30, loss = 0.54122660\n",
      "Iteration 31, loss = 0.54050256\n",
      "Iteration 32, loss = 0.53985378\n",
      "Iteration 33, loss = 0.53922982\n",
      "Iteration 34, loss = 0.53863463\n",
      "Iteration 35, loss = 0.53822397\n",
      "Iteration 36, loss = 0.53785145\n",
      "Iteration 37, loss = 0.53746320\n",
      "Iteration 38, loss = 0.53709952\n",
      "Iteration 39, loss = 0.53684133\n",
      "Iteration 40, loss = 0.53670443\n",
      "Iteration 41, loss = 0.53631442\n",
      "Iteration 42, loss = 0.53608838\n",
      "Iteration 43, loss = 0.53576516\n",
      "Iteration 44, loss = 0.53557183\n",
      "Iteration 45, loss = 0.53530629\n",
      "Iteration 46, loss = 0.53522744\n",
      "Iteration 47, loss = 0.53500503\n",
      "Iteration 48, loss = 0.53481719\n",
      "Iteration 49, loss = 0.53463209\n",
      "Iteration 50, loss = 0.53437259\n",
      "Iteration 51, loss = 0.53415785\n",
      "Iteration 52, loss = 0.53398577\n",
      "Iteration 53, loss = 0.53374401\n",
      "Iteration 54, loss = 0.53376165\n",
      "Iteration 55, loss = 0.53348082\n",
      "Iteration 56, loss = 0.53336856\n",
      "Iteration 57, loss = 0.53323067\n",
      "Iteration 58, loss = 0.53326726\n",
      "Iteration 59, loss = 0.53326318\n",
      "Iteration 60, loss = 0.53312510\n",
      "Iteration 61, loss = 0.53290194\n",
      "Iteration 62, loss = 0.53264826\n",
      "Iteration 63, loss = 0.53246574\n",
      "Iteration 64, loss = 0.53235773\n",
      "Iteration 65, loss = 0.53217825\n",
      "Iteration 66, loss = 0.53208352\n",
      "Iteration 67, loss = 0.53189938\n",
      "Iteration 68, loss = 0.53175344\n",
      "Iteration 69, loss = 0.53154938\n",
      "Iteration 70, loss = 0.53137520\n",
      "Iteration 71, loss = 0.53129716\n",
      "Iteration 72, loss = 0.53112882\n",
      "Iteration 73, loss = 0.53095495\n",
      "Iteration 74, loss = 0.53080173\n",
      "Iteration 75, loss = 0.53071443\n",
      "Iteration 76, loss = 0.53054260\n",
      "Iteration 77, loss = 0.53040545\n",
      "Iteration 78, loss = 0.53030849\n",
      "Iteration 79, loss = 0.53015172\n",
      "Iteration 80, loss = 0.52997929\n",
      "Iteration 81, loss = 0.52988253\n",
      "Iteration 82, loss = 0.52979801\n",
      "Iteration 83, loss = 0.52974011\n",
      "Iteration 84, loss = 0.52977781\n",
      "Iteration 85, loss = 0.53005952\n",
      "Iteration 86, loss = 0.53018353\n",
      "Iteration 87, loss = 0.52986900\n",
      "Iteration 88, loss = 0.52986795\n",
      "Iteration 89, loss = 0.52979356\n",
      "Iteration 90, loss = 0.52968486\n",
      "Iteration 91, loss = 0.52945527\n",
      "Iteration 92, loss = 0.52924427\n",
      "Iteration 93, loss = 0.52878934\n",
      "Iteration 94, loss = 0.52874782\n",
      "Iteration 95, loss = 0.52856274\n",
      "Iteration 96, loss = 0.52841006\n",
      "Iteration 97, loss = 0.52863071\n",
      "Iteration 98, loss = 0.52850553\n",
      "Iteration 99, loss = 0.52840017\n",
      "Iteration 100, loss = 0.52844284\n",
      "Iteration 101, loss = 0.52821136\n",
      "Iteration 102, loss = 0.52794848\n",
      "Iteration 103, loss = 0.52758332\n",
      "Iteration 104, loss = 0.52747213\n",
      "Iteration 105, loss = 0.52707624\n",
      "Iteration 106, loss = 0.52701651\n",
      "Iteration 107, loss = 0.52678767\n",
      "Iteration 108, loss = 0.52674158\n",
      "Iteration 109, loss = 0.52661013\n",
      "Iteration 110, loss = 0.52653020\n",
      "Iteration 111, loss = 0.52637881\n",
      "Iteration 112, loss = 0.52634503\n",
      "Iteration 113, loss = 0.52615917\n",
      "Iteration 114, loss = 0.52607740\n",
      "Iteration 115, loss = 0.52595191\n",
      "Iteration 116, loss = 0.52581645\n",
      "Iteration 117, loss = 0.52570142\n",
      "Iteration 118, loss = 0.52550637\n",
      "Iteration 119, loss = 0.52541047\n",
      "Iteration 120, loss = 0.52525102\n",
      "Iteration 121, loss = 0.52510321\n",
      "Iteration 122, loss = 0.52491551\n",
      "Iteration 123, loss = 0.52480033\n",
      "Iteration 124, loss = 0.52481720\n",
      "Iteration 125, loss = 0.52462038\n",
      "Iteration 126, loss = 0.52469822\n",
      "Iteration 127, loss = 0.52476698\n",
      "Iteration 128, loss = 0.52469609\n",
      "Iteration 129, loss = 0.52470244\n",
      "Iteration 130, loss = 0.52496832\n",
      "Iteration 131, loss = 0.52496705\n",
      "Iteration 132, loss = 0.52499574\n",
      "Iteration 133, loss = 0.52505431\n",
      "Iteration 134, loss = 0.52463832\n",
      "Iteration 135, loss = 0.52431308\n",
      "Iteration 136, loss = 0.52401017\n",
      "Iteration 137, loss = 0.52374660\n",
      "Iteration 138, loss = 0.52376485\n",
      "Iteration 139, loss = 0.52345692\n",
      "Iteration 140, loss = 0.52328723\n",
      "Iteration 141, loss = 0.52309593\n",
      "Iteration 142, loss = 0.52313418\n",
      "Iteration 143, loss = 0.52290642\n",
      "Iteration 144, loss = 0.52275060\n",
      "Iteration 145, loss = 0.52265869\n",
      "Iteration 146, loss = 0.52273107\n",
      "Iteration 147, loss = 0.52266497\n",
      "Iteration 148, loss = 0.52260413\n",
      "Iteration 149, loss = 0.52262647\n",
      "Iteration 150, loss = 0.52275574\n",
      "Iteration 151, loss = 0.52267447\n",
      "Iteration 152, loss = 0.52257275\n",
      "Iteration 153, loss = 0.52249344\n",
      "Iteration 154, loss = 0.52225483\n",
      "Iteration 155, loss = 0.52202714\n",
      "Iteration 156, loss = 0.52223638\n",
      "Iteration 157, loss = 0.52187284\n",
      "Iteration 158, loss = 0.52170775\n",
      "Iteration 159, loss = 0.52162279\n",
      "Iteration 160, loss = 0.52138300\n",
      "Iteration 161, loss = 0.52117389\n",
      "Iteration 162, loss = 0.52105006\n",
      "Iteration 163, loss = 0.52099920\n",
      "Iteration 164, loss = 0.52080283\n",
      "Iteration 165, loss = 0.52078283\n",
      "Iteration 166, loss = 0.52067873\n",
      "Iteration 167, loss = 0.52054592\n",
      "Iteration 168, loss = 0.52055998\n",
      "Iteration 169, loss = 0.52046709\n",
      "Iteration 170, loss = 0.52038264\n",
      "Iteration 171, loss = 0.52023222\n",
      "Iteration 172, loss = 0.52006869\n",
      "Iteration 173, loss = 0.51987700\n",
      "Iteration 174, loss = 0.51975990\n",
      "Iteration 175, loss = 0.51978612\n",
      "Iteration 176, loss = 0.51964558\n",
      "Iteration 177, loss = 0.51960705\n",
      "Iteration 178, loss = 0.51950081\n",
      "Iteration 179, loss = 0.51929199\n",
      "Iteration 180, loss = 0.51915057\n",
      "Iteration 181, loss = 0.51905732\n",
      "Iteration 182, loss = 0.51885277\n",
      "Iteration 183, loss = 0.51877287\n",
      "Iteration 184, loss = 0.51869346\n",
      "Iteration 185, loss = 0.51859019\n",
      "Iteration 186, loss = 0.51857575\n",
      "Iteration 187, loss = 0.51875252\n",
      "Iteration 188, loss = 0.51854957\n",
      "Iteration 189, loss = 0.51838899\n",
      "Iteration 190, loss = 0.51822998\n",
      "Iteration 191, loss = 0.51805092\n",
      "Iteration 192, loss = 0.51776707\n",
      "Iteration 193, loss = 0.51788070\n",
      "Iteration 194, loss = 0.51783670\n",
      "Iteration 195, loss = 0.51777520\n",
      "Iteration 196, loss = 0.51790256\n",
      "Iteration 197, loss = 0.51794146\n",
      "Iteration 198, loss = 0.51797945\n",
      "Iteration 199, loss = 0.51787684\n",
      "Iteration 200, loss = 0.51785082\n",
      "Iteration 201, loss = 0.51775758\n",
      "Iteration 202, loss = 0.51765274\n",
      "Iteration 203, loss = 0.51757238\n",
      "Iteration 204, loss = 0.51750222\n",
      "Iteration 205, loss = 0.51735979\n",
      "Iteration 206, loss = 0.51728291\n",
      "Iteration 207, loss = 0.51703747\n",
      "Iteration 208, loss = 0.51674901\n",
      "Iteration 209, loss = 0.51666104\n",
      "Iteration 210, loss = 0.51657946\n",
      "Iteration 211, loss = 0.51645130\n",
      "Iteration 212, loss = 0.51634159\n",
      "Iteration 213, loss = 0.51635089\n",
      "Iteration 214, loss = 0.51627462\n",
      "Iteration 215, loss = 0.51608706\n",
      "Iteration 216, loss = 0.51592467\n",
      "Iteration 217, loss = 0.51603183\n",
      "Iteration 218, loss = 0.51610912\n",
      "Iteration 219, loss = 0.51642822\n",
      "Iteration 220, loss = 0.51641587\n",
      "Iteration 221, loss = 0.51632536\n",
      "Iteration 222, loss = 0.51645543\n",
      "Iteration 223, loss = 0.51643553\n",
      "Iteration 224, loss = 0.51627065\n",
      "Iteration 225, loss = 0.51618457\n",
      "Iteration 226, loss = 0.51607899\n",
      "Iteration 227, loss = 0.51566494\n",
      "Iteration 228, loss = 0.51554400\n",
      "Iteration 229, loss = 0.51539863\n",
      "Iteration 230, loss = 0.51515295\n",
      "Iteration 231, loss = 0.51509026\n",
      "Iteration 232, loss = 0.51495244\n",
      "Iteration 233, loss = 0.51487556\n",
      "Iteration 234, loss = 0.51473548\n",
      "Iteration 235, loss = 0.51466094\n",
      "Iteration 236, loss = 0.51474123\n",
      "Iteration 237, loss = 0.51477455\n",
      "Iteration 238, loss = 0.51457303\n",
      "Iteration 239, loss = 0.51454234\n",
      "Iteration 240, loss = 0.51450899\n",
      "Iteration 241, loss = 0.51445650\n",
      "Iteration 242, loss = 0.51438689\n",
      "Iteration 243, loss = 0.51428692\n",
      "Iteration 244, loss = 0.51418323\n",
      "Iteration 245, loss = 0.51413498\n",
      "Iteration 246, loss = 0.51406047\n",
      "Iteration 247, loss = 0.51393389\n",
      "Iteration 248, loss = 0.51390145\n",
      "Iteration 249, loss = 0.51406886\n",
      "Iteration 250, loss = 0.51395391\n",
      "Iteration 251, loss = 0.51391454\n",
      "Iteration 252, loss = 0.51377524\n",
      "Iteration 253, loss = 0.51379777\n",
      "Iteration 254, loss = 0.51377070\n",
      "Iteration 255, loss = 0.51376700\n",
      "Iteration 256, loss = 0.51372871\n",
      "Iteration 257, loss = 0.51360611\n",
      "Iteration 258, loss = 0.51346092\n",
      "Iteration 259, loss = 0.51342632\n",
      "Iteration 260, loss = 0.51334897\n",
      "Iteration 261, loss = 0.51326147\n",
      "Iteration 262, loss = 0.51322544\n",
      "Iteration 263, loss = 0.51316598\n",
      "Iteration 264, loss = 0.51314854\n",
      "Iteration 265, loss = 0.51312430\n",
      "Iteration 266, loss = 0.51302100\n",
      "Iteration 267, loss = 0.51301163\n",
      "Iteration 268, loss = 0.51290360\n",
      "Iteration 269, loss = 0.51270497\n",
      "Iteration 270, loss = 0.51273208\n",
      "Iteration 271, loss = 0.51272523\n",
      "Iteration 272, loss = 0.51283242\n",
      "Iteration 273, loss = 0.51272888\n",
      "Iteration 274, loss = 0.51282691\n",
      "Iteration 275, loss = 0.51272374\n",
      "Iteration 276, loss = 0.51279313\n",
      "Iteration 277, loss = 0.51256807\n",
      "Iteration 278, loss = 0.51243156\n",
      "Iteration 279, loss = 0.51227515\n",
      "Iteration 280, loss = 0.51225407\n",
      "Iteration 281, loss = 0.51209478\n",
      "Iteration 282, loss = 0.51213216\n",
      "Iteration 283, loss = 0.51204504\n",
      "Iteration 284, loss = 0.51190156\n",
      "Iteration 285, loss = 0.51183715\n",
      "Iteration 286, loss = 0.51170419\n",
      "Iteration 287, loss = 0.51159533\n",
      "Iteration 288, loss = 0.51163872\n",
      "Iteration 289, loss = 0.51169067\n",
      "Iteration 290, loss = 0.51188520\n",
      "Iteration 291, loss = 0.51210639\n",
      "Iteration 292, loss = 0.51219119\n",
      "Iteration 293, loss = 0.51223148\n",
      "Iteration 294, loss = 0.51202153\n",
      "Iteration 295, loss = 0.51205361\n",
      "Iteration 296, loss = 0.51199637\n",
      "Iteration 297, loss = 0.51180263\n",
      "Iteration 298, loss = 0.51169965\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75904271\n",
      "Iteration 2, loss = 0.74087734\n",
      "Iteration 3, loss = 0.72366765\n",
      "Iteration 4, loss = 0.70754346\n",
      "Iteration 5, loss = 0.69223260\n",
      "Iteration 6, loss = 0.67798436\n",
      "Iteration 7, loss = 0.66453976\n",
      "Iteration 8, loss = 0.65165422\n",
      "Iteration 9, loss = 0.63995693\n",
      "Iteration 10, loss = 0.62869121\n",
      "Iteration 11, loss = 0.61881834\n",
      "Iteration 12, loss = 0.60896460\n",
      "Iteration 13, loss = 0.60012046\n",
      "Iteration 14, loss = 0.59280463\n",
      "Iteration 15, loss = 0.58519480\n",
      "Iteration 16, loss = 0.57898531\n",
      "Iteration 17, loss = 0.57314572\n",
      "Iteration 18, loss = 0.56743873\n",
      "Iteration 19, loss = 0.56256088\n",
      "Iteration 20, loss = 0.55810571\n",
      "Iteration 21, loss = 0.55363794\n",
      "Iteration 22, loss = 0.54993911\n",
      "Iteration 23, loss = 0.54637054\n",
      "Iteration 24, loss = 0.54284266\n",
      "Iteration 25, loss = 0.54014082\n",
      "Iteration 26, loss = 0.53730114\n",
      "Iteration 27, loss = 0.53512711\n",
      "Iteration 28, loss = 0.53269090\n",
      "Iteration 29, loss = 0.53062434\n",
      "Iteration 30, loss = 0.52912273\n",
      "Iteration 31, loss = 0.52758601\n",
      "Iteration 32, loss = 0.52581835\n",
      "Iteration 33, loss = 0.52474485\n",
      "Iteration 34, loss = 0.52369751\n",
      "Iteration 35, loss = 0.52270105\n",
      "Iteration 36, loss = 0.52206079\n",
      "Iteration 37, loss = 0.52141221\n",
      "Iteration 38, loss = 0.52081577\n",
      "Iteration 39, loss = 0.52033104\n",
      "Iteration 40, loss = 0.51990788\n",
      "Iteration 41, loss = 0.51957400\n",
      "Iteration 42, loss = 0.51924288\n",
      "Iteration 43, loss = 0.51885015\n",
      "Iteration 44, loss = 0.51872763\n",
      "Iteration 45, loss = 0.51843935\n",
      "Iteration 46, loss = 0.51827588\n",
      "Iteration 47, loss = 0.51804700\n",
      "Iteration 48, loss = 0.51785765\n",
      "Iteration 49, loss = 0.51773267\n",
      "Iteration 50, loss = 0.51760143\n",
      "Iteration 51, loss = 0.51751960\n",
      "Iteration 52, loss = 0.51735295\n",
      "Iteration 53, loss = 0.51722623\n",
      "Iteration 54, loss = 0.51710697\n",
      "Iteration 55, loss = 0.51689913\n",
      "Iteration 56, loss = 0.51687895\n",
      "Iteration 57, loss = 0.51666983\n",
      "Iteration 58, loss = 0.51653503\n",
      "Iteration 59, loss = 0.51638022\n",
      "Iteration 60, loss = 0.51634682\n",
      "Iteration 61, loss = 0.51616916\n",
      "Iteration 62, loss = 0.51615781\n",
      "Iteration 63, loss = 0.51610455\n",
      "Iteration 64, loss = 0.51590852\n",
      "Iteration 65, loss = 0.51586867\n",
      "Iteration 66, loss = 0.51567972\n",
      "Iteration 67, loss = 0.51559453\n",
      "Iteration 68, loss = 0.51548110\n",
      "Iteration 69, loss = 0.51535117\n",
      "Iteration 70, loss = 0.51530664\n",
      "Iteration 71, loss = 0.51523963\n",
      "Iteration 72, loss = 0.51505203\n",
      "Iteration 73, loss = 0.51485043\n",
      "Iteration 74, loss = 0.51480227\n",
      "Iteration 75, loss = 0.51464434\n",
      "Iteration 76, loss = 0.51452907\n",
      "Iteration 77, loss = 0.51452270\n",
      "Iteration 78, loss = 0.51439892\n",
      "Iteration 79, loss = 0.51419165\n",
      "Iteration 80, loss = 0.51410008\n",
      "Iteration 81, loss = 0.51397861\n",
      "Iteration 82, loss = 0.51385942\n",
      "Iteration 83, loss = 0.51386007\n",
      "Iteration 84, loss = 0.51371644\n",
      "Iteration 85, loss = 0.51371122\n",
      "Iteration 86, loss = 0.51362728\n",
      "Iteration 87, loss = 0.51348572\n",
      "Iteration 88, loss = 0.51341267\n",
      "Iteration 89, loss = 0.51332666\n",
      "Iteration 90, loss = 0.51318177\n",
      "Iteration 91, loss = 0.51312001\n",
      "Iteration 92, loss = 0.51311500\n",
      "Iteration 93, loss = 0.51306090\n",
      "Iteration 94, loss = 0.51308247\n",
      "Iteration 95, loss = 0.51306002\n",
      "Iteration 96, loss = 0.51294413\n",
      "Iteration 97, loss = 0.51286885\n",
      "Iteration 98, loss = 0.51274282\n",
      "Iteration 99, loss = 0.51267897\n",
      "Iteration 100, loss = 0.51261943\n",
      "Iteration 101, loss = 0.51252172\n",
      "Iteration 102, loss = 0.51243409\n",
      "Iteration 103, loss = 0.51237796\n",
      "Iteration 104, loss = 0.51226613\n",
      "Iteration 105, loss = 0.51219260\n",
      "Iteration 106, loss = 0.51211607\n",
      "Iteration 107, loss = 0.51203571\n",
      "Iteration 108, loss = 0.51212125\n",
      "Iteration 109, loss = 0.51186889\n",
      "Iteration 110, loss = 0.51182267\n",
      "Iteration 111, loss = 0.51182246\n",
      "Iteration 112, loss = 0.51186712\n",
      "Iteration 113, loss = 0.51169860\n",
      "Iteration 114, loss = 0.51165797\n",
      "Iteration 115, loss = 0.51158568\n",
      "Iteration 116, loss = 0.51161373\n",
      "Iteration 117, loss = 0.51164253\n",
      "Iteration 118, loss = 0.51158342\n",
      "Iteration 119, loss = 0.51159937\n",
      "Iteration 120, loss = 0.51165617\n",
      "Iteration 121, loss = 0.51147897\n",
      "Iteration 122, loss = 0.51142054\n",
      "Iteration 123, loss = 0.51122961\n",
      "Iteration 124, loss = 0.51120945\n",
      "Iteration 125, loss = 0.51109366\n",
      "Iteration 126, loss = 0.51112066\n",
      "Iteration 127, loss = 0.51102449\n",
      "Iteration 128, loss = 0.51094102\n",
      "Iteration 129, loss = 0.51087022\n",
      "Iteration 130, loss = 0.51080571\n",
      "Iteration 131, loss = 0.51068403\n",
      "Iteration 132, loss = 0.51068959\n",
      "Iteration 133, loss = 0.51068708\n",
      "Iteration 134, loss = 0.51054701\n",
      "Iteration 135, loss = 0.51067927\n",
      "Iteration 136, loss = 0.51057473\n",
      "Iteration 137, loss = 0.51066045\n",
      "Iteration 138, loss = 0.51073533\n",
      "Iteration 139, loss = 0.51076083\n",
      "Iteration 140, loss = 0.51077811\n",
      "Iteration 141, loss = 0.51075894\n",
      "Iteration 142, loss = 0.51073448\n",
      "Iteration 143, loss = 0.51062990\n",
      "Iteration 144, loss = 0.51049611\n",
      "Iteration 145, loss = 0.51029285\n",
      "Iteration 146, loss = 0.51021204\n",
      "Iteration 147, loss = 0.51015839\n",
      "Iteration 148, loss = 0.51002089\n",
      "Iteration 149, loss = 0.50994087\n",
      "Iteration 150, loss = 0.50994479\n",
      "Iteration 151, loss = 0.50988911\n",
      "Iteration 152, loss = 0.50997723\n",
      "Iteration 153, loss = 0.50994473\n",
      "Iteration 154, loss = 0.50995267\n",
      "Iteration 155, loss = 0.51000496\n",
      "Iteration 156, loss = 0.51006208\n",
      "Iteration 157, loss = 0.50998688\n",
      "Iteration 158, loss = 0.50990605\n",
      "Iteration 159, loss = 0.50989136\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66524362\n",
      "Iteration 2, loss = 0.65340826\n",
      "Iteration 3, loss = 0.64275839\n",
      "Iteration 4, loss = 0.63255114\n",
      "Iteration 5, loss = 0.62345887\n",
      "Iteration 6, loss = 0.61448971\n",
      "Iteration 7, loss = 0.60650977\n",
      "Iteration 8, loss = 0.59906591\n",
      "Iteration 9, loss = 0.59280984\n",
      "Iteration 10, loss = 0.58631426\n",
      "Iteration 11, loss = 0.58075325\n",
      "Iteration 12, loss = 0.57562481\n",
      "Iteration 13, loss = 0.57102205\n",
      "Iteration 14, loss = 0.56676112\n",
      "Iteration 15, loss = 0.56282986\n",
      "Iteration 16, loss = 0.55965968\n",
      "Iteration 17, loss = 0.55667915\n",
      "Iteration 18, loss = 0.55392887\n",
      "Iteration 19, loss = 0.55162523\n",
      "Iteration 20, loss = 0.54987304\n",
      "Iteration 21, loss = 0.54781548\n",
      "Iteration 22, loss = 0.54617533\n",
      "Iteration 23, loss = 0.54481133\n",
      "Iteration 24, loss = 0.54374743\n",
      "Iteration 25, loss = 0.54251868\n",
      "Iteration 26, loss = 0.54171304\n",
      "Iteration 27, loss = 0.54072355\n",
      "Iteration 28, loss = 0.54007270\n",
      "Iteration 29, loss = 0.53955172\n",
      "Iteration 30, loss = 0.53891277\n",
      "Iteration 31, loss = 0.53855319\n",
      "Iteration 32, loss = 0.53814211\n",
      "Iteration 33, loss = 0.53768963\n",
      "Iteration 34, loss = 0.53778353\n",
      "Iteration 35, loss = 0.53739185\n",
      "Iteration 36, loss = 0.53707161\n",
      "Iteration 37, loss = 0.53691240\n",
      "Iteration 38, loss = 0.53668225\n",
      "Iteration 39, loss = 0.53653064\n",
      "Iteration 40, loss = 0.53647451\n",
      "Iteration 41, loss = 0.53620422\n",
      "Iteration 42, loss = 0.53602018\n",
      "Iteration 43, loss = 0.53581589\n",
      "Iteration 44, loss = 0.53567444\n",
      "Iteration 45, loss = 0.53549157\n",
      "Iteration 46, loss = 0.53530144\n",
      "Iteration 47, loss = 0.53521398\n",
      "Iteration 48, loss = 0.53515945\n",
      "Iteration 49, loss = 0.53508980\n",
      "Iteration 50, loss = 0.53495711\n",
      "Iteration 51, loss = 0.53476018\n",
      "Iteration 52, loss = 0.53460763\n",
      "Iteration 53, loss = 0.53449290\n",
      "Iteration 54, loss = 0.53426532\n",
      "Iteration 55, loss = 0.53414145\n",
      "Iteration 56, loss = 0.53407779\n",
      "Iteration 57, loss = 0.53394508\n",
      "Iteration 58, loss = 0.53383858\n",
      "Iteration 59, loss = 0.53370100\n",
      "Iteration 60, loss = 0.53346841\n",
      "Iteration 61, loss = 0.53329074\n",
      "Iteration 62, loss = 0.53309095\n",
      "Iteration 63, loss = 0.53296897\n",
      "Iteration 64, loss = 0.53292288\n",
      "Iteration 65, loss = 0.53282622\n",
      "Iteration 66, loss = 0.53273753\n",
      "Iteration 67, loss = 0.53267708\n",
      "Iteration 68, loss = 0.53250889\n",
      "Iteration 69, loss = 0.53245783\n",
      "Iteration 70, loss = 0.53229166\n",
      "Iteration 71, loss = 0.53215699\n",
      "Iteration 72, loss = 0.53199677\n",
      "Iteration 73, loss = 0.53201274\n",
      "Iteration 74, loss = 0.53184831\n",
      "Iteration 75, loss = 0.53178880\n",
      "Iteration 76, loss = 0.53179529\n",
      "Iteration 77, loss = 0.53153092\n",
      "Iteration 78, loss = 0.53125136\n",
      "Iteration 79, loss = 0.53107925\n",
      "Iteration 80, loss = 0.53109471\n",
      "Iteration 81, loss = 0.53086734\n",
      "Iteration 82, loss = 0.53082145\n",
      "Iteration 83, loss = 0.53070076\n",
      "Iteration 84, loss = 0.53060142\n",
      "Iteration 85, loss = 0.53057538\n",
      "Iteration 86, loss = 0.53045430\n",
      "Iteration 87, loss = 0.53030850\n",
      "Iteration 88, loss = 0.53024074\n",
      "Iteration 89, loss = 0.53014339\n",
      "Iteration 90, loss = 0.53017733\n",
      "Iteration 91, loss = 0.52995369\n",
      "Iteration 92, loss = 0.52990638\n",
      "Iteration 93, loss = 0.52986197\n",
      "Iteration 94, loss = 0.52981727\n",
      "Iteration 95, loss = 0.52971437\n",
      "Iteration 96, loss = 0.52981122\n",
      "Iteration 97, loss = 0.52982746\n",
      "Iteration 98, loss = 0.52989141\n",
      "Iteration 99, loss = 0.52987724\n",
      "Iteration 100, loss = 0.52967800\n",
      "Iteration 101, loss = 0.52949135\n",
      "Iteration 102, loss = 0.52934357\n",
      "Iteration 103, loss = 0.52931602\n",
      "Iteration 104, loss = 0.52907170\n",
      "Iteration 105, loss = 0.52889379\n",
      "Iteration 106, loss = 0.52869950\n",
      "Iteration 107, loss = 0.52851523\n",
      "Iteration 108, loss = 0.52841651\n",
      "Iteration 109, loss = 0.52838612\n",
      "Iteration 110, loss = 0.52855547\n",
      "Iteration 111, loss = 0.52860859\n",
      "Iteration 112, loss = 0.52857526\n",
      "Iteration 113, loss = 0.52859584\n",
      "Iteration 114, loss = 0.52856909\n",
      "Iteration 115, loss = 0.52858870\n",
      "Iteration 116, loss = 0.52841788\n",
      "Iteration 117, loss = 0.52821258\n",
      "Iteration 118, loss = 0.52798315\n",
      "Iteration 119, loss = 0.52800098\n",
      "Iteration 120, loss = 0.52774440\n",
      "Iteration 121, loss = 0.52764206\n",
      "Iteration 122, loss = 0.52759521\n",
      "Iteration 123, loss = 0.52739156\n",
      "Iteration 124, loss = 0.52743383\n",
      "Iteration 125, loss = 0.52729549\n",
      "Iteration 126, loss = 0.52720714\n",
      "Iteration 127, loss = 0.52708078\n",
      "Iteration 128, loss = 0.52697450\n",
      "Iteration 129, loss = 0.52681673\n",
      "Iteration 130, loss = 0.52668804\n",
      "Iteration 131, loss = 0.52663434\n",
      "Iteration 132, loss = 0.52649257\n",
      "Iteration 133, loss = 0.52637886\n",
      "Iteration 134, loss = 0.52645874\n",
      "Iteration 135, loss = 0.52631749\n",
      "Iteration 136, loss = 0.52632307\n",
      "Iteration 137, loss = 0.52644352\n",
      "Iteration 138, loss = 0.52632218\n",
      "Iteration 139, loss = 0.52617415\n",
      "Iteration 140, loss = 0.52618079\n",
      "Iteration 141, loss = 0.52629242\n",
      "Iteration 142, loss = 0.52628494\n",
      "Iteration 143, loss = 0.52632833\n",
      "Iteration 144, loss = 0.52646348\n",
      "Iteration 145, loss = 0.52656371\n",
      "Iteration 146, loss = 0.52636632\n",
      "Iteration 147, loss = 0.52628368\n",
      "Iteration 148, loss = 0.52597015\n",
      "Iteration 149, loss = 0.52562961\n",
      "Iteration 150, loss = 0.52543514\n",
      "Iteration 151, loss = 0.52513765\n",
      "Iteration 152, loss = 0.52504805\n",
      "Iteration 153, loss = 0.52499867\n",
      "Iteration 154, loss = 0.52495818\n",
      "Iteration 155, loss = 0.52473332\n",
      "Iteration 156, loss = 0.52477811\n",
      "Iteration 157, loss = 0.52475821\n",
      "Iteration 158, loss = 0.52476440\n",
      "Iteration 159, loss = 0.52482830\n",
      "Iteration 160, loss = 0.52476473\n",
      "Iteration 161, loss = 0.52472325\n",
      "Iteration 162, loss = 0.52468246\n",
      "Iteration 163, loss = 0.52454249\n",
      "Iteration 164, loss = 0.52432677\n",
      "Iteration 165, loss = 0.52432224\n",
      "Iteration 166, loss = 0.52406150\n",
      "Iteration 167, loss = 0.52402893\n",
      "Iteration 168, loss = 0.52390368\n",
      "Iteration 169, loss = 0.52391672\n",
      "Iteration 170, loss = 0.52369307\n",
      "Iteration 171, loss = 0.52360975\n",
      "Iteration 172, loss = 0.52375137\n",
      "Iteration 173, loss = 0.52357819\n",
      "Iteration 174, loss = 0.52369275\n",
      "Iteration 175, loss = 0.52354655\n",
      "Iteration 176, loss = 0.52343514\n",
      "Iteration 177, loss = 0.52333885\n",
      "Iteration 178, loss = 0.52330415\n",
      "Iteration 179, loss = 0.52353862\n",
      "Iteration 180, loss = 0.52353357\n",
      "Iteration 181, loss = 0.52339051\n",
      "Iteration 182, loss = 0.52327166\n",
      "Iteration 183, loss = 0.52309605\n",
      "Iteration 184, loss = 0.52290935\n",
      "Iteration 185, loss = 0.52271726\n",
      "Iteration 186, loss = 0.52250513\n",
      "Iteration 187, loss = 0.52231890\n",
      "Iteration 188, loss = 0.52243271\n",
      "Iteration 189, loss = 0.52246053\n",
      "Iteration 190, loss = 0.52273165\n",
      "Iteration 191, loss = 0.52254397\n",
      "Iteration 192, loss = 0.52254581\n",
      "Iteration 193, loss = 0.52246953\n",
      "Iteration 194, loss = 0.52249124\n",
      "Iteration 195, loss = 0.52234152\n",
      "Iteration 196, loss = 0.52218601\n",
      "Iteration 197, loss = 0.52204100\n",
      "Iteration 198, loss = 0.52190523\n",
      "Iteration 199, loss = 0.52184882\n",
      "Iteration 200, loss = 0.52180936\n",
      "Iteration 201, loss = 0.52184459\n",
      "Iteration 202, loss = 0.52170760\n",
      "Iteration 203, loss = 0.52172779\n",
      "Iteration 204, loss = 0.52197827\n",
      "Iteration 205, loss = 0.52190592\n",
      "Iteration 206, loss = 0.52186159\n",
      "Iteration 207, loss = 0.52169230\n",
      "Iteration 208, loss = 0.52176302\n",
      "Iteration 209, loss = 0.52148518\n",
      "Iteration 210, loss = 0.52142635\n",
      "Iteration 211, loss = 0.52132094\n",
      "Iteration 212, loss = 0.52135049\n",
      "Iteration 213, loss = 0.52131208\n",
      "Iteration 214, loss = 0.52121897\n",
      "Iteration 215, loss = 0.52103087\n",
      "Iteration 216, loss = 0.52083166\n",
      "Iteration 217, loss = 0.52063645\n",
      "Iteration 218, loss = 0.52052321\n",
      "Iteration 219, loss = 0.52091334\n",
      "Iteration 220, loss = 0.52090080\n",
      "Iteration 221, loss = 0.52115569\n",
      "Iteration 222, loss = 0.52159071\n",
      "Iteration 223, loss = 0.52143810\n",
      "Iteration 224, loss = 0.52115042\n",
      "Iteration 225, loss = 0.52079572\n",
      "Iteration 226, loss = 0.52088694\n",
      "Iteration 227, loss = 0.52047190\n",
      "Iteration 228, loss = 0.52024123\n",
      "Iteration 229, loss = 0.52016556\n",
      "Iteration 230, loss = 0.51995582\n",
      "Iteration 231, loss = 0.52011403\n",
      "Iteration 232, loss = 0.52029070\n",
      "Iteration 233, loss = 0.51998199\n",
      "Iteration 234, loss = 0.51990265\n",
      "Iteration 235, loss = 0.51982615\n",
      "Iteration 236, loss = 0.51992547\n",
      "Iteration 237, loss = 0.51995162\n",
      "Iteration 238, loss = 0.52001607\n",
      "Iteration 239, loss = 0.52009073\n",
      "Iteration 240, loss = 0.52011698\n",
      "Iteration 241, loss = 0.52017794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.74986113\n",
      "Iteration 2, loss = 0.73272029\n",
      "Iteration 3, loss = 0.71590318\n",
      "Iteration 4, loss = 0.70035914\n",
      "Iteration 5, loss = 0.68599209\n",
      "Iteration 6, loss = 0.67237562\n",
      "Iteration 7, loss = 0.66032888\n",
      "Iteration 8, loss = 0.64863025\n",
      "Iteration 9, loss = 0.63765409\n",
      "Iteration 10, loss = 0.62797149\n",
      "Iteration 11, loss = 0.61849838\n",
      "Iteration 12, loss = 0.61001434\n",
      "Iteration 13, loss = 0.60206201\n",
      "Iteration 14, loss = 0.59453586\n",
      "Iteration 15, loss = 0.58771361\n",
      "Iteration 16, loss = 0.58168593\n",
      "Iteration 17, loss = 0.57596403\n",
      "Iteration 18, loss = 0.57094032\n",
      "Iteration 19, loss = 0.56648472\n",
      "Iteration 20, loss = 0.56210822\n",
      "Iteration 21, loss = 0.55838436\n",
      "Iteration 22, loss = 0.55508708\n",
      "Iteration 23, loss = 0.55231814\n",
      "Iteration 24, loss = 0.54980792\n",
      "Iteration 25, loss = 0.54735880\n",
      "Iteration 26, loss = 0.54561372\n",
      "Iteration 27, loss = 0.54368890\n",
      "Iteration 28, loss = 0.54218904\n",
      "Iteration 29, loss = 0.54077585\n",
      "Iteration 30, loss = 0.53947751\n",
      "Iteration 31, loss = 0.53850494\n",
      "Iteration 32, loss = 0.53716882\n",
      "Iteration 33, loss = 0.53644361\n",
      "Iteration 34, loss = 0.53565619\n",
      "Iteration 35, loss = 0.53530743\n",
      "Iteration 36, loss = 0.53468971\n",
      "Iteration 37, loss = 0.53413901\n",
      "Iteration 38, loss = 0.53381317\n",
      "Iteration 39, loss = 0.53362928\n",
      "Iteration 40, loss = 0.53332267\n",
      "Iteration 41, loss = 0.53307940\n",
      "Iteration 42, loss = 0.53283153\n",
      "Iteration 43, loss = 0.53272563\n",
      "Iteration 44, loss = 0.53248533\n",
      "Iteration 45, loss = 0.53234896\n",
      "Iteration 46, loss = 0.53220257\n",
      "Iteration 47, loss = 0.53206258\n",
      "Iteration 48, loss = 0.53212846\n",
      "Iteration 49, loss = 0.53194032\n",
      "Iteration 50, loss = 0.53198201\n",
      "Iteration 51, loss = 0.53179452\n",
      "Iteration 52, loss = 0.53172227\n",
      "Iteration 53, loss = 0.53163420\n",
      "Iteration 54, loss = 0.53147806\n",
      "Iteration 55, loss = 0.53134213\n",
      "Iteration 56, loss = 0.53120771\n",
      "Iteration 57, loss = 0.53113987\n",
      "Iteration 58, loss = 0.53109952\n",
      "Iteration 59, loss = 0.53099087\n",
      "Iteration 60, loss = 0.53082788\n",
      "Iteration 61, loss = 0.53075662\n",
      "Iteration 62, loss = 0.53063100\n",
      "Iteration 63, loss = 0.53053162\n",
      "Iteration 64, loss = 0.53043786\n",
      "Iteration 65, loss = 0.53034415\n",
      "Iteration 66, loss = 0.53024531\n",
      "Iteration 67, loss = 0.53025805\n",
      "Iteration 68, loss = 0.53017313\n",
      "Iteration 69, loss = 0.53003998\n",
      "Iteration 70, loss = 0.52985522\n",
      "Iteration 71, loss = 0.52969640\n",
      "Iteration 72, loss = 0.52956212\n",
      "Iteration 73, loss = 0.52977471\n",
      "Iteration 74, loss = 0.52963167\n",
      "Iteration 75, loss = 0.52960229\n",
      "Iteration 76, loss = 0.52948607\n",
      "Iteration 77, loss = 0.52936405\n",
      "Iteration 78, loss = 0.52924788\n",
      "Iteration 79, loss = 0.52913837\n",
      "Iteration 80, loss = 0.52904139\n",
      "Iteration 81, loss = 0.52891360\n",
      "Iteration 82, loss = 0.52875575\n",
      "Iteration 83, loss = 0.52869193\n",
      "Iteration 84, loss = 0.52848137\n",
      "Iteration 85, loss = 0.52835142\n",
      "Iteration 86, loss = 0.52831805\n",
      "Iteration 87, loss = 0.52805352\n",
      "Iteration 88, loss = 0.52801010\n",
      "Iteration 89, loss = 0.52786688\n",
      "Iteration 90, loss = 0.52773910\n",
      "Iteration 91, loss = 0.52762215\n",
      "Iteration 92, loss = 0.52757994\n",
      "Iteration 93, loss = 0.52744702\n",
      "Iteration 94, loss = 0.52731735\n",
      "Iteration 95, loss = 0.52731490\n",
      "Iteration 96, loss = 0.52717224\n",
      "Iteration 97, loss = 0.52704057\n",
      "Iteration 98, loss = 0.52696554\n",
      "Iteration 99, loss = 0.52706074\n",
      "Iteration 100, loss = 0.52692946\n",
      "Iteration 101, loss = 0.52685480\n",
      "Iteration 102, loss = 0.52676689\n",
      "Iteration 103, loss = 0.52674255\n",
      "Iteration 104, loss = 0.52670414\n",
      "Iteration 105, loss = 0.52661679\n",
      "Iteration 106, loss = 0.52657165\n",
      "Iteration 107, loss = 0.52649844\n",
      "Iteration 108, loss = 0.52636692\n",
      "Iteration 109, loss = 0.52649680\n",
      "Iteration 110, loss = 0.52625298\n",
      "Iteration 111, loss = 0.52623866\n",
      "Iteration 112, loss = 0.52617157\n",
      "Iteration 113, loss = 0.52624436\n",
      "Iteration 114, loss = 0.52612937\n",
      "Iteration 115, loss = 0.52609366\n",
      "Iteration 116, loss = 0.52593318\n",
      "Iteration 117, loss = 0.52582584\n",
      "Iteration 118, loss = 0.52569566\n",
      "Iteration 119, loss = 0.52562412\n",
      "Iteration 120, loss = 0.52550569\n",
      "Iteration 121, loss = 0.52542467\n",
      "Iteration 122, loss = 0.52527545\n",
      "Iteration 123, loss = 0.52516987\n",
      "Iteration 124, loss = 0.52504028\n",
      "Iteration 125, loss = 0.52505634\n",
      "Iteration 126, loss = 0.52525256\n",
      "Iteration 127, loss = 0.52557192\n",
      "Iteration 128, loss = 0.52573088\n",
      "Iteration 129, loss = 0.52583531\n",
      "Iteration 130, loss = 0.52590434\n",
      "Iteration 131, loss = 0.52575476\n",
      "Iteration 132, loss = 0.52561277\n",
      "Iteration 133, loss = 0.52536000\n",
      "Iteration 134, loss = 0.52531888\n",
      "Iteration 135, loss = 0.52497332\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66657353\n",
      "Iteration 2, loss = 0.65278931\n",
      "Iteration 3, loss = 0.64027760\n",
      "Iteration 4, loss = 0.62812893\n",
      "Iteration 5, loss = 0.61719134\n",
      "Iteration 6, loss = 0.60685037\n",
      "Iteration 7, loss = 0.59740986\n",
      "Iteration 8, loss = 0.58863556\n",
      "Iteration 9, loss = 0.58073627\n",
      "Iteration 10, loss = 0.57330203\n",
      "Iteration 11, loss = 0.56656148\n",
      "Iteration 12, loss = 0.55984243\n",
      "Iteration 13, loss = 0.55427381\n",
      "Iteration 14, loss = 0.54897098\n",
      "Iteration 15, loss = 0.54401775\n",
      "Iteration 16, loss = 0.53980623\n",
      "Iteration 17, loss = 0.53566400\n",
      "Iteration 18, loss = 0.53223932\n",
      "Iteration 19, loss = 0.52893835\n",
      "Iteration 20, loss = 0.52663886\n",
      "Iteration 21, loss = 0.52398225\n",
      "Iteration 22, loss = 0.52181022\n",
      "Iteration 23, loss = 0.51998434\n",
      "Iteration 24, loss = 0.51875850\n",
      "Iteration 25, loss = 0.51690362\n",
      "Iteration 26, loss = 0.51590206\n",
      "Iteration 27, loss = 0.51460764\n",
      "Iteration 28, loss = 0.51398196\n",
      "Iteration 29, loss = 0.51332198\n",
      "Iteration 30, loss = 0.51242220\n",
      "Iteration 31, loss = 0.51192895\n",
      "Iteration 32, loss = 0.51166869\n",
      "Iteration 33, loss = 0.51131035\n",
      "Iteration 34, loss = 0.51086461\n",
      "Iteration 35, loss = 0.51059849\n",
      "Iteration 36, loss = 0.51055552\n",
      "Iteration 37, loss = 0.51015129\n",
      "Iteration 38, loss = 0.51012584\n",
      "Iteration 39, loss = 0.51001469\n",
      "Iteration 40, loss = 0.50979877\n",
      "Iteration 41, loss = 0.50976423\n",
      "Iteration 42, loss = 0.50956682\n",
      "Iteration 43, loss = 0.50929090\n",
      "Iteration 44, loss = 0.50904772\n",
      "Iteration 45, loss = 0.50876794\n",
      "Iteration 46, loss = 0.50853885\n",
      "Iteration 47, loss = 0.50824551\n",
      "Iteration 48, loss = 0.50803139\n",
      "Iteration 49, loss = 0.50785721\n",
      "Iteration 50, loss = 0.50770501\n",
      "Iteration 51, loss = 0.50755206\n",
      "Iteration 52, loss = 0.50740700\n",
      "Iteration 53, loss = 0.50734074\n",
      "Iteration 54, loss = 0.50716921\n",
      "Iteration 55, loss = 0.50706051\n",
      "Iteration 56, loss = 0.50688456\n",
      "Iteration 57, loss = 0.50675672\n",
      "Iteration 58, loss = 0.50659887\n",
      "Iteration 59, loss = 0.50641877\n",
      "Iteration 60, loss = 0.50623692\n",
      "Iteration 61, loss = 0.50605393\n",
      "Iteration 62, loss = 0.50583163\n",
      "Iteration 63, loss = 0.50567619\n",
      "Iteration 64, loss = 0.50553124\n",
      "Iteration 65, loss = 0.50530102\n",
      "Iteration 66, loss = 0.50536563\n",
      "Iteration 67, loss = 0.50530601\n",
      "Iteration 68, loss = 0.50524036\n",
      "Iteration 69, loss = 0.50527452\n",
      "Iteration 70, loss = 0.50521443\n",
      "Iteration 71, loss = 0.50500463\n",
      "Iteration 72, loss = 0.50487672\n",
      "Iteration 73, loss = 0.50474574\n",
      "Iteration 74, loss = 0.50447671\n",
      "Iteration 75, loss = 0.50438534\n",
      "Iteration 76, loss = 0.50424998\n",
      "Iteration 77, loss = 0.50411832\n",
      "Iteration 78, loss = 0.50402419\n",
      "Iteration 79, loss = 0.50393417\n",
      "Iteration 80, loss = 0.50394873\n",
      "Iteration 81, loss = 0.50394726\n",
      "Iteration 82, loss = 0.50393706\n",
      "Iteration 83, loss = 0.50382908\n",
      "Iteration 84, loss = 0.50372198\n",
      "Iteration 85, loss = 0.50375630\n",
      "Iteration 86, loss = 0.50355410\n",
      "Iteration 87, loss = 0.50345584\n",
      "Iteration 88, loss = 0.50335427\n",
      "Iteration 89, loss = 0.50320233\n",
      "Iteration 90, loss = 0.50303611\n",
      "Iteration 91, loss = 0.50296154\n",
      "Iteration 92, loss = 0.50289717\n",
      "Iteration 93, loss = 0.50288211\n",
      "Iteration 94, loss = 0.50259368\n",
      "Iteration 95, loss = 0.50240325\n",
      "Iteration 96, loss = 0.50226570\n",
      "Iteration 97, loss = 0.50212473\n",
      "Iteration 98, loss = 0.50217358\n",
      "Iteration 99, loss = 0.50220969\n",
      "Iteration 100, loss = 0.50213765\n",
      "Iteration 101, loss = 0.50205455\n",
      "Iteration 102, loss = 0.50201589\n",
      "Iteration 103, loss = 0.50194746\n",
      "Iteration 104, loss = 0.50212552\n",
      "Iteration 105, loss = 0.50209495\n",
      "Iteration 106, loss = 0.50201021\n",
      "Iteration 107, loss = 0.50191583\n",
      "Iteration 108, loss = 0.50172810\n",
      "Iteration 109, loss = 0.50160015\n",
      "Iteration 110, loss = 0.50145035\n",
      "Iteration 111, loss = 0.50120789\n",
      "Iteration 112, loss = 0.50094924\n",
      "Iteration 113, loss = 0.50119081\n",
      "Iteration 114, loss = 0.50080535\n",
      "Iteration 115, loss = 0.50064607\n",
      "Iteration 116, loss = 0.50062458\n",
      "Iteration 117, loss = 0.50054128\n",
      "Iteration 118, loss = 0.50040645\n",
      "Iteration 119, loss = 0.50048151\n",
      "Iteration 120, loss = 0.50044544\n",
      "Iteration 121, loss = 0.50053798\n",
      "Iteration 122, loss = 0.50040651\n",
      "Iteration 123, loss = 0.50035171\n",
      "Iteration 124, loss = 0.50020831\n",
      "Iteration 125, loss = 0.50013404\n",
      "Iteration 126, loss = 0.50004015\n",
      "Iteration 127, loss = 0.49986867\n",
      "Iteration 128, loss = 0.49981920\n",
      "Iteration 129, loss = 0.49981271\n",
      "Iteration 130, loss = 0.49978669\n",
      "Iteration 131, loss = 0.49970879\n",
      "Iteration 132, loss = 0.49961539\n",
      "Iteration 133, loss = 0.49952333\n",
      "Iteration 134, loss = 0.49938642\n",
      "Iteration 135, loss = 0.49914969\n",
      "Iteration 136, loss = 0.49904761\n",
      "Iteration 137, loss = 0.49894029\n",
      "Iteration 138, loss = 0.49881142\n",
      "Iteration 139, loss = 0.49871754\n",
      "Iteration 140, loss = 0.49863785\n",
      "Iteration 141, loss = 0.49853796\n",
      "Iteration 142, loss = 0.49850273\n",
      "Iteration 143, loss = 0.49844114\n",
      "Iteration 144, loss = 0.49849299\n",
      "Iteration 145, loss = 0.49847196\n",
      "Iteration 146, loss = 0.49843519\n",
      "Iteration 147, loss = 0.49837189\n",
      "Iteration 148, loss = 0.49829543\n",
      "Iteration 149, loss = 0.49831864\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.76436592\n",
      "Iteration 2, loss = 0.74835978\n",
      "Iteration 3, loss = 0.73513703\n",
      "Iteration 4, loss = 0.72097257\n",
      "Iteration 5, loss = 0.70889717\n",
      "Iteration 6, loss = 0.69677456\n",
      "Iteration 7, loss = 0.68544419\n",
      "Iteration 8, loss = 0.67479104\n",
      "Iteration 9, loss = 0.66436088\n",
      "Iteration 10, loss = 0.65471988\n",
      "Iteration 11, loss = 0.64570429\n",
      "Iteration 12, loss = 0.63695933\n",
      "Iteration 13, loss = 0.62883766\n",
      "Iteration 14, loss = 0.62131234\n",
      "Iteration 15, loss = 0.61441516\n",
      "Iteration 16, loss = 0.60757081\n",
      "Iteration 17, loss = 0.60151982\n",
      "Iteration 18, loss = 0.59579622\n",
      "Iteration 19, loss = 0.59061413\n",
      "Iteration 20, loss = 0.58593413\n",
      "Iteration 21, loss = 0.58110529\n",
      "Iteration 22, loss = 0.57720474\n",
      "Iteration 23, loss = 0.57299436\n",
      "Iteration 24, loss = 0.56935303\n",
      "Iteration 25, loss = 0.56607785\n",
      "Iteration 26, loss = 0.56295486\n",
      "Iteration 27, loss = 0.55996058\n",
      "Iteration 28, loss = 0.55726743\n",
      "Iteration 29, loss = 0.55473644\n",
      "Iteration 30, loss = 0.55264974\n",
      "Iteration 31, loss = 0.55085321\n",
      "Iteration 32, loss = 0.54873781\n",
      "Iteration 33, loss = 0.54755512\n",
      "Iteration 34, loss = 0.54608384\n",
      "Iteration 35, loss = 0.54476133\n",
      "Iteration 36, loss = 0.54378052\n",
      "Iteration 37, loss = 0.54284411\n",
      "Iteration 38, loss = 0.54177064\n",
      "Iteration 39, loss = 0.54107451\n",
      "Iteration 40, loss = 0.54024369\n",
      "Iteration 41, loss = 0.53941636\n",
      "Iteration 42, loss = 0.53887679\n",
      "Iteration 43, loss = 0.53822792\n",
      "Iteration 44, loss = 0.53772265\n",
      "Iteration 45, loss = 0.53722142\n",
      "Iteration 46, loss = 0.53684748\n",
      "Iteration 47, loss = 0.53641916\n",
      "Iteration 48, loss = 0.53618851\n",
      "Iteration 49, loss = 0.53590322\n",
      "Iteration 50, loss = 0.53560516\n",
      "Iteration 51, loss = 0.53541968\n",
      "Iteration 52, loss = 0.53519790\n",
      "Iteration 53, loss = 0.53490771\n",
      "Iteration 54, loss = 0.53467845\n",
      "Iteration 55, loss = 0.53460209\n",
      "Iteration 56, loss = 0.53436288\n",
      "Iteration 57, loss = 0.53423890\n",
      "Iteration 58, loss = 0.53406370\n",
      "Iteration 59, loss = 0.53396155\n",
      "Iteration 60, loss = 0.53395283\n",
      "Iteration 61, loss = 0.53378201\n",
      "Iteration 62, loss = 0.53367847\n",
      "Iteration 63, loss = 0.53353922\n",
      "Iteration 64, loss = 0.53342581\n",
      "Iteration 65, loss = 0.53337137\n",
      "Iteration 66, loss = 0.53330488\n",
      "Iteration 67, loss = 0.53319408\n",
      "Iteration 68, loss = 0.53314838\n",
      "Iteration 69, loss = 0.53316711\n",
      "Iteration 70, loss = 0.53313354\n",
      "Iteration 71, loss = 0.53327432\n",
      "Iteration 72, loss = 0.53327781\n",
      "Iteration 73, loss = 0.53324743\n",
      "Iteration 74, loss = 0.53317586\n",
      "Iteration 75, loss = 0.53309447\n",
      "Iteration 76, loss = 0.53303788\n",
      "Iteration 77, loss = 0.53283957\n",
      "Iteration 78, loss = 0.53268418\n",
      "Iteration 79, loss = 0.53254499\n",
      "Iteration 80, loss = 0.53244468\n",
      "Iteration 81, loss = 0.53226828\n",
      "Iteration 82, loss = 0.53217589\n",
      "Iteration 83, loss = 0.53212265\n",
      "Iteration 84, loss = 0.53201713\n",
      "Iteration 85, loss = 0.53194125\n",
      "Iteration 86, loss = 0.53197372\n",
      "Iteration 87, loss = 0.53186896\n",
      "Iteration 88, loss = 0.53171087\n",
      "Iteration 89, loss = 0.53160009\n",
      "Iteration 90, loss = 0.53143462\n",
      "Iteration 91, loss = 0.53143979\n",
      "Iteration 92, loss = 0.53130799\n",
      "Iteration 93, loss = 0.53120627\n",
      "Iteration 94, loss = 0.53110227\n",
      "Iteration 95, loss = 0.53112618\n",
      "Iteration 96, loss = 0.53110294\n",
      "Iteration 97, loss = 0.53109489\n",
      "Iteration 98, loss = 0.53107290\n",
      "Iteration 99, loss = 0.53116540\n",
      "Iteration 100, loss = 0.53105573\n",
      "Iteration 101, loss = 0.53096417\n",
      "Iteration 102, loss = 0.53089284\n",
      "Iteration 103, loss = 0.53077410\n",
      "Iteration 104, loss = 0.53074713\n",
      "Iteration 105, loss = 0.53066835\n",
      "Iteration 106, loss = 0.53059577\n",
      "Iteration 107, loss = 0.53056271\n",
      "Iteration 108, loss = 0.53043839\n",
      "Iteration 109, loss = 0.53039352\n",
      "Iteration 110, loss = 0.53023103\n",
      "Iteration 111, loss = 0.53013574\n",
      "Iteration 112, loss = 0.52998498\n",
      "Iteration 113, loss = 0.52995208\n",
      "Iteration 114, loss = 0.52983262\n",
      "Iteration 115, loss = 0.52968186\n",
      "Iteration 116, loss = 0.52965402\n",
      "Iteration 117, loss = 0.52955217\n",
      "Iteration 118, loss = 0.52946467\n",
      "Iteration 119, loss = 0.52938857\n",
      "Iteration 120, loss = 0.52930908\n",
      "Iteration 121, loss = 0.52922894\n",
      "Iteration 122, loss = 0.52917056\n",
      "Iteration 123, loss = 0.52908726\n",
      "Iteration 124, loss = 0.52902662\n",
      "Iteration 125, loss = 0.52895807\n",
      "Iteration 126, loss = 0.52886851\n",
      "Iteration 127, loss = 0.52880113\n",
      "Iteration 128, loss = 0.52874149\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68030949\n",
      "Iteration 2, loss = 0.66648498\n",
      "Iteration 3, loss = 0.65375587\n",
      "Iteration 4, loss = 0.64152003\n",
      "Iteration 5, loss = 0.63056975\n",
      "Iteration 6, loss = 0.62032733\n",
      "Iteration 7, loss = 0.61059608\n",
      "Iteration 8, loss = 0.60177989\n",
      "Iteration 9, loss = 0.59361818\n",
      "Iteration 10, loss = 0.58626130\n",
      "Iteration 11, loss = 0.57949087\n",
      "Iteration 12, loss = 0.57316176\n",
      "Iteration 13, loss = 0.56776020\n",
      "Iteration 14, loss = 0.56287063\n",
      "Iteration 15, loss = 0.55839364\n",
      "Iteration 16, loss = 0.55438643\n",
      "Iteration 17, loss = 0.55095067\n",
      "Iteration 18, loss = 0.54791335\n",
      "Iteration 19, loss = 0.54523060\n",
      "Iteration 20, loss = 0.54306404\n",
      "Iteration 21, loss = 0.54140718\n",
      "Iteration 22, loss = 0.53923841\n",
      "Iteration 23, loss = 0.53816380\n",
      "Iteration 24, loss = 0.53678286\n",
      "Iteration 25, loss = 0.53593945\n",
      "Iteration 26, loss = 0.53525273\n",
      "Iteration 27, loss = 0.53461114\n",
      "Iteration 28, loss = 0.53396229\n",
      "Iteration 29, loss = 0.53344356\n",
      "Iteration 30, loss = 0.53303154\n",
      "Iteration 31, loss = 0.53280404\n",
      "Iteration 32, loss = 0.53230027\n",
      "Iteration 33, loss = 0.53202342\n",
      "Iteration 34, loss = 0.53176218\n",
      "Iteration 35, loss = 0.53163774\n",
      "Iteration 36, loss = 0.53156831\n",
      "Iteration 37, loss = 0.53137351\n",
      "Iteration 38, loss = 0.53121797\n",
      "Iteration 39, loss = 0.53116531\n",
      "Iteration 40, loss = 0.53093554\n",
      "Iteration 41, loss = 0.53070175\n",
      "Iteration 42, loss = 0.53052557\n",
      "Iteration 43, loss = 0.53032930\n",
      "Iteration 44, loss = 0.53005544\n",
      "Iteration 45, loss = 0.52991387\n",
      "Iteration 46, loss = 0.52969417\n",
      "Iteration 47, loss = 0.52943922\n",
      "Iteration 48, loss = 0.52926575\n",
      "Iteration 49, loss = 0.52934988\n",
      "Iteration 50, loss = 0.52907795\n",
      "Iteration 51, loss = 0.52900207\n",
      "Iteration 52, loss = 0.52885721\n",
      "Iteration 53, loss = 0.52877799\n",
      "Iteration 54, loss = 0.52857867\n",
      "Iteration 55, loss = 0.52854165\n",
      "Iteration 56, loss = 0.52847484\n",
      "Iteration 57, loss = 0.52841630\n",
      "Iteration 58, loss = 0.52834447\n",
      "Iteration 59, loss = 0.52832458\n",
      "Iteration 60, loss = 0.52813969\n",
      "Iteration 61, loss = 0.52810365\n",
      "Iteration 62, loss = 0.52784217\n",
      "Iteration 63, loss = 0.52770106\n",
      "Iteration 64, loss = 0.52763270\n",
      "Iteration 65, loss = 0.52744357\n",
      "Iteration 66, loss = 0.52736401\n",
      "Iteration 67, loss = 0.52731160\n",
      "Iteration 68, loss = 0.52727747\n",
      "Iteration 69, loss = 0.52712902\n",
      "Iteration 70, loss = 0.52700336\n",
      "Iteration 71, loss = 0.52686589\n",
      "Iteration 72, loss = 0.52677610\n",
      "Iteration 73, loss = 0.52661055\n",
      "Iteration 74, loss = 0.52663756\n",
      "Iteration 75, loss = 0.52647985\n",
      "Iteration 76, loss = 0.52632785\n",
      "Iteration 77, loss = 0.52625342\n",
      "Iteration 78, loss = 0.52615524\n",
      "Iteration 79, loss = 0.52605905\n",
      "Iteration 80, loss = 0.52605107\n",
      "Iteration 81, loss = 0.52596682\n",
      "Iteration 82, loss = 0.52606193\n",
      "Iteration 83, loss = 0.52593966\n",
      "Iteration 84, loss = 0.52589555\n",
      "Iteration 85, loss = 0.52586278\n",
      "Iteration 86, loss = 0.52579513\n",
      "Iteration 87, loss = 0.52573738\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66650455\n",
      "Iteration 2, loss = 0.65331743\n",
      "Iteration 3, loss = 0.64167439\n",
      "Iteration 4, loss = 0.63026053\n",
      "Iteration 5, loss = 0.61998886\n",
      "Iteration 6, loss = 0.60973009\n",
      "Iteration 7, loss = 0.60097268\n",
      "Iteration 8, loss = 0.59200194\n",
      "Iteration 9, loss = 0.58458589\n",
      "Iteration 10, loss = 0.57732953\n",
      "Iteration 11, loss = 0.57091565\n",
      "Iteration 12, loss = 0.56442668\n",
      "Iteration 13, loss = 0.55971194\n",
      "Iteration 14, loss = 0.55461667\n",
      "Iteration 15, loss = 0.55054441\n",
      "Iteration 16, loss = 0.54627552\n",
      "Iteration 17, loss = 0.54307221\n",
      "Iteration 18, loss = 0.53995969\n",
      "Iteration 19, loss = 0.53715996\n",
      "Iteration 20, loss = 0.53464316\n",
      "Iteration 21, loss = 0.53227226\n",
      "Iteration 22, loss = 0.53040686\n",
      "Iteration 23, loss = 0.52842492\n",
      "Iteration 24, loss = 0.52691286\n",
      "Iteration 25, loss = 0.52545465\n",
      "Iteration 26, loss = 0.52425975\n",
      "Iteration 27, loss = 0.52307103\n",
      "Iteration 28, loss = 0.52224244\n",
      "Iteration 29, loss = 0.52136884\n",
      "Iteration 30, loss = 0.52095083\n",
      "Iteration 31, loss = 0.52038516\n",
      "Iteration 32, loss = 0.51996007\n",
      "Iteration 33, loss = 0.51945837\n",
      "Iteration 34, loss = 0.51931268\n",
      "Iteration 35, loss = 0.51891286\n",
      "Iteration 36, loss = 0.51859652\n",
      "Iteration 37, loss = 0.51815244\n",
      "Iteration 38, loss = 0.51786166\n",
      "Iteration 39, loss = 0.51755247\n",
      "Iteration 40, loss = 0.51731681\n",
      "Iteration 41, loss = 0.51713943\n",
      "Iteration 42, loss = 0.51689484\n",
      "Iteration 43, loss = 0.51669691\n",
      "Iteration 44, loss = 0.51655359\n",
      "Iteration 45, loss = 0.51652307\n",
      "Iteration 46, loss = 0.51649323\n",
      "Iteration 47, loss = 0.51634010\n",
      "Iteration 48, loss = 0.51651025\n",
      "Iteration 49, loss = 0.51616020\n",
      "Iteration 50, loss = 0.51590704\n",
      "Iteration 51, loss = 0.51570931\n",
      "Iteration 52, loss = 0.51553628\n",
      "Iteration 53, loss = 0.51525010\n",
      "Iteration 54, loss = 0.51514186\n",
      "Iteration 55, loss = 0.51498476\n",
      "Iteration 56, loss = 0.51477016\n",
      "Iteration 57, loss = 0.51463463\n",
      "Iteration 58, loss = 0.51455390\n",
      "Iteration 59, loss = 0.51435419\n",
      "Iteration 60, loss = 0.51426175\n",
      "Iteration 61, loss = 0.51407777\n",
      "Iteration 62, loss = 0.51395832\n",
      "Iteration 63, loss = 0.51407985\n",
      "Iteration 64, loss = 0.51388028\n",
      "Iteration 65, loss = 0.51376441\n",
      "Iteration 66, loss = 0.51364688\n",
      "Iteration 67, loss = 0.51349784\n",
      "Iteration 68, loss = 0.51336595\n",
      "Iteration 69, loss = 0.51323780\n",
      "Iteration 70, loss = 0.51308818\n",
      "Iteration 71, loss = 0.51291144\n",
      "Iteration 72, loss = 0.51279775\n",
      "Iteration 73, loss = 0.51253876\n",
      "Iteration 74, loss = 0.51249273\n",
      "Iteration 75, loss = 0.51238901\n",
      "Iteration 76, loss = 0.51234961\n",
      "Iteration 77, loss = 0.51237094\n",
      "Iteration 78, loss = 0.51223466\n",
      "Iteration 79, loss = 0.51206084\n",
      "Iteration 80, loss = 0.51206033\n",
      "Iteration 81, loss = 0.51183420\n",
      "Iteration 82, loss = 0.51182030\n",
      "Iteration 83, loss = 0.51176648\n",
      "Iteration 84, loss = 0.51170559\n",
      "Iteration 85, loss = 0.51168480\n",
      "Iteration 86, loss = 0.51157668\n",
      "Iteration 87, loss = 0.51145475\n",
      "Iteration 88, loss = 0.51127815\n",
      "Iteration 89, loss = 0.51127976\n",
      "Iteration 90, loss = 0.51108440\n",
      "Iteration 91, loss = 0.51093999\n",
      "Iteration 92, loss = 0.51099981\n",
      "Iteration 93, loss = 0.51071348\n",
      "Iteration 94, loss = 0.51060947\n",
      "Iteration 95, loss = 0.51061409\n",
      "Iteration 96, loss = 0.51059983\n",
      "Iteration 97, loss = 0.51070545\n",
      "Iteration 98, loss = 0.51070710\n",
      "Iteration 99, loss = 0.51060543\n",
      "Iteration 100, loss = 0.51047009\n",
      "Iteration 101, loss = 0.51037931\n",
      "Iteration 102, loss = 0.51027340\n",
      "Iteration 103, loss = 0.51021098\n",
      "Iteration 104, loss = 0.51008646\n",
      "Iteration 105, loss = 0.50998647\n",
      "Iteration 106, loss = 0.50992602\n",
      "Iteration 107, loss = 0.50974957\n",
      "Iteration 108, loss = 0.50966216\n",
      "Iteration 109, loss = 0.50958248\n",
      "Iteration 110, loss = 0.50938041\n",
      "Iteration 111, loss = 0.50935715\n",
      "Iteration 112, loss = 0.50916988\n",
      "Iteration 113, loss = 0.50906766\n",
      "Iteration 114, loss = 0.50906940\n",
      "Iteration 115, loss = 0.50899436\n",
      "Iteration 116, loss = 0.50888894\n",
      "Iteration 117, loss = 0.50895292\n",
      "Iteration 118, loss = 0.50882719\n",
      "Iteration 119, loss = 0.50878664\n",
      "Iteration 120, loss = 0.50871699\n",
      "Iteration 121, loss = 0.50864300\n",
      "Iteration 122, loss = 0.50851693\n",
      "Iteration 123, loss = 0.50839891\n",
      "Iteration 124, loss = 0.50831805\n",
      "Iteration 125, loss = 0.50826825\n",
      "Iteration 126, loss = 0.50832510\n",
      "Iteration 127, loss = 0.50829081\n",
      "Iteration 128, loss = 0.50833537\n",
      "Iteration 129, loss = 0.50816675\n",
      "Iteration 130, loss = 0.50803203\n",
      "Iteration 131, loss = 0.50784936\n",
      "Iteration 132, loss = 0.50775490\n",
      "Iteration 133, loss = 0.50767433\n",
      "Iteration 134, loss = 0.50748934\n",
      "Iteration 135, loss = 0.50735090\n",
      "Iteration 136, loss = 0.50719759\n",
      "Iteration 137, loss = 0.50720912\n",
      "Iteration 138, loss = 0.50735281\n",
      "Iteration 139, loss = 0.50725409\n",
      "Iteration 140, loss = 0.50716702\n",
      "Iteration 141, loss = 0.50714713\n",
      "Iteration 142, loss = 0.50712564\n",
      "Iteration 143, loss = 0.50705526\n",
      "Iteration 144, loss = 0.50696277\n",
      "Iteration 145, loss = 0.50686856\n",
      "Iteration 146, loss = 0.50682566\n",
      "Iteration 147, loss = 0.50677436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70725064\n",
      "Iteration 2, loss = 0.69188482\n",
      "Iteration 3, loss = 0.67761379\n",
      "Iteration 4, loss = 0.66465938\n",
      "Iteration 5, loss = 0.65171629\n",
      "Iteration 6, loss = 0.63978074\n",
      "Iteration 7, loss = 0.62951127\n",
      "Iteration 8, loss = 0.61930721\n",
      "Iteration 9, loss = 0.61011472\n",
      "Iteration 10, loss = 0.60185532\n",
      "Iteration 11, loss = 0.59411093\n",
      "Iteration 12, loss = 0.58754187\n",
      "Iteration 13, loss = 0.58119249\n",
      "Iteration 14, loss = 0.57546862\n",
      "Iteration 15, loss = 0.57071340\n",
      "Iteration 16, loss = 0.56578317\n",
      "Iteration 17, loss = 0.56179853\n",
      "Iteration 18, loss = 0.55799003\n",
      "Iteration 19, loss = 0.55454482\n",
      "Iteration 20, loss = 0.55172080\n",
      "Iteration 21, loss = 0.54925588\n",
      "Iteration 22, loss = 0.54660080\n",
      "Iteration 23, loss = 0.54482030\n",
      "Iteration 24, loss = 0.54342123\n",
      "Iteration 25, loss = 0.54205734\n",
      "Iteration 26, loss = 0.54083148\n",
      "Iteration 27, loss = 0.53985801\n",
      "Iteration 28, loss = 0.53892890\n",
      "Iteration 29, loss = 0.53828080\n",
      "Iteration 30, loss = 0.53737394\n",
      "Iteration 31, loss = 0.53692333\n",
      "Iteration 32, loss = 0.53651901\n",
      "Iteration 33, loss = 0.53623810\n",
      "Iteration 34, loss = 0.53586365\n",
      "Iteration 35, loss = 0.53564208\n",
      "Iteration 36, loss = 0.53549925\n",
      "Iteration 37, loss = 0.53550946\n",
      "Iteration 38, loss = 0.53540320\n",
      "Iteration 39, loss = 0.53527984\n",
      "Iteration 40, loss = 0.53528729\n",
      "Iteration 41, loss = 0.53517462\n",
      "Iteration 42, loss = 0.53543169\n",
      "Iteration 43, loss = 0.53527398\n",
      "Iteration 44, loss = 0.53527747\n",
      "Iteration 45, loss = 0.53520523\n",
      "Iteration 46, loss = 0.53510932\n",
      "Iteration 47, loss = 0.53497052\n",
      "Iteration 48, loss = 0.53494302\n",
      "Iteration 49, loss = 0.53478425\n",
      "Iteration 50, loss = 0.53456601\n",
      "Iteration 51, loss = 0.53454673\n",
      "Iteration 52, loss = 0.53431150\n",
      "Iteration 53, loss = 0.53415366\n",
      "Iteration 54, loss = 0.53401477\n",
      "Iteration 55, loss = 0.53389848\n",
      "Iteration 56, loss = 0.53373668\n",
      "Iteration 57, loss = 0.53361052\n",
      "Iteration 58, loss = 0.53351746\n",
      "Iteration 59, loss = 0.53345354\n",
      "Iteration 60, loss = 0.53334503\n",
      "Iteration 61, loss = 0.53323884\n",
      "Iteration 62, loss = 0.53317372\n",
      "Iteration 63, loss = 0.53310571\n",
      "Iteration 64, loss = 0.53296062\n",
      "Iteration 65, loss = 0.53309236\n",
      "Iteration 66, loss = 0.53285792\n",
      "Iteration 67, loss = 0.53275092\n",
      "Iteration 68, loss = 0.53265237\n",
      "Iteration 69, loss = 0.53266722\n",
      "Iteration 70, loss = 0.53258559\n",
      "Iteration 71, loss = 0.53260965\n",
      "Iteration 72, loss = 0.53253400\n",
      "Iteration 73, loss = 0.53242765\n",
      "Iteration 74, loss = 0.53245328\n",
      "Iteration 75, loss = 0.53237498\n",
      "Iteration 76, loss = 0.53235522\n",
      "Iteration 77, loss = 0.53227135\n",
      "Iteration 78, loss = 0.53220777\n",
      "Iteration 79, loss = 0.53214908\n",
      "Iteration 80, loss = 0.53202092\n",
      "Iteration 81, loss = 0.53192834\n",
      "Iteration 82, loss = 0.53185075\n",
      "Iteration 83, loss = 0.53180701\n",
      "Iteration 84, loss = 0.53176711\n",
      "Iteration 85, loss = 0.53177137\n",
      "Iteration 86, loss = 0.53176946\n",
      "Iteration 87, loss = 0.53179369\n",
      "Iteration 88, loss = 0.53162985\n",
      "Iteration 89, loss = 0.53165019\n",
      "Iteration 90, loss = 0.53153536\n",
      "Iteration 91, loss = 0.53149401\n",
      "Iteration 92, loss = 0.53147726\n",
      "Iteration 93, loss = 0.53135718\n",
      "Iteration 94, loss = 0.53131612\n",
      "Iteration 95, loss = 0.53121994\n",
      "Iteration 96, loss = 0.53108856\n",
      "Iteration 97, loss = 0.53097881\n",
      "Iteration 98, loss = 0.53093568\n",
      "Iteration 99, loss = 0.53096668\n",
      "Iteration 100, loss = 0.53077406\n",
      "Iteration 101, loss = 0.53089734\n",
      "Iteration 102, loss = 0.53074830\n",
      "Iteration 103, loss = 0.53063004\n",
      "Iteration 104, loss = 0.53063054\n",
      "Iteration 105, loss = 0.53062705\n",
      "Iteration 106, loss = 0.53061628\n",
      "Iteration 107, loss = 0.53061081\n",
      "Iteration 108, loss = 0.53060322\n",
      "Iteration 109, loss = 0.53039339\n",
      "Iteration 110, loss = 0.53037758\n",
      "Iteration 111, loss = 0.53029478\n",
      "Iteration 112, loss = 0.53018642\n",
      "Iteration 113, loss = 0.53017242\n",
      "Iteration 114, loss = 0.53009379\n",
      "Iteration 115, loss = 0.53011394\n",
      "Iteration 116, loss = 0.53007287\n",
      "Iteration 117, loss = 0.53009942\n",
      "Iteration 118, loss = 0.53007017\n",
      "Iteration 119, loss = 0.53007637\n",
      "Iteration 120, loss = 0.53010278\n",
      "Iteration 121, loss = 0.53002702\n",
      "Iteration 122, loss = 0.52992484\n",
      "Iteration 123, loss = 0.52981144\n",
      "Iteration 124, loss = 0.52989790\n",
      "Iteration 125, loss = 0.52981187\n",
      "Iteration 126, loss = 0.52978400\n",
      "Iteration 127, loss = 0.52970255\n",
      "Iteration 128, loss = 0.52981505\n",
      "Iteration 129, loss = 0.52980028\n",
      "Iteration 130, loss = 0.52977808\n",
      "Iteration 131, loss = 0.52984579\n",
      "Iteration 132, loss = 0.52982344\n",
      "Iteration 133, loss = 0.52985672\n",
      "Iteration 134, loss = 0.52961572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.75807787\n",
      "Iteration 2, loss = 0.73942611\n",
      "Iteration 3, loss = 0.72294849\n",
      "Iteration 4, loss = 0.70658964\n",
      "Iteration 5, loss = 0.69165004\n",
      "Iteration 6, loss = 0.67788580\n",
      "Iteration 7, loss = 0.66495298\n",
      "Iteration 8, loss = 0.65235171\n",
      "Iteration 9, loss = 0.64161256\n",
      "Iteration 10, loss = 0.63075480\n",
      "Iteration 11, loss = 0.62129571\n",
      "Iteration 12, loss = 0.61271289\n",
      "Iteration 13, loss = 0.60470568\n",
      "Iteration 14, loss = 0.59718839\n",
      "Iteration 15, loss = 0.59042925\n",
      "Iteration 16, loss = 0.58442830\n",
      "Iteration 17, loss = 0.57954297\n",
      "Iteration 18, loss = 0.57444907\n",
      "Iteration 19, loss = 0.56968728\n",
      "Iteration 20, loss = 0.56643036\n",
      "Iteration 21, loss = 0.56292189\n",
      "Iteration 22, loss = 0.56021875\n",
      "Iteration 23, loss = 0.55756609\n",
      "Iteration 24, loss = 0.55529316\n",
      "Iteration 25, loss = 0.55340826\n",
      "Iteration 26, loss = 0.55190223\n",
      "Iteration 27, loss = 0.55041107\n",
      "Iteration 28, loss = 0.54931052\n",
      "Iteration 29, loss = 0.54805290\n",
      "Iteration 30, loss = 0.54750570\n",
      "Iteration 31, loss = 0.54646515\n",
      "Iteration 32, loss = 0.54588728\n",
      "Iteration 33, loss = 0.54556201\n",
      "Iteration 34, loss = 0.54493302\n",
      "Iteration 35, loss = 0.54447424\n",
      "Iteration 36, loss = 0.54428289\n",
      "Iteration 37, loss = 0.54388444\n",
      "Iteration 38, loss = 0.54360274\n",
      "Iteration 39, loss = 0.54330792\n",
      "Iteration 40, loss = 0.54298515\n",
      "Iteration 41, loss = 0.54277804\n",
      "Iteration 42, loss = 0.54244407\n",
      "Iteration 43, loss = 0.54219229\n",
      "Iteration 44, loss = 0.54185164\n",
      "Iteration 45, loss = 0.54164385\n",
      "Iteration 46, loss = 0.54145003\n",
      "Iteration 47, loss = 0.54115012\n",
      "Iteration 48, loss = 0.54090016\n",
      "Iteration 49, loss = 0.54075944\n",
      "Iteration 50, loss = 0.54048903\n",
      "Iteration 51, loss = 0.54045059\n",
      "Iteration 52, loss = 0.54013953\n",
      "Iteration 53, loss = 0.53995655\n",
      "Iteration 54, loss = 0.53974283\n",
      "Iteration 55, loss = 0.53961064\n",
      "Iteration 56, loss = 0.53936325\n",
      "Iteration 57, loss = 0.53950753\n",
      "Iteration 58, loss = 0.53924558\n",
      "Iteration 59, loss = 0.53912784\n",
      "Iteration 60, loss = 0.53891773\n",
      "Iteration 61, loss = 0.53873525\n",
      "Iteration 62, loss = 0.53855739\n",
      "Iteration 63, loss = 0.53832705\n",
      "Iteration 64, loss = 0.53812922\n",
      "Iteration 65, loss = 0.53803128\n",
      "Iteration 66, loss = 0.53798762\n",
      "Iteration 67, loss = 0.53780210\n",
      "Iteration 68, loss = 0.53770479\n",
      "Iteration 69, loss = 0.53753520\n",
      "Iteration 70, loss = 0.53743760\n",
      "Iteration 71, loss = 0.53739610\n",
      "Iteration 72, loss = 0.53710194\n",
      "Iteration 73, loss = 0.53698491\n",
      "Iteration 74, loss = 0.53686928\n",
      "Iteration 75, loss = 0.53679099\n",
      "Iteration 76, loss = 0.53659700\n",
      "Iteration 77, loss = 0.53647747\n",
      "Iteration 78, loss = 0.53632291\n",
      "Iteration 79, loss = 0.53619695\n",
      "Iteration 80, loss = 0.53611961\n",
      "Iteration 81, loss = 0.53599850\n",
      "Iteration 82, loss = 0.53588518\n",
      "Iteration 83, loss = 0.53581856\n",
      "Iteration 84, loss = 0.53569054\n",
      "Iteration 85, loss = 0.53560539\n",
      "Iteration 86, loss = 0.53547950\n",
      "Iteration 87, loss = 0.53535375\n",
      "Iteration 88, loss = 0.53522683\n",
      "Iteration 89, loss = 0.53515382\n",
      "Iteration 90, loss = 0.53500090\n",
      "Iteration 91, loss = 0.53488870\n",
      "Iteration 92, loss = 0.53473793\n",
      "Iteration 93, loss = 0.53484449\n",
      "Iteration 94, loss = 0.53464575\n",
      "Iteration 95, loss = 0.53463203\n",
      "Iteration 96, loss = 0.53444667\n",
      "Iteration 97, loss = 0.53424867\n",
      "Iteration 98, loss = 0.53412604\n",
      "Iteration 99, loss = 0.53417330\n",
      "Iteration 100, loss = 0.53401904\n",
      "Iteration 101, loss = 0.53398606\n",
      "Iteration 102, loss = 0.53407450\n",
      "Iteration 103, loss = 0.53386877\n",
      "Iteration 104, loss = 0.53376484\n",
      "Iteration 105, loss = 0.53367923\n",
      "Iteration 106, loss = 0.53362242\n",
      "Iteration 107, loss = 0.53356748\n",
      "Iteration 108, loss = 0.53346029\n",
      "Iteration 109, loss = 0.53329718\n",
      "Iteration 110, loss = 0.53325872\n",
      "Iteration 111, loss = 0.53316315\n",
      "Iteration 112, loss = 0.53334354\n",
      "Iteration 113, loss = 0.53329162\n",
      "Iteration 114, loss = 0.53330041\n",
      "Iteration 115, loss = 0.53328332\n",
      "Iteration 116, loss = 0.53332083\n",
      "Iteration 117, loss = 0.53330157\n",
      "Iteration 118, loss = 0.53336907\n",
      "Iteration 119, loss = 0.53327978\n",
      "Iteration 120, loss = 0.53329091\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73897186\n",
      "Iteration 2, loss = 0.71764287\n",
      "Iteration 3, loss = 0.69842347\n",
      "Iteration 4, loss = 0.68032218\n",
      "Iteration 5, loss = 0.66332120\n",
      "Iteration 6, loss = 0.64835057\n",
      "Iteration 7, loss = 0.63358184\n",
      "Iteration 8, loss = 0.62099433\n",
      "Iteration 9, loss = 0.60949618\n",
      "Iteration 10, loss = 0.59813607\n",
      "Iteration 11, loss = 0.58768918\n",
      "Iteration 12, loss = 0.57911721\n",
      "Iteration 13, loss = 0.57094769\n",
      "Iteration 14, loss = 0.56303590\n",
      "Iteration 15, loss = 0.55631049\n",
      "Iteration 16, loss = 0.55051475\n",
      "Iteration 17, loss = 0.54514081\n",
      "Iteration 18, loss = 0.54035888\n",
      "Iteration 19, loss = 0.53586862\n",
      "Iteration 20, loss = 0.53255713\n",
      "Iteration 21, loss = 0.52924686\n",
      "Iteration 22, loss = 0.52650038\n",
      "Iteration 23, loss = 0.52436161\n",
      "Iteration 24, loss = 0.52258224\n",
      "Iteration 25, loss = 0.52080073\n",
      "Iteration 26, loss = 0.51977304\n",
      "Iteration 27, loss = 0.51848297\n",
      "Iteration 28, loss = 0.51768575\n",
      "Iteration 29, loss = 0.51708275\n",
      "Iteration 30, loss = 0.51629967\n",
      "Iteration 31, loss = 0.51607553\n",
      "Iteration 32, loss = 0.51581272\n",
      "Iteration 33, loss = 0.51547302\n",
      "Iteration 34, loss = 0.51516719\n",
      "Iteration 35, loss = 0.51503840\n",
      "Iteration 36, loss = 0.51486941\n",
      "Iteration 37, loss = 0.51475448\n",
      "Iteration 38, loss = 0.51449406\n",
      "Iteration 39, loss = 0.51415124\n",
      "Iteration 40, loss = 0.51391140\n",
      "Iteration 41, loss = 0.51364180\n",
      "Iteration 42, loss = 0.51344282\n",
      "Iteration 43, loss = 0.51323490\n",
      "Iteration 44, loss = 0.51300885\n",
      "Iteration 45, loss = 0.51285199\n",
      "Iteration 46, loss = 0.51295046\n",
      "Iteration 47, loss = 0.51290744\n",
      "Iteration 48, loss = 0.51279212\n",
      "Iteration 49, loss = 0.51270008\n",
      "Iteration 50, loss = 0.51256450\n",
      "Iteration 51, loss = 0.51254574\n",
      "Iteration 52, loss = 0.51256832\n",
      "Iteration 53, loss = 0.51254333\n",
      "Iteration 54, loss = 0.51252460\n",
      "Iteration 55, loss = 0.51241272\n",
      "Iteration 56, loss = 0.51230921\n",
      "Iteration 57, loss = 0.51219834\n",
      "Iteration 58, loss = 0.51200586\n",
      "Iteration 59, loss = 0.51187874\n",
      "Iteration 60, loss = 0.51163941\n",
      "Iteration 61, loss = 0.51130213\n",
      "Iteration 62, loss = 0.51102616\n",
      "Iteration 63, loss = 0.51079057\n",
      "Iteration 64, loss = 0.51038520\n",
      "Iteration 65, loss = 0.51020544\n",
      "Iteration 66, loss = 0.51035581\n",
      "Iteration 67, loss = 0.51079228\n",
      "Iteration 68, loss = 0.51062767\n",
      "Iteration 69, loss = 0.51047856\n",
      "Iteration 70, loss = 0.51026422\n",
      "Iteration 71, loss = 0.51003326\n",
      "Iteration 72, loss = 0.50995136\n",
      "Iteration 73, loss = 0.51008904\n",
      "Iteration 74, loss = 0.50976435\n",
      "Iteration 75, loss = 0.50959499\n",
      "Iteration 76, loss = 0.50959179\n",
      "Iteration 77, loss = 0.50941404\n",
      "Iteration 78, loss = 0.50927277\n",
      "Iteration 79, loss = 0.50924515\n",
      "Iteration 80, loss = 0.50921742\n",
      "Iteration 81, loss = 0.50924806\n",
      "Iteration 82, loss = 0.50905571\n",
      "Iteration 83, loss = 0.50887086\n",
      "Iteration 84, loss = 0.50865142\n",
      "Iteration 85, loss = 0.50839468\n",
      "Iteration 86, loss = 0.50834358\n",
      "Iteration 87, loss = 0.50809436\n",
      "Iteration 88, loss = 0.50824627\n",
      "Iteration 89, loss = 0.50814977\n",
      "Iteration 90, loss = 0.50809623\n",
      "Iteration 91, loss = 0.50810440\n",
      "Iteration 92, loss = 0.50820153\n",
      "Iteration 93, loss = 0.50832209\n",
      "Iteration 94, loss = 0.50827433\n",
      "Iteration 95, loss = 0.50814853\n",
      "Iteration 96, loss = 0.50806120\n",
      "Iteration 97, loss = 0.50796851\n",
      "Iteration 98, loss = 0.50777971\n",
      "Iteration 99, loss = 0.50756006\n",
      "Iteration 100, loss = 0.50724009\n",
      "Iteration 101, loss = 0.50726920\n",
      "Iteration 102, loss = 0.50672575\n",
      "Iteration 103, loss = 0.50664824\n",
      "Iteration 104, loss = 0.50651802\n",
      "Iteration 105, loss = 0.50644046\n",
      "Iteration 106, loss = 0.50629294\n",
      "Iteration 107, loss = 0.50645838\n",
      "Iteration 108, loss = 0.50616074\n",
      "Iteration 109, loss = 0.50601414\n",
      "Iteration 110, loss = 0.50595456\n",
      "Iteration 111, loss = 0.50588926\n",
      "Iteration 112, loss = 0.50583410\n",
      "Iteration 113, loss = 0.50574776\n",
      "Iteration 114, loss = 0.50571547\n",
      "Iteration 115, loss = 0.50561879\n",
      "Iteration 116, loss = 0.50551936\n",
      "Iteration 117, loss = 0.50547367\n",
      "Iteration 118, loss = 0.50533442\n",
      "Iteration 119, loss = 0.50523883\n",
      "Iteration 120, loss = 0.50515249\n",
      "Iteration 121, loss = 0.50498900\n",
      "Iteration 122, loss = 0.50503103\n",
      "Iteration 123, loss = 0.50480651\n",
      "Iteration 124, loss = 0.50470532\n",
      "Iteration 125, loss = 0.50459582\n",
      "Iteration 126, loss = 0.50449751\n",
      "Iteration 127, loss = 0.50452022\n",
      "Iteration 128, loss = 0.50444798\n",
      "Iteration 129, loss = 0.50451379\n",
      "Iteration 130, loss = 0.50449558\n",
      "Iteration 131, loss = 0.50450154\n",
      "Iteration 132, loss = 0.50442353\n",
      "Iteration 133, loss = 0.50441820\n",
      "Iteration 134, loss = 0.50418353\n",
      "Iteration 135, loss = 0.50409104\n",
      "Iteration 136, loss = 0.50399284\n",
      "Iteration 137, loss = 0.50380620\n",
      "Iteration 138, loss = 0.50398460\n",
      "Iteration 139, loss = 0.50386333\n",
      "Iteration 140, loss = 0.50388792\n",
      "Iteration 141, loss = 0.50376939\n",
      "Iteration 142, loss = 0.50383616\n",
      "Iteration 143, loss = 0.50400869\n",
      "Iteration 144, loss = 0.50368931\n",
      "Iteration 145, loss = 0.50350610\n",
      "Iteration 146, loss = 0.50321587\n",
      "Iteration 147, loss = 0.50312672\n",
      "Iteration 148, loss = 0.50299808\n",
      "Iteration 149, loss = 0.50307156\n",
      "Iteration 150, loss = 0.50294643\n",
      "Iteration 151, loss = 0.50291134\n",
      "Iteration 152, loss = 0.50271571\n",
      "Iteration 153, loss = 0.50249503\n",
      "Iteration 154, loss = 0.50233040\n",
      "Iteration 155, loss = 0.50247117\n",
      "Iteration 156, loss = 0.50229750\n",
      "Iteration 157, loss = 0.50252009\n",
      "Iteration 158, loss = 0.50245909\n",
      "Iteration 159, loss = 0.50253187\n",
      "Iteration 160, loss = 0.50249446\n",
      "Iteration 161, loss = 0.50244781\n",
      "Iteration 162, loss = 0.50241062\n",
      "Iteration 163, loss = 0.50226991\n",
      "Iteration 164, loss = 0.50222113\n",
      "Iteration 165, loss = 0.50194187\n",
      "Iteration 166, loss = 0.50161782\n",
      "Iteration 167, loss = 0.50140800\n",
      "Iteration 168, loss = 0.50139050\n",
      "Iteration 169, loss = 0.50164457\n",
      "Iteration 170, loss = 0.50171829\n",
      "Iteration 171, loss = 0.50156337\n",
      "Iteration 172, loss = 0.50136287\n",
      "Iteration 173, loss = 0.50125713\n",
      "Iteration 174, loss = 0.50109113\n",
      "Iteration 175, loss = 0.50116525\n",
      "Iteration 176, loss = 0.50106169\n",
      "Iteration 177, loss = 0.50102977\n",
      "Iteration 178, loss = 0.50098448\n",
      "Iteration 179, loss = 0.50100019\n",
      "Iteration 180, loss = 0.50076155\n",
      "Iteration 181, loss = 0.50069368\n",
      "Iteration 182, loss = 0.50053465\n",
      "Iteration 183, loss = 0.50027728\n",
      "Iteration 184, loss = 0.50017881\n",
      "Iteration 185, loss = 0.50009213\n",
      "Iteration 186, loss = 0.50008778\n",
      "Iteration 187, loss = 0.49997299\n",
      "Iteration 188, loss = 0.49986438\n",
      "Iteration 189, loss = 0.49987188\n",
      "Iteration 190, loss = 0.49983364\n",
      "Iteration 191, loss = 0.49973695\n",
      "Iteration 192, loss = 0.49975447\n",
      "Iteration 193, loss = 0.49968698\n",
      "Iteration 194, loss = 0.49961246\n",
      "Iteration 195, loss = 0.49955831\n",
      "Iteration 196, loss = 0.49955410\n",
      "Iteration 197, loss = 0.49981134\n",
      "Iteration 198, loss = 0.49978173\n",
      "Iteration 199, loss = 0.49967668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65496633\n",
      "Iteration 2, loss = 0.64196900\n",
      "Iteration 3, loss = 0.62989235\n",
      "Iteration 4, loss = 0.61901156\n",
      "Iteration 5, loss = 0.60993874\n",
      "Iteration 6, loss = 0.60220553\n",
      "Iteration 7, loss = 0.59482280\n",
      "Iteration 8, loss = 0.58900688\n",
      "Iteration 9, loss = 0.58348325\n",
      "Iteration 10, loss = 0.57953656\n",
      "Iteration 11, loss = 0.57613297\n",
      "Iteration 12, loss = 0.57289279\n",
      "Iteration 13, loss = 0.57084356\n",
      "Iteration 14, loss = 0.56914061\n",
      "Iteration 15, loss = 0.56763791\n",
      "Iteration 16, loss = 0.56728865\n",
      "Iteration 17, loss = 0.56651373\n",
      "Iteration 18, loss = 0.56621788\n",
      "Iteration 19, loss = 0.56570106\n",
      "Iteration 20, loss = 0.56523841\n",
      "Iteration 21, loss = 0.56463640\n",
      "Iteration 22, loss = 0.56453861\n",
      "Iteration 23, loss = 0.56424709\n",
      "Iteration 24, loss = 0.56387662\n",
      "Iteration 25, loss = 0.56351846\n",
      "Iteration 26, loss = 0.56316879\n",
      "Iteration 27, loss = 0.56305577\n",
      "Iteration 28, loss = 0.56259926\n",
      "Iteration 29, loss = 0.56240057\n",
      "Iteration 30, loss = 0.56193438\n",
      "Iteration 31, loss = 0.56177387\n",
      "Iteration 32, loss = 0.56143420\n",
      "Iteration 33, loss = 0.56145213\n",
      "Iteration 34, loss = 0.56118635\n",
      "Iteration 35, loss = 0.56111557\n",
      "Iteration 36, loss = 0.56102724\n",
      "Iteration 37, loss = 0.56088446\n",
      "Iteration 38, loss = 0.56089982\n",
      "Iteration 39, loss = 0.56083589\n",
      "Iteration 40, loss = 0.56091296\n",
      "Iteration 41, loss = 0.56083665\n",
      "Iteration 42, loss = 0.56060470\n",
      "Iteration 43, loss = 0.56031090\n",
      "Iteration 44, loss = 0.56016417\n",
      "Iteration 45, loss = 0.55996099\n",
      "Iteration 46, loss = 0.55971415\n",
      "Iteration 47, loss = 0.55960584\n",
      "Iteration 48, loss = 0.55945350\n",
      "Iteration 49, loss = 0.55930923\n",
      "Iteration 50, loss = 0.55909518\n",
      "Iteration 51, loss = 0.55897667\n",
      "Iteration 52, loss = 0.55886138\n",
      "Iteration 53, loss = 0.55873520\n",
      "Iteration 54, loss = 0.55850112\n",
      "Iteration 55, loss = 0.55829923\n",
      "Iteration 56, loss = 0.55812283\n",
      "Iteration 57, loss = 0.55801802\n",
      "Iteration 58, loss = 0.55795134\n",
      "Iteration 59, loss = 0.55795358\n",
      "Iteration 60, loss = 0.55789412\n",
      "Iteration 61, loss = 0.55772116\n",
      "Iteration 62, loss = 0.55756115\n",
      "Iteration 63, loss = 0.55742446\n",
      "Iteration 64, loss = 0.55731986\n",
      "Iteration 65, loss = 0.55728699\n",
      "Iteration 66, loss = 0.55700504\n",
      "Iteration 67, loss = 0.55699579\n",
      "Iteration 68, loss = 0.55694885\n",
      "Iteration 69, loss = 0.55691017\n",
      "Iteration 70, loss = 0.55686831\n",
      "Iteration 71, loss = 0.55680062\n",
      "Iteration 72, loss = 0.55677363\n",
      "Iteration 73, loss = 0.55682408\n",
      "Iteration 74, loss = 0.55663541\n",
      "Iteration 75, loss = 0.55660299\n",
      "Iteration 76, loss = 0.55646564\n",
      "Iteration 77, loss = 0.55624607\n",
      "Iteration 78, loss = 0.55611808\n",
      "Iteration 79, loss = 0.55589648\n",
      "Iteration 80, loss = 0.55580001\n",
      "Iteration 81, loss = 0.55556350\n",
      "Iteration 82, loss = 0.55558995\n",
      "Iteration 83, loss = 0.55527305\n",
      "Iteration 84, loss = 0.55519786\n",
      "Iteration 85, loss = 0.55525139\n",
      "Iteration 86, loss = 0.55507011\n",
      "Iteration 87, loss = 0.55492126\n",
      "Iteration 88, loss = 0.55483252\n",
      "Iteration 89, loss = 0.55467653\n",
      "Iteration 90, loss = 0.55460599\n",
      "Iteration 91, loss = 0.55466809\n",
      "Iteration 92, loss = 0.55446262\n",
      "Iteration 93, loss = 0.55461190\n",
      "Iteration 94, loss = 0.55453555\n",
      "Iteration 95, loss = 0.55444657\n",
      "Iteration 96, loss = 0.55441586\n",
      "Iteration 97, loss = 0.55448429\n",
      "Iteration 98, loss = 0.55426271\n",
      "Iteration 99, loss = 0.55388104\n",
      "Iteration 100, loss = 0.55403045\n",
      "Iteration 101, loss = 0.55355616\n",
      "Iteration 102, loss = 0.55381496\n",
      "Iteration 103, loss = 0.55381258\n",
      "Iteration 104, loss = 0.55364455\n",
      "Iteration 105, loss = 0.55334281\n",
      "Iteration 106, loss = 0.55308144\n",
      "Iteration 107, loss = 0.55342148\n",
      "Iteration 108, loss = 0.55364712\n",
      "Iteration 109, loss = 0.55331436\n",
      "Iteration 110, loss = 0.55326177\n",
      "Iteration 111, loss = 0.55314454\n",
      "Iteration 112, loss = 0.55294736\n",
      "Iteration 113, loss = 0.55264257\n",
      "Iteration 114, loss = 0.55258842\n",
      "Iteration 115, loss = 0.55228618\n",
      "Iteration 116, loss = 0.55203779\n",
      "Iteration 117, loss = 0.55206241\n",
      "Iteration 118, loss = 0.55189216\n",
      "Iteration 119, loss = 0.55175531\n",
      "Iteration 120, loss = 0.55179545\n",
      "Iteration 121, loss = 0.55203696\n",
      "Iteration 122, loss = 0.55195212\n",
      "Iteration 123, loss = 0.55191487\n",
      "Iteration 124, loss = 0.55178831\n",
      "Iteration 125, loss = 0.55192492\n",
      "Iteration 126, loss = 0.55183824\n",
      "Iteration 127, loss = 0.55222228\n",
      "Iteration 128, loss = 0.55225528\n",
      "Iteration 129, loss = 0.55233153\n",
      "Iteration 130, loss = 0.55231440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66689570\n",
      "Iteration 2, loss = 0.65043616\n",
      "Iteration 3, loss = 0.63528494\n",
      "Iteration 4, loss = 0.62172583\n",
      "Iteration 5, loss = 0.60981277\n",
      "Iteration 6, loss = 0.59876075\n",
      "Iteration 7, loss = 0.58982737\n",
      "Iteration 8, loss = 0.58042671\n",
      "Iteration 9, loss = 0.57315771\n",
      "Iteration 10, loss = 0.56683987\n",
      "Iteration 11, loss = 0.56108620\n",
      "Iteration 12, loss = 0.55577011\n",
      "Iteration 13, loss = 0.55226587\n",
      "Iteration 14, loss = 0.54871111\n",
      "Iteration 15, loss = 0.54590990\n",
      "Iteration 16, loss = 0.54356318\n",
      "Iteration 17, loss = 0.54163819\n",
      "Iteration 18, loss = 0.54043529\n",
      "Iteration 19, loss = 0.53873111\n",
      "Iteration 20, loss = 0.53797419\n",
      "Iteration 21, loss = 0.53695561\n",
      "Iteration 22, loss = 0.53626775\n",
      "Iteration 23, loss = 0.53563713\n",
      "Iteration 24, loss = 0.53514754\n",
      "Iteration 25, loss = 0.53472054\n",
      "Iteration 26, loss = 0.53432556\n",
      "Iteration 27, loss = 0.53404847\n",
      "Iteration 28, loss = 0.53384239\n",
      "Iteration 29, loss = 0.53374179\n",
      "Iteration 30, loss = 0.53344130\n",
      "Iteration 31, loss = 0.53322708\n",
      "Iteration 32, loss = 0.53320332\n",
      "Iteration 33, loss = 0.53300402\n",
      "Iteration 34, loss = 0.53291398\n",
      "Iteration 35, loss = 0.53276815\n",
      "Iteration 36, loss = 0.53242770\n",
      "Iteration 37, loss = 0.53218238\n",
      "Iteration 38, loss = 0.53184180\n",
      "Iteration 39, loss = 0.53183112\n",
      "Iteration 40, loss = 0.53161002\n",
      "Iteration 41, loss = 0.53141182\n",
      "Iteration 42, loss = 0.53133069\n",
      "Iteration 43, loss = 0.53127374\n",
      "Iteration 44, loss = 0.53105708\n",
      "Iteration 45, loss = 0.53093159\n",
      "Iteration 46, loss = 0.53073600\n",
      "Iteration 47, loss = 0.53062007\n",
      "Iteration 48, loss = 0.53043236\n",
      "Iteration 49, loss = 0.53022651\n",
      "Iteration 50, loss = 0.53002258\n",
      "Iteration 51, loss = 0.52986752\n",
      "Iteration 52, loss = 0.52983227\n",
      "Iteration 53, loss = 0.53009545\n",
      "Iteration 54, loss = 0.53007665\n",
      "Iteration 55, loss = 0.52996256\n",
      "Iteration 56, loss = 0.52976420\n",
      "Iteration 57, loss = 0.52947997\n",
      "Iteration 58, loss = 0.52930372\n",
      "Iteration 59, loss = 0.52917255\n",
      "Iteration 60, loss = 0.52891128\n",
      "Iteration 61, loss = 0.52935067\n",
      "Iteration 62, loss = 0.52947995\n",
      "Iteration 63, loss = 0.52973543\n",
      "Iteration 64, loss = 0.52996397\n",
      "Iteration 65, loss = 0.52993865\n",
      "Iteration 66, loss = 0.52963262\n",
      "Iteration 67, loss = 0.52919670\n",
      "Iteration 68, loss = 0.52883651\n",
      "Iteration 69, loss = 0.52854707\n",
      "Iteration 70, loss = 0.52829948\n",
      "Iteration 71, loss = 0.52849017\n",
      "Iteration 72, loss = 0.52838977\n",
      "Iteration 73, loss = 0.52875894\n",
      "Iteration 74, loss = 0.52874749\n",
      "Iteration 75, loss = 0.52897847\n",
      "Iteration 76, loss = 0.52922408\n",
      "Iteration 77, loss = 0.52907744\n",
      "Iteration 78, loss = 0.52900130\n",
      "Iteration 79, loss = 0.52862539\n",
      "Iteration 80, loss = 0.52801843\n",
      "Iteration 81, loss = 0.52801342\n",
      "Iteration 82, loss = 0.52772517\n",
      "Iteration 83, loss = 0.52751596\n",
      "Iteration 84, loss = 0.52721313\n",
      "Iteration 85, loss = 0.52712142\n",
      "Iteration 86, loss = 0.52692379\n",
      "Iteration 87, loss = 0.52683408\n",
      "Iteration 88, loss = 0.52680775\n",
      "Iteration 89, loss = 0.52672546\n",
      "Iteration 90, loss = 0.52652140\n",
      "Iteration 91, loss = 0.52638890\n",
      "Iteration 92, loss = 0.52622728\n",
      "Iteration 93, loss = 0.52621139\n",
      "Iteration 94, loss = 0.52599127\n",
      "Iteration 95, loss = 0.52593768\n",
      "Iteration 96, loss = 0.52584741\n",
      "Iteration 97, loss = 0.52582201\n",
      "Iteration 98, loss = 0.52572962\n",
      "Iteration 99, loss = 0.52565687\n",
      "Iteration 100, loss = 0.52561443\n",
      "Iteration 101, loss = 0.52552709\n",
      "Iteration 102, loss = 0.52540103\n",
      "Iteration 103, loss = 0.52538277\n",
      "Iteration 104, loss = 0.52529395\n",
      "Iteration 105, loss = 0.52523884\n",
      "Iteration 106, loss = 0.52517391\n",
      "Iteration 107, loss = 0.52514090\n",
      "Iteration 108, loss = 0.52507620\n",
      "Iteration 109, loss = 0.52498553\n",
      "Iteration 110, loss = 0.52483904\n",
      "Iteration 111, loss = 0.52481158\n",
      "Iteration 112, loss = 0.52483679\n",
      "Iteration 113, loss = 0.52550414\n",
      "Iteration 114, loss = 0.52547368\n",
      "Iteration 115, loss = 0.52560146\n",
      "Iteration 116, loss = 0.52554837\n",
      "Iteration 117, loss = 0.52539290\n",
      "Iteration 118, loss = 0.52521105\n",
      "Iteration 119, loss = 0.52521183\n",
      "Iteration 120, loss = 0.52496717\n",
      "Iteration 121, loss = 0.52493024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68373153\n",
      "Iteration 2, loss = 0.66737395\n",
      "Iteration 3, loss = 0.65267478\n",
      "Iteration 4, loss = 0.64001186\n",
      "Iteration 5, loss = 0.62810390\n",
      "Iteration 6, loss = 0.61814873\n",
      "Iteration 7, loss = 0.60949589\n",
      "Iteration 8, loss = 0.60126032\n",
      "Iteration 9, loss = 0.59496391\n",
      "Iteration 10, loss = 0.58980853\n",
      "Iteration 11, loss = 0.58427898\n",
      "Iteration 12, loss = 0.58104368\n",
      "Iteration 13, loss = 0.57807841\n",
      "Iteration 14, loss = 0.57560610\n",
      "Iteration 15, loss = 0.57328451\n",
      "Iteration 16, loss = 0.57182999\n",
      "Iteration 17, loss = 0.57042033\n",
      "Iteration 18, loss = 0.56924266\n",
      "Iteration 19, loss = 0.56833483\n",
      "Iteration 20, loss = 0.56754296\n",
      "Iteration 21, loss = 0.56717324\n",
      "Iteration 22, loss = 0.56690769\n",
      "Iteration 23, loss = 0.56694415\n",
      "Iteration 24, loss = 0.56665571\n",
      "Iteration 25, loss = 0.56649186\n",
      "Iteration 26, loss = 0.56613967\n",
      "Iteration 27, loss = 0.56608148\n",
      "Iteration 28, loss = 0.56555083\n",
      "Iteration 29, loss = 0.56542089\n",
      "Iteration 30, loss = 0.56539988\n",
      "Iteration 31, loss = 0.56533624\n",
      "Iteration 32, loss = 0.56497135\n",
      "Iteration 33, loss = 0.56451759\n",
      "Iteration 34, loss = 0.56426112\n",
      "Iteration 35, loss = 0.56390577\n",
      "Iteration 36, loss = 0.56381606\n",
      "Iteration 37, loss = 0.56360503\n",
      "Iteration 38, loss = 0.56352088\n",
      "Iteration 39, loss = 0.56326688\n",
      "Iteration 40, loss = 0.56308452\n",
      "Iteration 41, loss = 0.56290901\n",
      "Iteration 42, loss = 0.56299527\n",
      "Iteration 43, loss = 0.56262851\n",
      "Iteration 44, loss = 0.56255286\n",
      "Iteration 45, loss = 0.56240548\n",
      "Iteration 46, loss = 0.56220234\n",
      "Iteration 47, loss = 0.56209015\n",
      "Iteration 48, loss = 0.56187854\n",
      "Iteration 49, loss = 0.56177383\n",
      "Iteration 50, loss = 0.56169229\n",
      "Iteration 51, loss = 0.56164479\n",
      "Iteration 52, loss = 0.56156824\n",
      "Iteration 53, loss = 0.56146597\n",
      "Iteration 54, loss = 0.56149334\n",
      "Iteration 55, loss = 0.56159102\n",
      "Iteration 56, loss = 0.56157357\n",
      "Iteration 57, loss = 0.56154886\n",
      "Iteration 58, loss = 0.56133108\n",
      "Iteration 59, loss = 0.56116554\n",
      "Iteration 60, loss = 0.56085920\n",
      "Iteration 61, loss = 0.56068498\n",
      "Iteration 62, loss = 0.56062713\n",
      "Iteration 63, loss = 0.56052227\n",
      "Iteration 64, loss = 0.56018796\n",
      "Iteration 65, loss = 0.56007065\n",
      "Iteration 66, loss = 0.55994633\n",
      "Iteration 67, loss = 0.55992225\n",
      "Iteration 68, loss = 0.55977049\n",
      "Iteration 69, loss = 0.55960803\n",
      "Iteration 70, loss = 0.55945677\n",
      "Iteration 71, loss = 0.55927879\n",
      "Iteration 72, loss = 0.55921625\n",
      "Iteration 73, loss = 0.55919169\n",
      "Iteration 74, loss = 0.55880713\n",
      "Iteration 75, loss = 0.55879810\n",
      "Iteration 76, loss = 0.55899750\n",
      "Iteration 77, loss = 0.55876851\n",
      "Iteration 78, loss = 0.55878883\n",
      "Iteration 79, loss = 0.55866690\n",
      "Iteration 80, loss = 0.55852141\n",
      "Iteration 81, loss = 0.55854175\n",
      "Iteration 82, loss = 0.55839743\n",
      "Iteration 83, loss = 0.55834098\n",
      "Iteration 84, loss = 0.55826649\n",
      "Iteration 85, loss = 0.55829060\n",
      "Iteration 86, loss = 0.55814669\n",
      "Iteration 87, loss = 0.55791832\n",
      "Iteration 88, loss = 0.55762850\n",
      "Iteration 89, loss = 0.55745875\n",
      "Iteration 90, loss = 0.55736335\n",
      "Iteration 91, loss = 0.55713189\n",
      "Iteration 92, loss = 0.55694266\n",
      "Iteration 93, loss = 0.55697367\n",
      "Iteration 94, loss = 0.55667717\n",
      "Iteration 95, loss = 0.55659599\n",
      "Iteration 96, loss = 0.55653868\n",
      "Iteration 97, loss = 0.55639422\n",
      "Iteration 98, loss = 0.55632496\n",
      "Iteration 99, loss = 0.55613394\n",
      "Iteration 100, loss = 0.55599785\n",
      "Iteration 101, loss = 0.55589295\n",
      "Iteration 102, loss = 0.55585508\n",
      "Iteration 103, loss = 0.55580259\n",
      "Iteration 104, loss = 0.55575561\n",
      "Iteration 105, loss = 0.55579421\n",
      "Iteration 106, loss = 0.55594123\n",
      "Iteration 107, loss = 0.55622642\n",
      "Iteration 108, loss = 0.55605271\n",
      "Iteration 109, loss = 0.55599892\n",
      "Iteration 110, loss = 0.55599361\n",
      "Iteration 111, loss = 0.55582136\n",
      "Iteration 112, loss = 0.55523710\n",
      "Iteration 113, loss = 0.55466080\n",
      "Iteration 114, loss = 0.55457015\n",
      "Iteration 115, loss = 0.55434410\n",
      "Iteration 116, loss = 0.55476666\n",
      "Iteration 117, loss = 0.55485725\n",
      "Iteration 118, loss = 0.55487197\n",
      "Iteration 119, loss = 0.55488965\n",
      "Iteration 120, loss = 0.55459388\n",
      "Iteration 121, loss = 0.55446237\n",
      "Iteration 122, loss = 0.55419017\n",
      "Iteration 123, loss = 0.55392889\n",
      "Iteration 124, loss = 0.55376898\n",
      "Iteration 125, loss = 0.55364216\n",
      "Iteration 126, loss = 0.55351524\n",
      "Iteration 127, loss = 0.55345810\n",
      "Iteration 128, loss = 0.55377379\n",
      "Iteration 129, loss = 0.55400210\n",
      "Iteration 130, loss = 0.55379019\n",
      "Iteration 131, loss = 0.55349464\n",
      "Iteration 132, loss = 0.55319361\n",
      "Iteration 133, loss = 0.55297665\n",
      "Iteration 134, loss = 0.55292379\n",
      "Iteration 135, loss = 0.55292512\n",
      "Iteration 136, loss = 0.55282094\n",
      "Iteration 137, loss = 0.55273449\n",
      "Iteration 138, loss = 0.55282360\n",
      "Iteration 139, loss = 0.55262496\n",
      "Iteration 140, loss = 0.55239644\n",
      "Iteration 141, loss = 0.55218206\n",
      "Iteration 142, loss = 0.55211627\n",
      "Iteration 143, loss = 0.55206316\n",
      "Iteration 144, loss = 0.55199390\n",
      "Iteration 145, loss = 0.55216829\n",
      "Iteration 146, loss = 0.55223458\n",
      "Iteration 147, loss = 0.55238878\n",
      "Iteration 148, loss = 0.55235249\n",
      "Iteration 149, loss = 0.55233229\n",
      "Iteration 150, loss = 0.55217162\n",
      "Iteration 151, loss = 0.55191843\n",
      "Iteration 152, loss = 0.55156469\n",
      "Iteration 153, loss = 0.55165166\n",
      "Iteration 154, loss = 0.55113957\n",
      "Iteration 155, loss = 0.55092007\n",
      "Iteration 156, loss = 0.55083925\n",
      "Iteration 157, loss = 0.55071906\n",
      "Iteration 158, loss = 0.55079332\n",
      "Iteration 159, loss = 0.55083897\n",
      "Iteration 160, loss = 0.55088695\n",
      "Iteration 161, loss = 0.55080422\n",
      "Iteration 162, loss = 0.55063866\n",
      "Iteration 163, loss = 0.55046388\n",
      "Iteration 164, loss = 0.55055940\n",
      "Iteration 165, loss = 0.55040044\n",
      "Iteration 166, loss = 0.55019300\n",
      "Iteration 167, loss = 0.54987063\n",
      "Iteration 168, loss = 0.54978089\n",
      "Iteration 169, loss = 0.54949919\n",
      "Iteration 170, loss = 0.54944554\n",
      "Iteration 171, loss = 0.54942379\n",
      "Iteration 172, loss = 0.54951793\n",
      "Iteration 173, loss = 0.54943375\n",
      "Iteration 174, loss = 0.54931011\n",
      "Iteration 175, loss = 0.54922632\n",
      "Iteration 176, loss = 0.54914835\n",
      "Iteration 177, loss = 0.54909705\n",
      "Iteration 178, loss = 0.54900581\n",
      "Iteration 179, loss = 0.54881581\n",
      "Iteration 180, loss = 0.54878013\n",
      "Iteration 181, loss = 0.54874876\n",
      "Iteration 182, loss = 0.54865367\n",
      "Iteration 183, loss = 0.54841453\n",
      "Iteration 184, loss = 0.54840925\n",
      "Iteration 185, loss = 0.54823683\n",
      "Iteration 186, loss = 0.54818966\n",
      "Iteration 187, loss = 0.54799212\n",
      "Iteration 188, loss = 0.54784646\n",
      "Iteration 189, loss = 0.54773639\n",
      "Iteration 190, loss = 0.54766194\n",
      "Iteration 191, loss = 0.54756123\n",
      "Iteration 192, loss = 0.54760964\n",
      "Iteration 193, loss = 0.54766758\n",
      "Iteration 194, loss = 0.54759498\n",
      "Iteration 195, loss = 0.54760539\n",
      "Iteration 196, loss = 0.54726531\n",
      "Iteration 197, loss = 0.54736927\n",
      "Iteration 198, loss = 0.54707393\n",
      "Iteration 199, loss = 0.54694113\n",
      "Iteration 200, loss = 0.54687887\n",
      "Iteration 201, loss = 0.54678158\n",
      "Iteration 202, loss = 0.54665991\n",
      "Iteration 203, loss = 0.54648036\n",
      "Iteration 204, loss = 0.54637882\n",
      "Iteration 205, loss = 0.54643784\n",
      "Iteration 206, loss = 0.54614948\n",
      "Iteration 207, loss = 0.54612872\n",
      "Iteration 208, loss = 0.54611279\n",
      "Iteration 209, loss = 0.54615870\n",
      "Iteration 210, loss = 0.54622946\n",
      "Iteration 211, loss = 0.54613684\n",
      "Iteration 212, loss = 0.54614364\n",
      "Iteration 213, loss = 0.54620893\n",
      "Iteration 214, loss = 0.54628164\n",
      "Iteration 215, loss = 0.54654316\n",
      "Iteration 216, loss = 0.54680763\n",
      "Iteration 217, loss = 0.54721349\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66121651\n",
      "Iteration 2, loss = 0.64558781\n",
      "Iteration 3, loss = 0.63182283\n",
      "Iteration 4, loss = 0.61938990\n",
      "Iteration 5, loss = 0.60965684\n",
      "Iteration 6, loss = 0.60102234\n",
      "Iteration 7, loss = 0.59348017\n",
      "Iteration 8, loss = 0.58674097\n",
      "Iteration 9, loss = 0.58099602\n",
      "Iteration 10, loss = 0.57683814\n",
      "Iteration 11, loss = 0.57280036\n",
      "Iteration 12, loss = 0.56961285\n",
      "Iteration 13, loss = 0.56671451\n",
      "Iteration 14, loss = 0.56451634\n",
      "Iteration 15, loss = 0.56293847\n",
      "Iteration 16, loss = 0.56126903\n",
      "Iteration 17, loss = 0.55998115\n",
      "Iteration 18, loss = 0.55927803\n",
      "Iteration 19, loss = 0.55872359\n",
      "Iteration 20, loss = 0.55806035\n",
      "Iteration 21, loss = 0.55773065\n",
      "Iteration 22, loss = 0.55727804\n",
      "Iteration 23, loss = 0.55717394\n",
      "Iteration 24, loss = 0.55672066\n",
      "Iteration 25, loss = 0.55656226\n",
      "Iteration 26, loss = 0.55626115\n",
      "Iteration 27, loss = 0.55598908\n",
      "Iteration 28, loss = 0.55570518\n",
      "Iteration 29, loss = 0.55544932\n",
      "Iteration 30, loss = 0.55508448\n",
      "Iteration 31, loss = 0.55483533\n",
      "Iteration 32, loss = 0.55466284\n",
      "Iteration 33, loss = 0.55457112\n",
      "Iteration 34, loss = 0.55459192\n",
      "Iteration 35, loss = 0.55445727\n",
      "Iteration 36, loss = 0.55419662\n",
      "Iteration 37, loss = 0.55413406\n",
      "Iteration 38, loss = 0.55417837\n",
      "Iteration 39, loss = 0.55417973\n",
      "Iteration 40, loss = 0.55405856\n",
      "Iteration 41, loss = 0.55374098\n",
      "Iteration 42, loss = 0.55340650\n",
      "Iteration 43, loss = 0.55321618\n",
      "Iteration 44, loss = 0.55275426\n",
      "Iteration 45, loss = 0.55249385\n",
      "Iteration 46, loss = 0.55233847\n",
      "Iteration 47, loss = 0.55208866\n",
      "Iteration 48, loss = 0.55181614\n",
      "Iteration 49, loss = 0.55169795\n",
      "Iteration 50, loss = 0.55156878\n",
      "Iteration 51, loss = 0.55146638\n",
      "Iteration 52, loss = 0.55132730\n",
      "Iteration 53, loss = 0.55109613\n",
      "Iteration 54, loss = 0.55102020\n",
      "Iteration 55, loss = 0.55089265\n",
      "Iteration 56, loss = 0.55075495\n",
      "Iteration 57, loss = 0.55065723\n",
      "Iteration 58, loss = 0.55066836\n",
      "Iteration 59, loss = 0.55043164\n",
      "Iteration 60, loss = 0.55053558\n",
      "Iteration 61, loss = 0.55063853\n",
      "Iteration 62, loss = 0.55039108\n",
      "Iteration 63, loss = 0.55049581\n",
      "Iteration 64, loss = 0.54994237\n",
      "Iteration 65, loss = 0.54994092\n",
      "Iteration 66, loss = 0.54963434\n",
      "Iteration 67, loss = 0.54956820\n",
      "Iteration 68, loss = 0.54924213\n",
      "Iteration 69, loss = 0.54906105\n",
      "Iteration 70, loss = 0.54908865\n",
      "Iteration 71, loss = 0.54892140\n",
      "Iteration 72, loss = 0.54883061\n",
      "Iteration 73, loss = 0.54904843\n",
      "Iteration 74, loss = 0.54926476\n",
      "Iteration 75, loss = 0.54928457\n",
      "Iteration 76, loss = 0.54942481\n",
      "Iteration 77, loss = 0.54936066\n",
      "Iteration 78, loss = 0.54904784\n",
      "Iteration 79, loss = 0.54852587\n",
      "Iteration 80, loss = 0.54817229\n",
      "Iteration 81, loss = 0.54797923\n",
      "Iteration 82, loss = 0.54781962\n",
      "Iteration 83, loss = 0.54765352\n",
      "Iteration 84, loss = 0.54773175\n",
      "Iteration 85, loss = 0.54750625\n",
      "Iteration 86, loss = 0.54740384\n",
      "Iteration 87, loss = 0.54723349\n",
      "Iteration 88, loss = 0.54703352\n",
      "Iteration 89, loss = 0.54697897\n",
      "Iteration 90, loss = 0.54660428\n",
      "Iteration 91, loss = 0.54649077\n",
      "Iteration 92, loss = 0.54642752\n",
      "Iteration 93, loss = 0.54638359\n",
      "Iteration 94, loss = 0.54656650\n",
      "Iteration 95, loss = 0.54669540\n",
      "Iteration 96, loss = 0.54674890\n",
      "Iteration 97, loss = 0.54671972\n",
      "Iteration 98, loss = 0.54675188\n",
      "Iteration 99, loss = 0.54662977\n",
      "Iteration 100, loss = 0.54635183\n",
      "Iteration 101, loss = 0.54598883\n",
      "Iteration 102, loss = 0.54551309\n",
      "Iteration 103, loss = 0.54513947\n",
      "Iteration 104, loss = 0.54498146\n",
      "Iteration 105, loss = 0.54480529\n",
      "Iteration 106, loss = 0.54468888\n",
      "Iteration 107, loss = 0.54477306\n",
      "Iteration 108, loss = 0.54426369\n",
      "Iteration 109, loss = 0.54452002\n",
      "Iteration 110, loss = 0.54430572\n",
      "Iteration 111, loss = 0.54438645\n",
      "Iteration 112, loss = 0.54435400\n",
      "Iteration 113, loss = 0.54428741\n",
      "Iteration 114, loss = 0.54429685\n",
      "Iteration 115, loss = 0.54418678\n",
      "Iteration 116, loss = 0.54394725\n",
      "Iteration 117, loss = 0.54380939\n",
      "Iteration 118, loss = 0.54356966\n",
      "Iteration 119, loss = 0.54349876\n",
      "Iteration 120, loss = 0.54353163\n",
      "Iteration 121, loss = 0.54327697\n",
      "Iteration 122, loss = 0.54318844\n",
      "Iteration 123, loss = 0.54301088\n",
      "Iteration 124, loss = 0.54288167\n",
      "Iteration 125, loss = 0.54288703\n",
      "Iteration 126, loss = 0.54258243\n",
      "Iteration 127, loss = 0.54284948\n",
      "Iteration 128, loss = 0.54263476\n",
      "Iteration 129, loss = 0.54247433\n",
      "Iteration 130, loss = 0.54243721\n",
      "Iteration 131, loss = 0.54212443\n",
      "Iteration 132, loss = 0.54214838\n",
      "Iteration 133, loss = 0.54196613\n",
      "Iteration 134, loss = 0.54185742\n",
      "Iteration 135, loss = 0.54205769\n",
      "Iteration 136, loss = 0.54182406\n",
      "Iteration 137, loss = 0.54203512\n",
      "Iteration 138, loss = 0.54207685\n",
      "Iteration 139, loss = 0.54193672\n",
      "Iteration 140, loss = 0.54178272\n",
      "Iteration 141, loss = 0.54164411\n",
      "Iteration 142, loss = 0.54126506\n",
      "Iteration 143, loss = 0.54114114\n",
      "Iteration 144, loss = 0.54094160\n",
      "Iteration 145, loss = 0.54078045\n",
      "Iteration 146, loss = 0.54065171\n",
      "Iteration 147, loss = 0.54086232\n",
      "Iteration 148, loss = 0.54084546\n",
      "Iteration 149, loss = 0.54080914\n",
      "Iteration 150, loss = 0.54066668\n",
      "Iteration 151, loss = 0.54028201\n",
      "Iteration 152, loss = 0.53996348\n",
      "Iteration 153, loss = 0.53972091\n",
      "Iteration 154, loss = 0.53979291\n",
      "Iteration 155, loss = 0.53967276\n",
      "Iteration 156, loss = 0.53971194\n",
      "Iteration 157, loss = 0.54002973\n",
      "Iteration 158, loss = 0.54011408\n",
      "Iteration 159, loss = 0.54007679\n",
      "Iteration 160, loss = 0.53995381\n",
      "Iteration 161, loss = 0.53962427\n",
      "Iteration 162, loss = 0.53941313\n",
      "Iteration 163, loss = 0.53934995\n",
      "Iteration 164, loss = 0.53908866\n",
      "Iteration 165, loss = 0.53874399\n",
      "Iteration 166, loss = 0.53863957\n",
      "Iteration 167, loss = 0.53849182\n",
      "Iteration 168, loss = 0.53856073\n",
      "Iteration 169, loss = 0.53895525\n",
      "Iteration 170, loss = 0.53901327\n",
      "Iteration 171, loss = 0.53884316\n",
      "Iteration 172, loss = 0.53870715\n",
      "Iteration 173, loss = 0.53821951\n",
      "Iteration 174, loss = 0.53805221\n",
      "Iteration 175, loss = 0.53778362\n",
      "Iteration 176, loss = 0.53748128\n",
      "Iteration 177, loss = 0.53768309\n",
      "Iteration 178, loss = 0.53733398\n",
      "Iteration 179, loss = 0.53713321\n",
      "Iteration 180, loss = 0.53707533\n",
      "Iteration 181, loss = 0.53690986\n",
      "Iteration 182, loss = 0.53713247\n",
      "Iteration 183, loss = 0.53708936\n",
      "Iteration 184, loss = 0.53705730\n",
      "Iteration 185, loss = 0.53708770\n",
      "Iteration 186, loss = 0.53683714\n",
      "Iteration 187, loss = 0.53668772\n",
      "Iteration 188, loss = 0.53684378\n",
      "Iteration 189, loss = 0.53670542\n",
      "Iteration 190, loss = 0.53647381\n",
      "Iteration 191, loss = 0.53628612\n",
      "Iteration 192, loss = 0.53599755\n",
      "Iteration 193, loss = 0.53583734\n",
      "Iteration 194, loss = 0.53574605\n",
      "Iteration 195, loss = 0.53589703\n",
      "Iteration 196, loss = 0.53575961\n",
      "Iteration 197, loss = 0.53572199\n",
      "Iteration 198, loss = 0.53562317\n",
      "Iteration 199, loss = 0.53550369\n",
      "Iteration 200, loss = 0.53521351\n",
      "Iteration 201, loss = 0.53508928\n",
      "Iteration 202, loss = 0.53477959\n",
      "Iteration 203, loss = 0.53498760\n",
      "Iteration 204, loss = 0.53493953\n",
      "Iteration 205, loss = 0.53495920\n",
      "Iteration 206, loss = 0.53460534\n",
      "Iteration 207, loss = 0.53418724\n",
      "Iteration 208, loss = 0.53403974\n",
      "Iteration 209, loss = 0.53399835\n",
      "Iteration 210, loss = 0.53408335\n",
      "Iteration 211, loss = 0.53395917\n",
      "Iteration 212, loss = 0.53397852\n",
      "Iteration 213, loss = 0.53400905\n",
      "Iteration 214, loss = 0.53400954\n",
      "Iteration 215, loss = 0.53406291\n",
      "Iteration 216, loss = 0.53390129\n",
      "Iteration 217, loss = 0.53363458\n",
      "Iteration 218, loss = 0.53336106\n",
      "Iteration 219, loss = 0.53314062\n",
      "Iteration 220, loss = 0.53287631\n",
      "Iteration 221, loss = 0.53264394\n",
      "Iteration 222, loss = 0.53297470\n",
      "Iteration 223, loss = 0.53276060\n",
      "Iteration 224, loss = 0.53279246\n",
      "Iteration 225, loss = 0.53254700\n",
      "Iteration 226, loss = 0.53243459\n",
      "Iteration 227, loss = 0.53227365\n",
      "Iteration 228, loss = 0.53229177\n",
      "Iteration 229, loss = 0.53241572\n",
      "Iteration 230, loss = 0.53207249\n",
      "Iteration 231, loss = 0.53211392\n",
      "Iteration 232, loss = 0.53193928\n",
      "Iteration 233, loss = 0.53168394\n",
      "Iteration 234, loss = 0.53144483\n",
      "Iteration 235, loss = 0.53141629\n",
      "Iteration 236, loss = 0.53120364\n",
      "Iteration 237, loss = 0.53110292\n",
      "Iteration 238, loss = 0.53139303\n",
      "Iteration 239, loss = 0.53123661\n",
      "Iteration 240, loss = 0.53099884\n",
      "Iteration 241, loss = 0.53073082\n",
      "Iteration 242, loss = 0.53091643\n",
      "Iteration 243, loss = 0.53118090\n",
      "Iteration 244, loss = 0.53149609\n",
      "Iteration 245, loss = 0.53173695\n",
      "Iteration 246, loss = 0.53188802\n",
      "Iteration 247, loss = 0.53156473\n",
      "Iteration 248, loss = 0.53105562\n",
      "Iteration 249, loss = 0.53085148\n",
      "Iteration 250, loss = 0.53057074\n",
      "Iteration 251, loss = 0.53029211\n",
      "Iteration 252, loss = 0.53011268\n",
      "Iteration 253, loss = 0.52988873\n",
      "Iteration 254, loss = 0.52983998\n",
      "Iteration 255, loss = 0.52974625\n",
      "Iteration 256, loss = 0.52971785\n",
      "Iteration 257, loss = 0.52974578\n",
      "Iteration 258, loss = 0.52988619\n",
      "Iteration 259, loss = 0.53033493\n",
      "Iteration 260, loss = 0.53003718\n",
      "Iteration 261, loss = 0.53001547\n",
      "Iteration 262, loss = 0.52938969\n",
      "Iteration 263, loss = 0.52937609\n",
      "Iteration 264, loss = 0.52934626\n",
      "Iteration 265, loss = 0.52915214\n",
      "Iteration 266, loss = 0.52917816\n",
      "Iteration 267, loss = 0.52903919\n",
      "Iteration 268, loss = 0.52896355\n",
      "Iteration 269, loss = 0.52915228\n",
      "Iteration 270, loss = 0.52911564\n",
      "Iteration 271, loss = 0.52901813\n",
      "Iteration 272, loss = 0.52877665\n",
      "Iteration 273, loss = 0.52931080\n",
      "Iteration 274, loss = 0.52915118\n",
      "Iteration 275, loss = 0.52928014\n",
      "Iteration 276, loss = 0.52904254\n",
      "Iteration 277, loss = 0.52841888\n",
      "Iteration 278, loss = 0.52783212\n",
      "Iteration 279, loss = 0.52738625\n",
      "Iteration 280, loss = 0.52742638\n",
      "Iteration 281, loss = 0.52778855\n",
      "Iteration 282, loss = 0.52788329\n",
      "Iteration 283, loss = 0.52818775\n",
      "Iteration 284, loss = 0.52793667\n",
      "Iteration 285, loss = 0.52739655\n",
      "Iteration 286, loss = 0.52697589\n",
      "Iteration 287, loss = 0.52677948\n",
      "Iteration 288, loss = 0.52688380\n",
      "Iteration 289, loss = 0.52712765\n",
      "Iteration 290, loss = 0.52731112\n",
      "Iteration 291, loss = 0.52693338\n",
      "Iteration 292, loss = 0.52702369\n",
      "Iteration 293, loss = 0.52815015\n",
      "Iteration 294, loss = 0.52810342\n",
      "Iteration 295, loss = 0.52783005\n",
      "Iteration 296, loss = 0.52710754\n",
      "Iteration 297, loss = 0.52614270\n",
      "Iteration 298, loss = 0.52583418\n",
      "Iteration 299, loss = 0.52611031\n",
      "Iteration 300, loss = 0.52587200\n",
      "Iteration 301, loss = 0.52588914\n",
      "Iteration 302, loss = 0.52607902\n",
      "Iteration 303, loss = 0.52606107\n",
      "Iteration 304, loss = 0.52634752\n",
      "Iteration 305, loss = 0.52602094\n",
      "Iteration 306, loss = 0.52563265\n",
      "Iteration 307, loss = 0.52520715\n",
      "Iteration 308, loss = 0.52518226\n",
      "Iteration 309, loss = 0.52543914\n",
      "Iteration 310, loss = 0.52560493\n",
      "Iteration 311, loss = 0.52531221\n",
      "Iteration 312, loss = 0.52518925\n",
      "Iteration 313, loss = 0.52481463\n",
      "Iteration 314, loss = 0.52451235\n",
      "Iteration 315, loss = 0.52417170\n",
      "Iteration 316, loss = 0.52429503\n",
      "Iteration 317, loss = 0.52435685\n",
      "Iteration 318, loss = 0.52400804\n",
      "Iteration 319, loss = 0.52395104\n",
      "Iteration 320, loss = 0.52366730\n",
      "Iteration 321, loss = 0.52358826\n",
      "Iteration 322, loss = 0.52347430\n",
      "Iteration 323, loss = 0.52376279\n",
      "Iteration 324, loss = 0.52395768\n",
      "Iteration 325, loss = 0.52429898\n",
      "Iteration 326, loss = 0.52448520\n",
      "Iteration 327, loss = 0.52470997\n",
      "Iteration 328, loss = 0.52470791\n",
      "Iteration 329, loss = 0.52435455\n",
      "Iteration 330, loss = 0.52409838\n",
      "Iteration 331, loss = 0.52362093\n",
      "Iteration 332, loss = 0.52331355\n",
      "Iteration 333, loss = 0.52296189\n",
      "Iteration 334, loss = 0.52291131\n",
      "Iteration 335, loss = 0.52309158\n",
      "Iteration 336, loss = 0.52315105\n",
      "Iteration 337, loss = 0.52301450\n",
      "Iteration 338, loss = 0.52308209\n",
      "Iteration 339, loss = 0.52289095\n",
      "Iteration 340, loss = 0.52298621\n",
      "Iteration 341, loss = 0.52281586\n",
      "Iteration 342, loss = 0.52244854\n",
      "Iteration 343, loss = 0.52228824\n",
      "Iteration 344, loss = 0.52227606\n",
      "Iteration 345, loss = 0.52265167\n",
      "Iteration 346, loss = 0.52299478\n",
      "Iteration 347, loss = 0.52299090\n",
      "Iteration 348, loss = 0.52285689\n",
      "Iteration 349, loss = 0.52255322\n",
      "Iteration 350, loss = 0.52227608\n",
      "Iteration 351, loss = 0.52199622\n",
      "Iteration 352, loss = 0.52194226\n",
      "Iteration 353, loss = 0.52168164\n",
      "Iteration 354, loss = 0.52155638\n",
      "Iteration 355, loss = 0.52141132\n",
      "Iteration 356, loss = 0.52132638\n",
      "Iteration 357, loss = 0.52103512\n",
      "Iteration 358, loss = 0.52082139\n",
      "Iteration 359, loss = 0.52125542\n",
      "Iteration 360, loss = 0.52117733\n",
      "Iteration 361, loss = 0.52131043\n",
      "Iteration 362, loss = 0.52138081\n",
      "Iteration 363, loss = 0.52127081\n",
      "Iteration 364, loss = 0.52121144\n",
      "Iteration 365, loss = 0.52080859\n",
      "Iteration 366, loss = 0.52087068\n",
      "Iteration 367, loss = 0.52040470\n",
      "Iteration 368, loss = 0.52097476\n",
      "Iteration 369, loss = 0.52152432\n",
      "Iteration 370, loss = 0.52180549\n",
      "Iteration 371, loss = 0.52170788\n",
      "Iteration 372, loss = 0.52188487\n",
      "Iteration 373, loss = 0.52150629\n",
      "Iteration 374, loss = 0.52127970\n",
      "Iteration 375, loss = 0.52101010\n",
      "Iteration 376, loss = 0.52060001\n",
      "Iteration 377, loss = 0.52037252\n",
      "Iteration 378, loss = 0.52035822\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70286814\n",
      "Iteration 2, loss = 0.68387614\n",
      "Iteration 3, loss = 0.66621251\n",
      "Iteration 4, loss = 0.65109798\n",
      "Iteration 5, loss = 0.63623373\n",
      "Iteration 6, loss = 0.62366545\n",
      "Iteration 7, loss = 0.61113840\n",
      "Iteration 8, loss = 0.60082569\n",
      "Iteration 9, loss = 0.59083602\n",
      "Iteration 10, loss = 0.58239934\n",
      "Iteration 11, loss = 0.57482986\n",
      "Iteration 12, loss = 0.56837129\n",
      "Iteration 13, loss = 0.56262777\n",
      "Iteration 14, loss = 0.55725819\n",
      "Iteration 15, loss = 0.55322602\n",
      "Iteration 16, loss = 0.54965059\n",
      "Iteration 17, loss = 0.54678649\n",
      "Iteration 18, loss = 0.54434815\n",
      "Iteration 19, loss = 0.54234344\n",
      "Iteration 20, loss = 0.54072074\n",
      "Iteration 21, loss = 0.53946647\n",
      "Iteration 22, loss = 0.53879655\n",
      "Iteration 23, loss = 0.53786755\n",
      "Iteration 24, loss = 0.53779344\n",
      "Iteration 25, loss = 0.53696042\n",
      "Iteration 26, loss = 0.53696857\n",
      "Iteration 27, loss = 0.53665275\n",
      "Iteration 28, loss = 0.53637822\n",
      "Iteration 29, loss = 0.53612709\n",
      "Iteration 30, loss = 0.53586835\n",
      "Iteration 31, loss = 0.53572633\n",
      "Iteration 32, loss = 0.53569235\n",
      "Iteration 33, loss = 0.53542869\n",
      "Iteration 34, loss = 0.53517184\n",
      "Iteration 35, loss = 0.53508390\n",
      "Iteration 36, loss = 0.53474367\n",
      "Iteration 37, loss = 0.53454772\n",
      "Iteration 38, loss = 0.53439342\n",
      "Iteration 39, loss = 0.53451563\n",
      "Iteration 40, loss = 0.53407491\n",
      "Iteration 41, loss = 0.53403510\n",
      "Iteration 42, loss = 0.53384974\n",
      "Iteration 43, loss = 0.53384248\n",
      "Iteration 44, loss = 0.53385455\n",
      "Iteration 45, loss = 0.53368892\n",
      "Iteration 46, loss = 0.53363899\n",
      "Iteration 47, loss = 0.53356103\n",
      "Iteration 48, loss = 0.53366275\n",
      "Iteration 49, loss = 0.53347412\n",
      "Iteration 50, loss = 0.53329468\n",
      "Iteration 51, loss = 0.53304253\n",
      "Iteration 52, loss = 0.53310633\n",
      "Iteration 53, loss = 0.53312674\n",
      "Iteration 54, loss = 0.53290472\n",
      "Iteration 55, loss = 0.53271525\n",
      "Iteration 56, loss = 0.53252497\n",
      "Iteration 57, loss = 0.53229674\n",
      "Iteration 58, loss = 0.53208726\n",
      "Iteration 59, loss = 0.53197538\n",
      "Iteration 60, loss = 0.53187280\n",
      "Iteration 61, loss = 0.53196866\n",
      "Iteration 62, loss = 0.53204527\n",
      "Iteration 63, loss = 0.53218234\n",
      "Iteration 64, loss = 0.53212294\n",
      "Iteration 65, loss = 0.53218784\n",
      "Iteration 66, loss = 0.53222447\n",
      "Iteration 67, loss = 0.53204424\n",
      "Iteration 68, loss = 0.53182719\n",
      "Iteration 69, loss = 0.53158621\n",
      "Iteration 70, loss = 0.53133938\n",
      "Iteration 71, loss = 0.53131826\n",
      "Iteration 72, loss = 0.53100708\n",
      "Iteration 73, loss = 0.53099460\n",
      "Iteration 74, loss = 0.53076832\n",
      "Iteration 75, loss = 0.53066012\n",
      "Iteration 76, loss = 0.53067284\n",
      "Iteration 77, loss = 0.53067434\n",
      "Iteration 78, loss = 0.53068738\n",
      "Iteration 79, loss = 0.53049351\n",
      "Iteration 80, loss = 0.53039437\n",
      "Iteration 81, loss = 0.53020904\n",
      "Iteration 82, loss = 0.53006432\n",
      "Iteration 83, loss = 0.52999776\n",
      "Iteration 84, loss = 0.52984161\n",
      "Iteration 85, loss = 0.52989915\n",
      "Iteration 86, loss = 0.52997569\n",
      "Iteration 87, loss = 0.52990973\n",
      "Iteration 88, loss = 0.52992048\n",
      "Iteration 89, loss = 0.52990358\n",
      "Iteration 90, loss = 0.52981670\n",
      "Iteration 91, loss = 0.52966086\n",
      "Iteration 92, loss = 0.52985354\n",
      "Iteration 93, loss = 0.52978259\n",
      "Iteration 94, loss = 0.52966632\n",
      "Iteration 95, loss = 0.52954246\n",
      "Iteration 96, loss = 0.52937813\n",
      "Iteration 97, loss = 0.52913481\n",
      "Iteration 98, loss = 0.52903934\n",
      "Iteration 99, loss = 0.52882036\n",
      "Iteration 100, loss = 0.52869530\n",
      "Iteration 101, loss = 0.52850146\n",
      "Iteration 102, loss = 0.52840247\n",
      "Iteration 103, loss = 0.52833620\n",
      "Iteration 104, loss = 0.52831818\n",
      "Iteration 105, loss = 0.52808436\n",
      "Iteration 106, loss = 0.52815790\n",
      "Iteration 107, loss = 0.52797445\n",
      "Iteration 108, loss = 0.52801564\n",
      "Iteration 109, loss = 0.52801101\n",
      "Iteration 110, loss = 0.52792481\n",
      "Iteration 111, loss = 0.52790210\n",
      "Iteration 112, loss = 0.52783809\n",
      "Iteration 113, loss = 0.52780377\n",
      "Iteration 114, loss = 0.52763795\n",
      "Iteration 115, loss = 0.52755496\n",
      "Iteration 116, loss = 0.52739348\n",
      "Iteration 117, loss = 0.52724205\n",
      "Iteration 118, loss = 0.52723296\n",
      "Iteration 119, loss = 0.52702020\n",
      "Iteration 120, loss = 0.52717882\n",
      "Iteration 121, loss = 0.52701797\n",
      "Iteration 122, loss = 0.52699732\n",
      "Iteration 123, loss = 0.52682853\n",
      "Iteration 124, loss = 0.52677830\n",
      "Iteration 125, loss = 0.52680667\n",
      "Iteration 126, loss = 0.52660661\n",
      "Iteration 127, loss = 0.52651792\n",
      "Iteration 128, loss = 0.52638070\n",
      "Iteration 129, loss = 0.52628460\n",
      "Iteration 130, loss = 0.52639204\n",
      "Iteration 131, loss = 0.52633063\n",
      "Iteration 132, loss = 0.52640359\n",
      "Iteration 133, loss = 0.52635529\n",
      "Iteration 134, loss = 0.52626173\n",
      "Iteration 135, loss = 0.52632633\n",
      "Iteration 136, loss = 0.52650717\n",
      "Iteration 137, loss = 0.52654251\n",
      "Iteration 138, loss = 0.52652242\n",
      "Iteration 139, loss = 0.52667633\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70943392\n",
      "Iteration 2, loss = 0.68791923\n",
      "Iteration 3, loss = 0.66944087\n",
      "Iteration 4, loss = 0.65264505\n",
      "Iteration 5, loss = 0.63688470\n",
      "Iteration 6, loss = 0.62303556\n",
      "Iteration 7, loss = 0.61112021\n",
      "Iteration 8, loss = 0.59987592\n",
      "Iteration 9, loss = 0.59089296\n",
      "Iteration 10, loss = 0.58256812\n",
      "Iteration 11, loss = 0.57519742\n",
      "Iteration 12, loss = 0.56863399\n",
      "Iteration 13, loss = 0.56389947\n",
      "Iteration 14, loss = 0.56009566\n",
      "Iteration 15, loss = 0.55658658\n",
      "Iteration 16, loss = 0.55414187\n",
      "Iteration 17, loss = 0.55256107\n",
      "Iteration 18, loss = 0.55081739\n",
      "Iteration 19, loss = 0.54986710\n",
      "Iteration 20, loss = 0.54899166\n",
      "Iteration 21, loss = 0.54878277\n",
      "Iteration 22, loss = 0.54840139\n",
      "Iteration 23, loss = 0.54844726\n",
      "Iteration 24, loss = 0.54827441\n",
      "Iteration 25, loss = 0.54815546\n",
      "Iteration 26, loss = 0.54787982\n",
      "Iteration 27, loss = 0.54779924\n",
      "Iteration 28, loss = 0.54781660\n",
      "Iteration 29, loss = 0.54784043\n",
      "Iteration 30, loss = 0.54767817\n",
      "Iteration 31, loss = 0.54748497\n",
      "Iteration 32, loss = 0.54706091\n",
      "Iteration 33, loss = 0.54662511\n",
      "Iteration 34, loss = 0.54639740\n",
      "Iteration 35, loss = 0.54607160\n",
      "Iteration 36, loss = 0.54573900\n",
      "Iteration 37, loss = 0.54559026\n",
      "Iteration 38, loss = 0.54541854\n",
      "Iteration 39, loss = 0.54520591\n",
      "Iteration 40, loss = 0.54505910\n",
      "Iteration 41, loss = 0.54492784\n",
      "Iteration 42, loss = 0.54475462\n",
      "Iteration 43, loss = 0.54472057\n",
      "Iteration 44, loss = 0.54458197\n",
      "Iteration 45, loss = 0.54439423\n",
      "Iteration 46, loss = 0.54428243\n",
      "Iteration 47, loss = 0.54406351\n",
      "Iteration 48, loss = 0.54388581\n",
      "Iteration 49, loss = 0.54396440\n",
      "Iteration 50, loss = 0.54378693\n",
      "Iteration 51, loss = 0.54356820\n",
      "Iteration 52, loss = 0.54359063\n",
      "Iteration 53, loss = 0.54355534\n",
      "Iteration 54, loss = 0.54344265\n",
      "Iteration 55, loss = 0.54329153\n",
      "Iteration 56, loss = 0.54323685\n",
      "Iteration 57, loss = 0.54318340\n",
      "Iteration 58, loss = 0.54299307\n",
      "Iteration 59, loss = 0.54278790\n",
      "Iteration 60, loss = 0.54298487\n",
      "Iteration 61, loss = 0.54283224\n",
      "Iteration 62, loss = 0.54276681\n",
      "Iteration 63, loss = 0.54280063\n",
      "Iteration 64, loss = 0.54273485\n",
      "Iteration 65, loss = 0.54286718\n",
      "Iteration 66, loss = 0.54279279\n",
      "Iteration 67, loss = 0.54284112\n",
      "Iteration 68, loss = 0.54269093\n",
      "Iteration 69, loss = 0.54255454\n",
      "Iteration 70, loss = 0.54251657\n",
      "Iteration 71, loss = 0.54230320\n",
      "Iteration 72, loss = 0.54211866\n",
      "Iteration 73, loss = 0.54216449\n",
      "Iteration 74, loss = 0.54185881\n",
      "Iteration 75, loss = 0.54176252\n",
      "Iteration 76, loss = 0.54176305\n",
      "Iteration 77, loss = 0.54169454\n",
      "Iteration 78, loss = 0.54163345\n",
      "Iteration 79, loss = 0.54169706\n",
      "Iteration 80, loss = 0.54190893\n",
      "Iteration 81, loss = 0.54174596\n",
      "Iteration 82, loss = 0.54158321\n",
      "Iteration 83, loss = 0.54129217\n",
      "Iteration 84, loss = 0.54110214\n",
      "Iteration 85, loss = 0.54121294\n",
      "Iteration 86, loss = 0.54161487\n",
      "Iteration 87, loss = 0.54149209\n",
      "Iteration 88, loss = 0.54148422\n",
      "Iteration 89, loss = 0.54141242\n",
      "Iteration 90, loss = 0.54139111\n",
      "Iteration 91, loss = 0.54119089\n",
      "Iteration 92, loss = 0.54108947\n",
      "Iteration 93, loss = 0.54113587\n",
      "Iteration 94, loss = 0.54104875\n",
      "Iteration 95, loss = 0.54094039\n",
      "Iteration 96, loss = 0.54101858\n",
      "Iteration 97, loss = 0.54091841\n",
      "Iteration 98, loss = 0.54097008\n",
      "Iteration 99, loss = 0.54108118\n",
      "Iteration 100, loss = 0.54108941\n",
      "Iteration 101, loss = 0.54090391\n",
      "Iteration 102, loss = 0.54095311\n",
      "Iteration 103, loss = 0.54078320\n",
      "Iteration 104, loss = 0.54056164\n",
      "Iteration 105, loss = 0.54044042\n",
      "Iteration 106, loss = 0.54005718\n",
      "Iteration 107, loss = 0.53987914\n",
      "Iteration 108, loss = 0.53997455\n",
      "Iteration 109, loss = 0.53988040\n",
      "Iteration 110, loss = 0.53974615\n",
      "Iteration 111, loss = 0.53966317\n",
      "Iteration 112, loss = 0.53950519\n",
      "Iteration 113, loss = 0.53946123\n",
      "Iteration 114, loss = 0.53944953\n",
      "Iteration 115, loss = 0.53952306\n",
      "Iteration 116, loss = 0.53949686\n",
      "Iteration 117, loss = 0.53968284\n",
      "Iteration 118, loss = 0.53975943\n",
      "Iteration 119, loss = 0.53992035\n",
      "Iteration 120, loss = 0.54011986\n",
      "Iteration 121, loss = 0.54036998\n",
      "Iteration 122, loss = 0.54040297\n",
      "Iteration 123, loss = 0.54019652\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71063854\n",
      "Iteration 2, loss = 0.68942275\n",
      "Iteration 3, loss = 0.67123351\n",
      "Iteration 4, loss = 0.65367696\n",
      "Iteration 5, loss = 0.63816385\n",
      "Iteration 6, loss = 0.62383104\n",
      "Iteration 7, loss = 0.60997658\n",
      "Iteration 8, loss = 0.59851109\n",
      "Iteration 9, loss = 0.58759996\n",
      "Iteration 10, loss = 0.57872148\n",
      "Iteration 11, loss = 0.57020914\n",
      "Iteration 12, loss = 0.56285942\n",
      "Iteration 13, loss = 0.55704914\n",
      "Iteration 14, loss = 0.55097579\n",
      "Iteration 15, loss = 0.54672985\n",
      "Iteration 16, loss = 0.54263877\n",
      "Iteration 17, loss = 0.53955303\n",
      "Iteration 18, loss = 0.53646988\n",
      "Iteration 19, loss = 0.53391969\n",
      "Iteration 20, loss = 0.53222279\n",
      "Iteration 21, loss = 0.53004431\n",
      "Iteration 22, loss = 0.52878904\n",
      "Iteration 23, loss = 0.52728963\n",
      "Iteration 24, loss = 0.52667273\n",
      "Iteration 25, loss = 0.52576461\n",
      "Iteration 26, loss = 0.52536844\n",
      "Iteration 27, loss = 0.52480577\n",
      "Iteration 28, loss = 0.52452014\n",
      "Iteration 29, loss = 0.52418222\n",
      "Iteration 30, loss = 0.52391609\n",
      "Iteration 31, loss = 0.52371407\n",
      "Iteration 32, loss = 0.52348931\n",
      "Iteration 33, loss = 0.52320894\n",
      "Iteration 34, loss = 0.52310549\n",
      "Iteration 35, loss = 0.52269336\n",
      "Iteration 36, loss = 0.52231294\n",
      "Iteration 37, loss = 0.52188655\n",
      "Iteration 38, loss = 0.52159028\n",
      "Iteration 39, loss = 0.52143705\n",
      "Iteration 40, loss = 0.52103581\n",
      "Iteration 41, loss = 0.52077983\n",
      "Iteration 42, loss = 0.52072636\n",
      "Iteration 43, loss = 0.52106965\n",
      "Iteration 44, loss = 0.52099751\n",
      "Iteration 45, loss = 0.52120449\n",
      "Iteration 46, loss = 0.52114114\n",
      "Iteration 47, loss = 0.52106656\n",
      "Iteration 48, loss = 0.52099882\n",
      "Iteration 49, loss = 0.52082429\n",
      "Iteration 50, loss = 0.52049836\n",
      "Iteration 51, loss = 0.52013681\n",
      "Iteration 52, loss = 0.51986544\n",
      "Iteration 53, loss = 0.51956669\n",
      "Iteration 54, loss = 0.51935647\n",
      "Iteration 55, loss = 0.51901563\n",
      "Iteration 56, loss = 0.51871590\n",
      "Iteration 57, loss = 0.51861162\n",
      "Iteration 58, loss = 0.51815031\n",
      "Iteration 59, loss = 0.51811575\n",
      "Iteration 60, loss = 0.51784053\n",
      "Iteration 61, loss = 0.51770765\n",
      "Iteration 62, loss = 0.51759490\n",
      "Iteration 63, loss = 0.51748631\n",
      "Iteration 64, loss = 0.51743903\n",
      "Iteration 65, loss = 0.51730753\n",
      "Iteration 66, loss = 0.51724032\n",
      "Iteration 67, loss = 0.51728808\n",
      "Iteration 68, loss = 0.51709672\n",
      "Iteration 69, loss = 0.51723982\n",
      "Iteration 70, loss = 0.51704820\n",
      "Iteration 71, loss = 0.51693181\n",
      "Iteration 72, loss = 0.51664506\n",
      "Iteration 73, loss = 0.51643153\n",
      "Iteration 74, loss = 0.51618992\n",
      "Iteration 75, loss = 0.51592545\n",
      "Iteration 76, loss = 0.51581279\n",
      "Iteration 77, loss = 0.51558096\n",
      "Iteration 78, loss = 0.51553980\n",
      "Iteration 79, loss = 0.51548993\n",
      "Iteration 80, loss = 0.51559280\n",
      "Iteration 81, loss = 0.51577038\n",
      "Iteration 82, loss = 0.51584230\n",
      "Iteration 83, loss = 0.51586328\n",
      "Iteration 84, loss = 0.51603631\n",
      "Iteration 85, loss = 0.51595300\n",
      "Iteration 86, loss = 0.51580541\n",
      "Iteration 87, loss = 0.51550996\n",
      "Iteration 88, loss = 0.51538386\n",
      "Iteration 89, loss = 0.51533403\n",
      "Iteration 90, loss = 0.51523471\n",
      "Iteration 91, loss = 0.51511688\n",
      "Iteration 92, loss = 0.51523962\n",
      "Iteration 93, loss = 0.51476167\n",
      "Iteration 94, loss = 0.51427807\n",
      "Iteration 95, loss = 0.51402207\n",
      "Iteration 96, loss = 0.51373972\n",
      "Iteration 97, loss = 0.51396516\n",
      "Iteration 98, loss = 0.51399465\n",
      "Iteration 99, loss = 0.51433939\n",
      "Iteration 100, loss = 0.51421409\n",
      "Iteration 101, loss = 0.51406362\n",
      "Iteration 102, loss = 0.51408924\n",
      "Iteration 103, loss = 0.51367830\n",
      "Iteration 104, loss = 0.51362061\n",
      "Iteration 105, loss = 0.51351438\n",
      "Iteration 106, loss = 0.51335835\n",
      "Iteration 107, loss = 0.51326804\n",
      "Iteration 108, loss = 0.51316231\n",
      "Iteration 109, loss = 0.51332196\n",
      "Iteration 110, loss = 0.51323326\n",
      "Iteration 111, loss = 0.51334384\n",
      "Iteration 112, loss = 0.51325012\n",
      "Iteration 113, loss = 0.51348876\n",
      "Iteration 114, loss = 0.51346259\n",
      "Iteration 115, loss = 0.51330538\n",
      "Iteration 116, loss = 0.51304461\n",
      "Iteration 117, loss = 0.51305269\n",
      "Iteration 118, loss = 0.51298141\n",
      "Iteration 119, loss = 0.51298917\n",
      "Iteration 120, loss = 0.51281682\n",
      "Iteration 121, loss = 0.51240207\n",
      "Iteration 122, loss = 0.51224493\n",
      "Iteration 123, loss = 0.51206759\n",
      "Iteration 124, loss = 0.51181596\n",
      "Iteration 125, loss = 0.51200110\n",
      "Iteration 126, loss = 0.51210176\n",
      "Iteration 127, loss = 0.51197382\n",
      "Iteration 128, loss = 0.51178768\n",
      "Iteration 129, loss = 0.51163294\n",
      "Iteration 130, loss = 0.51142144\n",
      "Iteration 131, loss = 0.51119065\n",
      "Iteration 132, loss = 0.51101626\n",
      "Iteration 133, loss = 0.51105572\n",
      "Iteration 134, loss = 0.51101416\n",
      "Iteration 135, loss = 0.51118210\n",
      "Iteration 136, loss = 0.51110341\n",
      "Iteration 137, loss = 0.51111224\n",
      "Iteration 138, loss = 0.51126459\n",
      "Iteration 139, loss = 0.51104416\n",
      "Iteration 140, loss = 0.51090858\n",
      "Iteration 141, loss = 0.51083677\n",
      "Iteration 142, loss = 0.51068510\n",
      "Iteration 143, loss = 0.51057816\n",
      "Iteration 144, loss = 0.51057278\n",
      "Iteration 145, loss = 0.51047861\n",
      "Iteration 146, loss = 0.51042704\n",
      "Iteration 147, loss = 0.51022823\n",
      "Iteration 148, loss = 0.51021532\n",
      "Iteration 149, loss = 0.51027661\n",
      "Iteration 150, loss = 0.51025010\n",
      "Iteration 151, loss = 0.51037677\n",
      "Iteration 152, loss = 0.51029945\n",
      "Iteration 153, loss = 0.51020847\n",
      "Iteration 154, loss = 0.51001279\n",
      "Iteration 155, loss = 0.51009820\n",
      "Iteration 156, loss = 0.50990768\n",
      "Iteration 157, loss = 0.50992527\n",
      "Iteration 158, loss = 0.50992938\n",
      "Iteration 159, loss = 0.50998705\n",
      "Iteration 160, loss = 0.51001537\n",
      "Iteration 161, loss = 0.50998410\n",
      "Iteration 162, loss = 0.50991989\n",
      "Iteration 163, loss = 0.50990867\n",
      "Iteration 164, loss = 0.50952274\n",
      "Iteration 165, loss = 0.50944448\n",
      "Iteration 166, loss = 0.50948899\n",
      "Iteration 167, loss = 0.50950563\n",
      "Iteration 168, loss = 0.50935130\n",
      "Iteration 169, loss = 0.50915585\n",
      "Iteration 170, loss = 0.50891806\n",
      "Iteration 171, loss = 0.50893026\n",
      "Iteration 172, loss = 0.50870969\n",
      "Iteration 173, loss = 0.50849073\n",
      "Iteration 174, loss = 0.50853922\n",
      "Iteration 175, loss = 0.50852265\n",
      "Iteration 176, loss = 0.50833007\n",
      "Iteration 177, loss = 0.50817562\n",
      "Iteration 178, loss = 0.50794562\n",
      "Iteration 179, loss = 0.50774439\n",
      "Iteration 180, loss = 0.50769593\n",
      "Iteration 181, loss = 0.50791238\n",
      "Iteration 182, loss = 0.50827860\n",
      "Iteration 183, loss = 0.50855454\n",
      "Iteration 184, loss = 0.50857711\n",
      "Iteration 185, loss = 0.50858579\n",
      "Iteration 186, loss = 0.50840218\n",
      "Iteration 187, loss = 0.50811778\n",
      "Iteration 188, loss = 0.50784816\n",
      "Iteration 189, loss = 0.50755175\n",
      "Iteration 190, loss = 0.50741283\n",
      "Iteration 191, loss = 0.50716754\n",
      "Iteration 192, loss = 0.50705927\n",
      "Iteration 193, loss = 0.50693328\n",
      "Iteration 194, loss = 0.50724222\n",
      "Iteration 195, loss = 0.50700564\n",
      "Iteration 196, loss = 0.50684018\n",
      "Iteration 197, loss = 0.50662751\n",
      "Iteration 198, loss = 0.50646932\n",
      "Iteration 199, loss = 0.50668410\n",
      "Iteration 200, loss = 0.50641396\n",
      "Iteration 201, loss = 0.50635067\n",
      "Iteration 202, loss = 0.50618753\n",
      "Iteration 203, loss = 0.50636908\n",
      "Iteration 204, loss = 0.50599407\n",
      "Iteration 205, loss = 0.50610299\n",
      "Iteration 206, loss = 0.50612648\n",
      "Iteration 207, loss = 0.50607883\n",
      "Iteration 208, loss = 0.50626409\n",
      "Iteration 209, loss = 0.50617644\n",
      "Iteration 210, loss = 0.50588281\n",
      "Iteration 211, loss = 0.50580281\n",
      "Iteration 212, loss = 0.50596166\n",
      "Iteration 213, loss = 0.50578520\n",
      "Iteration 214, loss = 0.50573448\n",
      "Iteration 215, loss = 0.50570095\n",
      "Iteration 216, loss = 0.50553013\n",
      "Iteration 217, loss = 0.50545306\n",
      "Iteration 218, loss = 0.50551379\n",
      "Iteration 219, loss = 0.50567006\n",
      "Iteration 220, loss = 0.50578619\n",
      "Iteration 221, loss = 0.50578491\n",
      "Iteration 222, loss = 0.50563539\n",
      "Iteration 223, loss = 0.50557377\n",
      "Iteration 224, loss = 0.50562868\n",
      "Iteration 225, loss = 0.50543688\n",
      "Iteration 226, loss = 0.50600308\n",
      "Iteration 227, loss = 0.50531609\n",
      "Iteration 228, loss = 0.50526169\n",
      "Iteration 229, loss = 0.50486873\n",
      "Iteration 230, loss = 0.50476198\n",
      "Iteration 231, loss = 0.50470716\n",
      "Iteration 232, loss = 0.50455909\n",
      "Iteration 233, loss = 0.50511423\n",
      "Iteration 234, loss = 0.50507422\n",
      "Iteration 235, loss = 0.50501853\n",
      "Iteration 236, loss = 0.50471665\n",
      "Iteration 237, loss = 0.50445223\n",
      "Iteration 238, loss = 0.50420461\n",
      "Iteration 239, loss = 0.50446264\n",
      "Iteration 240, loss = 0.50496950\n",
      "Iteration 241, loss = 0.50488753\n",
      "Iteration 242, loss = 0.50474088\n",
      "Iteration 243, loss = 0.50444785\n",
      "Iteration 244, loss = 0.50413164\n",
      "Iteration 245, loss = 0.50456618\n",
      "Iteration 246, loss = 0.50459534\n",
      "Iteration 247, loss = 0.50491852\n",
      "Iteration 248, loss = 0.50516527\n",
      "Iteration 249, loss = 0.50508038\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69733251\n",
      "Iteration 2, loss = 0.67637577\n",
      "Iteration 3, loss = 0.65652206\n",
      "Iteration 4, loss = 0.64057468\n",
      "Iteration 5, loss = 0.62524560\n",
      "Iteration 6, loss = 0.61166364\n",
      "Iteration 7, loss = 0.60030572\n",
      "Iteration 8, loss = 0.58953802\n",
      "Iteration 9, loss = 0.58014564\n",
      "Iteration 10, loss = 0.57301503\n",
      "Iteration 11, loss = 0.56608945\n",
      "Iteration 12, loss = 0.56027948\n",
      "Iteration 13, loss = 0.55588525\n",
      "Iteration 14, loss = 0.55177191\n",
      "Iteration 15, loss = 0.54851282\n",
      "Iteration 16, loss = 0.54600115\n",
      "Iteration 17, loss = 0.54364583\n",
      "Iteration 18, loss = 0.54228862\n",
      "Iteration 19, loss = 0.54073806\n",
      "Iteration 20, loss = 0.53971972\n",
      "Iteration 21, loss = 0.53883296\n",
      "Iteration 22, loss = 0.53837089\n",
      "Iteration 23, loss = 0.53792012\n",
      "Iteration 24, loss = 0.53719646\n",
      "Iteration 25, loss = 0.53684177\n",
      "Iteration 26, loss = 0.53635721\n",
      "Iteration 27, loss = 0.53601983\n",
      "Iteration 28, loss = 0.53560182\n",
      "Iteration 29, loss = 0.53505113\n",
      "Iteration 30, loss = 0.53490034\n",
      "Iteration 31, loss = 0.53449951\n",
      "Iteration 32, loss = 0.53438349\n",
      "Iteration 33, loss = 0.53415682\n",
      "Iteration 34, loss = 0.53422446\n",
      "Iteration 35, loss = 0.53412362\n",
      "Iteration 36, loss = 0.53403330\n",
      "Iteration 37, loss = 0.53407884\n",
      "Iteration 38, loss = 0.53398225\n",
      "Iteration 39, loss = 0.53378995\n",
      "Iteration 40, loss = 0.53356367\n",
      "Iteration 41, loss = 0.53338970\n",
      "Iteration 42, loss = 0.53311419\n",
      "Iteration 43, loss = 0.53292964\n",
      "Iteration 44, loss = 0.53277310\n",
      "Iteration 45, loss = 0.53259611\n",
      "Iteration 46, loss = 0.53242324\n",
      "Iteration 47, loss = 0.53225416\n",
      "Iteration 48, loss = 0.53190883\n",
      "Iteration 49, loss = 0.53166947\n",
      "Iteration 50, loss = 0.53157152\n",
      "Iteration 51, loss = 0.53113798\n",
      "Iteration 52, loss = 0.53091462\n",
      "Iteration 53, loss = 0.53087519\n",
      "Iteration 54, loss = 0.53071159\n",
      "Iteration 55, loss = 0.53044495\n",
      "Iteration 56, loss = 0.53033368\n",
      "Iteration 57, loss = 0.53010186\n",
      "Iteration 58, loss = 0.53007729\n",
      "Iteration 59, loss = 0.52996506\n",
      "Iteration 60, loss = 0.53001595\n",
      "Iteration 61, loss = 0.52981274\n",
      "Iteration 62, loss = 0.52977314\n",
      "Iteration 63, loss = 0.52955997\n",
      "Iteration 64, loss = 0.52943746\n",
      "Iteration 65, loss = 0.52934217\n",
      "Iteration 66, loss = 0.52955216\n",
      "Iteration 67, loss = 0.52934400\n",
      "Iteration 68, loss = 0.52933215\n",
      "Iteration 69, loss = 0.52944364\n",
      "Iteration 70, loss = 0.52905650\n",
      "Iteration 71, loss = 0.52880004\n",
      "Iteration 72, loss = 0.52870247\n",
      "Iteration 73, loss = 0.52839953\n",
      "Iteration 74, loss = 0.52834136\n",
      "Iteration 75, loss = 0.52810972\n",
      "Iteration 76, loss = 0.52819025\n",
      "Iteration 77, loss = 0.52800285\n",
      "Iteration 78, loss = 0.52781175\n",
      "Iteration 79, loss = 0.52771051\n",
      "Iteration 80, loss = 0.52778646\n",
      "Iteration 81, loss = 0.52767654\n",
      "Iteration 82, loss = 0.52774841\n",
      "Iteration 83, loss = 0.52771589\n",
      "Iteration 84, loss = 0.52773780\n",
      "Iteration 85, loss = 0.52768614\n",
      "Iteration 86, loss = 0.52742929\n",
      "Iteration 87, loss = 0.52752320\n",
      "Iteration 88, loss = 0.52735033\n",
      "Iteration 89, loss = 0.52719260\n",
      "Iteration 90, loss = 0.52715513\n",
      "Iteration 91, loss = 0.52699072\n",
      "Iteration 92, loss = 0.52677765\n",
      "Iteration 93, loss = 0.52661559\n",
      "Iteration 94, loss = 0.52643393\n",
      "Iteration 95, loss = 0.52634509\n",
      "Iteration 96, loss = 0.52627426\n",
      "Iteration 97, loss = 0.52620589\n",
      "Iteration 98, loss = 0.52600001\n",
      "Iteration 99, loss = 0.52577806\n",
      "Iteration 100, loss = 0.52570364\n",
      "Iteration 101, loss = 0.52577195\n",
      "Iteration 102, loss = 0.52561155\n",
      "Iteration 103, loss = 0.52570854\n",
      "Iteration 104, loss = 0.52620202\n",
      "Iteration 105, loss = 0.52642582\n",
      "Iteration 106, loss = 0.52646476\n",
      "Iteration 107, loss = 0.52671778\n",
      "Iteration 108, loss = 0.52679928\n",
      "Iteration 109, loss = 0.52706017\n",
      "Iteration 110, loss = 0.52715560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71276362\n",
      "Iteration 2, loss = 0.69257655\n",
      "Iteration 3, loss = 0.67372722\n",
      "Iteration 4, loss = 0.65731402\n",
      "Iteration 5, loss = 0.64160453\n",
      "Iteration 6, loss = 0.62810801\n",
      "Iteration 7, loss = 0.61518251\n",
      "Iteration 8, loss = 0.60496565\n",
      "Iteration 9, loss = 0.59477078\n",
      "Iteration 10, loss = 0.58641066\n",
      "Iteration 11, loss = 0.57937812\n",
      "Iteration 12, loss = 0.57273998\n",
      "Iteration 13, loss = 0.56723632\n",
      "Iteration 14, loss = 0.56236346\n",
      "Iteration 15, loss = 0.55844871\n",
      "Iteration 16, loss = 0.55459755\n",
      "Iteration 17, loss = 0.55199289\n",
      "Iteration 18, loss = 0.54909355\n",
      "Iteration 19, loss = 0.54727566\n",
      "Iteration 20, loss = 0.54545918\n",
      "Iteration 21, loss = 0.54374003\n",
      "Iteration 22, loss = 0.54265797\n",
      "Iteration 23, loss = 0.54187731\n",
      "Iteration 24, loss = 0.54126669\n",
      "Iteration 25, loss = 0.54076640\n",
      "Iteration 26, loss = 0.54022402\n",
      "Iteration 27, loss = 0.54007119\n",
      "Iteration 28, loss = 0.54007487\n",
      "Iteration 29, loss = 0.53973178\n",
      "Iteration 30, loss = 0.53950434\n",
      "Iteration 31, loss = 0.53930804\n",
      "Iteration 32, loss = 0.53922845\n",
      "Iteration 33, loss = 0.53895265\n",
      "Iteration 34, loss = 0.53858711\n",
      "Iteration 35, loss = 0.53839107\n",
      "Iteration 36, loss = 0.53804305\n",
      "Iteration 37, loss = 0.53768855\n",
      "Iteration 38, loss = 0.53755802\n",
      "Iteration 39, loss = 0.53719789\n",
      "Iteration 40, loss = 0.53689122\n",
      "Iteration 41, loss = 0.53662672\n",
      "Iteration 42, loss = 0.53657839\n",
      "Iteration 43, loss = 0.53639275\n",
      "Iteration 44, loss = 0.53621152\n",
      "Iteration 45, loss = 0.53602575\n",
      "Iteration 46, loss = 0.53593633\n",
      "Iteration 47, loss = 0.53572461\n",
      "Iteration 48, loss = 0.53569410\n",
      "Iteration 49, loss = 0.53562641\n",
      "Iteration 50, loss = 0.53534290\n",
      "Iteration 51, loss = 0.53508402\n",
      "Iteration 52, loss = 0.53477918\n",
      "Iteration 53, loss = 0.53469824\n",
      "Iteration 54, loss = 0.53454663\n",
      "Iteration 55, loss = 0.53427871\n",
      "Iteration 56, loss = 0.53416244\n",
      "Iteration 57, loss = 0.53392190\n",
      "Iteration 58, loss = 0.53399897\n",
      "Iteration 59, loss = 0.53377033\n",
      "Iteration 60, loss = 0.53359113\n",
      "Iteration 61, loss = 0.53341019\n",
      "Iteration 62, loss = 0.53311899\n",
      "Iteration 63, loss = 0.53294507\n",
      "Iteration 64, loss = 0.53274951\n",
      "Iteration 65, loss = 0.53261111\n",
      "Iteration 66, loss = 0.53241877\n",
      "Iteration 67, loss = 0.53237345\n",
      "Iteration 68, loss = 0.53241401\n",
      "Iteration 69, loss = 0.53217759\n",
      "Iteration 70, loss = 0.53204415\n",
      "Iteration 71, loss = 0.53190205\n",
      "Iteration 72, loss = 0.53185859\n",
      "Iteration 73, loss = 0.53184067\n",
      "Iteration 74, loss = 0.53165439\n",
      "Iteration 75, loss = 0.53178343\n",
      "Iteration 76, loss = 0.53171834\n",
      "Iteration 77, loss = 0.53168514\n",
      "Iteration 78, loss = 0.53164785\n",
      "Iteration 79, loss = 0.53149541\n",
      "Iteration 80, loss = 0.53131560\n",
      "Iteration 81, loss = 0.53112166\n",
      "Iteration 82, loss = 0.53116837\n",
      "Iteration 83, loss = 0.53077200\n",
      "Iteration 84, loss = 0.53067713\n",
      "Iteration 85, loss = 0.53062287\n",
      "Iteration 86, loss = 0.53047107\n",
      "Iteration 87, loss = 0.53041075\n",
      "Iteration 88, loss = 0.53037020\n",
      "Iteration 89, loss = 0.53015643\n",
      "Iteration 90, loss = 0.53009228\n",
      "Iteration 91, loss = 0.52992139\n",
      "Iteration 92, loss = 0.52965127\n",
      "Iteration 93, loss = 0.52943130\n",
      "Iteration 94, loss = 0.52918219\n",
      "Iteration 95, loss = 0.52926633\n",
      "Iteration 96, loss = 0.52892783\n",
      "Iteration 97, loss = 0.52879576\n",
      "Iteration 98, loss = 0.52860502\n",
      "Iteration 99, loss = 0.52863293\n",
      "Iteration 100, loss = 0.52854279\n",
      "Iteration 101, loss = 0.52856237\n",
      "Iteration 102, loss = 0.52827251\n",
      "Iteration 103, loss = 0.52820945\n",
      "Iteration 104, loss = 0.52810191\n",
      "Iteration 105, loss = 0.52809199\n",
      "Iteration 106, loss = 0.52802983\n",
      "Iteration 107, loss = 0.52788094\n",
      "Iteration 108, loss = 0.52776229\n",
      "Iteration 109, loss = 0.52777893\n",
      "Iteration 110, loss = 0.52764907\n",
      "Iteration 111, loss = 0.52776349\n",
      "Iteration 112, loss = 0.52779193\n",
      "Iteration 113, loss = 0.52770021\n",
      "Iteration 114, loss = 0.52749870\n",
      "Iteration 115, loss = 0.52789419\n",
      "Iteration 116, loss = 0.52750432\n",
      "Iteration 117, loss = 0.52718669\n",
      "Iteration 118, loss = 0.52715108\n",
      "Iteration 119, loss = 0.52704355\n",
      "Iteration 120, loss = 0.52697324\n",
      "Iteration 121, loss = 0.52705745\n",
      "Iteration 122, loss = 0.52679499\n",
      "Iteration 123, loss = 0.52683429\n",
      "Iteration 124, loss = 0.52647335\n",
      "Iteration 125, loss = 0.52662190\n",
      "Iteration 126, loss = 0.52647682\n",
      "Iteration 127, loss = 0.52662156\n",
      "Iteration 128, loss = 0.52673779\n",
      "Iteration 129, loss = 0.52650276\n",
      "Iteration 130, loss = 0.52643408\n",
      "Iteration 131, loss = 0.52606677\n",
      "Iteration 132, loss = 0.52607674\n",
      "Iteration 133, loss = 0.52627933\n",
      "Iteration 134, loss = 0.52598479\n",
      "Iteration 135, loss = 0.52602047\n",
      "Iteration 136, loss = 0.52588445\n",
      "Iteration 137, loss = 0.52582580\n",
      "Iteration 138, loss = 0.52569588\n",
      "Iteration 139, loss = 0.52536962\n",
      "Iteration 140, loss = 0.52523080\n",
      "Iteration 141, loss = 0.52510751\n",
      "Iteration 142, loss = 0.52508156\n",
      "Iteration 143, loss = 0.52499885\n",
      "Iteration 144, loss = 0.52459297\n",
      "Iteration 145, loss = 0.52449532\n",
      "Iteration 146, loss = 0.52450151\n",
      "Iteration 147, loss = 0.52464094\n",
      "Iteration 148, loss = 0.52465726\n",
      "Iteration 149, loss = 0.52477926\n",
      "Iteration 150, loss = 0.52444080\n",
      "Iteration 151, loss = 0.52448164\n",
      "Iteration 152, loss = 0.52437045\n",
      "Iteration 153, loss = 0.52426472\n",
      "Iteration 154, loss = 0.52445815\n",
      "Iteration 155, loss = 0.52427791\n",
      "Iteration 156, loss = 0.52399327\n",
      "Iteration 157, loss = 0.52402854\n",
      "Iteration 158, loss = 0.52389809\n",
      "Iteration 159, loss = 0.52375024\n",
      "Iteration 160, loss = 0.52373490\n",
      "Iteration 161, loss = 0.52352917\n",
      "Iteration 162, loss = 0.52338420\n",
      "Iteration 163, loss = 0.52311761\n",
      "Iteration 164, loss = 0.52315420\n",
      "Iteration 165, loss = 0.52306212\n",
      "Iteration 166, loss = 0.52300374\n",
      "Iteration 167, loss = 0.52292376\n",
      "Iteration 168, loss = 0.52286194\n",
      "Iteration 169, loss = 0.52286330\n",
      "Iteration 170, loss = 0.52290464\n",
      "Iteration 171, loss = 0.52260441\n",
      "Iteration 172, loss = 0.52254020\n",
      "Iteration 173, loss = 0.52238140\n",
      "Iteration 174, loss = 0.52230152\n",
      "Iteration 175, loss = 0.52234751\n",
      "Iteration 176, loss = 0.52252683\n",
      "Iteration 177, loss = 0.52276473\n",
      "Iteration 178, loss = 0.52287416\n",
      "Iteration 179, loss = 0.52296879\n",
      "Iteration 180, loss = 0.52299364\n",
      "Iteration 181, loss = 0.52314354\n",
      "Iteration 182, loss = 0.52319701\n",
      "Iteration 183, loss = 0.52330255\n",
      "Iteration 184, loss = 0.52332957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.71590009\n",
      "Iteration 2, loss = 0.69157388\n",
      "Iteration 3, loss = 0.66964714\n",
      "Iteration 4, loss = 0.64870394\n",
      "Iteration 5, loss = 0.63142841\n",
      "Iteration 6, loss = 0.61495714\n",
      "Iteration 7, loss = 0.60109367\n",
      "Iteration 8, loss = 0.58930978\n",
      "Iteration 9, loss = 0.57812325\n",
      "Iteration 10, loss = 0.56919034\n",
      "Iteration 11, loss = 0.56112720\n",
      "Iteration 12, loss = 0.55485776\n",
      "Iteration 13, loss = 0.54900766\n",
      "Iteration 14, loss = 0.54444949\n",
      "Iteration 15, loss = 0.54089852\n",
      "Iteration 16, loss = 0.53728873\n",
      "Iteration 17, loss = 0.53516126\n",
      "Iteration 18, loss = 0.53302415\n",
      "Iteration 19, loss = 0.53133053\n",
      "Iteration 20, loss = 0.53016188\n",
      "Iteration 21, loss = 0.52927243\n",
      "Iteration 22, loss = 0.52843040\n",
      "Iteration 23, loss = 0.52810065\n",
      "Iteration 24, loss = 0.52748716\n",
      "Iteration 25, loss = 0.52742798\n",
      "Iteration 26, loss = 0.52725784\n",
      "Iteration 27, loss = 0.52718551\n",
      "Iteration 28, loss = 0.52746022\n",
      "Iteration 29, loss = 0.52733186\n",
      "Iteration 30, loss = 0.52733804\n",
      "Iteration 31, loss = 0.52736477\n",
      "Iteration 32, loss = 0.52725742\n",
      "Iteration 33, loss = 0.52712536\n",
      "Iteration 34, loss = 0.52702857\n",
      "Iteration 35, loss = 0.52675728\n",
      "Iteration 36, loss = 0.52663761\n",
      "Iteration 37, loss = 0.52639814\n",
      "Iteration 38, loss = 0.52626633\n",
      "Iteration 39, loss = 0.52616941\n",
      "Iteration 40, loss = 0.52607161\n",
      "Iteration 41, loss = 0.52595470\n",
      "Iteration 42, loss = 0.52593459\n",
      "Iteration 43, loss = 0.52585802\n",
      "Iteration 44, loss = 0.52564307\n",
      "Iteration 45, loss = 0.52560058\n",
      "Iteration 46, loss = 0.52537939\n",
      "Iteration 47, loss = 0.52534820\n",
      "Iteration 48, loss = 0.52504680\n",
      "Iteration 49, loss = 0.52504376\n",
      "Iteration 50, loss = 0.52533026\n",
      "Iteration 51, loss = 0.52553950\n",
      "Iteration 52, loss = 0.52562042\n",
      "Iteration 53, loss = 0.52569475\n",
      "Iteration 54, loss = 0.52596477\n",
      "Iteration 55, loss = 0.52616620\n",
      "Iteration 56, loss = 0.52614383\n",
      "Iteration 57, loss = 0.52599909\n",
      "Iteration 58, loss = 0.52585533\n",
      "Iteration 59, loss = 0.52569944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.72793495\n",
      "Iteration 2, loss = 0.70539658\n",
      "Iteration 3, loss = 0.68455462\n",
      "Iteration 4, loss = 0.66551087\n",
      "Iteration 5, loss = 0.64883899\n",
      "Iteration 6, loss = 0.63372871\n",
      "Iteration 7, loss = 0.62053686\n",
      "Iteration 8, loss = 0.60888031\n",
      "Iteration 9, loss = 0.59844663\n",
      "Iteration 10, loss = 0.58937125\n",
      "Iteration 11, loss = 0.58128199\n",
      "Iteration 12, loss = 0.57493514\n",
      "Iteration 13, loss = 0.56896822\n",
      "Iteration 14, loss = 0.56437972\n",
      "Iteration 15, loss = 0.56031054\n",
      "Iteration 16, loss = 0.55723147\n",
      "Iteration 17, loss = 0.55457250\n",
      "Iteration 18, loss = 0.55267136\n",
      "Iteration 19, loss = 0.55095648\n",
      "Iteration 20, loss = 0.55019371\n",
      "Iteration 21, loss = 0.54956717\n",
      "Iteration 22, loss = 0.54910458\n",
      "Iteration 23, loss = 0.54864046\n",
      "Iteration 24, loss = 0.54833780\n",
      "Iteration 25, loss = 0.54819614\n",
      "Iteration 26, loss = 0.54796490\n",
      "Iteration 27, loss = 0.54770332\n",
      "Iteration 28, loss = 0.54751694\n",
      "Iteration 29, loss = 0.54724899\n",
      "Iteration 30, loss = 0.54695123\n",
      "Iteration 31, loss = 0.54659161\n",
      "Iteration 32, loss = 0.54624467\n",
      "Iteration 33, loss = 0.54598527\n",
      "Iteration 34, loss = 0.54559754\n",
      "Iteration 35, loss = 0.54530059\n",
      "Iteration 36, loss = 0.54509431\n",
      "Iteration 37, loss = 0.54494790\n",
      "Iteration 38, loss = 0.54496789\n",
      "Iteration 39, loss = 0.54481917\n",
      "Iteration 40, loss = 0.54470263\n",
      "Iteration 41, loss = 0.54457019\n",
      "Iteration 42, loss = 0.54426027\n",
      "Iteration 43, loss = 0.54415051\n",
      "Iteration 44, loss = 0.54385159\n",
      "Iteration 45, loss = 0.54369787\n",
      "Iteration 46, loss = 0.54353841\n",
      "Iteration 47, loss = 0.54340618\n",
      "Iteration 48, loss = 0.54325763\n",
      "Iteration 49, loss = 0.54323607\n",
      "Iteration 50, loss = 0.54313544\n",
      "Iteration 51, loss = 0.54309152\n",
      "Iteration 52, loss = 0.54299432\n",
      "Iteration 53, loss = 0.54322028\n",
      "Iteration 54, loss = 0.54300059\n",
      "Iteration 55, loss = 0.54276342\n",
      "Iteration 56, loss = 0.54263091\n",
      "Iteration 57, loss = 0.54254452\n",
      "Iteration 58, loss = 0.54209599\n",
      "Iteration 59, loss = 0.54189740\n",
      "Iteration 60, loss = 0.54165210\n",
      "Iteration 61, loss = 0.54140201\n",
      "Iteration 62, loss = 0.54170424\n",
      "Iteration 63, loss = 0.54180999\n",
      "Iteration 64, loss = 0.54219634\n",
      "Iteration 65, loss = 0.54234443\n",
      "Iteration 66, loss = 0.54237908\n",
      "Iteration 67, loss = 0.54185775\n",
      "Iteration 68, loss = 0.54127535\n",
      "Iteration 69, loss = 0.54071912\n",
      "Iteration 70, loss = 0.54022409\n",
      "Iteration 71, loss = 0.53995883\n",
      "Iteration 72, loss = 0.54001996\n",
      "Iteration 73, loss = 0.54012073\n",
      "Iteration 74, loss = 0.53998525\n",
      "Iteration 75, loss = 0.54013565\n",
      "Iteration 76, loss = 0.54007943\n",
      "Iteration 77, loss = 0.54003997\n",
      "Iteration 78, loss = 0.54002221\n",
      "Iteration 79, loss = 0.53992601\n",
      "Iteration 80, loss = 0.53989667\n",
      "Iteration 81, loss = 0.53960186\n",
      "Iteration 82, loss = 0.53940827\n",
      "Iteration 83, loss = 0.53931765\n",
      "Iteration 84, loss = 0.53922322\n",
      "Iteration 85, loss = 0.53918444\n",
      "Iteration 86, loss = 0.53925129\n",
      "Iteration 87, loss = 0.53937403\n",
      "Iteration 88, loss = 0.53910576\n",
      "Iteration 89, loss = 0.53896195\n",
      "Iteration 90, loss = 0.53842494\n",
      "Iteration 91, loss = 0.53814847\n",
      "Iteration 92, loss = 0.53769718\n",
      "Iteration 93, loss = 0.53790338\n",
      "Iteration 94, loss = 0.53761557\n",
      "Iteration 95, loss = 0.53753006\n",
      "Iteration 96, loss = 0.53753674\n",
      "Iteration 97, loss = 0.53731690\n",
      "Iteration 98, loss = 0.53696374\n",
      "Iteration 99, loss = 0.53692288\n",
      "Iteration 100, loss = 0.53644140\n",
      "Iteration 101, loss = 0.53629165\n",
      "Iteration 102, loss = 0.53633902\n",
      "Iteration 103, loss = 0.53638056\n",
      "Iteration 104, loss = 0.53637700\n",
      "Iteration 105, loss = 0.53647361\n",
      "Iteration 106, loss = 0.53635747\n",
      "Iteration 107, loss = 0.53625349\n",
      "Iteration 108, loss = 0.53604160\n",
      "Iteration 109, loss = 0.53588091\n",
      "Iteration 110, loss = 0.53561518\n",
      "Iteration 111, loss = 0.53536456\n",
      "Iteration 112, loss = 0.53540467\n",
      "Iteration 113, loss = 0.53544770\n",
      "Iteration 114, loss = 0.53548678\n",
      "Iteration 115, loss = 0.53528137\n",
      "Iteration 116, loss = 0.53517826\n",
      "Iteration 117, loss = 0.53502752\n",
      "Iteration 118, loss = 0.53500306\n",
      "Iteration 119, loss = 0.53475830\n",
      "Iteration 120, loss = 0.53443686\n",
      "Iteration 121, loss = 0.53403413\n",
      "Iteration 122, loss = 0.53389318\n",
      "Iteration 123, loss = 0.53379444\n",
      "Iteration 124, loss = 0.53373923\n",
      "Iteration 125, loss = 0.53347508\n",
      "Iteration 126, loss = 0.53375217\n",
      "Iteration 127, loss = 0.53340354\n",
      "Iteration 128, loss = 0.53343747\n",
      "Iteration 129, loss = 0.53299862\n",
      "Iteration 130, loss = 0.53363166\n",
      "Iteration 131, loss = 0.53350293\n",
      "Iteration 132, loss = 0.53330486\n",
      "Iteration 133, loss = 0.53301996\n",
      "Iteration 134, loss = 0.53263475\n",
      "Iteration 135, loss = 0.53234079\n",
      "Iteration 136, loss = 0.53205798\n",
      "Iteration 137, loss = 0.53185207\n",
      "Iteration 138, loss = 0.53170451\n",
      "Iteration 139, loss = 0.53136281\n",
      "Iteration 140, loss = 0.53114826\n",
      "Iteration 141, loss = 0.53140681\n",
      "Iteration 142, loss = 0.53105516\n",
      "Iteration 143, loss = 0.53095975\n",
      "Iteration 144, loss = 0.53124826\n",
      "Iteration 145, loss = 0.53116126\n",
      "Iteration 146, loss = 0.53081705\n",
      "Iteration 147, loss = 0.53075402\n",
      "Iteration 148, loss = 0.53059102\n",
      "Iteration 149, loss = 0.53065466\n",
      "Iteration 150, loss = 0.53064395\n",
      "Iteration 151, loss = 0.53044987\n",
      "Iteration 152, loss = 0.53022422\n",
      "Iteration 153, loss = 0.53028333\n",
      "Iteration 154, loss = 0.53035315\n",
      "Iteration 155, loss = 0.53009396\n",
      "Iteration 156, loss = 0.53003817\n",
      "Iteration 157, loss = 0.52979036\n",
      "Iteration 158, loss = 0.52976579\n",
      "Iteration 159, loss = 0.53030157\n",
      "Iteration 160, loss = 0.53055148\n",
      "Iteration 161, loss = 0.53098571\n",
      "Iteration 162, loss = 0.53124589\n",
      "Iteration 163, loss = 0.53083039\n",
      "Iteration 164, loss = 0.53020609\n",
      "Iteration 165, loss = 0.52967596\n",
      "Iteration 166, loss = 0.52943566\n",
      "Iteration 167, loss = 0.52946716\n",
      "Iteration 168, loss = 0.52950339\n",
      "Iteration 169, loss = 0.52944266\n",
      "Iteration 170, loss = 0.52898441\n",
      "Iteration 171, loss = 0.52858328\n",
      "Iteration 172, loss = 0.52798891\n",
      "Iteration 173, loss = 0.52735262\n",
      "Iteration 174, loss = 0.52739873\n",
      "Iteration 175, loss = 0.52709096\n",
      "Iteration 176, loss = 0.52677559\n",
      "Iteration 177, loss = 0.52642687\n",
      "Iteration 178, loss = 0.52650959\n",
      "Iteration 179, loss = 0.52687128\n",
      "Iteration 180, loss = 0.52651408\n",
      "Iteration 181, loss = 0.52630507\n",
      "Iteration 182, loss = 0.52610799\n",
      "Iteration 183, loss = 0.52580583\n",
      "Iteration 184, loss = 0.52569553\n",
      "Iteration 185, loss = 0.52550162\n",
      "Iteration 186, loss = 0.52560221\n",
      "Iteration 187, loss = 0.52568936\n",
      "Iteration 188, loss = 0.52582423\n",
      "Iteration 189, loss = 0.52587141\n",
      "Iteration 190, loss = 0.52642332\n",
      "Iteration 191, loss = 0.52678257\n",
      "Iteration 192, loss = 0.52639134\n",
      "Iteration 193, loss = 0.52588828\n",
      "Iteration 194, loss = 0.52556001\n",
      "Iteration 195, loss = 0.52484356\n",
      "Iteration 196, loss = 0.52499294\n",
      "Iteration 197, loss = 0.52427749\n",
      "Iteration 198, loss = 0.52427703\n",
      "Iteration 199, loss = 0.52425580\n",
      "Iteration 200, loss = 0.52432399\n",
      "Iteration 201, loss = 0.52428008\n",
      "Iteration 202, loss = 0.52429468\n",
      "Iteration 203, loss = 0.52442705\n",
      "Iteration 204, loss = 0.52435635\n",
      "Iteration 205, loss = 0.52440303\n",
      "Iteration 206, loss = 0.52417859\n",
      "Iteration 207, loss = 0.52393034\n",
      "Iteration 208, loss = 0.52399211\n",
      "Iteration 209, loss = 0.52366412\n",
      "Iteration 210, loss = 0.52335138\n",
      "Iteration 211, loss = 0.52310440\n",
      "Iteration 212, loss = 0.52295692\n",
      "Iteration 213, loss = 0.52296458\n",
      "Iteration 214, loss = 0.52269872\n",
      "Iteration 215, loss = 0.52248696\n",
      "Iteration 216, loss = 0.52250597\n",
      "Iteration 217, loss = 0.52237598\n",
      "Iteration 218, loss = 0.52258184\n",
      "Iteration 219, loss = 0.52242476\n",
      "Iteration 220, loss = 0.52239227\n",
      "Iteration 221, loss = 0.52221198\n",
      "Iteration 222, loss = 0.52214615\n",
      "Iteration 223, loss = 0.52195692\n",
      "Iteration 224, loss = 0.52162336\n",
      "Iteration 225, loss = 0.52154640\n",
      "Iteration 226, loss = 0.52132072\n",
      "Iteration 227, loss = 0.52109871\n",
      "Iteration 228, loss = 0.52123557\n",
      "Iteration 229, loss = 0.52132366\n",
      "Iteration 230, loss = 0.52127731\n",
      "Iteration 231, loss = 0.52121416\n",
      "Iteration 232, loss = 0.52126889\n",
      "Iteration 233, loss = 0.52152213\n",
      "Iteration 234, loss = 0.52181189\n",
      "Iteration 235, loss = 0.52194796\n",
      "Iteration 236, loss = 0.52236547\n",
      "Iteration 237, loss = 0.52251658\n",
      "Iteration 238, loss = 0.52247564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67916531\n",
      "Iteration 2, loss = 0.65943515\n",
      "Iteration 3, loss = 0.64175991\n",
      "Iteration 4, loss = 0.62587813\n",
      "Iteration 5, loss = 0.61157296\n",
      "Iteration 6, loss = 0.59932306\n",
      "Iteration 7, loss = 0.58769050\n",
      "Iteration 8, loss = 0.57790131\n",
      "Iteration 9, loss = 0.56928018\n",
      "Iteration 10, loss = 0.56161227\n",
      "Iteration 11, loss = 0.55493774\n",
      "Iteration 12, loss = 0.54976951\n",
      "Iteration 13, loss = 0.54503080\n",
      "Iteration 14, loss = 0.54138731\n",
      "Iteration 15, loss = 0.53818607\n",
      "Iteration 16, loss = 0.53589484\n",
      "Iteration 17, loss = 0.53418400\n",
      "Iteration 18, loss = 0.53330558\n",
      "Iteration 19, loss = 0.53196364\n",
      "Iteration 20, loss = 0.53112304\n",
      "Iteration 21, loss = 0.53075716\n",
      "Iteration 22, loss = 0.53013400\n",
      "Iteration 23, loss = 0.52948170\n",
      "Iteration 24, loss = 0.52906213\n",
      "Iteration 25, loss = 0.52875092\n",
      "Iteration 26, loss = 0.52831366\n",
      "Iteration 27, loss = 0.52796900\n",
      "Iteration 28, loss = 0.52772492\n",
      "Iteration 29, loss = 0.52778150\n",
      "Iteration 30, loss = 0.52752549\n",
      "Iteration 31, loss = 0.52750958\n",
      "Iteration 32, loss = 0.52768103\n",
      "Iteration 33, loss = 0.52749387\n",
      "Iteration 34, loss = 0.52730777\n",
      "Iteration 35, loss = 0.52719787\n",
      "Iteration 36, loss = 0.52683119\n",
      "Iteration 37, loss = 0.52661253\n",
      "Iteration 38, loss = 0.52676931\n",
      "Iteration 39, loss = 0.52628011\n",
      "Iteration 40, loss = 0.52618942\n",
      "Iteration 41, loss = 0.52611073\n",
      "Iteration 42, loss = 0.52590022\n",
      "Iteration 43, loss = 0.52546942\n",
      "Iteration 44, loss = 0.52530276\n",
      "Iteration 45, loss = 0.52529200\n",
      "Iteration 46, loss = 0.52498267\n",
      "Iteration 47, loss = 0.52492800\n",
      "Iteration 48, loss = 0.52466431\n",
      "Iteration 49, loss = 0.52445210\n",
      "Iteration 50, loss = 0.52428761\n",
      "Iteration 51, loss = 0.52417912\n",
      "Iteration 52, loss = 0.52407988\n",
      "Iteration 53, loss = 0.52402664\n",
      "Iteration 54, loss = 0.52410794\n",
      "Iteration 55, loss = 0.52393922\n",
      "Iteration 56, loss = 0.52394752\n",
      "Iteration 57, loss = 0.52374937\n",
      "Iteration 58, loss = 0.52379040\n",
      "Iteration 59, loss = 0.52364005\n",
      "Iteration 60, loss = 0.52338566\n",
      "Iteration 61, loss = 0.52330477\n",
      "Iteration 62, loss = 0.52325452\n",
      "Iteration 63, loss = 0.52300454\n",
      "Iteration 64, loss = 0.52288664\n",
      "Iteration 65, loss = 0.52283731\n",
      "Iteration 66, loss = 0.52281719\n",
      "Iteration 67, loss = 0.52285905\n",
      "Iteration 68, loss = 0.52289788\n",
      "Iteration 69, loss = 0.52285879\n",
      "Iteration 70, loss = 0.52266042\n",
      "Iteration 71, loss = 0.52286176\n",
      "Iteration 72, loss = 0.52240065\n",
      "Iteration 73, loss = 0.52210670\n",
      "Iteration 74, loss = 0.52179188\n",
      "Iteration 75, loss = 0.52160734\n",
      "Iteration 76, loss = 0.52178630\n",
      "Iteration 77, loss = 0.52196063\n",
      "Iteration 78, loss = 0.52219456\n",
      "Iteration 79, loss = 0.52220288\n",
      "Iteration 80, loss = 0.52220519\n",
      "Iteration 81, loss = 0.52203762\n",
      "Iteration 82, loss = 0.52180060\n",
      "Iteration 83, loss = 0.52174075\n",
      "Iteration 84, loss = 0.52165616\n",
      "Iteration 85, loss = 0.52146115\n",
      "Iteration 86, loss = 0.52111592\n",
      "Iteration 87, loss = 0.52075533\n",
      "Iteration 88, loss = 0.52044245\n",
      "Iteration 89, loss = 0.52037908\n",
      "Iteration 90, loss = 0.52034061\n",
      "Iteration 91, loss = 0.52021156\n",
      "Iteration 92, loss = 0.52010776\n",
      "Iteration 93, loss = 0.51992798\n",
      "Iteration 94, loss = 0.51966298\n",
      "Iteration 95, loss = 0.51954310\n",
      "Iteration 96, loss = 0.51936017\n",
      "Iteration 97, loss = 0.51931064\n",
      "Iteration 98, loss = 0.51980555\n",
      "Iteration 99, loss = 0.51974965\n",
      "Iteration 100, loss = 0.51978336\n",
      "Iteration 101, loss = 0.51996709\n",
      "Iteration 102, loss = 0.51965542\n",
      "Iteration 103, loss = 0.51956406\n",
      "Iteration 104, loss = 0.51900481\n",
      "Iteration 105, loss = 0.51860922\n",
      "Iteration 106, loss = 0.51845947\n",
      "Iteration 107, loss = 0.51820143\n",
      "Iteration 108, loss = 0.51845779\n",
      "Iteration 109, loss = 0.51834912\n",
      "Iteration 110, loss = 0.51860279\n",
      "Iteration 111, loss = 0.51856665\n",
      "Iteration 112, loss = 0.51868285\n",
      "Iteration 113, loss = 0.51883371\n",
      "Iteration 114, loss = 0.51870190\n",
      "Iteration 115, loss = 0.51855968\n",
      "Iteration 116, loss = 0.51815440\n",
      "Iteration 117, loss = 0.51796655\n",
      "Iteration 118, loss = 0.51764421\n",
      "Iteration 119, loss = 0.51743638\n",
      "Iteration 120, loss = 0.51722691\n",
      "Iteration 121, loss = 0.51731921\n",
      "Iteration 122, loss = 0.51718444\n",
      "Iteration 123, loss = 0.51755481\n",
      "Iteration 124, loss = 0.51774610\n",
      "Iteration 125, loss = 0.51757896\n",
      "Iteration 126, loss = 0.51725585\n",
      "Iteration 127, loss = 0.51689804\n",
      "Iteration 128, loss = 0.51653517\n",
      "Iteration 129, loss = 0.51630495\n",
      "Iteration 130, loss = 0.51663039\n",
      "Iteration 131, loss = 0.51650144\n",
      "Iteration 132, loss = 0.51687538\n",
      "Iteration 133, loss = 0.51722658\n",
      "Iteration 134, loss = 0.51712592\n",
      "Iteration 135, loss = 0.51720163\n",
      "Iteration 136, loss = 0.51673693\n",
      "Iteration 137, loss = 0.51646710\n",
      "Iteration 138, loss = 0.51655115\n",
      "Iteration 139, loss = 0.51612823\n",
      "Iteration 140, loss = 0.51569964\n",
      "Iteration 141, loss = 0.51574890\n",
      "Iteration 142, loss = 0.51538418\n",
      "Iteration 143, loss = 0.51526370\n",
      "Iteration 144, loss = 0.51499865\n",
      "Iteration 145, loss = 0.51485803\n",
      "Iteration 146, loss = 0.51492707\n",
      "Iteration 147, loss = 0.51468121\n",
      "Iteration 148, loss = 0.51455393\n",
      "Iteration 149, loss = 0.51459784\n",
      "Iteration 150, loss = 0.51458252\n",
      "Iteration 151, loss = 0.51448981\n",
      "Iteration 152, loss = 0.51471383\n",
      "Iteration 153, loss = 0.51415413\n",
      "Iteration 154, loss = 0.51383968\n",
      "Iteration 155, loss = 0.51386786\n",
      "Iteration 156, loss = 0.51385701\n",
      "Iteration 157, loss = 0.51375663\n",
      "Iteration 158, loss = 0.51370101\n",
      "Iteration 159, loss = 0.51361611\n",
      "Iteration 160, loss = 0.51350302\n",
      "Iteration 161, loss = 0.51344422\n",
      "Iteration 162, loss = 0.51318824\n",
      "Iteration 163, loss = 0.51291765\n",
      "Iteration 164, loss = 0.51279824\n",
      "Iteration 165, loss = 0.51311921\n",
      "Iteration 166, loss = 0.51303589\n",
      "Iteration 167, loss = 0.51301781\n",
      "Iteration 168, loss = 0.51291695\n",
      "Iteration 169, loss = 0.51290224\n",
      "Iteration 170, loss = 0.51291923\n",
      "Iteration 171, loss = 0.51284989\n",
      "Iteration 172, loss = 0.51262698\n",
      "Iteration 173, loss = 0.51246882\n",
      "Iteration 174, loss = 0.51233794\n",
      "Iteration 175, loss = 0.51199869\n",
      "Iteration 176, loss = 0.51185823\n",
      "Iteration 177, loss = 0.51179928\n",
      "Iteration 178, loss = 0.51197806\n",
      "Iteration 179, loss = 0.51216177\n",
      "Iteration 180, loss = 0.51325580\n",
      "Iteration 181, loss = 0.51296774\n",
      "Iteration 182, loss = 0.51258065\n",
      "Iteration 183, loss = 0.51208525\n",
      "Iteration 184, loss = 0.51197729\n",
      "Iteration 185, loss = 0.51127596\n",
      "Iteration 186, loss = 0.51104606\n",
      "Iteration 187, loss = 0.51191627\n",
      "Iteration 188, loss = 0.51218358\n",
      "Iteration 189, loss = 0.51206562\n",
      "Iteration 190, loss = 0.51187928\n",
      "Iteration 191, loss = 0.51177146\n",
      "Iteration 192, loss = 0.51156735\n",
      "Iteration 193, loss = 0.51156343\n",
      "Iteration 194, loss = 0.51166284\n",
      "Iteration 195, loss = 0.51167608\n",
      "Iteration 196, loss = 0.51134701\n",
      "Iteration 197, loss = 0.51080789\n",
      "Iteration 198, loss = 0.51072094\n",
      "Iteration 199, loss = 0.51017536\n",
      "Iteration 200, loss = 0.51022410\n",
      "Iteration 201, loss = 0.51002711\n",
      "Iteration 202, loss = 0.50987278\n",
      "Iteration 203, loss = 0.50982269\n",
      "Iteration 204, loss = 0.50973106\n",
      "Iteration 205, loss = 0.50958863\n",
      "Iteration 206, loss = 0.50960506\n",
      "Iteration 207, loss = 0.50940925\n",
      "Iteration 208, loss = 0.50949019\n",
      "Iteration 209, loss = 0.50942012\n",
      "Iteration 210, loss = 0.50940755\n",
      "Iteration 211, loss = 0.50925461\n",
      "Iteration 212, loss = 0.50920398\n",
      "Iteration 213, loss = 0.50906224\n",
      "Iteration 214, loss = 0.50899035\n",
      "Iteration 215, loss = 0.50904662\n",
      "Iteration 216, loss = 0.50916778\n",
      "Iteration 217, loss = 0.50959181\n",
      "Iteration 218, loss = 0.50950092\n",
      "Iteration 219, loss = 0.50952852\n",
      "Iteration 220, loss = 0.50944520\n",
      "Iteration 221, loss = 0.50964096\n",
      "Iteration 222, loss = 0.50965216\n",
      "Iteration 223, loss = 0.50930951\n",
      "Iteration 224, loss = 0.50890188\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.73988640\n",
      "Iteration 2, loss = 0.71359213\n",
      "Iteration 3, loss = 0.69119355\n",
      "Iteration 4, loss = 0.66846677\n",
      "Iteration 5, loss = 0.65011933\n",
      "Iteration 6, loss = 0.63347848\n",
      "Iteration 7, loss = 0.61808488\n",
      "Iteration 8, loss = 0.60494781\n",
      "Iteration 9, loss = 0.59280325\n",
      "Iteration 10, loss = 0.58210082\n",
      "Iteration 11, loss = 0.57397216\n",
      "Iteration 12, loss = 0.56535098\n",
      "Iteration 13, loss = 0.55897746\n",
      "Iteration 14, loss = 0.55317470\n",
      "Iteration 15, loss = 0.54878960\n",
      "Iteration 16, loss = 0.54473815\n",
      "Iteration 17, loss = 0.54101188\n",
      "Iteration 18, loss = 0.53854532\n",
      "Iteration 19, loss = 0.53648496\n",
      "Iteration 20, loss = 0.53401985\n",
      "Iteration 21, loss = 0.53232219\n",
      "Iteration 22, loss = 0.53093059\n",
      "Iteration 23, loss = 0.52989305\n",
      "Iteration 24, loss = 0.52883560\n",
      "Iteration 25, loss = 0.52796077\n",
      "Iteration 26, loss = 0.52750507\n",
      "Iteration 27, loss = 0.52704279\n",
      "Iteration 28, loss = 0.52660830\n",
      "Iteration 29, loss = 0.52672428\n",
      "Iteration 30, loss = 0.52624724\n",
      "Iteration 31, loss = 0.52601973\n",
      "Iteration 32, loss = 0.52598197\n",
      "Iteration 33, loss = 0.52595458\n",
      "Iteration 34, loss = 0.52565821\n",
      "Iteration 35, loss = 0.52552243\n",
      "Iteration 36, loss = 0.52545176\n",
      "Iteration 37, loss = 0.52542039\n",
      "Iteration 38, loss = 0.52534319\n",
      "Iteration 39, loss = 0.52517768\n",
      "Iteration 40, loss = 0.52499092\n",
      "Iteration 41, loss = 0.52489140\n",
      "Iteration 42, loss = 0.52469360\n",
      "Iteration 43, loss = 0.52450254\n",
      "Iteration 44, loss = 0.52442529\n",
      "Iteration 45, loss = 0.52436683\n",
      "Iteration 46, loss = 0.52407630\n",
      "Iteration 47, loss = 0.52398365\n",
      "Iteration 48, loss = 0.52403716\n",
      "Iteration 49, loss = 0.52395468\n",
      "Iteration 50, loss = 0.52386431\n",
      "Iteration 51, loss = 0.52376410\n",
      "Iteration 52, loss = 0.52359525\n",
      "Iteration 53, loss = 0.52341248\n",
      "Iteration 54, loss = 0.52333320\n",
      "Iteration 55, loss = 0.52313473\n",
      "Iteration 56, loss = 0.52304381\n",
      "Iteration 57, loss = 0.52296019\n",
      "Iteration 58, loss = 0.52298323\n",
      "Iteration 59, loss = 0.52294488\n",
      "Iteration 60, loss = 0.52283759\n",
      "Iteration 61, loss = 0.52275753\n",
      "Iteration 62, loss = 0.52269512\n",
      "Iteration 63, loss = 0.52258740\n",
      "Iteration 64, loss = 0.52250594\n",
      "Iteration 65, loss = 0.52231751\n",
      "Iteration 66, loss = 0.52250137\n",
      "Iteration 67, loss = 0.52223311\n",
      "Iteration 68, loss = 0.52195896\n",
      "Iteration 69, loss = 0.52212301\n",
      "Iteration 70, loss = 0.52193742\n",
      "Iteration 71, loss = 0.52186097\n",
      "Iteration 72, loss = 0.52187869\n",
      "Iteration 73, loss = 0.52174446\n",
      "Iteration 74, loss = 0.52161949\n",
      "Iteration 75, loss = 0.52153463\n",
      "Iteration 76, loss = 0.52146231\n",
      "Iteration 77, loss = 0.52129708\n",
      "Iteration 78, loss = 0.52114147\n",
      "Iteration 79, loss = 0.52102059\n",
      "Iteration 80, loss = 0.52094730\n",
      "Iteration 81, loss = 0.52094814\n",
      "Iteration 82, loss = 0.52085041\n",
      "Iteration 83, loss = 0.52109924\n",
      "Iteration 84, loss = 0.52093826\n",
      "Iteration 85, loss = 0.52098934\n",
      "Iteration 86, loss = 0.52090988\n",
      "Iteration 87, loss = 0.52083084\n",
      "Iteration 88, loss = 0.52064392\n",
      "Iteration 89, loss = 0.52048953\n",
      "Iteration 90, loss = 0.52035179\n",
      "Iteration 91, loss = 0.52022730\n",
      "Iteration 92, loss = 0.52000166\n",
      "Iteration 93, loss = 0.52003414\n",
      "Iteration 94, loss = 0.51994031\n",
      "Iteration 95, loss = 0.51982234\n",
      "Iteration 96, loss = 0.51987252\n",
      "Iteration 97, loss = 0.51972862\n",
      "Iteration 98, loss = 0.51961290\n",
      "Iteration 99, loss = 0.51976245\n",
      "Iteration 100, loss = 0.51979770\n",
      "Iteration 101, loss = 0.52003412\n",
      "Iteration 102, loss = 0.51987102\n",
      "Iteration 103, loss = 0.51967721\n",
      "Iteration 104, loss = 0.51951300\n",
      "Iteration 105, loss = 0.51917130\n",
      "Iteration 106, loss = 0.51884729\n",
      "Iteration 107, loss = 0.51908033\n",
      "Iteration 108, loss = 0.51874826\n",
      "Iteration 109, loss = 0.51864818\n",
      "Iteration 110, loss = 0.51877800\n",
      "Iteration 111, loss = 0.51893122\n",
      "Iteration 112, loss = 0.51905167\n",
      "Iteration 113, loss = 0.51921780\n",
      "Iteration 114, loss = 0.51939197\n",
      "Iteration 115, loss = 0.51930655\n",
      "Iteration 116, loss = 0.51919444\n",
      "Iteration 117, loss = 0.51905108\n",
      "Iteration 118, loss = 0.51868036\n",
      "Iteration 119, loss = 0.51855338\n",
      "Iteration 120, loss = 0.51828244\n",
      "Iteration 121, loss = 0.51814888\n",
      "Iteration 122, loss = 0.51793537\n",
      "Iteration 123, loss = 0.51786668\n",
      "Iteration 124, loss = 0.51776122\n",
      "Iteration 125, loss = 0.51778490\n",
      "Iteration 126, loss = 0.51786701\n",
      "Iteration 127, loss = 0.51828510\n",
      "Iteration 128, loss = 0.51848720\n",
      "Iteration 129, loss = 0.51847937\n",
      "Iteration 130, loss = 0.51848299\n",
      "Iteration 131, loss = 0.51803543\n",
      "Iteration 132, loss = 0.51790279\n",
      "Iteration 133, loss = 0.51754019\n",
      "Iteration 134, loss = 0.51741587\n",
      "Iteration 135, loss = 0.51750243\n",
      "Iteration 136, loss = 0.51729522\n",
      "Iteration 137, loss = 0.51718806\n",
      "Iteration 138, loss = 0.51713410\n",
      "Iteration 139, loss = 0.51699584\n",
      "Iteration 140, loss = 0.51710043\n",
      "Iteration 141, loss = 0.51711399\n",
      "Iteration 142, loss = 0.51692731\n",
      "Iteration 143, loss = 0.51673394\n",
      "Iteration 144, loss = 0.51674075\n",
      "Iteration 145, loss = 0.51651975\n",
      "Iteration 146, loss = 0.51649369\n",
      "Iteration 147, loss = 0.51650684\n",
      "Iteration 148, loss = 0.51640099\n",
      "Iteration 149, loss = 0.51624719\n",
      "Iteration 150, loss = 0.51610267\n",
      "Iteration 151, loss = 0.51596622\n",
      "Iteration 152, loss = 0.51585570\n",
      "Iteration 153, loss = 0.51585035\n",
      "Iteration 154, loss = 0.51590653\n",
      "Iteration 155, loss = 0.51593858\n",
      "Iteration 156, loss = 0.51614949\n",
      "Iteration 157, loss = 0.51631705\n",
      "Iteration 158, loss = 0.51650156\n",
      "Iteration 159, loss = 0.51671902\n",
      "Iteration 160, loss = 0.51672503\n",
      "Iteration 161, loss = 0.51662243\n",
      "Iteration 162, loss = 0.51631321\n",
      "Iteration 163, loss = 0.51619814\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70219799\n",
      "Iteration 2, loss = 0.67986937\n",
      "Iteration 3, loss = 0.65874871\n",
      "Iteration 4, loss = 0.64022280\n",
      "Iteration 5, loss = 0.62354672\n",
      "Iteration 6, loss = 0.60853895\n",
      "Iteration 7, loss = 0.59457472\n",
      "Iteration 8, loss = 0.58268574\n",
      "Iteration 9, loss = 0.57287159\n",
      "Iteration 10, loss = 0.56298877\n",
      "Iteration 11, loss = 0.55582474\n",
      "Iteration 12, loss = 0.54877434\n",
      "Iteration 13, loss = 0.54336566\n",
      "Iteration 14, loss = 0.53881030\n",
      "Iteration 15, loss = 0.53544037\n",
      "Iteration 16, loss = 0.53243770\n",
      "Iteration 17, loss = 0.53013580\n",
      "Iteration 18, loss = 0.52835239\n",
      "Iteration 19, loss = 0.52739522\n",
      "Iteration 20, loss = 0.52633494\n",
      "Iteration 21, loss = 0.52517273\n",
      "Iteration 22, loss = 0.52459312\n",
      "Iteration 23, loss = 0.52398732\n",
      "Iteration 24, loss = 0.52323999\n",
      "Iteration 25, loss = 0.52318838\n",
      "Iteration 26, loss = 0.52257372\n",
      "Iteration 27, loss = 0.52228619\n",
      "Iteration 28, loss = 0.52202219\n",
      "Iteration 29, loss = 0.52177320\n",
      "Iteration 30, loss = 0.52155951\n",
      "Iteration 31, loss = 0.52146978\n",
      "Iteration 32, loss = 0.52106805\n",
      "Iteration 33, loss = 0.52080679\n",
      "Iteration 34, loss = 0.52063580\n",
      "Iteration 35, loss = 0.52065520\n",
      "Iteration 36, loss = 0.52055876\n",
      "Iteration 37, loss = 0.52031899\n",
      "Iteration 38, loss = 0.52021200\n",
      "Iteration 39, loss = 0.52000589\n",
      "Iteration 40, loss = 0.51974561\n",
      "Iteration 41, loss = 0.51958952\n",
      "Iteration 42, loss = 0.51933695\n",
      "Iteration 43, loss = 0.51916788\n",
      "Iteration 44, loss = 0.51893218\n",
      "Iteration 45, loss = 0.51878324\n",
      "Iteration 46, loss = 0.51858657\n",
      "Iteration 47, loss = 0.51850091\n",
      "Iteration 48, loss = 0.51822813\n",
      "Iteration 49, loss = 0.51818721\n",
      "Iteration 50, loss = 0.51821810\n",
      "Iteration 51, loss = 0.51795104\n",
      "Iteration 52, loss = 0.51779615\n",
      "Iteration 53, loss = 0.51771252\n",
      "Iteration 54, loss = 0.51759706\n",
      "Iteration 55, loss = 0.51746373\n",
      "Iteration 56, loss = 0.51726749\n",
      "Iteration 57, loss = 0.51704130\n",
      "Iteration 58, loss = 0.51700006\n",
      "Iteration 59, loss = 0.51678968\n",
      "Iteration 60, loss = 0.51660577\n",
      "Iteration 61, loss = 0.51640983\n",
      "Iteration 62, loss = 0.51637504\n",
      "Iteration 63, loss = 0.51614838\n",
      "Iteration 64, loss = 0.51602711\n",
      "Iteration 65, loss = 0.51594969\n",
      "Iteration 66, loss = 0.51587056\n",
      "Iteration 67, loss = 0.51610569\n",
      "Iteration 68, loss = 0.51630606\n",
      "Iteration 69, loss = 0.51632995\n",
      "Iteration 70, loss = 0.51629228\n",
      "Iteration 71, loss = 0.51645816\n",
      "Iteration 72, loss = 0.51676247\n",
      "Iteration 73, loss = 0.51664663\n",
      "Iteration 74, loss = 0.51651827\n",
      "Iteration 75, loss = 0.51620875\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.65569792\n",
      "Iteration 2, loss = 0.63860206\n",
      "Iteration 3, loss = 0.62491647\n",
      "Iteration 4, loss = 0.61183880\n",
      "Iteration 5, loss = 0.60029491\n",
      "Iteration 6, loss = 0.59092097\n",
      "Iteration 7, loss = 0.58206821\n",
      "Iteration 8, loss = 0.57424262\n",
      "Iteration 9, loss = 0.56863008\n",
      "Iteration 10, loss = 0.56358140\n",
      "Iteration 11, loss = 0.55920803\n",
      "Iteration 12, loss = 0.55605475\n",
      "Iteration 13, loss = 0.55337253\n",
      "Iteration 14, loss = 0.55175010\n",
      "Iteration 15, loss = 0.55004364\n",
      "Iteration 16, loss = 0.54939435\n",
      "Iteration 17, loss = 0.54843437\n",
      "Iteration 18, loss = 0.54811970\n",
      "Iteration 19, loss = 0.54767832\n",
      "Iteration 20, loss = 0.54743498\n",
      "Iteration 21, loss = 0.54689365\n",
      "Iteration 22, loss = 0.54669355\n",
      "Iteration 23, loss = 0.54642533\n",
      "Iteration 24, loss = 0.54633724\n",
      "Iteration 25, loss = 0.54620689\n",
      "Iteration 26, loss = 0.54603886\n",
      "Iteration 27, loss = 0.54588683\n",
      "Iteration 28, loss = 0.54572590\n",
      "Iteration 29, loss = 0.54543562\n",
      "Iteration 30, loss = 0.54512674\n",
      "Iteration 31, loss = 0.54488031\n",
      "Iteration 32, loss = 0.54475284\n",
      "Iteration 33, loss = 0.54460839\n",
      "Iteration 34, loss = 0.54435244\n",
      "Iteration 35, loss = 0.54417970\n",
      "Iteration 36, loss = 0.54396442\n",
      "Iteration 37, loss = 0.54408756\n",
      "Iteration 38, loss = 0.54384643\n",
      "Iteration 39, loss = 0.54368605\n",
      "Iteration 40, loss = 0.54339342\n",
      "Iteration 41, loss = 0.54305618\n",
      "Iteration 42, loss = 0.54295656\n",
      "Iteration 43, loss = 0.54240766\n",
      "Iteration 44, loss = 0.54216717\n",
      "Iteration 45, loss = 0.54187573\n",
      "Iteration 46, loss = 0.54166446\n",
      "Iteration 47, loss = 0.54159414\n",
      "Iteration 48, loss = 0.54174801\n",
      "Iteration 49, loss = 0.54171981\n",
      "Iteration 50, loss = 0.54180650\n",
      "Iteration 51, loss = 0.54199725\n",
      "Iteration 52, loss = 0.54212034\n",
      "Iteration 53, loss = 0.54187578\n",
      "Iteration 54, loss = 0.54155624\n",
      "Iteration 55, loss = 0.54128789\n",
      "Iteration 56, loss = 0.54117263\n",
      "Iteration 57, loss = 0.54113499\n",
      "Iteration 58, loss = 0.54083359\n",
      "Iteration 59, loss = 0.54055991\n",
      "Iteration 60, loss = 0.54027662\n",
      "Iteration 61, loss = 0.53995814\n",
      "Iteration 62, loss = 0.53988139\n",
      "Iteration 63, loss = 0.53932245\n",
      "Iteration 64, loss = 0.53911407\n",
      "Iteration 65, loss = 0.53909622\n",
      "Iteration 66, loss = 0.53904396\n",
      "Iteration 67, loss = 0.53902540\n",
      "Iteration 68, loss = 0.53902507\n",
      "Iteration 69, loss = 0.53890196\n",
      "Iteration 70, loss = 0.53881194\n",
      "Iteration 71, loss = 0.53855190\n",
      "Iteration 72, loss = 0.53830768\n",
      "Iteration 73, loss = 0.53807019\n",
      "Iteration 74, loss = 0.53792191\n",
      "Iteration 75, loss = 0.53793624\n",
      "Iteration 76, loss = 0.53770563\n",
      "Iteration 77, loss = 0.53746160\n",
      "Iteration 78, loss = 0.53750067\n",
      "Iteration 79, loss = 0.53747730\n",
      "Iteration 80, loss = 0.53755886\n",
      "Iteration 81, loss = 0.53753019\n",
      "Iteration 82, loss = 0.53734761\n",
      "Iteration 83, loss = 0.53725888\n",
      "Iteration 84, loss = 0.53711764\n",
      "Iteration 85, loss = 0.53676549\n",
      "Iteration 86, loss = 0.53647998\n",
      "Iteration 87, loss = 0.53639591\n",
      "Iteration 88, loss = 0.53642881\n",
      "Iteration 89, loss = 0.53646923\n",
      "Iteration 90, loss = 0.53672077\n",
      "Iteration 91, loss = 0.53643915\n",
      "Iteration 92, loss = 0.53620866\n",
      "Iteration 93, loss = 0.53587402\n",
      "Iteration 94, loss = 0.53564253\n",
      "Iteration 95, loss = 0.53570400\n",
      "Iteration 96, loss = 0.53559637\n",
      "Iteration 97, loss = 0.53556790\n",
      "Iteration 98, loss = 0.53530943\n",
      "Iteration 99, loss = 0.53517322\n",
      "Iteration 100, loss = 0.53506006\n",
      "Iteration 101, loss = 0.53469324\n",
      "Iteration 102, loss = 0.53466721\n",
      "Iteration 103, loss = 0.53478740\n",
      "Iteration 104, loss = 0.53473582\n",
      "Iteration 105, loss = 0.53467996\n",
      "Iteration 106, loss = 0.53477470\n",
      "Iteration 107, loss = 0.53464849\n",
      "Iteration 108, loss = 0.53449901\n",
      "Iteration 109, loss = 0.53443809\n",
      "Iteration 110, loss = 0.53401975\n",
      "Iteration 111, loss = 0.53400782\n",
      "Iteration 112, loss = 0.53389332\n",
      "Iteration 113, loss = 0.53356580\n",
      "Iteration 114, loss = 0.53318067\n",
      "Iteration 115, loss = 0.53284367\n",
      "Iteration 116, loss = 0.53290152\n",
      "Iteration 117, loss = 0.53270500\n",
      "Iteration 118, loss = 0.53289760\n",
      "Iteration 119, loss = 0.53301132\n",
      "Iteration 120, loss = 0.53300945\n",
      "Iteration 121, loss = 0.53314465\n",
      "Iteration 122, loss = 0.53297601\n",
      "Iteration 123, loss = 0.53262891\n",
      "Iteration 124, loss = 0.53258305\n",
      "Iteration 125, loss = 0.53247025\n",
      "Iteration 126, loss = 0.53222546\n",
      "Iteration 127, loss = 0.53211570\n",
      "Iteration 128, loss = 0.53184443\n",
      "Iteration 129, loss = 0.53176042\n",
      "Iteration 130, loss = 0.53156171\n",
      "Iteration 131, loss = 0.53133799\n",
      "Iteration 132, loss = 0.53130949\n",
      "Iteration 133, loss = 0.53114555\n",
      "Iteration 134, loss = 0.53110829\n",
      "Iteration 135, loss = 0.53111818\n",
      "Iteration 136, loss = 0.53090503\n",
      "Iteration 137, loss = 0.53069247\n",
      "Iteration 138, loss = 0.53044174\n",
      "Iteration 139, loss = 0.53032007\n",
      "Iteration 140, loss = 0.53015164\n",
      "Iteration 141, loss = 0.53000816\n",
      "Iteration 142, loss = 0.53052153\n",
      "Iteration 143, loss = 0.53132387\n",
      "Iteration 144, loss = 0.53205154\n",
      "Iteration 145, loss = 0.53230285\n",
      "Iteration 146, loss = 0.53211963\n",
      "Iteration 147, loss = 0.53145738\n",
      "Iteration 148, loss = 0.53093044\n",
      "Iteration 149, loss = 0.53055579\n",
      "Iteration 150, loss = 0.53003693\n",
      "Iteration 151, loss = 0.52980430\n",
      "Iteration 152, loss = 0.52944925\n",
      "Iteration 153, loss = 0.52915169\n",
      "Iteration 154, loss = 0.52920797\n",
      "Iteration 155, loss = 0.52889261\n",
      "Iteration 156, loss = 0.52909325\n",
      "Iteration 157, loss = 0.52955405\n",
      "Iteration 158, loss = 0.52968689\n",
      "Iteration 159, loss = 0.52987920\n",
      "Iteration 160, loss = 0.53016481\n",
      "Iteration 161, loss = 0.53008816\n",
      "Iteration 162, loss = 0.52974668\n",
      "Iteration 163, loss = 0.52947011\n",
      "Iteration 164, loss = 0.52913856\n",
      "Iteration 165, loss = 0.52860976\n",
      "Iteration 166, loss = 0.52860682\n",
      "Iteration 167, loss = 0.52804192\n",
      "Iteration 168, loss = 0.52804598\n",
      "Iteration 169, loss = 0.52814709\n",
      "Iteration 170, loss = 0.52797615\n",
      "Iteration 171, loss = 0.52809682\n",
      "Iteration 172, loss = 0.52831852\n",
      "Iteration 173, loss = 0.52835944\n",
      "Iteration 174, loss = 0.52830392\n",
      "Iteration 175, loss = 0.52809015\n",
      "Iteration 176, loss = 0.52758563\n",
      "Iteration 177, loss = 0.52709987\n",
      "Iteration 178, loss = 0.52696268\n",
      "Iteration 179, loss = 0.52670747\n",
      "Iteration 180, loss = 0.52655454\n",
      "Iteration 181, loss = 0.52667857\n",
      "Iteration 182, loss = 0.52695261\n",
      "Iteration 183, loss = 0.52715571\n",
      "Iteration 184, loss = 0.52735300\n",
      "Iteration 185, loss = 0.52731160\n",
      "Iteration 186, loss = 0.52700897\n",
      "Iteration 187, loss = 0.52672455\n",
      "Iteration 188, loss = 0.52675318\n",
      "Iteration 189, loss = 0.52703199\n",
      "Iteration 190, loss = 0.52730424\n",
      "Iteration 191, loss = 0.52785582\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.66796419\n",
      "Iteration 2, loss = 0.64539701\n",
      "Iteration 3, loss = 0.62514774\n",
      "Iteration 4, loss = 0.60654201\n",
      "Iteration 5, loss = 0.59085943\n",
      "Iteration 6, loss = 0.57520704\n",
      "Iteration 7, loss = 0.56234432\n",
      "Iteration 8, loss = 0.55150383\n",
      "Iteration 9, loss = 0.54067124\n",
      "Iteration 10, loss = 0.53219579\n",
      "Iteration 11, loss = 0.52502942\n",
      "Iteration 12, loss = 0.51932807\n",
      "Iteration 13, loss = 0.51438572\n",
      "Iteration 14, loss = 0.51036727\n",
      "Iteration 15, loss = 0.50653223\n",
      "Iteration 16, loss = 0.50419529\n",
      "Iteration 17, loss = 0.50180597\n",
      "Iteration 18, loss = 0.50020953\n",
      "Iteration 19, loss = 0.49857709\n",
      "Iteration 20, loss = 0.49744345\n",
      "Iteration 21, loss = 0.49717138\n",
      "Iteration 22, loss = 0.49616964\n",
      "Iteration 23, loss = 0.49557223\n",
      "Iteration 24, loss = 0.49541723\n",
      "Iteration 25, loss = 0.49522330\n",
      "Iteration 26, loss = 0.49467431\n",
      "Iteration 27, loss = 0.49429513\n",
      "Iteration 28, loss = 0.49389385\n",
      "Iteration 29, loss = 0.49368996\n",
      "Iteration 30, loss = 0.49344078\n",
      "Iteration 31, loss = 0.49325410\n",
      "Iteration 32, loss = 0.49302977\n",
      "Iteration 33, loss = 0.49293642\n",
      "Iteration 34, loss = 0.49286658\n",
      "Iteration 35, loss = 0.49257186\n",
      "Iteration 36, loss = 0.49251356\n",
      "Iteration 37, loss = 0.49220069\n",
      "Iteration 38, loss = 0.49184420\n",
      "Iteration 39, loss = 0.49141284\n",
      "Iteration 40, loss = 0.49129204\n",
      "Iteration 41, loss = 0.49096975\n",
      "Iteration 42, loss = 0.49077308\n",
      "Iteration 43, loss = 0.49095565\n",
      "Iteration 44, loss = 0.49066709\n",
      "Iteration 45, loss = 0.49054684\n",
      "Iteration 46, loss = 0.49057331\n",
      "Iteration 47, loss = 0.49044037\n",
      "Iteration 48, loss = 0.49001595\n",
      "Iteration 49, loss = 0.48973876\n",
      "Iteration 50, loss = 0.48943966\n",
      "Iteration 51, loss = 0.48906680\n",
      "Iteration 52, loss = 0.48878878\n",
      "Iteration 53, loss = 0.48883359\n",
      "Iteration 54, loss = 0.48877450\n",
      "Iteration 55, loss = 0.48860974\n",
      "Iteration 56, loss = 0.48849186\n",
      "Iteration 57, loss = 0.48835063\n",
      "Iteration 58, loss = 0.48824339\n",
      "Iteration 59, loss = 0.48813781\n",
      "Iteration 60, loss = 0.48794651\n",
      "Iteration 61, loss = 0.48785467\n",
      "Iteration 62, loss = 0.48759914\n",
      "Iteration 63, loss = 0.48738468\n",
      "Iteration 64, loss = 0.48724006\n",
      "Iteration 65, loss = 0.48704756\n",
      "Iteration 66, loss = 0.48679993\n",
      "Iteration 67, loss = 0.48673978\n",
      "Iteration 68, loss = 0.48643996\n",
      "Iteration 69, loss = 0.48619976\n",
      "Iteration 70, loss = 0.48639326\n",
      "Iteration 71, loss = 0.48633637\n",
      "Iteration 72, loss = 0.48624297\n",
      "Iteration 73, loss = 0.48612256\n",
      "Iteration 74, loss = 0.48581913\n",
      "Iteration 75, loss = 0.48569105\n",
      "Iteration 76, loss = 0.48560747\n",
      "Iteration 77, loss = 0.48554210\n",
      "Iteration 78, loss = 0.48556105\n",
      "Iteration 79, loss = 0.48532385\n",
      "Iteration 80, loss = 0.48520591\n",
      "Iteration 81, loss = 0.48496561\n",
      "Iteration 82, loss = 0.48488504\n",
      "Iteration 83, loss = 0.48485976\n",
      "Iteration 84, loss = 0.48486174\n",
      "Iteration 85, loss = 0.48466723\n",
      "Iteration 86, loss = 0.48430841\n",
      "Iteration 87, loss = 0.48386214\n",
      "Iteration 88, loss = 0.48351599\n",
      "Iteration 89, loss = 0.48399328\n",
      "Iteration 90, loss = 0.48368598\n",
      "Iteration 91, loss = 0.48357750\n",
      "Iteration 92, loss = 0.48355380\n",
      "Iteration 93, loss = 0.48336735\n",
      "Iteration 94, loss = 0.48323234\n",
      "Iteration 95, loss = 0.48304489\n",
      "Iteration 96, loss = 0.48304982\n",
      "Iteration 97, loss = 0.48276688\n",
      "Iteration 98, loss = 0.48266428\n",
      "Iteration 99, loss = 0.48252149\n",
      "Iteration 100, loss = 0.48233591\n",
      "Iteration 101, loss = 0.48223260\n",
      "Iteration 102, loss = 0.48211918\n",
      "Iteration 103, loss = 0.48199035\n",
      "Iteration 104, loss = 0.48177419\n",
      "Iteration 105, loss = 0.48165197\n",
      "Iteration 106, loss = 0.48158372\n",
      "Iteration 107, loss = 0.48163835\n",
      "Iteration 108, loss = 0.48149260\n",
      "Iteration 109, loss = 0.48142214\n",
      "Iteration 110, loss = 0.48138714\n",
      "Iteration 111, loss = 0.48140412\n",
      "Iteration 112, loss = 0.48133368\n",
      "Iteration 113, loss = 0.48146109\n",
      "Iteration 114, loss = 0.48166453\n",
      "Iteration 115, loss = 0.48165390\n",
      "Iteration 116, loss = 0.48112831\n",
      "Iteration 117, loss = 0.48089348\n",
      "Iteration 118, loss = 0.48044668\n",
      "Iteration 119, loss = 0.48028282\n",
      "Iteration 120, loss = 0.47983482\n",
      "Iteration 121, loss = 0.47989664\n",
      "Iteration 122, loss = 0.47955255\n",
      "Iteration 123, loss = 0.47935364\n",
      "Iteration 124, loss = 0.47929571\n",
      "Iteration 125, loss = 0.47937253\n",
      "Iteration 126, loss = 0.47914276\n",
      "Iteration 127, loss = 0.47888925\n",
      "Iteration 128, loss = 0.47876788\n",
      "Iteration 129, loss = 0.47870352\n",
      "Iteration 130, loss = 0.47873990\n",
      "Iteration 131, loss = 0.47864235\n",
      "Iteration 132, loss = 0.47882419\n",
      "Iteration 133, loss = 0.47874012\n",
      "Iteration 134, loss = 0.47874276\n",
      "Iteration 135, loss = 0.47841968\n",
      "Iteration 136, loss = 0.47814842\n",
      "Iteration 137, loss = 0.47793260\n",
      "Iteration 138, loss = 0.47766227\n",
      "Iteration 139, loss = 0.47753634\n",
      "Iteration 140, loss = 0.47753493\n",
      "Iteration 141, loss = 0.47748980\n",
      "Iteration 142, loss = 0.47743648\n",
      "Iteration 143, loss = 0.47773697\n",
      "Iteration 144, loss = 0.47765676\n",
      "Iteration 145, loss = 0.47757164\n",
      "Iteration 146, loss = 0.47757610\n",
      "Iteration 147, loss = 0.47738034\n",
      "Iteration 148, loss = 0.47715646\n",
      "Iteration 149, loss = 0.47722201\n",
      "Iteration 150, loss = 0.47740206\n",
      "Iteration 151, loss = 0.47683927\n",
      "Iteration 152, loss = 0.47659955\n",
      "Iteration 153, loss = 0.47596577\n",
      "Iteration 154, loss = 0.47600967\n",
      "Iteration 155, loss = 0.47548586\n",
      "Iteration 156, loss = 0.47537313\n",
      "Iteration 157, loss = 0.47524100\n",
      "Iteration 158, loss = 0.47513998\n",
      "Iteration 159, loss = 0.47523832\n",
      "Iteration 160, loss = 0.47576094\n",
      "Iteration 161, loss = 0.47593040\n",
      "Iteration 162, loss = 0.47578314\n",
      "Iteration 163, loss = 0.47538777\n",
      "Iteration 164, loss = 0.47543515\n",
      "Iteration 165, loss = 0.47486926\n",
      "Iteration 166, loss = 0.47469921\n",
      "Iteration 167, loss = 0.47478234\n",
      "Iteration 168, loss = 0.47475216\n",
      "Iteration 169, loss = 0.47483392\n",
      "Iteration 170, loss = 0.47493054\n",
      "Iteration 171, loss = 0.47550615\n",
      "Iteration 172, loss = 0.47483834\n",
      "Iteration 173, loss = 0.47446713\n",
      "Iteration 174, loss = 0.47379999\n",
      "Iteration 175, loss = 0.47345474\n",
      "Iteration 176, loss = 0.47315910\n",
      "Iteration 177, loss = 0.47312287\n",
      "Iteration 178, loss = 0.47282481\n",
      "Iteration 179, loss = 0.47262636\n",
      "Iteration 180, loss = 0.47339711\n",
      "Iteration 181, loss = 0.47353322\n",
      "Iteration 182, loss = 0.47357615\n",
      "Iteration 183, loss = 0.47369729\n",
      "Iteration 184, loss = 0.47365282\n",
      "Iteration 185, loss = 0.47333302\n",
      "Iteration 186, loss = 0.47327583\n",
      "Iteration 187, loss = 0.47293101\n",
      "Iteration 188, loss = 0.47295963\n",
      "Iteration 189, loss = 0.47264106\n",
      "Iteration 190, loss = 0.47211207\n",
      "Iteration 191, loss = 0.47195001\n",
      "Iteration 192, loss = 0.47154479\n",
      "Iteration 193, loss = 0.47114152\n",
      "Iteration 194, loss = 0.47095073\n",
      "Iteration 195, loss = 0.47073935\n",
      "Iteration 196, loss = 0.47076702\n",
      "Iteration 197, loss = 0.47097431\n",
      "Iteration 198, loss = 0.47079719\n",
      "Iteration 199, loss = 0.47099401\n",
      "Iteration 200, loss = 0.47071340\n",
      "Iteration 201, loss = 0.47043472\n",
      "Iteration 202, loss = 0.47061637\n",
      "Iteration 203, loss = 0.47029088\n",
      "Iteration 204, loss = 0.47017915\n",
      "Iteration 205, loss = 0.47000299\n",
      "Iteration 206, loss = 0.47036408\n",
      "Iteration 207, loss = 0.47006722\n",
      "Iteration 208, loss = 0.46989686\n",
      "Iteration 209, loss = 0.46942398\n",
      "Iteration 210, loss = 0.46922190\n",
      "Iteration 211, loss = 0.46889863\n",
      "Iteration 212, loss = 0.46888907\n",
      "Iteration 213, loss = 0.46888689\n",
      "Iteration 214, loss = 0.46901477\n",
      "Iteration 215, loss = 0.46903142\n",
      "Iteration 216, loss = 0.46868400\n",
      "Iteration 217, loss = 0.46828068\n",
      "Iteration 218, loss = 0.46799116\n",
      "Iteration 219, loss = 0.46851757\n",
      "Iteration 220, loss = 0.46842388\n",
      "Iteration 221, loss = 0.46837011\n",
      "Iteration 222, loss = 0.46837528\n",
      "Iteration 223, loss = 0.46814285\n",
      "Iteration 224, loss = 0.46774906\n",
      "Iteration 225, loss = 0.46733940\n",
      "Iteration 226, loss = 0.46754769\n",
      "Iteration 227, loss = 0.46752209\n",
      "Iteration 228, loss = 0.46753041\n",
      "Iteration 229, loss = 0.46807865\n",
      "Iteration 230, loss = 0.46790682\n",
      "Iteration 231, loss = 0.46741420\n",
      "Iteration 232, loss = 0.46688559\n",
      "Iteration 233, loss = 0.46617619\n",
      "Iteration 234, loss = 0.46620955\n",
      "Iteration 235, loss = 0.46674951\n",
      "Iteration 236, loss = 0.46724725\n",
      "Iteration 237, loss = 0.46800173\n",
      "Iteration 238, loss = 0.46799414\n",
      "Iteration 239, loss = 0.46709318\n",
      "Iteration 240, loss = 0.46628323\n",
      "Iteration 241, loss = 0.46539646\n",
      "Iteration 242, loss = 0.46553681\n",
      "Iteration 243, loss = 0.46581402\n",
      "Iteration 244, loss = 0.46583927\n",
      "Iteration 245, loss = 0.46589283\n",
      "Iteration 246, loss = 0.46569725\n",
      "Iteration 247, loss = 0.46542335\n",
      "Iteration 248, loss = 0.46515600\n",
      "Iteration 249, loss = 0.46514873\n",
      "Iteration 250, loss = 0.46503838\n",
      "Iteration 251, loss = 0.46513749\n",
      "Iteration 252, loss = 0.46495947\n",
      "Iteration 253, loss = 0.46451586\n",
      "Iteration 254, loss = 0.46446195\n",
      "Iteration 255, loss = 0.46465338\n",
      "Iteration 256, loss = 0.46428788\n",
      "Iteration 257, loss = 0.46431617\n",
      "Iteration 258, loss = 0.46388781\n",
      "Iteration 259, loss = 0.46376885\n",
      "Iteration 260, loss = 0.46351790\n",
      "Iteration 261, loss = 0.46454957\n",
      "Iteration 262, loss = 0.46433108\n",
      "Iteration 263, loss = 0.46429726\n",
      "Iteration 264, loss = 0.46414315\n",
      "Iteration 265, loss = 0.46372801\n",
      "Iteration 266, loss = 0.46345682\n",
      "Iteration 267, loss = 0.46337657\n",
      "Iteration 268, loss = 0.46355176\n",
      "Iteration 269, loss = 0.46442671\n",
      "Iteration 270, loss = 0.46418790\n",
      "Iteration 271, loss = 0.46430023\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67385364\n",
      "Iteration 2, loss = 0.65084003\n",
      "Iteration 3, loss = 0.63267958\n",
      "Iteration 4, loss = 0.61468558\n",
      "Iteration 5, loss = 0.59979051\n",
      "Iteration 6, loss = 0.58541074\n",
      "Iteration 7, loss = 0.57397576\n",
      "Iteration 8, loss = 0.56365712\n",
      "Iteration 9, loss = 0.55409868\n",
      "Iteration 10, loss = 0.54593246\n",
      "Iteration 11, loss = 0.53920155\n",
      "Iteration 12, loss = 0.53369825\n",
      "Iteration 13, loss = 0.52866124\n",
      "Iteration 14, loss = 0.52485764\n",
      "Iteration 15, loss = 0.52160800\n",
      "Iteration 16, loss = 0.51933054\n",
      "Iteration 17, loss = 0.51761826\n",
      "Iteration 18, loss = 0.51582325\n",
      "Iteration 19, loss = 0.51487061\n",
      "Iteration 20, loss = 0.51390342\n",
      "Iteration 21, loss = 0.51400573\n",
      "Iteration 22, loss = 0.51328160\n",
      "Iteration 23, loss = 0.51302540\n",
      "Iteration 24, loss = 0.51279639\n",
      "Iteration 25, loss = 0.51254817\n",
      "Iteration 26, loss = 0.51228954\n",
      "Iteration 27, loss = 0.51214212\n",
      "Iteration 28, loss = 0.51196368\n",
      "Iteration 29, loss = 0.51188308\n",
      "Iteration 30, loss = 0.51168961\n",
      "Iteration 31, loss = 0.51155193\n",
      "Iteration 32, loss = 0.51159830\n",
      "Iteration 33, loss = 0.51170555\n",
      "Iteration 34, loss = 0.51152167\n",
      "Iteration 35, loss = 0.51156771\n",
      "Iteration 36, loss = 0.51160531\n",
      "Iteration 37, loss = 0.51191778\n",
      "Iteration 38, loss = 0.51211552\n",
      "Iteration 39, loss = 0.51225702\n",
      "Iteration 40, loss = 0.51224295\n",
      "Iteration 41, loss = 0.51239465\n",
      "Iteration 42, loss = 0.51249664\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69550965\n",
      "Iteration 2, loss = 0.67685620\n",
      "Iteration 3, loss = 0.65962807\n",
      "Iteration 4, loss = 0.64517107\n",
      "Iteration 5, loss = 0.63197829\n",
      "Iteration 6, loss = 0.61986429\n",
      "Iteration 7, loss = 0.61007341\n",
      "Iteration 8, loss = 0.60099090\n",
      "Iteration 9, loss = 0.59373627\n",
      "Iteration 10, loss = 0.58755996\n",
      "Iteration 11, loss = 0.58236036\n",
      "Iteration 12, loss = 0.57799334\n",
      "Iteration 13, loss = 0.57522806\n",
      "Iteration 14, loss = 0.57281556\n",
      "Iteration 15, loss = 0.57091467\n",
      "Iteration 16, loss = 0.56941771\n",
      "Iteration 17, loss = 0.56900399\n",
      "Iteration 18, loss = 0.56791726\n",
      "Iteration 19, loss = 0.56753734\n",
      "Iteration 20, loss = 0.56744564\n",
      "Iteration 21, loss = 0.56721070\n",
      "Iteration 22, loss = 0.56730192\n",
      "Iteration 23, loss = 0.56745384\n",
      "Iteration 24, loss = 0.56757525\n",
      "Iteration 25, loss = 0.56738810\n",
      "Iteration 26, loss = 0.56735326\n",
      "Iteration 27, loss = 0.56690546\n",
      "Iteration 28, loss = 0.56665690\n",
      "Iteration 29, loss = 0.56661283\n",
      "Iteration 30, loss = 0.56629602\n",
      "Iteration 31, loss = 0.56563697\n",
      "Iteration 32, loss = 0.56511107\n",
      "Iteration 33, loss = 0.56489494\n",
      "Iteration 34, loss = 0.56472848\n",
      "Iteration 35, loss = 0.56460783\n",
      "Iteration 36, loss = 0.56441954\n",
      "Iteration 37, loss = 0.56423081\n",
      "Iteration 38, loss = 0.56409214\n",
      "Iteration 39, loss = 0.56381448\n",
      "Iteration 40, loss = 0.56363386\n",
      "Iteration 41, loss = 0.56353129\n",
      "Iteration 42, loss = 0.56330554\n",
      "Iteration 43, loss = 0.56314656\n",
      "Iteration 44, loss = 0.56306336\n",
      "Iteration 45, loss = 0.56294743\n",
      "Iteration 46, loss = 0.56279786\n",
      "Iteration 47, loss = 0.56267615\n",
      "Iteration 48, loss = 0.56250411\n",
      "Iteration 49, loss = 0.56236985\n",
      "Iteration 50, loss = 0.56220028\n",
      "Iteration 51, loss = 0.56223990\n",
      "Iteration 52, loss = 0.56221645\n",
      "Iteration 53, loss = 0.56213862\n",
      "Iteration 54, loss = 0.56217037\n",
      "Iteration 55, loss = 0.56200049\n",
      "Iteration 56, loss = 0.56187156\n",
      "Iteration 57, loss = 0.56191494\n",
      "Iteration 58, loss = 0.56168466\n",
      "Iteration 59, loss = 0.56145483\n",
      "Iteration 60, loss = 0.56127131\n",
      "Iteration 61, loss = 0.56113697\n",
      "Iteration 62, loss = 0.56099359\n",
      "Iteration 63, loss = 0.56085588\n",
      "Iteration 64, loss = 0.56076883\n",
      "Iteration 65, loss = 0.56071986\n",
      "Iteration 66, loss = 0.56095093\n",
      "Iteration 67, loss = 0.56072666\n",
      "Iteration 68, loss = 0.56075467\n",
      "Iteration 69, loss = 0.56097444\n",
      "Iteration 70, loss = 0.56114707\n",
      "Iteration 71, loss = 0.56119894\n",
      "Iteration 72, loss = 0.56110859\n",
      "Iteration 73, loss = 0.56076653\n",
      "Iteration 74, loss = 0.56057608\n",
      "Iteration 75, loss = 0.56041229\n",
      "Iteration 76, loss = 0.56007652\n",
      "Iteration 77, loss = 0.55994664\n",
      "Iteration 78, loss = 0.55982627\n",
      "Iteration 79, loss = 0.55971924\n",
      "Iteration 80, loss = 0.55966937\n",
      "Iteration 81, loss = 0.55945532\n",
      "Iteration 82, loss = 0.55929037\n",
      "Iteration 83, loss = 0.55911744\n",
      "Iteration 84, loss = 0.55897066\n",
      "Iteration 85, loss = 0.55900154\n",
      "Iteration 86, loss = 0.55897435\n",
      "Iteration 87, loss = 0.55857277\n",
      "Iteration 88, loss = 0.55851668\n",
      "Iteration 89, loss = 0.55827125\n",
      "Iteration 90, loss = 0.55799094\n",
      "Iteration 91, loss = 0.55816281\n",
      "Iteration 92, loss = 0.55808271\n",
      "Iteration 93, loss = 0.55813619\n",
      "Iteration 94, loss = 0.55808764\n",
      "Iteration 95, loss = 0.55771399\n",
      "Iteration 96, loss = 0.55740709\n",
      "Iteration 97, loss = 0.55736616\n",
      "Iteration 98, loss = 0.55726167\n",
      "Iteration 99, loss = 0.55722377\n",
      "Iteration 100, loss = 0.55712623\n",
      "Iteration 101, loss = 0.55689519\n",
      "Iteration 102, loss = 0.55674590\n",
      "Iteration 103, loss = 0.55658426\n",
      "Iteration 104, loss = 0.55654885\n",
      "Iteration 105, loss = 0.55641456\n",
      "Iteration 106, loss = 0.55630611\n",
      "Iteration 107, loss = 0.55641090\n",
      "Iteration 108, loss = 0.55622929\n",
      "Iteration 109, loss = 0.55630205\n",
      "Iteration 110, loss = 0.55603294\n",
      "Iteration 111, loss = 0.55597591\n",
      "Iteration 112, loss = 0.55607277\n",
      "Iteration 113, loss = 0.55646098\n",
      "Iteration 114, loss = 0.55637801\n",
      "Iteration 115, loss = 0.55621970\n",
      "Iteration 116, loss = 0.55593423\n",
      "Iteration 117, loss = 0.55545190\n",
      "Iteration 118, loss = 0.55555714\n",
      "Iteration 119, loss = 0.55499468\n",
      "Iteration 120, loss = 0.55507026\n",
      "Iteration 121, loss = 0.55511772\n",
      "Iteration 122, loss = 0.55504845\n",
      "Iteration 123, loss = 0.55545954\n",
      "Iteration 124, loss = 0.55526296\n",
      "Iteration 125, loss = 0.55506327\n",
      "Iteration 126, loss = 0.55479500\n",
      "Iteration 127, loss = 0.55437327\n",
      "Iteration 128, loss = 0.55451227\n",
      "Iteration 129, loss = 0.55433241\n",
      "Iteration 130, loss = 0.55413074\n",
      "Iteration 131, loss = 0.55392130\n",
      "Iteration 132, loss = 0.55370425\n",
      "Iteration 133, loss = 0.55347014\n",
      "Iteration 134, loss = 0.55352599\n",
      "Iteration 135, loss = 0.55317304\n",
      "Iteration 136, loss = 0.55342880\n",
      "Iteration 137, loss = 0.55337316\n",
      "Iteration 138, loss = 0.55318000\n",
      "Iteration 139, loss = 0.55304812\n",
      "Iteration 140, loss = 0.55297251\n",
      "Iteration 141, loss = 0.55312758\n",
      "Iteration 142, loss = 0.55290865\n",
      "Iteration 143, loss = 0.55266880\n",
      "Iteration 144, loss = 0.55259770\n",
      "Iteration 145, loss = 0.55252444\n",
      "Iteration 146, loss = 0.55242399\n",
      "Iteration 147, loss = 0.55234169\n",
      "Iteration 148, loss = 0.55222142\n",
      "Iteration 149, loss = 0.55208188\n",
      "Iteration 150, loss = 0.55189834\n",
      "Iteration 151, loss = 0.55187604\n",
      "Iteration 152, loss = 0.55221256\n",
      "Iteration 153, loss = 0.55210176\n",
      "Iteration 154, loss = 0.55195436\n",
      "Iteration 155, loss = 0.55176130\n",
      "Iteration 156, loss = 0.55166839\n",
      "Iteration 157, loss = 0.55164318\n",
      "Iteration 158, loss = 0.55129261\n",
      "Iteration 159, loss = 0.55148598\n",
      "Iteration 160, loss = 0.55114227\n",
      "Iteration 161, loss = 0.55107827\n",
      "Iteration 162, loss = 0.55077699\n",
      "Iteration 163, loss = 0.55075173\n",
      "Iteration 164, loss = 0.55067317\n",
      "Iteration 165, loss = 0.55085473\n",
      "Iteration 166, loss = 0.55045149\n",
      "Iteration 167, loss = 0.55028360\n",
      "Iteration 168, loss = 0.55039501\n",
      "Iteration 169, loss = 0.55040731\n",
      "Iteration 170, loss = 0.55035812\n",
      "Iteration 171, loss = 0.55041088\n",
      "Iteration 172, loss = 0.55066111\n",
      "Iteration 173, loss = 0.55024922\n",
      "Iteration 174, loss = 0.55000547\n",
      "Iteration 175, loss = 0.54959132\n",
      "Iteration 176, loss = 0.54955978\n",
      "Iteration 177, loss = 0.54961941\n",
      "Iteration 178, loss = 0.54901487\n",
      "Iteration 179, loss = 0.54919013\n",
      "Iteration 180, loss = 0.54895003\n",
      "Iteration 181, loss = 0.54884283\n",
      "Iteration 182, loss = 0.54887128\n",
      "Iteration 183, loss = 0.54895240\n",
      "Iteration 184, loss = 0.54886359\n",
      "Iteration 185, loss = 0.54892185\n",
      "Iteration 186, loss = 0.54862468\n",
      "Iteration 187, loss = 0.54866868\n",
      "Iteration 188, loss = 0.54824659\n",
      "Iteration 189, loss = 0.54809594\n",
      "Iteration 190, loss = 0.54839982\n",
      "Iteration 191, loss = 0.54923593\n",
      "Iteration 192, loss = 0.55022239\n",
      "Iteration 193, loss = 0.55037111\n",
      "Iteration 194, loss = 0.55020487\n",
      "Iteration 195, loss = 0.54967153\n",
      "Iteration 196, loss = 0.54920995\n",
      "Iteration 197, loss = 0.54815425\n",
      "Iteration 198, loss = 0.54766347\n",
      "Iteration 199, loss = 0.54702101\n",
      "Iteration 200, loss = 0.54695722\n",
      "Iteration 201, loss = 0.54700531\n",
      "Iteration 202, loss = 0.54741948\n",
      "Iteration 203, loss = 0.54755388\n",
      "Iteration 204, loss = 0.54767273\n",
      "Iteration 205, loss = 0.54770187\n",
      "Iteration 206, loss = 0.54800005\n",
      "Iteration 207, loss = 0.54796392\n",
      "Iteration 208, loss = 0.54793105\n",
      "Iteration 209, loss = 0.54767708\n",
      "Iteration 210, loss = 0.54744260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69398915\n",
      "Iteration 2, loss = 0.67352915\n",
      "Iteration 3, loss = 0.65561652\n",
      "Iteration 4, loss = 0.63760724\n",
      "Iteration 5, loss = 0.62419359\n",
      "Iteration 6, loss = 0.61059403\n",
      "Iteration 7, loss = 0.59995053\n",
      "Iteration 8, loss = 0.58998909\n",
      "Iteration 9, loss = 0.58132230\n",
      "Iteration 10, loss = 0.57345710\n",
      "Iteration 11, loss = 0.56741091\n",
      "Iteration 12, loss = 0.56131671\n",
      "Iteration 13, loss = 0.55720769\n",
      "Iteration 14, loss = 0.55348065\n",
      "Iteration 15, loss = 0.55040390\n",
      "Iteration 16, loss = 0.54797983\n",
      "Iteration 17, loss = 0.54615878\n",
      "Iteration 18, loss = 0.54452551\n",
      "Iteration 19, loss = 0.54394013\n",
      "Iteration 20, loss = 0.54259432\n",
      "Iteration 21, loss = 0.54235774\n",
      "Iteration 22, loss = 0.54166840\n",
      "Iteration 23, loss = 0.54131207\n",
      "Iteration 24, loss = 0.54112684\n",
      "Iteration 25, loss = 0.54082520\n",
      "Iteration 26, loss = 0.54055704\n",
      "Iteration 27, loss = 0.54049398\n",
      "Iteration 28, loss = 0.54008469\n",
      "Iteration 29, loss = 0.53977991\n",
      "Iteration 30, loss = 0.53975016\n",
      "Iteration 31, loss = 0.53947548\n",
      "Iteration 32, loss = 0.53935418\n",
      "Iteration 33, loss = 0.53915162\n",
      "Iteration 34, loss = 0.53906639\n",
      "Iteration 35, loss = 0.53906188\n",
      "Iteration 36, loss = 0.53901308\n",
      "Iteration 37, loss = 0.53872285\n",
      "Iteration 38, loss = 0.53861729\n",
      "Iteration 39, loss = 0.53819840\n",
      "Iteration 40, loss = 0.53801178\n",
      "Iteration 41, loss = 0.53784675\n",
      "Iteration 42, loss = 0.53756217\n",
      "Iteration 43, loss = 0.53754841\n",
      "Iteration 44, loss = 0.53784018\n",
      "Iteration 45, loss = 0.53762436\n",
      "Iteration 46, loss = 0.53750706\n",
      "Iteration 47, loss = 0.53739940\n",
      "Iteration 48, loss = 0.53712869\n",
      "Iteration 49, loss = 0.53696404\n",
      "Iteration 50, loss = 0.53664584\n",
      "Iteration 51, loss = 0.53690217\n",
      "Iteration 52, loss = 0.53660345\n",
      "Iteration 53, loss = 0.53639325\n",
      "Iteration 54, loss = 0.53622702\n",
      "Iteration 55, loss = 0.53612943\n",
      "Iteration 56, loss = 0.53601055\n",
      "Iteration 57, loss = 0.53588500\n",
      "Iteration 58, loss = 0.53603640\n",
      "Iteration 59, loss = 0.53589867\n",
      "Iteration 60, loss = 0.53605753\n",
      "Iteration 61, loss = 0.53623978\n",
      "Iteration 62, loss = 0.53603126\n",
      "Iteration 63, loss = 0.53616089\n",
      "Iteration 64, loss = 0.53591369\n",
      "Iteration 65, loss = 0.53567087\n",
      "Iteration 66, loss = 0.53530611\n",
      "Iteration 67, loss = 0.53515011\n",
      "Iteration 68, loss = 0.53474413\n",
      "Iteration 69, loss = 0.53451461\n",
      "Iteration 70, loss = 0.53445445\n",
      "Iteration 71, loss = 0.53441259\n",
      "Iteration 72, loss = 0.53400138\n",
      "Iteration 73, loss = 0.53400456\n",
      "Iteration 74, loss = 0.53393624\n",
      "Iteration 75, loss = 0.53420830\n",
      "Iteration 76, loss = 0.53440386\n",
      "Iteration 77, loss = 0.53481164\n",
      "Iteration 78, loss = 0.53526200\n",
      "Iteration 79, loss = 0.53528293\n",
      "Iteration 80, loss = 0.53487683\n",
      "Iteration 81, loss = 0.53426253\n",
      "Iteration 82, loss = 0.53385979\n",
      "Iteration 83, loss = 0.53353975\n",
      "Iteration 84, loss = 0.53293606\n",
      "Iteration 85, loss = 0.53308021\n",
      "Iteration 86, loss = 0.53304752\n",
      "Iteration 87, loss = 0.53291658\n",
      "Iteration 88, loss = 0.53282702\n",
      "Iteration 89, loss = 0.53274903\n",
      "Iteration 90, loss = 0.53267796\n",
      "Iteration 91, loss = 0.53241828\n",
      "Iteration 92, loss = 0.53225444\n",
      "Iteration 93, loss = 0.53216840\n",
      "Iteration 94, loss = 0.53227470\n",
      "Iteration 95, loss = 0.53250581\n",
      "Iteration 96, loss = 0.53263756\n",
      "Iteration 97, loss = 0.53257563\n",
      "Iteration 98, loss = 0.53241922\n",
      "Iteration 99, loss = 0.53221966\n",
      "Iteration 100, loss = 0.53205990\n",
      "Iteration 101, loss = 0.53191898\n",
      "Iteration 102, loss = 0.53172072\n",
      "Iteration 103, loss = 0.53168468\n",
      "Iteration 104, loss = 0.53180001\n",
      "Iteration 105, loss = 0.53148773\n",
      "Iteration 106, loss = 0.53143073\n",
      "Iteration 107, loss = 0.53109034\n",
      "Iteration 108, loss = 0.53098713\n",
      "Iteration 109, loss = 0.53069393\n",
      "Iteration 110, loss = 0.53052432\n",
      "Iteration 111, loss = 0.53058784\n",
      "Iteration 112, loss = 0.53037078\n",
      "Iteration 113, loss = 0.53025487\n",
      "Iteration 114, loss = 0.53022364\n",
      "Iteration 115, loss = 0.53012480\n",
      "Iteration 116, loss = 0.53020425\n",
      "Iteration 117, loss = 0.52989360\n",
      "Iteration 118, loss = 0.52986375\n",
      "Iteration 119, loss = 0.52991090\n",
      "Iteration 120, loss = 0.52985482\n",
      "Iteration 121, loss = 0.52989040\n",
      "Iteration 122, loss = 0.52975374\n",
      "Iteration 123, loss = 0.52955463\n",
      "Iteration 124, loss = 0.52965821\n",
      "Iteration 125, loss = 0.52942528\n",
      "Iteration 126, loss = 0.52930954\n",
      "Iteration 127, loss = 0.52913453\n",
      "Iteration 128, loss = 0.52903386\n",
      "Iteration 129, loss = 0.52905885\n",
      "Iteration 130, loss = 0.52914904\n",
      "Iteration 131, loss = 0.52915665\n",
      "Iteration 132, loss = 0.52911293\n",
      "Iteration 133, loss = 0.52912016\n",
      "Iteration 134, loss = 0.52927536\n",
      "Iteration 135, loss = 0.52919845\n",
      "Iteration 136, loss = 0.52898840\n",
      "Iteration 137, loss = 0.52914837\n",
      "Iteration 138, loss = 0.52924926\n",
      "Iteration 139, loss = 0.52898419\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69917360\n",
      "Iteration 2, loss = 0.67163941\n",
      "Iteration 3, loss = 0.64688122\n",
      "Iteration 4, loss = 0.62798943\n",
      "Iteration 5, loss = 0.61037281\n",
      "Iteration 6, loss = 0.59685181\n",
      "Iteration 7, loss = 0.58422333\n",
      "Iteration 8, loss = 0.57514737\n",
      "Iteration 9, loss = 0.56851780\n",
      "Iteration 10, loss = 0.56328527\n",
      "Iteration 11, loss = 0.55873753\n",
      "Iteration 12, loss = 0.55704366\n",
      "Iteration 13, loss = 0.55499062\n",
      "Iteration 14, loss = 0.55447985\n",
      "Iteration 15, loss = 0.55421130\n",
      "Iteration 16, loss = 0.55361232\n",
      "Iteration 17, loss = 0.55364277\n",
      "Iteration 18, loss = 0.55338801\n",
      "Iteration 19, loss = 0.55315832\n",
      "Iteration 20, loss = 0.55326811\n",
      "Iteration 21, loss = 0.55278474\n",
      "Iteration 22, loss = 0.55234185\n",
      "Iteration 23, loss = 0.55204894\n",
      "Iteration 24, loss = 0.55140031\n",
      "Iteration 25, loss = 0.55102207\n",
      "Iteration 26, loss = 0.55050286\n",
      "Iteration 27, loss = 0.55030868\n",
      "Iteration 28, loss = 0.54998889\n",
      "Iteration 29, loss = 0.54963856\n",
      "Iteration 30, loss = 0.54950201\n",
      "Iteration 31, loss = 0.54946022\n",
      "Iteration 32, loss = 0.54941310\n",
      "Iteration 33, loss = 0.54906470\n",
      "Iteration 34, loss = 0.54902241\n",
      "Iteration 35, loss = 0.54911291\n",
      "Iteration 36, loss = 0.54898989\n",
      "Iteration 37, loss = 0.54888457\n",
      "Iteration 38, loss = 0.54863555\n",
      "Iteration 39, loss = 0.54846159\n",
      "Iteration 40, loss = 0.54808466\n",
      "Iteration 41, loss = 0.54778347\n",
      "Iteration 42, loss = 0.54802688\n",
      "Iteration 43, loss = 0.54749448\n",
      "Iteration 44, loss = 0.54728486\n",
      "Iteration 45, loss = 0.54750517\n",
      "Iteration 46, loss = 0.54703452\n",
      "Iteration 47, loss = 0.54668343\n",
      "Iteration 48, loss = 0.54641723\n",
      "Iteration 49, loss = 0.54647376\n",
      "Iteration 50, loss = 0.54601266\n",
      "Iteration 51, loss = 0.54594209\n",
      "Iteration 52, loss = 0.54594371\n",
      "Iteration 53, loss = 0.54590252\n",
      "Iteration 54, loss = 0.54563083\n",
      "Iteration 55, loss = 0.54542912\n",
      "Iteration 56, loss = 0.54531037\n",
      "Iteration 57, loss = 0.54517724\n",
      "Iteration 58, loss = 0.54566859\n",
      "Iteration 59, loss = 0.54555884\n",
      "Iteration 60, loss = 0.54536052\n",
      "Iteration 61, loss = 0.54532117\n",
      "Iteration 62, loss = 0.54522584\n",
      "Iteration 63, loss = 0.54497008\n",
      "Iteration 64, loss = 0.54469269\n",
      "Iteration 65, loss = 0.54404663\n",
      "Iteration 66, loss = 0.54336680\n",
      "Iteration 67, loss = 0.54369591\n",
      "Iteration 68, loss = 0.54445532\n",
      "Iteration 69, loss = 0.54582108\n",
      "Iteration 70, loss = 0.54640146\n",
      "Iteration 71, loss = 0.54625551\n",
      "Iteration 72, loss = 0.54556221\n",
      "Iteration 73, loss = 0.54517678\n",
      "Iteration 74, loss = 0.54455519\n",
      "Iteration 75, loss = 0.54419340\n",
      "Iteration 76, loss = 0.54383812\n",
      "Iteration 77, loss = 0.54316105\n",
      "Iteration 78, loss = 0.54331491\n",
      "Iteration 79, loss = 0.54363537\n",
      "Iteration 80, loss = 0.54394848\n",
      "Iteration 81, loss = 0.54375110\n",
      "Iteration 82, loss = 0.54308483\n",
      "Iteration 83, loss = 0.54249941\n",
      "Iteration 84, loss = 0.54156401\n",
      "Iteration 85, loss = 0.54161262\n",
      "Iteration 86, loss = 0.54095226\n",
      "Iteration 87, loss = 0.54082964\n",
      "Iteration 88, loss = 0.54110136\n",
      "Iteration 89, loss = 0.54136293\n",
      "Iteration 90, loss = 0.54094031\n",
      "Iteration 91, loss = 0.54036838\n",
      "Iteration 92, loss = 0.54026955\n",
      "Iteration 93, loss = 0.53984459\n",
      "Iteration 94, loss = 0.54004936\n",
      "Iteration 95, loss = 0.53969210\n",
      "Iteration 96, loss = 0.53947982\n",
      "Iteration 97, loss = 0.53940004\n",
      "Iteration 98, loss = 0.53919828\n",
      "Iteration 99, loss = 0.53878406\n",
      "Iteration 100, loss = 0.53886631\n",
      "Iteration 101, loss = 0.53868231\n",
      "Iteration 102, loss = 0.53846056\n",
      "Iteration 103, loss = 0.53829666\n",
      "Iteration 104, loss = 0.53837151\n",
      "Iteration 105, loss = 0.53830489\n",
      "Iteration 106, loss = 0.53803726\n",
      "Iteration 107, loss = 0.53793957\n",
      "Iteration 108, loss = 0.53774659\n",
      "Iteration 109, loss = 0.53758936\n",
      "Iteration 110, loss = 0.53720806\n",
      "Iteration 111, loss = 0.53706842\n",
      "Iteration 112, loss = 0.53692562\n",
      "Iteration 113, loss = 0.53664759\n",
      "Iteration 114, loss = 0.53663510\n",
      "Iteration 115, loss = 0.53649678\n",
      "Iteration 116, loss = 0.53683040\n",
      "Iteration 117, loss = 0.53724440\n",
      "Iteration 118, loss = 0.53736498\n",
      "Iteration 119, loss = 0.53742425\n",
      "Iteration 120, loss = 0.53806988\n",
      "Iteration 121, loss = 0.53766691\n",
      "Iteration 122, loss = 0.53703658\n",
      "Iteration 123, loss = 0.53646820\n",
      "Iteration 124, loss = 0.53590318\n",
      "Iteration 125, loss = 0.53513922\n",
      "Iteration 126, loss = 0.53507972\n",
      "Iteration 127, loss = 0.53463564\n",
      "Iteration 128, loss = 0.53448756\n",
      "Iteration 129, loss = 0.53401882\n",
      "Iteration 130, loss = 0.53394090\n",
      "Iteration 131, loss = 0.53408182\n",
      "Iteration 132, loss = 0.53410518\n",
      "Iteration 133, loss = 0.53419863\n",
      "Iteration 134, loss = 0.53402239\n",
      "Iteration 135, loss = 0.53387818\n",
      "Iteration 136, loss = 0.53449047\n",
      "Iteration 137, loss = 0.53373595\n",
      "Iteration 138, loss = 0.53289912\n",
      "Iteration 139, loss = 0.53253890\n",
      "Iteration 140, loss = 0.53216042\n",
      "Iteration 141, loss = 0.53215621\n",
      "Iteration 142, loss = 0.53244348\n",
      "Iteration 143, loss = 0.53266226\n",
      "Iteration 144, loss = 0.53264389\n",
      "Iteration 145, loss = 0.53222972\n",
      "Iteration 146, loss = 0.53223192\n",
      "Iteration 147, loss = 0.53147030\n",
      "Iteration 148, loss = 0.53115974\n",
      "Iteration 149, loss = 0.53080689\n",
      "Iteration 150, loss = 0.53096112\n",
      "Iteration 151, loss = 0.53110902\n",
      "Iteration 152, loss = 0.53106500\n",
      "Iteration 153, loss = 0.53067860\n",
      "Iteration 154, loss = 0.53063151\n",
      "Iteration 155, loss = 0.53025147\n",
      "Iteration 156, loss = 0.53016117\n",
      "Iteration 157, loss = 0.53020301\n",
      "Iteration 158, loss = 0.52966851\n",
      "Iteration 159, loss = 0.52923653\n",
      "Iteration 160, loss = 0.53013626\n",
      "Iteration 161, loss = 0.53019800\n",
      "Iteration 162, loss = 0.53039430\n",
      "Iteration 163, loss = 0.53066281\n",
      "Iteration 164, loss = 0.53045398\n",
      "Iteration 165, loss = 0.53038370\n",
      "Iteration 166, loss = 0.52993392\n",
      "Iteration 167, loss = 0.52942902\n",
      "Iteration 168, loss = 0.52897330\n",
      "Iteration 169, loss = 0.52858621\n",
      "Iteration 170, loss = 0.52874397\n",
      "Iteration 171, loss = 0.52857166\n",
      "Iteration 172, loss = 0.52960277\n",
      "Iteration 173, loss = 0.52932536\n",
      "Iteration 174, loss = 0.52797642\n",
      "Iteration 175, loss = 0.52752475\n",
      "Iteration 176, loss = 0.52745345\n",
      "Iteration 177, loss = 0.52733579\n",
      "Iteration 178, loss = 0.52717060\n",
      "Iteration 179, loss = 0.52688713\n",
      "Iteration 180, loss = 0.52674329\n",
      "Iteration 181, loss = 0.52645073\n",
      "Iteration 182, loss = 0.52626178\n",
      "Iteration 183, loss = 0.52606371\n",
      "Iteration 184, loss = 0.52584203\n",
      "Iteration 185, loss = 0.52580885\n",
      "Iteration 186, loss = 0.52570951\n",
      "Iteration 187, loss = 0.52659888\n",
      "Iteration 188, loss = 0.52668069\n",
      "Iteration 189, loss = 0.52676098\n",
      "Iteration 190, loss = 0.52682080\n",
      "Iteration 191, loss = 0.52737241\n",
      "Iteration 192, loss = 0.52711016\n",
      "Iteration 193, loss = 0.52648461\n",
      "Iteration 194, loss = 0.52573068\n",
      "Iteration 195, loss = 0.52515812\n",
      "Iteration 196, loss = 0.52460101\n",
      "Iteration 197, loss = 0.52445994\n",
      "Iteration 198, loss = 0.52458157\n",
      "Iteration 199, loss = 0.52503025\n",
      "Iteration 200, loss = 0.52456816\n",
      "Iteration 201, loss = 0.52389791\n",
      "Iteration 202, loss = 0.52347349\n",
      "Iteration 203, loss = 0.52451216\n",
      "Iteration 204, loss = 0.52454247\n",
      "Iteration 205, loss = 0.52465420\n",
      "Iteration 206, loss = 0.52389728\n",
      "Iteration 207, loss = 0.52328697\n",
      "Iteration 208, loss = 0.52294645\n",
      "Iteration 209, loss = 0.52337733\n",
      "Iteration 210, loss = 0.52342331\n",
      "Iteration 211, loss = 0.52344083\n",
      "Iteration 212, loss = 0.52341279\n",
      "Iteration 213, loss = 0.52320281\n",
      "Iteration 214, loss = 0.52268450\n",
      "Iteration 215, loss = 0.52261178\n",
      "Iteration 216, loss = 0.52231624\n",
      "Iteration 217, loss = 0.52283288\n",
      "Iteration 218, loss = 0.52333317\n",
      "Iteration 219, loss = 0.52363465\n",
      "Iteration 220, loss = 0.52377398\n",
      "Iteration 221, loss = 0.52301246\n",
      "Iteration 222, loss = 0.52181759\n",
      "Iteration 223, loss = 0.52157909\n",
      "Iteration 224, loss = 0.52243229\n",
      "Iteration 225, loss = 0.52264839\n",
      "Iteration 226, loss = 0.52288083\n",
      "Iteration 227, loss = 0.52280790\n",
      "Iteration 228, loss = 0.52203640\n",
      "Iteration 229, loss = 0.52133863\n",
      "Iteration 230, loss = 0.52115868\n",
      "Iteration 231, loss = 0.52170417\n",
      "Iteration 232, loss = 0.52166114\n",
      "Iteration 233, loss = 0.52106756\n",
      "Iteration 234, loss = 0.52107855\n",
      "Iteration 235, loss = 0.52061868\n",
      "Iteration 236, loss = 0.52089757\n",
      "Iteration 237, loss = 0.52016469\n",
      "Iteration 238, loss = 0.52009450\n",
      "Iteration 239, loss = 0.52010525\n",
      "Iteration 240, loss = 0.52042335\n",
      "Iteration 241, loss = 0.52031624\n",
      "Iteration 242, loss = 0.52048676\n",
      "Iteration 243, loss = 0.52017908\n",
      "Iteration 244, loss = 0.51983568\n",
      "Iteration 245, loss = 0.51925946\n",
      "Iteration 246, loss = 0.51895173\n",
      "Iteration 247, loss = 0.51866183\n",
      "Iteration 248, loss = 0.51851563\n",
      "Iteration 249, loss = 0.51922835\n",
      "Iteration 250, loss = 0.51991342\n",
      "Iteration 251, loss = 0.51957842\n",
      "Iteration 252, loss = 0.51880786\n",
      "Iteration 253, loss = 0.51834896\n",
      "Iteration 254, loss = 0.51780774\n",
      "Iteration 255, loss = 0.51848751\n",
      "Iteration 256, loss = 0.51910371\n",
      "Iteration 257, loss = 0.52026305\n",
      "Iteration 258, loss = 0.51958420\n",
      "Iteration 259, loss = 0.51914420\n",
      "Iteration 260, loss = 0.51876567\n",
      "Iteration 261, loss = 0.51838152\n",
      "Iteration 262, loss = 0.51851229\n",
      "Iteration 263, loss = 0.51849211\n",
      "Iteration 264, loss = 0.51839307\n",
      "Iteration 265, loss = 0.51828634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67629000\n",
      "Iteration 2, loss = 0.65217993\n",
      "Iteration 3, loss = 0.63087473\n",
      "Iteration 4, loss = 0.61513564\n",
      "Iteration 5, loss = 0.59966332\n",
      "Iteration 6, loss = 0.58900837\n",
      "Iteration 7, loss = 0.58011094\n",
      "Iteration 8, loss = 0.57343572\n",
      "Iteration 9, loss = 0.56763624\n",
      "Iteration 10, loss = 0.56408594\n",
      "Iteration 11, loss = 0.56120448\n",
      "Iteration 12, loss = 0.55864474\n",
      "Iteration 13, loss = 0.55766231\n",
      "Iteration 14, loss = 0.55627401\n",
      "Iteration 15, loss = 0.55580969\n",
      "Iteration 16, loss = 0.55573374\n",
      "Iteration 17, loss = 0.55544417\n",
      "Iteration 18, loss = 0.55456500\n",
      "Iteration 19, loss = 0.55419533\n",
      "Iteration 20, loss = 0.55418532\n",
      "Iteration 21, loss = 0.55371991\n",
      "Iteration 22, loss = 0.55343945\n",
      "Iteration 23, loss = 0.55317851\n",
      "Iteration 24, loss = 0.55271186\n",
      "Iteration 25, loss = 0.55271170\n",
      "Iteration 26, loss = 0.55238285\n",
      "Iteration 27, loss = 0.55227783\n",
      "Iteration 28, loss = 0.55219105\n",
      "Iteration 29, loss = 0.55198031\n",
      "Iteration 30, loss = 0.55173295\n",
      "Iteration 31, loss = 0.55140982\n",
      "Iteration 32, loss = 0.55108584\n",
      "Iteration 33, loss = 0.55105093\n",
      "Iteration 34, loss = 0.55108614\n",
      "Iteration 35, loss = 0.55057010\n",
      "Iteration 36, loss = 0.55055498\n",
      "Iteration 37, loss = 0.55030304\n",
      "Iteration 38, loss = 0.55011768\n",
      "Iteration 39, loss = 0.55003660\n",
      "Iteration 40, loss = 0.54983793\n",
      "Iteration 41, loss = 0.54978551\n",
      "Iteration 42, loss = 0.54978910\n",
      "Iteration 43, loss = 0.54970214\n",
      "Iteration 44, loss = 0.54964770\n",
      "Iteration 45, loss = 0.54949495\n",
      "Iteration 46, loss = 0.54937702\n",
      "Iteration 47, loss = 0.54918597\n",
      "Iteration 48, loss = 0.54905491\n",
      "Iteration 49, loss = 0.54894055\n",
      "Iteration 50, loss = 0.54869168\n",
      "Iteration 51, loss = 0.54845208\n",
      "Iteration 52, loss = 0.54820200\n",
      "Iteration 53, loss = 0.54803743\n",
      "Iteration 54, loss = 0.54780840\n",
      "Iteration 55, loss = 0.54771760\n",
      "Iteration 56, loss = 0.54772426\n",
      "Iteration 57, loss = 0.54752144\n",
      "Iteration 58, loss = 0.54741501\n",
      "Iteration 59, loss = 0.54727261\n",
      "Iteration 60, loss = 0.54707491\n",
      "Iteration 61, loss = 0.54703563\n",
      "Iteration 62, loss = 0.54697328\n",
      "Iteration 63, loss = 0.54694675\n",
      "Iteration 64, loss = 0.54685885\n",
      "Iteration 65, loss = 0.54673034\n",
      "Iteration 66, loss = 0.54684349\n",
      "Iteration 67, loss = 0.54677259\n",
      "Iteration 68, loss = 0.54670742\n",
      "Iteration 69, loss = 0.54666029\n",
      "Iteration 70, loss = 0.54660634\n",
      "Iteration 71, loss = 0.54619610\n",
      "Iteration 72, loss = 0.54586308\n",
      "Iteration 73, loss = 0.54559168\n",
      "Iteration 74, loss = 0.54636429\n",
      "Iteration 75, loss = 0.54610511\n",
      "Iteration 76, loss = 0.54632686\n",
      "Iteration 77, loss = 0.54669192\n",
      "Iteration 78, loss = 0.54690005\n",
      "Iteration 79, loss = 0.54704887\n",
      "Iteration 80, loss = 0.54679399\n",
      "Iteration 81, loss = 0.54632516\n",
      "Iteration 82, loss = 0.54557316\n",
      "Iteration 83, loss = 0.54536343\n",
      "Iteration 84, loss = 0.54510236\n",
      "Iteration 85, loss = 0.54506230\n",
      "Iteration 86, loss = 0.54514517\n",
      "Iteration 87, loss = 0.54524825\n",
      "Iteration 88, loss = 0.54508405\n",
      "Iteration 89, loss = 0.54488184\n",
      "Iteration 90, loss = 0.54470132\n",
      "Iteration 91, loss = 0.54439882\n",
      "Iteration 92, loss = 0.54402209\n",
      "Iteration 93, loss = 0.54413535\n",
      "Iteration 94, loss = 0.54401836\n",
      "Iteration 95, loss = 0.54363028\n",
      "Iteration 96, loss = 0.54356859\n",
      "Iteration 97, loss = 0.54343295\n",
      "Iteration 98, loss = 0.54351226\n",
      "Iteration 99, loss = 0.54378034\n",
      "Iteration 100, loss = 0.54427847\n",
      "Iteration 101, loss = 0.54454231\n",
      "Iteration 102, loss = 0.54459216\n",
      "Iteration 103, loss = 0.54481500\n",
      "Iteration 104, loss = 0.54472548\n",
      "Iteration 105, loss = 0.54443603\n",
      "Iteration 106, loss = 0.54405488\n",
      "Iteration 107, loss = 0.54332552\n",
      "Iteration 108, loss = 0.54256985\n",
      "Iteration 109, loss = 0.54223132\n",
      "Iteration 110, loss = 0.54204373\n",
      "Iteration 111, loss = 0.54222355\n",
      "Iteration 112, loss = 0.54235590\n",
      "Iteration 113, loss = 0.54252845\n",
      "Iteration 114, loss = 0.54258115\n",
      "Iteration 115, loss = 0.54234611\n",
      "Iteration 116, loss = 0.54184757\n",
      "Iteration 117, loss = 0.54166924\n",
      "Iteration 118, loss = 0.54121347\n",
      "Iteration 119, loss = 0.54113549\n",
      "Iteration 120, loss = 0.54184926\n",
      "Iteration 121, loss = 0.54190296\n",
      "Iteration 122, loss = 0.54233999\n",
      "Iteration 123, loss = 0.54244393\n",
      "Iteration 124, loss = 0.54283929\n",
      "Iteration 125, loss = 0.54266981\n",
      "Iteration 126, loss = 0.54231524\n",
      "Iteration 127, loss = 0.54185215\n",
      "Iteration 128, loss = 0.54153581\n",
      "Iteration 129, loss = 0.54113605\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.67125320\n",
      "Iteration 2, loss = 0.64363950\n",
      "Iteration 3, loss = 0.61920328\n",
      "Iteration 4, loss = 0.59853955\n",
      "Iteration 5, loss = 0.58136431\n",
      "Iteration 6, loss = 0.56632193\n",
      "Iteration 7, loss = 0.55512514\n",
      "Iteration 8, loss = 0.54559802\n",
      "Iteration 9, loss = 0.53806253\n",
      "Iteration 10, loss = 0.53363191\n",
      "Iteration 11, loss = 0.52916793\n",
      "Iteration 12, loss = 0.52690265\n",
      "Iteration 13, loss = 0.52525839\n",
      "Iteration 14, loss = 0.52419816\n",
      "Iteration 15, loss = 0.52431524\n",
      "Iteration 16, loss = 0.52439186\n",
      "Iteration 17, loss = 0.52448188\n",
      "Iteration 18, loss = 0.52452728\n",
      "Iteration 19, loss = 0.52471412\n",
      "Iteration 20, loss = 0.52532592\n",
      "Iteration 21, loss = 0.52549777\n",
      "Iteration 22, loss = 0.52511305\n",
      "Iteration 23, loss = 0.52491416\n",
      "Iteration 24, loss = 0.52420937\n",
      "Iteration 25, loss = 0.52322377\n",
      "Iteration 26, loss = 0.52241061\n",
      "Iteration 27, loss = 0.52159356\n",
      "Iteration 28, loss = 0.52057472\n",
      "Iteration 29, loss = 0.52019443\n",
      "Iteration 30, loss = 0.51981039\n",
      "Iteration 31, loss = 0.51948755\n",
      "Iteration 32, loss = 0.51946290\n",
      "Iteration 33, loss = 0.51941634\n",
      "Iteration 34, loss = 0.51931013\n",
      "Iteration 35, loss = 0.51875389\n",
      "Iteration 36, loss = 0.51872107\n",
      "Iteration 37, loss = 0.51849723\n",
      "Iteration 38, loss = 0.51876917\n",
      "Iteration 39, loss = 0.51927165\n",
      "Iteration 40, loss = 0.51984068\n",
      "Iteration 41, loss = 0.51955854\n",
      "Iteration 42, loss = 0.51890785\n",
      "Iteration 43, loss = 0.51802624\n",
      "Iteration 44, loss = 0.51727695\n",
      "Iteration 45, loss = 0.51674452\n",
      "Iteration 46, loss = 0.51626546\n",
      "Iteration 47, loss = 0.51610616\n",
      "Iteration 48, loss = 0.51577345\n",
      "Iteration 49, loss = 0.51594700\n",
      "Iteration 50, loss = 0.51577722\n",
      "Iteration 51, loss = 0.51588613\n",
      "Iteration 52, loss = 0.51585072\n",
      "Iteration 53, loss = 0.51575672\n",
      "Iteration 54, loss = 0.51519816\n",
      "Iteration 55, loss = 0.51509373\n",
      "Iteration 56, loss = 0.51519048\n",
      "Iteration 57, loss = 0.51504234\n",
      "Iteration 58, loss = 0.51457935\n",
      "Iteration 59, loss = 0.51447523\n",
      "Iteration 60, loss = 0.51388856\n",
      "Iteration 61, loss = 0.51404634\n",
      "Iteration 62, loss = 0.51391016\n",
      "Iteration 63, loss = 0.51383056\n",
      "Iteration 64, loss = 0.51369749\n",
      "Iteration 65, loss = 0.51370987\n",
      "Iteration 66, loss = 0.51363242\n",
      "Iteration 67, loss = 0.51346671\n",
      "Iteration 68, loss = 0.51317331\n",
      "Iteration 69, loss = 0.51296659\n",
      "Iteration 70, loss = 0.51275358\n",
      "Iteration 71, loss = 0.51282414\n",
      "Iteration 72, loss = 0.51251052\n",
      "Iteration 73, loss = 0.51242280\n",
      "Iteration 74, loss = 0.51240250\n",
      "Iteration 75, loss = 0.51264260\n",
      "Iteration 76, loss = 0.51234557\n",
      "Iteration 77, loss = 0.51219968\n",
      "Iteration 78, loss = 0.51193806\n",
      "Iteration 79, loss = 0.51209948\n",
      "Iteration 80, loss = 0.51181278\n",
      "Iteration 81, loss = 0.51164672\n",
      "Iteration 82, loss = 0.51137492\n",
      "Iteration 83, loss = 0.51149886\n",
      "Iteration 84, loss = 0.51125082\n",
      "Iteration 85, loss = 0.51091913\n",
      "Iteration 86, loss = 0.51044288\n",
      "Iteration 87, loss = 0.51021033\n",
      "Iteration 88, loss = 0.50977965\n",
      "Iteration 89, loss = 0.50968763\n",
      "Iteration 90, loss = 0.50963365\n",
      "Iteration 91, loss = 0.50992000\n",
      "Iteration 92, loss = 0.50954765\n",
      "Iteration 93, loss = 0.50923870\n",
      "Iteration 94, loss = 0.50889873\n",
      "Iteration 95, loss = 0.50932676\n",
      "Iteration 96, loss = 0.50848982\n",
      "Iteration 97, loss = 0.50836868\n",
      "Iteration 98, loss = 0.50834458\n",
      "Iteration 99, loss = 0.50820428\n",
      "Iteration 100, loss = 0.50819972\n",
      "Iteration 101, loss = 0.50790507\n",
      "Iteration 102, loss = 0.50759854\n",
      "Iteration 103, loss = 0.50725573\n",
      "Iteration 104, loss = 0.50729485\n",
      "Iteration 105, loss = 0.50711424\n",
      "Iteration 106, loss = 0.50675504\n",
      "Iteration 107, loss = 0.50650176\n",
      "Iteration 108, loss = 0.50633446\n",
      "Iteration 109, loss = 0.50646679\n",
      "Iteration 110, loss = 0.50641007\n",
      "Iteration 111, loss = 0.50618404\n",
      "Iteration 112, loss = 0.50629831\n",
      "Iteration 113, loss = 0.50663319\n",
      "Iteration 114, loss = 0.50667269\n",
      "Iteration 115, loss = 0.50662595\n",
      "Iteration 116, loss = 0.50651399\n",
      "Iteration 117, loss = 0.50642309\n",
      "Iteration 118, loss = 0.50574443\n",
      "Iteration 119, loss = 0.50496899\n",
      "Iteration 120, loss = 0.50452796\n",
      "Iteration 121, loss = 0.50451939\n",
      "Iteration 122, loss = 0.50504823\n",
      "Iteration 123, loss = 0.50579989\n",
      "Iteration 124, loss = 0.50641123\n",
      "Iteration 125, loss = 0.50611647\n",
      "Iteration 126, loss = 0.50588407\n",
      "Iteration 127, loss = 0.50435832\n",
      "Iteration 128, loss = 0.50392257\n",
      "Iteration 129, loss = 0.50355693\n",
      "Iteration 130, loss = 0.50342870\n",
      "Iteration 131, loss = 0.50309204\n",
      "Iteration 132, loss = 0.50300459\n",
      "Iteration 133, loss = 0.50298989\n",
      "Iteration 134, loss = 0.50312365\n",
      "Iteration 135, loss = 0.50394926\n",
      "Iteration 136, loss = 0.50545301\n",
      "Iteration 137, loss = 0.50527244\n",
      "Iteration 138, loss = 0.50439381\n",
      "Iteration 139, loss = 0.50336817\n",
      "Iteration 140, loss = 0.50321901\n",
      "Iteration 141, loss = 0.50225352\n",
      "Iteration 142, loss = 0.50189923\n",
      "Iteration 143, loss = 0.50252667\n",
      "Iteration 144, loss = 0.50321248\n",
      "Iteration 145, loss = 0.50384566\n",
      "Iteration 146, loss = 0.50397411\n",
      "Iteration 147, loss = 0.50352034\n",
      "Iteration 148, loss = 0.50244076\n",
      "Iteration 149, loss = 0.50232119\n",
      "Iteration 150, loss = 0.50195844\n",
      "Iteration 151, loss = 0.50182908\n",
      "Iteration 152, loss = 0.50144219\n",
      "Iteration 153, loss = 0.50168092\n",
      "Iteration 154, loss = 0.50107947\n",
      "Iteration 155, loss = 0.50104586\n",
      "Iteration 156, loss = 0.50084902\n",
      "Iteration 157, loss = 0.50044242\n",
      "Iteration 158, loss = 0.50003232\n",
      "Iteration 159, loss = 0.49976833\n",
      "Iteration 160, loss = 0.49958529\n",
      "Iteration 161, loss = 0.49933807\n",
      "Iteration 162, loss = 0.49914581\n",
      "Iteration 163, loss = 0.49908762\n",
      "Iteration 164, loss = 0.49879036\n",
      "Iteration 165, loss = 0.49863110\n",
      "Iteration 166, loss = 0.49844005\n",
      "Iteration 167, loss = 0.49813623\n",
      "Iteration 168, loss = 0.49806695\n",
      "Iteration 169, loss = 0.49800280\n",
      "Iteration 170, loss = 0.49783761\n",
      "Iteration 171, loss = 0.49788763\n",
      "Iteration 172, loss = 0.49794830\n",
      "Iteration 173, loss = 0.49808715\n",
      "Iteration 174, loss = 0.49819618\n",
      "Iteration 175, loss = 0.49799588\n",
      "Iteration 176, loss = 0.49727923\n",
      "Iteration 177, loss = 0.49716290\n",
      "Iteration 178, loss = 0.49727227\n",
      "Iteration 179, loss = 0.49796316\n",
      "Iteration 180, loss = 0.49743288\n",
      "Iteration 181, loss = 0.49819879\n",
      "Iteration 182, loss = 0.49716415\n",
      "Iteration 183, loss = 0.49662780\n",
      "Iteration 184, loss = 0.49610746\n",
      "Iteration 185, loss = 0.49654852\n",
      "Iteration 186, loss = 0.49626423\n",
      "Iteration 187, loss = 0.49616179\n",
      "Iteration 188, loss = 0.49648835\n",
      "Iteration 189, loss = 0.49639602\n",
      "Iteration 190, loss = 0.49628804\n",
      "Iteration 191, loss = 0.49570738\n",
      "Iteration 192, loss = 0.49546630\n",
      "Iteration 193, loss = 0.49511860\n",
      "Iteration 194, loss = 0.49504425\n",
      "Iteration 195, loss = 0.49536571\n",
      "Iteration 196, loss = 0.49539265\n",
      "Iteration 197, loss = 0.49541092\n",
      "Iteration 198, loss = 0.49541469\n",
      "Iteration 199, loss = 0.49508124\n",
      "Iteration 200, loss = 0.49453054\n",
      "Iteration 201, loss = 0.49474516\n",
      "Iteration 202, loss = 0.49488032\n",
      "Iteration 203, loss = 0.49499210\n",
      "Iteration 204, loss = 0.49421565\n",
      "Iteration 205, loss = 0.49432382\n",
      "Iteration 206, loss = 0.49441068\n",
      "Iteration 207, loss = 0.49498195\n",
      "Iteration 208, loss = 0.49524582\n",
      "Iteration 209, loss = 0.49530350\n",
      "Iteration 210, loss = 0.49539469\n",
      "Iteration 211, loss = 0.49543016\n",
      "Iteration 212, loss = 0.49467697\n",
      "Iteration 213, loss = 0.49438277\n",
      "Iteration 214, loss = 0.49336280\n",
      "Iteration 215, loss = 0.49333254\n",
      "Iteration 216, loss = 0.49241207\n",
      "Iteration 217, loss = 0.49241433\n",
      "Iteration 218, loss = 0.49263647\n",
      "Iteration 219, loss = 0.49316548\n",
      "Iteration 220, loss = 0.49316433\n",
      "Iteration 221, loss = 0.49285979\n",
      "Iteration 222, loss = 0.49229284\n",
      "Iteration 223, loss = 0.49240222\n",
      "Iteration 224, loss = 0.49197403\n",
      "Iteration 225, loss = 0.49172425\n",
      "Iteration 226, loss = 0.49173251\n",
      "Iteration 227, loss = 0.49205775\n",
      "Iteration 228, loss = 0.49233494\n",
      "Iteration 229, loss = 0.49221782\n",
      "Iteration 230, loss = 0.49217833\n",
      "Iteration 231, loss = 0.49241145\n",
      "Iteration 232, loss = 0.49181279\n",
      "Iteration 233, loss = 0.49111908\n",
      "Iteration 234, loss = 0.49118598\n",
      "Iteration 235, loss = 0.49100910\n",
      "Iteration 236, loss = 0.49191910\n",
      "Iteration 237, loss = 0.49206713\n",
      "Iteration 238, loss = 0.49225161\n",
      "Iteration 239, loss = 0.49179134\n",
      "Iteration 240, loss = 0.49081494\n",
      "Iteration 241, loss = 0.49068810\n",
      "Iteration 242, loss = 0.49069136\n",
      "Iteration 243, loss = 0.49052191\n",
      "Iteration 244, loss = 0.49048604\n",
      "Iteration 245, loss = 0.49075170\n",
      "Iteration 246, loss = 0.49034177\n",
      "Iteration 247, loss = 0.49032810\n",
      "Iteration 248, loss = 0.49019975\n",
      "Iteration 249, loss = 0.49045749\n",
      "Iteration 250, loss = 0.49048720\n",
      "Iteration 251, loss = 0.49027642\n",
      "Iteration 252, loss = 0.48975305\n",
      "Iteration 253, loss = 0.48937763\n",
      "Iteration 254, loss = 0.48929545\n",
      "Iteration 255, loss = 0.48904208\n",
      "Iteration 256, loss = 0.48890623\n",
      "Iteration 257, loss = 0.48884450\n",
      "Iteration 258, loss = 0.48850895\n",
      "Iteration 259, loss = 0.48842750\n",
      "Iteration 260, loss = 0.48802785\n",
      "Iteration 261, loss = 0.48816331\n",
      "Iteration 262, loss = 0.48860378\n",
      "Iteration 263, loss = 0.48853004\n",
      "Iteration 264, loss = 0.48817383\n",
      "Iteration 265, loss = 0.48824626\n",
      "Iteration 266, loss = 0.48811077\n",
      "Iteration 267, loss = 0.48802452\n",
      "Iteration 268, loss = 0.48826380\n",
      "Iteration 269, loss = 0.48832121\n",
      "Iteration 270, loss = 0.48839090\n",
      "Iteration 271, loss = 0.48803592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68712582\n",
      "Iteration 2, loss = 0.65835439\n",
      "Iteration 3, loss = 0.63652750\n",
      "Iteration 4, loss = 0.61590630\n",
      "Iteration 5, loss = 0.59964861\n",
      "Iteration 6, loss = 0.58678570\n",
      "Iteration 7, loss = 0.57485670\n",
      "Iteration 8, loss = 0.56553864\n",
      "Iteration 9, loss = 0.55990461\n",
      "Iteration 10, loss = 0.55467273\n",
      "Iteration 11, loss = 0.55020684\n",
      "Iteration 12, loss = 0.54832387\n",
      "Iteration 13, loss = 0.54663485\n",
      "Iteration 14, loss = 0.54592982\n",
      "Iteration 15, loss = 0.54494679\n",
      "Iteration 16, loss = 0.54483327\n",
      "Iteration 17, loss = 0.54455892\n",
      "Iteration 18, loss = 0.54439146\n",
      "Iteration 19, loss = 0.54487810\n",
      "Iteration 20, loss = 0.54514904\n",
      "Iteration 21, loss = 0.54485995\n",
      "Iteration 22, loss = 0.54457909\n",
      "Iteration 23, loss = 0.54455625\n",
      "Iteration 24, loss = 0.54412246\n",
      "Iteration 25, loss = 0.54372807\n",
      "Iteration 26, loss = 0.54330446\n",
      "Iteration 27, loss = 0.54299580\n",
      "Iteration 28, loss = 0.54234978\n",
      "Iteration 29, loss = 0.54186100\n",
      "Iteration 30, loss = 0.54138928\n",
      "Iteration 31, loss = 0.54081545\n",
      "Iteration 32, loss = 0.54069243\n",
      "Iteration 33, loss = 0.54036823\n",
      "Iteration 34, loss = 0.54011201\n",
      "Iteration 35, loss = 0.53985356\n",
      "Iteration 36, loss = 0.53981451\n",
      "Iteration 37, loss = 0.53994527\n",
      "Iteration 38, loss = 0.53957241\n",
      "Iteration 39, loss = 0.53941478\n",
      "Iteration 40, loss = 0.53924195\n",
      "Iteration 41, loss = 0.53914249\n",
      "Iteration 42, loss = 0.53904070\n",
      "Iteration 43, loss = 0.53882727\n",
      "Iteration 44, loss = 0.53856274\n",
      "Iteration 45, loss = 0.53841276\n",
      "Iteration 46, loss = 0.53813434\n",
      "Iteration 47, loss = 0.53782905\n",
      "Iteration 48, loss = 0.53771034\n",
      "Iteration 49, loss = 0.53718759\n",
      "Iteration 50, loss = 0.53679037\n",
      "Iteration 51, loss = 0.53692544\n",
      "Iteration 52, loss = 0.53675531\n",
      "Iteration 53, loss = 0.53682670\n",
      "Iteration 54, loss = 0.53686118\n",
      "Iteration 55, loss = 0.53685966\n",
      "Iteration 56, loss = 0.53678826\n",
      "Iteration 57, loss = 0.53677550\n",
      "Iteration 58, loss = 0.53613983\n",
      "Iteration 59, loss = 0.53597886\n",
      "Iteration 60, loss = 0.53545350\n",
      "Iteration 61, loss = 0.53521584\n",
      "Iteration 62, loss = 0.53549204\n",
      "Iteration 63, loss = 0.53527251\n",
      "Iteration 64, loss = 0.53511061\n",
      "Iteration 65, loss = 0.53478739\n",
      "Iteration 66, loss = 0.53467996\n",
      "Iteration 67, loss = 0.53473468\n",
      "Iteration 68, loss = 0.53438301\n",
      "Iteration 69, loss = 0.53454748\n",
      "Iteration 70, loss = 0.53445800\n",
      "Iteration 71, loss = 0.53436720\n",
      "Iteration 72, loss = 0.53430633\n",
      "Iteration 73, loss = 0.53396939\n",
      "Iteration 74, loss = 0.53383683\n",
      "Iteration 75, loss = 0.53382447\n",
      "Iteration 76, loss = 0.53343849\n",
      "Iteration 77, loss = 0.53328028\n",
      "Iteration 78, loss = 0.53279477\n",
      "Iteration 79, loss = 0.53208023\n",
      "Iteration 80, loss = 0.53188824\n",
      "Iteration 81, loss = 0.53250100\n",
      "Iteration 82, loss = 0.53279786\n",
      "Iteration 83, loss = 0.53330289\n",
      "Iteration 84, loss = 0.53320808\n",
      "Iteration 85, loss = 0.53298200\n",
      "Iteration 86, loss = 0.53281199\n",
      "Iteration 87, loss = 0.53212785\n",
      "Iteration 88, loss = 0.53122468\n",
      "Iteration 89, loss = 0.53064810\n",
      "Iteration 90, loss = 0.53096946\n",
      "Iteration 91, loss = 0.53080096\n",
      "Iteration 92, loss = 0.53002712\n",
      "Iteration 93, loss = 0.53002510\n",
      "Iteration 94, loss = 0.52953261\n",
      "Iteration 95, loss = 0.52901245\n",
      "Iteration 96, loss = 0.52884356\n",
      "Iteration 97, loss = 0.52871660\n",
      "Iteration 98, loss = 0.52848210\n",
      "Iteration 99, loss = 0.52819506\n",
      "Iteration 100, loss = 0.52799954\n",
      "Iteration 101, loss = 0.52822829\n",
      "Iteration 102, loss = 0.52798757\n",
      "Iteration 103, loss = 0.52802551\n",
      "Iteration 104, loss = 0.52838658\n",
      "Iteration 105, loss = 0.52822870\n",
      "Iteration 106, loss = 0.52820801\n",
      "Iteration 107, loss = 0.52815562\n",
      "Iteration 108, loss = 0.52817615\n",
      "Iteration 109, loss = 0.52764703\n",
      "Iteration 110, loss = 0.52709293\n",
      "Iteration 111, loss = 0.52641798\n",
      "Iteration 112, loss = 0.52594554\n",
      "Iteration 113, loss = 0.52579946\n",
      "Iteration 114, loss = 0.52583755\n",
      "Iteration 115, loss = 0.52558658\n",
      "Iteration 116, loss = 0.52548358\n",
      "Iteration 117, loss = 0.52513329\n",
      "Iteration 118, loss = 0.52527105\n",
      "Iteration 119, loss = 0.52473460\n",
      "Iteration 120, loss = 0.52464681\n",
      "Iteration 121, loss = 0.52402761\n",
      "Iteration 122, loss = 0.52374774\n",
      "Iteration 123, loss = 0.52402658\n",
      "Iteration 124, loss = 0.52438863\n",
      "Iteration 125, loss = 0.52484278\n",
      "Iteration 126, loss = 0.52525683\n",
      "Iteration 127, loss = 0.52475060\n",
      "Iteration 128, loss = 0.52433974\n",
      "Iteration 129, loss = 0.52313856\n",
      "Iteration 130, loss = 0.52289233\n",
      "Iteration 131, loss = 0.52247466\n",
      "Iteration 132, loss = 0.52269076\n",
      "Iteration 133, loss = 0.52269264\n",
      "Iteration 134, loss = 0.52278218\n",
      "Iteration 135, loss = 0.52297236\n",
      "Iteration 136, loss = 0.52369682\n",
      "Iteration 137, loss = 0.52336805\n",
      "Iteration 138, loss = 0.52271448\n",
      "Iteration 139, loss = 0.52240293\n",
      "Iteration 140, loss = 0.52162067\n",
      "Iteration 141, loss = 0.52206801\n",
      "Iteration 142, loss = 0.52127861\n",
      "Iteration 143, loss = 0.52129280\n",
      "Iteration 144, loss = 0.52115844\n",
      "Iteration 145, loss = 0.52108332\n",
      "Iteration 146, loss = 0.52065906\n",
      "Iteration 147, loss = 0.52048682\n",
      "Iteration 148, loss = 0.52124401\n",
      "Iteration 149, loss = 0.52152268\n",
      "Iteration 150, loss = 0.52251079\n",
      "Iteration 151, loss = 0.52178756\n",
      "Iteration 152, loss = 0.52141934\n",
      "Iteration 153, loss = 0.52117407\n",
      "Iteration 154, loss = 0.52162245\n",
      "Iteration 155, loss = 0.52110938\n",
      "Iteration 156, loss = 0.52087379\n",
      "Iteration 157, loss = 0.52121978\n",
      "Iteration 158, loss = 0.52088749\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69013174\n",
      "Iteration 2, loss = 0.66214285\n",
      "Iteration 3, loss = 0.63632112\n",
      "Iteration 4, loss = 0.61511771\n",
      "Iteration 5, loss = 0.59558995\n",
      "Iteration 6, loss = 0.58142891\n",
      "Iteration 7, loss = 0.56644698\n",
      "Iteration 8, loss = 0.55724250\n",
      "Iteration 9, loss = 0.54812982\n",
      "Iteration 10, loss = 0.54163409\n",
      "Iteration 11, loss = 0.53717179\n",
      "Iteration 12, loss = 0.53291172\n",
      "Iteration 13, loss = 0.53054678\n",
      "Iteration 14, loss = 0.52921720\n",
      "Iteration 15, loss = 0.52794777\n",
      "Iteration 16, loss = 0.52833762\n",
      "Iteration 17, loss = 0.52762009\n",
      "Iteration 18, loss = 0.52841250\n",
      "Iteration 19, loss = 0.52843906\n",
      "Iteration 20, loss = 0.52871664\n",
      "Iteration 21, loss = 0.52856069\n",
      "Iteration 22, loss = 0.52840541\n",
      "Iteration 23, loss = 0.52776611\n",
      "Iteration 24, loss = 0.52733219\n",
      "Iteration 25, loss = 0.52685678\n",
      "Iteration 26, loss = 0.52629717\n",
      "Iteration 27, loss = 0.52607588\n",
      "Iteration 28, loss = 0.52599086\n",
      "Iteration 29, loss = 0.52554696\n",
      "Iteration 30, loss = 0.52494280\n",
      "Iteration 31, loss = 0.52478975\n",
      "Iteration 32, loss = 0.52414041\n",
      "Iteration 33, loss = 0.52398422\n",
      "Iteration 34, loss = 0.52369211\n",
      "Iteration 35, loss = 0.52392045\n",
      "Iteration 36, loss = 0.52367526\n",
      "Iteration 37, loss = 0.52394611\n",
      "Iteration 38, loss = 0.52369844\n",
      "Iteration 39, loss = 0.52364083\n",
      "Iteration 40, loss = 0.52323192\n",
      "Iteration 41, loss = 0.52281747\n",
      "Iteration 42, loss = 0.52222677\n",
      "Iteration 43, loss = 0.52217628\n",
      "Iteration 44, loss = 0.52196502\n",
      "Iteration 45, loss = 0.52212447\n",
      "Iteration 46, loss = 0.52222217\n",
      "Iteration 47, loss = 0.52218188\n",
      "Iteration 48, loss = 0.52190594\n",
      "Iteration 49, loss = 0.52164071\n",
      "Iteration 50, loss = 0.52134565\n",
      "Iteration 51, loss = 0.52098221\n",
      "Iteration 52, loss = 0.52100313\n",
      "Iteration 53, loss = 0.52082815\n",
      "Iteration 54, loss = 0.52094392\n",
      "Iteration 55, loss = 0.52142666\n",
      "Iteration 56, loss = 0.52175487\n",
      "Iteration 57, loss = 0.52201729\n",
      "Iteration 58, loss = 0.52169365\n",
      "Iteration 59, loss = 0.52135003\n",
      "Iteration 60, loss = 0.52037981\n",
      "Iteration 61, loss = 0.51981228\n",
      "Iteration 62, loss = 0.51952509\n",
      "Iteration 63, loss = 0.51923231\n",
      "Iteration 64, loss = 0.52012959\n",
      "Iteration 65, loss = 0.51988674\n",
      "Iteration 66, loss = 0.51994699\n",
      "Iteration 67, loss = 0.52005316\n",
      "Iteration 68, loss = 0.51990394\n",
      "Iteration 69, loss = 0.51941088\n",
      "Iteration 70, loss = 0.51930742\n",
      "Iteration 71, loss = 0.51902331\n",
      "Iteration 72, loss = 0.51861454\n",
      "Iteration 73, loss = 0.51841872\n",
      "Iteration 74, loss = 0.51820662\n",
      "Iteration 75, loss = 0.51844485\n",
      "Iteration 76, loss = 0.51821808\n",
      "Iteration 77, loss = 0.51788230\n",
      "Iteration 78, loss = 0.51782064\n",
      "Iteration 79, loss = 0.51768459\n",
      "Iteration 80, loss = 0.51768163\n",
      "Iteration 81, loss = 0.51794978\n",
      "Iteration 82, loss = 0.51790385\n",
      "Iteration 83, loss = 0.51782623\n",
      "Iteration 84, loss = 0.51767613\n",
      "Iteration 85, loss = 0.51761438\n",
      "Iteration 86, loss = 0.51745025\n",
      "Iteration 87, loss = 0.51742257\n",
      "Iteration 88, loss = 0.51721474\n",
      "Iteration 89, loss = 0.51717603\n",
      "Iteration 90, loss = 0.51703857\n",
      "Iteration 91, loss = 0.51698972\n",
      "Iteration 92, loss = 0.51699934\n",
      "Iteration 93, loss = 0.51707937\n",
      "Iteration 94, loss = 0.51676887\n",
      "Iteration 95, loss = 0.51652783\n",
      "Iteration 96, loss = 0.51627206\n",
      "Iteration 97, loss = 0.51591878\n",
      "Iteration 98, loss = 0.51587715\n",
      "Iteration 99, loss = 0.51598121\n",
      "Iteration 100, loss = 0.51552274\n",
      "Iteration 101, loss = 0.51541912\n",
      "Iteration 102, loss = 0.51548141\n",
      "Iteration 103, loss = 0.51542249\n",
      "Iteration 104, loss = 0.51549132\n",
      "Iteration 105, loss = 0.51570031\n",
      "Iteration 106, loss = 0.51591305\n",
      "Iteration 107, loss = 0.51596270\n",
      "Iteration 108, loss = 0.51553686\n",
      "Iteration 109, loss = 0.51529563\n",
      "Iteration 110, loss = 0.51480235\n",
      "Iteration 111, loss = 0.51466709\n",
      "Iteration 112, loss = 0.51442360\n",
      "Iteration 113, loss = 0.51458734\n",
      "Iteration 114, loss = 0.51423045\n",
      "Iteration 115, loss = 0.51402150\n",
      "Iteration 116, loss = 0.51370665\n",
      "Iteration 117, loss = 0.51398232\n",
      "Iteration 118, loss = 0.51360084\n",
      "Iteration 119, loss = 0.51326826\n",
      "Iteration 120, loss = 0.51388114\n",
      "Iteration 121, loss = 0.51499405\n",
      "Iteration 122, loss = 0.51619172\n",
      "Iteration 123, loss = 0.51589091\n",
      "Iteration 124, loss = 0.51567748\n",
      "Iteration 125, loss = 0.51492822\n",
      "Iteration 126, loss = 0.51451742\n",
      "Iteration 127, loss = 0.51418313\n",
      "Iteration 128, loss = 0.51336892\n",
      "Iteration 129, loss = 0.51300342\n",
      "Iteration 130, loss = 0.51259308\n",
      "Iteration 131, loss = 0.51243802\n",
      "Iteration 132, loss = 0.51210879\n",
      "Iteration 133, loss = 0.51231353\n",
      "Iteration 134, loss = 0.51241391\n",
      "Iteration 135, loss = 0.51269911\n",
      "Iteration 136, loss = 0.51287776\n",
      "Iteration 137, loss = 0.51287150\n",
      "Iteration 138, loss = 0.51239395\n",
      "Iteration 139, loss = 0.51192405\n",
      "Iteration 140, loss = 0.51183928\n",
      "Iteration 141, loss = 0.51189343\n",
      "Iteration 142, loss = 0.51173242\n",
      "Iteration 143, loss = 0.51128852\n",
      "Iteration 144, loss = 0.51098865\n",
      "Iteration 145, loss = 0.51089354\n",
      "Iteration 146, loss = 0.51097841\n",
      "Iteration 147, loss = 0.51112556\n",
      "Iteration 148, loss = 0.51084507\n",
      "Iteration 149, loss = 0.51078385\n",
      "Iteration 150, loss = 0.51037719\n",
      "Iteration 151, loss = 0.51049831\n",
      "Iteration 152, loss = 0.51041446\n",
      "Iteration 153, loss = 0.51030368\n",
      "Iteration 154, loss = 0.51025144\n",
      "Iteration 155, loss = 0.51018677\n",
      "Iteration 156, loss = 0.50993634\n",
      "Iteration 157, loss = 0.51011846\n",
      "Iteration 158, loss = 0.50981724\n",
      "Iteration 159, loss = 0.50989242\n",
      "Iteration 160, loss = 0.50958225\n",
      "Iteration 161, loss = 0.50957258\n",
      "Iteration 162, loss = 0.50927410\n",
      "Iteration 163, loss = 0.50902683\n",
      "Iteration 164, loss = 0.50883978\n",
      "Iteration 165, loss = 0.50898297\n",
      "Iteration 166, loss = 0.50875804\n",
      "Iteration 167, loss = 0.50890339\n",
      "Iteration 168, loss = 0.50900183\n",
      "Iteration 169, loss = 0.50870154\n",
      "Iteration 170, loss = 0.50816572\n",
      "Iteration 171, loss = 0.50810496\n",
      "Iteration 172, loss = 0.50819055\n",
      "Iteration 173, loss = 0.50802173\n",
      "Iteration 174, loss = 0.50812871\n",
      "Iteration 175, loss = 0.50802975\n",
      "Iteration 176, loss = 0.50777140\n",
      "Iteration 177, loss = 0.50747941\n",
      "Iteration 178, loss = 0.50752185\n",
      "Iteration 179, loss = 0.50702184\n",
      "Iteration 180, loss = 0.50726964\n",
      "Iteration 181, loss = 0.50721484\n",
      "Iteration 182, loss = 0.50739731\n",
      "Iteration 183, loss = 0.50750299\n",
      "Iteration 184, loss = 0.50715498\n",
      "Iteration 185, loss = 0.50698376\n",
      "Iteration 186, loss = 0.50695727\n",
      "Iteration 187, loss = 0.50693035\n",
      "Iteration 188, loss = 0.50692971\n",
      "Iteration 189, loss = 0.50684074\n",
      "Iteration 190, loss = 0.50668820\n",
      "Iteration 191, loss = 0.50662150\n",
      "Iteration 192, loss = 0.50654699\n",
      "Iteration 193, loss = 0.50640385\n",
      "Iteration 194, loss = 0.50652431\n",
      "Iteration 195, loss = 0.50620340\n",
      "Iteration 196, loss = 0.50595216\n",
      "Iteration 197, loss = 0.50608844\n",
      "Iteration 198, loss = 0.50600230\n",
      "Iteration 199, loss = 0.50607189\n",
      "Iteration 200, loss = 0.50593623\n",
      "Iteration 201, loss = 0.50598758\n",
      "Iteration 202, loss = 0.50598347\n",
      "Iteration 203, loss = 0.50583123\n",
      "Iteration 204, loss = 0.50622389\n",
      "Iteration 205, loss = 0.50635817\n",
      "Iteration 206, loss = 0.50591612\n",
      "Iteration 207, loss = 0.50515805\n",
      "Iteration 208, loss = 0.50476186\n",
      "Iteration 209, loss = 0.50460089\n",
      "Iteration 210, loss = 0.50473200\n",
      "Iteration 211, loss = 0.50505076\n",
      "Iteration 212, loss = 0.50577067\n",
      "Iteration 213, loss = 0.50629731\n",
      "Iteration 214, loss = 0.50646363\n",
      "Iteration 215, loss = 0.50654969\n",
      "Iteration 216, loss = 0.50634069\n",
      "Iteration 217, loss = 0.50532939\n",
      "Iteration 218, loss = 0.50506078\n",
      "Iteration 219, loss = 0.50571829\n",
      "Iteration 220, loss = 0.50536477\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68429285\n",
      "Iteration 2, loss = 0.65456806\n",
      "Iteration 3, loss = 0.63107821\n",
      "Iteration 4, loss = 0.61043036\n",
      "Iteration 5, loss = 0.59325128\n",
      "Iteration 6, loss = 0.57855025\n",
      "Iteration 7, loss = 0.56638869\n",
      "Iteration 8, loss = 0.55774314\n",
      "Iteration 9, loss = 0.54990242\n",
      "Iteration 10, loss = 0.54325723\n",
      "Iteration 11, loss = 0.53997096\n",
      "Iteration 12, loss = 0.53692160\n",
      "Iteration 13, loss = 0.53499789\n",
      "Iteration 14, loss = 0.53408556\n",
      "Iteration 15, loss = 0.53390862\n",
      "Iteration 16, loss = 0.53327789\n",
      "Iteration 17, loss = 0.53302376\n",
      "Iteration 18, loss = 0.53261388\n",
      "Iteration 19, loss = 0.53230702\n",
      "Iteration 20, loss = 0.53180133\n",
      "Iteration 21, loss = 0.53147755\n",
      "Iteration 22, loss = 0.53081146\n",
      "Iteration 23, loss = 0.53047544\n",
      "Iteration 24, loss = 0.53011770\n",
      "Iteration 25, loss = 0.52969143\n",
      "Iteration 26, loss = 0.52942615\n",
      "Iteration 27, loss = 0.52941041\n",
      "Iteration 28, loss = 0.52923217\n",
      "Iteration 29, loss = 0.52907675\n",
      "Iteration 30, loss = 0.52893809\n",
      "Iteration 31, loss = 0.52890632\n",
      "Iteration 32, loss = 0.52883094\n",
      "Iteration 33, loss = 0.52865225\n",
      "Iteration 34, loss = 0.52851838\n",
      "Iteration 35, loss = 0.52823853\n",
      "Iteration 36, loss = 0.52772251\n",
      "Iteration 37, loss = 0.52717548\n",
      "Iteration 38, loss = 0.52687398\n",
      "Iteration 39, loss = 0.52646633\n",
      "Iteration 40, loss = 0.52626847\n",
      "Iteration 41, loss = 0.52619915\n",
      "Iteration 42, loss = 0.52629772\n",
      "Iteration 43, loss = 0.52637938\n",
      "Iteration 44, loss = 0.52625070\n",
      "Iteration 45, loss = 0.52607927\n",
      "Iteration 46, loss = 0.52580354\n",
      "Iteration 47, loss = 0.52537836\n",
      "Iteration 48, loss = 0.52463764\n",
      "Iteration 49, loss = 0.52424140\n",
      "Iteration 50, loss = 0.52393414\n",
      "Iteration 51, loss = 0.52373583\n",
      "Iteration 52, loss = 0.52367912\n",
      "Iteration 53, loss = 0.52384670\n",
      "Iteration 54, loss = 0.52360414\n",
      "Iteration 55, loss = 0.52332163\n",
      "Iteration 56, loss = 0.52308395\n",
      "Iteration 57, loss = 0.52280478\n",
      "Iteration 58, loss = 0.52249796\n",
      "Iteration 59, loss = 0.52235893\n",
      "Iteration 60, loss = 0.52224412\n",
      "Iteration 61, loss = 0.52199722\n",
      "Iteration 62, loss = 0.52162268\n",
      "Iteration 63, loss = 0.52153678\n",
      "Iteration 64, loss = 0.52141495\n",
      "Iteration 65, loss = 0.52133552\n",
      "Iteration 66, loss = 0.52131682\n",
      "Iteration 67, loss = 0.52159816\n",
      "Iteration 68, loss = 0.52147114\n",
      "Iteration 69, loss = 0.52144987\n",
      "Iteration 70, loss = 0.52114501\n",
      "Iteration 71, loss = 0.52131874\n",
      "Iteration 72, loss = 0.52107389\n",
      "Iteration 73, loss = 0.52106637\n",
      "Iteration 74, loss = 0.52099340\n",
      "Iteration 75, loss = 0.52078677\n",
      "Iteration 76, loss = 0.52066906\n",
      "Iteration 77, loss = 0.52049566\n",
      "Iteration 78, loss = 0.52020140\n",
      "Iteration 79, loss = 0.52020425\n",
      "Iteration 80, loss = 0.52010870\n",
      "Iteration 81, loss = 0.51988960\n",
      "Iteration 82, loss = 0.51978121\n",
      "Iteration 83, loss = 0.51944282\n",
      "Iteration 84, loss = 0.51881492\n",
      "Iteration 85, loss = 0.51815688\n",
      "Iteration 86, loss = 0.51770870\n",
      "Iteration 87, loss = 0.51798800\n",
      "Iteration 88, loss = 0.51813639\n",
      "Iteration 89, loss = 0.51795962\n",
      "Iteration 90, loss = 0.51767391\n",
      "Iteration 91, loss = 0.51732901\n",
      "Iteration 92, loss = 0.51702253\n",
      "Iteration 93, loss = 0.51673391\n",
      "Iteration 94, loss = 0.51637411\n",
      "Iteration 95, loss = 0.51664413\n",
      "Iteration 96, loss = 0.51671564\n",
      "Iteration 97, loss = 0.51678366\n",
      "Iteration 98, loss = 0.51667322\n",
      "Iteration 99, loss = 0.51668779\n",
      "Iteration 100, loss = 0.51654446\n",
      "Iteration 101, loss = 0.51619708\n",
      "Iteration 102, loss = 0.51564706\n",
      "Iteration 103, loss = 0.51546611\n",
      "Iteration 104, loss = 0.51511686\n",
      "Iteration 105, loss = 0.51492435\n",
      "Iteration 106, loss = 0.51463464\n",
      "Iteration 107, loss = 0.51496455\n",
      "Iteration 108, loss = 0.51465302\n",
      "Iteration 109, loss = 0.51449219\n",
      "Iteration 110, loss = 0.51431831\n",
      "Iteration 111, loss = 0.51487224\n",
      "Iteration 112, loss = 0.51477385\n",
      "Iteration 113, loss = 0.51456715\n",
      "Iteration 114, loss = 0.51434675\n",
      "Iteration 115, loss = 0.51403009\n",
      "Iteration 116, loss = 0.51357027\n",
      "Iteration 117, loss = 0.51307173\n",
      "Iteration 118, loss = 0.51310234\n",
      "Iteration 119, loss = 0.51249224\n",
      "Iteration 120, loss = 0.51233522\n",
      "Iteration 121, loss = 0.51188748\n",
      "Iteration 122, loss = 0.51207042\n",
      "Iteration 123, loss = 0.51191405\n",
      "Iteration 124, loss = 0.51182575\n",
      "Iteration 125, loss = 0.51171390\n",
      "Iteration 126, loss = 0.51186819\n",
      "Iteration 127, loss = 0.51160933\n",
      "Iteration 128, loss = 0.51211736\n",
      "Iteration 129, loss = 0.51183493\n",
      "Iteration 130, loss = 0.51155049\n",
      "Iteration 131, loss = 0.51088082\n",
      "Iteration 132, loss = 0.51021965\n",
      "Iteration 133, loss = 0.50976406\n",
      "Iteration 134, loss = 0.51021902\n",
      "Iteration 135, loss = 0.51083825\n",
      "Iteration 136, loss = 0.51128748\n",
      "Iteration 137, loss = 0.51140089\n",
      "Iteration 138, loss = 0.51146106\n",
      "Iteration 139, loss = 0.51084376\n",
      "Iteration 140, loss = 0.51011896\n",
      "Iteration 141, loss = 0.50909164\n",
      "Iteration 142, loss = 0.50865838\n",
      "Iteration 143, loss = 0.50829621\n",
      "Iteration 144, loss = 0.50799412\n",
      "Iteration 145, loss = 0.50822269\n",
      "Iteration 146, loss = 0.50814660\n",
      "Iteration 147, loss = 0.50792321\n",
      "Iteration 148, loss = 0.50785594\n",
      "Iteration 149, loss = 0.50803940\n",
      "Iteration 150, loss = 0.50759062\n",
      "Iteration 151, loss = 0.50755664\n",
      "Iteration 152, loss = 0.50742210\n",
      "Iteration 153, loss = 0.50687272\n",
      "Iteration 154, loss = 0.50706357\n",
      "Iteration 155, loss = 0.50663796\n",
      "Iteration 156, loss = 0.50651907\n",
      "Iteration 157, loss = 0.50608849\n",
      "Iteration 158, loss = 0.50660947\n",
      "Iteration 159, loss = 0.50682267\n",
      "Iteration 160, loss = 0.50669606\n",
      "Iteration 161, loss = 0.50653959\n",
      "Iteration 162, loss = 0.50650898\n",
      "Iteration 163, loss = 0.50634689\n",
      "Iteration 164, loss = 0.50625614\n",
      "Iteration 165, loss = 0.50631575\n",
      "Iteration 166, loss = 0.50607126\n",
      "Iteration 167, loss = 0.50561949\n",
      "Iteration 168, loss = 0.50573101\n",
      "Iteration 169, loss = 0.50559948\n",
      "Iteration 170, loss = 0.50552285\n",
      "Iteration 171, loss = 0.50481515\n",
      "Iteration 172, loss = 0.50432161\n",
      "Iteration 173, loss = 0.50452318\n",
      "Iteration 174, loss = 0.50518537\n",
      "Iteration 175, loss = 0.50590048\n",
      "Iteration 176, loss = 0.50551890\n",
      "Iteration 177, loss = 0.50503705\n",
      "Iteration 178, loss = 0.50549494\n",
      "Iteration 179, loss = 0.50629067\n",
      "Iteration 180, loss = 0.50650080\n",
      "Iteration 181, loss = 0.50614939\n",
      "Iteration 182, loss = 0.50590850\n",
      "Iteration 183, loss = 0.50508480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68832483\n",
      "Iteration 2, loss = 0.65783025\n",
      "Iteration 3, loss = 0.63316976\n",
      "Iteration 4, loss = 0.61043046\n",
      "Iteration 5, loss = 0.59197941\n",
      "Iteration 6, loss = 0.57616598\n",
      "Iteration 7, loss = 0.56259511\n",
      "Iteration 8, loss = 0.55224830\n",
      "Iteration 9, loss = 0.54312521\n",
      "Iteration 10, loss = 0.53628269\n",
      "Iteration 11, loss = 0.53134941\n",
      "Iteration 12, loss = 0.52715226\n",
      "Iteration 13, loss = 0.52440291\n",
      "Iteration 14, loss = 0.52195172\n",
      "Iteration 15, loss = 0.52074220\n",
      "Iteration 16, loss = 0.51969294\n",
      "Iteration 17, loss = 0.51897600\n",
      "Iteration 18, loss = 0.51903326\n",
      "Iteration 19, loss = 0.51845257\n",
      "Iteration 20, loss = 0.51825228\n",
      "Iteration 21, loss = 0.51818637\n",
      "Iteration 22, loss = 0.51825988\n",
      "Iteration 23, loss = 0.51807812\n",
      "Iteration 24, loss = 0.51790886\n",
      "Iteration 25, loss = 0.51736913\n",
      "Iteration 26, loss = 0.51692482\n",
      "Iteration 27, loss = 0.51677967\n",
      "Iteration 28, loss = 0.51617955\n",
      "Iteration 29, loss = 0.51588173\n",
      "Iteration 30, loss = 0.51536260\n",
      "Iteration 31, loss = 0.51501475\n",
      "Iteration 32, loss = 0.51456942\n",
      "Iteration 33, loss = 0.51426014\n",
      "Iteration 34, loss = 0.51412030\n",
      "Iteration 35, loss = 0.51370749\n",
      "Iteration 36, loss = 0.51350093\n",
      "Iteration 37, loss = 0.51335482\n",
      "Iteration 38, loss = 0.51329221\n",
      "Iteration 39, loss = 0.51288895\n",
      "Iteration 40, loss = 0.51295080\n",
      "Iteration 41, loss = 0.51261194\n",
      "Iteration 42, loss = 0.51232942\n",
      "Iteration 43, loss = 0.51222444\n",
      "Iteration 44, loss = 0.51183859\n",
      "Iteration 45, loss = 0.51165604\n",
      "Iteration 46, loss = 0.51147316\n",
      "Iteration 47, loss = 0.51133887\n",
      "Iteration 48, loss = 0.51121501\n",
      "Iteration 49, loss = 0.51108184\n",
      "Iteration 50, loss = 0.51091362\n",
      "Iteration 51, loss = 0.51081997\n",
      "Iteration 52, loss = 0.51096827\n",
      "Iteration 53, loss = 0.51081219\n",
      "Iteration 54, loss = 0.51078410\n",
      "Iteration 55, loss = 0.51068465\n",
      "Iteration 56, loss = 0.51015486\n",
      "Iteration 57, loss = 0.50956490\n",
      "Iteration 58, loss = 0.51005879\n",
      "Iteration 59, loss = 0.50978998\n",
      "Iteration 60, loss = 0.51123968\n",
      "Iteration 61, loss = 0.51151724\n",
      "Iteration 62, loss = 0.51145018\n",
      "Iteration 63, loss = 0.51096231\n",
      "Iteration 64, loss = 0.51077154\n",
      "Iteration 65, loss = 0.51061069\n",
      "Iteration 66, loss = 0.51018455\n",
      "Iteration 67, loss = 0.50998905\n",
      "Iteration 68, loss = 0.50945457\n",
      "Iteration 69, loss = 0.50922994\n",
      "Iteration 70, loss = 0.50893986\n",
      "Iteration 71, loss = 0.50880188\n",
      "Iteration 72, loss = 0.50846843\n",
      "Iteration 73, loss = 0.50859336\n",
      "Iteration 74, loss = 0.50850631\n",
      "Iteration 75, loss = 0.50812247\n",
      "Iteration 76, loss = 0.50795905\n",
      "Iteration 77, loss = 0.50771474\n",
      "Iteration 78, loss = 0.50753390\n",
      "Iteration 79, loss = 0.50731448\n",
      "Iteration 80, loss = 0.50719276\n",
      "Iteration 81, loss = 0.50704813\n",
      "Iteration 82, loss = 0.50680483\n",
      "Iteration 83, loss = 0.50673424\n",
      "Iteration 84, loss = 0.50666986\n",
      "Iteration 85, loss = 0.50685061\n",
      "Iteration 86, loss = 0.50658367\n",
      "Iteration 87, loss = 0.50641115\n",
      "Iteration 88, loss = 0.50621371\n",
      "Iteration 89, loss = 0.50619913\n",
      "Iteration 90, loss = 0.50597641\n",
      "Iteration 91, loss = 0.50584558\n",
      "Iteration 92, loss = 0.50550967\n",
      "Iteration 93, loss = 0.50554440\n",
      "Iteration 94, loss = 0.50565601\n",
      "Iteration 95, loss = 0.50552871\n",
      "Iteration 96, loss = 0.50508910\n",
      "Iteration 97, loss = 0.50511209\n",
      "Iteration 98, loss = 0.50477524\n",
      "Iteration 99, loss = 0.50479922\n",
      "Iteration 100, loss = 0.50476623\n",
      "Iteration 101, loss = 0.50450622\n",
      "Iteration 102, loss = 0.50465630\n",
      "Iteration 103, loss = 0.50482317\n",
      "Iteration 104, loss = 0.50513043\n",
      "Iteration 105, loss = 0.50563791\n",
      "Iteration 106, loss = 0.50531973\n",
      "Iteration 107, loss = 0.50480357\n",
      "Iteration 108, loss = 0.50451018\n",
      "Iteration 109, loss = 0.50422252\n",
      "Iteration 110, loss = 0.50387412\n",
      "Iteration 111, loss = 0.50347400\n",
      "Iteration 112, loss = 0.50324285\n",
      "Iteration 113, loss = 0.50305248\n",
      "Iteration 114, loss = 0.50293227\n",
      "Iteration 115, loss = 0.50286536\n",
      "Iteration 116, loss = 0.50285343\n",
      "Iteration 117, loss = 0.50295532\n",
      "Iteration 118, loss = 0.50252030\n",
      "Iteration 119, loss = 0.50236787\n",
      "Iteration 120, loss = 0.50240507\n",
      "Iteration 121, loss = 0.50210772\n",
      "Iteration 122, loss = 0.50192691\n",
      "Iteration 123, loss = 0.50187662\n",
      "Iteration 124, loss = 0.50163596\n",
      "Iteration 125, loss = 0.50159964\n",
      "Iteration 126, loss = 0.50148198\n",
      "Iteration 127, loss = 0.50134706\n",
      "Iteration 128, loss = 0.50127558\n",
      "Iteration 129, loss = 0.50127963\n",
      "Iteration 130, loss = 0.50132530\n",
      "Iteration 131, loss = 0.50121501\n",
      "Iteration 132, loss = 0.50096753\n",
      "Iteration 133, loss = 0.50098765\n",
      "Iteration 134, loss = 0.50076166\n",
      "Iteration 135, loss = 0.50093633\n",
      "Iteration 136, loss = 0.50118576\n",
      "Iteration 137, loss = 0.50132082\n",
      "Iteration 138, loss = 0.50104917\n",
      "Iteration 139, loss = 0.50103543\n",
      "Iteration 140, loss = 0.50092081\n",
      "Iteration 141, loss = 0.50097780\n",
      "Iteration 142, loss = 0.50077494\n",
      "Iteration 143, loss = 0.50031912\n",
      "Iteration 144, loss = 0.49986952\n",
      "Iteration 145, loss = 0.49959491\n",
      "Iteration 146, loss = 0.49926976\n",
      "Iteration 147, loss = 0.49896544\n",
      "Iteration 148, loss = 0.49881216\n",
      "Iteration 149, loss = 0.49880945\n",
      "Iteration 150, loss = 0.49912003\n",
      "Iteration 151, loss = 0.49924296\n",
      "Iteration 152, loss = 0.49943635\n",
      "Iteration 153, loss = 0.49928734\n",
      "Iteration 154, loss = 0.49903962\n",
      "Iteration 155, loss = 0.49874922\n",
      "Iteration 156, loss = 0.49874056\n",
      "Iteration 157, loss = 0.49888364\n",
      "Iteration 158, loss = 0.49876066\n",
      "Iteration 159, loss = 0.49921054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69072870\n",
      "Iteration 2, loss = 0.66161688\n",
      "Iteration 3, loss = 0.63516772\n",
      "Iteration 4, loss = 0.61323205\n",
      "Iteration 5, loss = 0.59392137\n",
      "Iteration 6, loss = 0.57750481\n",
      "Iteration 7, loss = 0.56333247\n",
      "Iteration 8, loss = 0.55454486\n",
      "Iteration 9, loss = 0.54484330\n",
      "Iteration 10, loss = 0.53824601\n",
      "Iteration 11, loss = 0.53352305\n",
      "Iteration 12, loss = 0.52982251\n",
      "Iteration 13, loss = 0.52681635\n",
      "Iteration 14, loss = 0.52487454\n",
      "Iteration 15, loss = 0.52315131\n",
      "Iteration 16, loss = 0.52312677\n",
      "Iteration 17, loss = 0.52240319\n",
      "Iteration 18, loss = 0.52205047\n",
      "Iteration 19, loss = 0.52179477\n",
      "Iteration 20, loss = 0.52251665\n",
      "Iteration 21, loss = 0.52266251\n",
      "Iteration 22, loss = 0.52263299\n",
      "Iteration 23, loss = 0.52290012\n",
      "Iteration 24, loss = 0.52290196\n",
      "Iteration 25, loss = 0.52248417\n",
      "Iteration 26, loss = 0.52173042\n",
      "Iteration 27, loss = 0.52095971\n",
      "Iteration 28, loss = 0.52019426\n",
      "Iteration 29, loss = 0.51978981\n",
      "Iteration 30, loss = 0.51950404\n",
      "Iteration 31, loss = 0.51897514\n",
      "Iteration 32, loss = 0.51863504\n",
      "Iteration 33, loss = 0.51876587\n",
      "Iteration 34, loss = 0.51885403\n",
      "Iteration 35, loss = 0.51873164\n",
      "Iteration 36, loss = 0.51848756\n",
      "Iteration 37, loss = 0.51812582\n",
      "Iteration 38, loss = 0.51781855\n",
      "Iteration 39, loss = 0.51765783\n",
      "Iteration 40, loss = 0.51719284\n",
      "Iteration 41, loss = 0.51705216\n",
      "Iteration 42, loss = 0.51683985\n",
      "Iteration 43, loss = 0.51662567\n",
      "Iteration 44, loss = 0.51685269\n",
      "Iteration 45, loss = 0.51626629\n",
      "Iteration 46, loss = 0.51595320\n",
      "Iteration 47, loss = 0.51593907\n",
      "Iteration 48, loss = 0.51609418\n",
      "Iteration 49, loss = 0.51599586\n",
      "Iteration 50, loss = 0.51577715\n",
      "Iteration 51, loss = 0.51561416\n",
      "Iteration 52, loss = 0.51541007\n",
      "Iteration 53, loss = 0.51510967\n",
      "Iteration 54, loss = 0.51491933\n",
      "Iteration 55, loss = 0.51472924\n",
      "Iteration 56, loss = 0.51459564\n",
      "Iteration 57, loss = 0.51461707\n",
      "Iteration 58, loss = 0.51437418\n",
      "Iteration 59, loss = 0.51419693\n",
      "Iteration 60, loss = 0.51401654\n",
      "Iteration 61, loss = 0.51414717\n",
      "Iteration 62, loss = 0.51456870\n",
      "Iteration 63, loss = 0.51407346\n",
      "Iteration 64, loss = 0.51380457\n",
      "Iteration 65, loss = 0.51386058\n",
      "Iteration 66, loss = 0.51367687\n",
      "Iteration 67, loss = 0.51362022\n",
      "Iteration 68, loss = 0.51363656\n",
      "Iteration 69, loss = 0.51371032\n",
      "Iteration 70, loss = 0.51390745\n",
      "Iteration 71, loss = 0.51357341\n",
      "Iteration 72, loss = 0.51338975\n",
      "Iteration 73, loss = 0.51308846\n",
      "Iteration 74, loss = 0.51285433\n",
      "Iteration 75, loss = 0.51269212\n",
      "Iteration 76, loss = 0.51263940\n",
      "Iteration 77, loss = 0.51262707\n",
      "Iteration 78, loss = 0.51212328\n",
      "Iteration 79, loss = 0.51196106\n",
      "Iteration 80, loss = 0.51168744\n",
      "Iteration 81, loss = 0.51148331\n",
      "Iteration 82, loss = 0.51135894\n",
      "Iteration 83, loss = 0.51122828\n",
      "Iteration 84, loss = 0.51151323\n",
      "Iteration 85, loss = 0.51154702\n",
      "Iteration 86, loss = 0.51147287\n",
      "Iteration 87, loss = 0.51145069\n",
      "Iteration 88, loss = 0.51138003\n",
      "Iteration 89, loss = 0.51114019\n",
      "Iteration 90, loss = 0.51097957\n",
      "Iteration 91, loss = 0.51075944\n",
      "Iteration 92, loss = 0.51066783\n",
      "Iteration 93, loss = 0.51062651\n",
      "Iteration 94, loss = 0.51028319\n",
      "Iteration 95, loss = 0.51003384\n",
      "Iteration 96, loss = 0.50997334\n",
      "Iteration 97, loss = 0.50989212\n",
      "Iteration 98, loss = 0.50975937\n",
      "Iteration 99, loss = 0.51035008\n",
      "Iteration 100, loss = 0.51043590\n",
      "Iteration 101, loss = 0.50997848\n",
      "Iteration 102, loss = 0.50953130\n",
      "Iteration 103, loss = 0.50913715\n",
      "Iteration 104, loss = 0.50914562\n",
      "Iteration 105, loss = 0.50904456\n",
      "Iteration 106, loss = 0.50940880\n",
      "Iteration 107, loss = 0.50958316\n",
      "Iteration 108, loss = 0.50955076\n",
      "Iteration 109, loss = 0.51000553\n",
      "Iteration 110, loss = 0.50981441\n",
      "Iteration 111, loss = 0.50943603\n",
      "Iteration 112, loss = 0.50920962\n",
      "Iteration 113, loss = 0.50872198\n",
      "Iteration 114, loss = 0.50884996\n",
      "Iteration 115, loss = 0.50837866\n",
      "Iteration 116, loss = 0.50781419\n",
      "Iteration 117, loss = 0.50766089\n",
      "Iteration 118, loss = 0.50760435\n",
      "Iteration 119, loss = 0.50728689\n",
      "Iteration 120, loss = 0.50731308\n",
      "Iteration 121, loss = 0.50747868\n",
      "Iteration 122, loss = 0.50720022\n",
      "Iteration 123, loss = 0.50701214\n",
      "Iteration 124, loss = 0.50675300\n",
      "Iteration 125, loss = 0.50661008\n",
      "Iteration 126, loss = 0.50651275\n",
      "Iteration 127, loss = 0.50623775\n",
      "Iteration 128, loss = 0.50622148\n",
      "Iteration 129, loss = 0.50599218\n",
      "Iteration 130, loss = 0.50619575\n",
      "Iteration 131, loss = 0.50584440\n",
      "Iteration 132, loss = 0.50569765\n",
      "Iteration 133, loss = 0.50542004\n",
      "Iteration 134, loss = 0.50556047\n",
      "Iteration 135, loss = 0.50577283\n",
      "Iteration 136, loss = 0.50570763\n",
      "Iteration 137, loss = 0.50551501\n",
      "Iteration 138, loss = 0.50532662\n",
      "Iteration 139, loss = 0.50503124\n",
      "Iteration 140, loss = 0.50482847\n",
      "Iteration 141, loss = 0.50479581\n",
      "Iteration 142, loss = 0.50457806\n",
      "Iteration 143, loss = 0.50463619\n",
      "Iteration 144, loss = 0.50474683\n",
      "Iteration 145, loss = 0.50477360\n",
      "Iteration 146, loss = 0.50449636\n",
      "Iteration 147, loss = 0.50420784\n",
      "Iteration 148, loss = 0.50391611\n",
      "Iteration 149, loss = 0.50429282\n",
      "Iteration 150, loss = 0.50384614\n",
      "Iteration 151, loss = 0.50378539\n",
      "Iteration 152, loss = 0.50399939\n",
      "Iteration 153, loss = 0.50413002\n",
      "Iteration 154, loss = 0.50420464\n",
      "Iteration 155, loss = 0.50399710\n",
      "Iteration 156, loss = 0.50357504\n",
      "Iteration 157, loss = 0.50337777\n",
      "Iteration 158, loss = 0.50313415\n",
      "Iteration 159, loss = 0.50318045\n",
      "Iteration 160, loss = 0.50276768\n",
      "Iteration 161, loss = 0.50256679\n",
      "Iteration 162, loss = 0.50253790\n",
      "Iteration 163, loss = 0.50232320\n",
      "Iteration 164, loss = 0.50205669\n",
      "Iteration 165, loss = 0.50167860\n",
      "Iteration 166, loss = 0.50147534\n",
      "Iteration 167, loss = 0.50136603\n",
      "Iteration 168, loss = 0.50171855\n",
      "Iteration 169, loss = 0.50120543\n",
      "Iteration 170, loss = 0.50107707\n",
      "Iteration 171, loss = 0.50089468\n",
      "Iteration 172, loss = 0.50093841\n",
      "Iteration 173, loss = 0.50135049\n",
      "Iteration 174, loss = 0.50091771\n",
      "Iteration 175, loss = 0.50107043\n",
      "Iteration 176, loss = 0.50076780\n",
      "Iteration 177, loss = 0.50064885\n",
      "Iteration 178, loss = 0.50067496\n",
      "Iteration 179, loss = 0.50099048\n",
      "Iteration 180, loss = 0.50171124\n",
      "Iteration 181, loss = 0.50235418\n",
      "Iteration 182, loss = 0.50302717\n",
      "Iteration 183, loss = 0.50294138\n",
      "Iteration 184, loss = 0.50243709\n",
      "Iteration 185, loss = 0.50112313\n",
      "Iteration 186, loss = 0.50036516\n",
      "Iteration 187, loss = 0.50002567\n",
      "Iteration 188, loss = 0.49978858\n",
      "Iteration 189, loss = 0.49924507\n",
      "Iteration 190, loss = 0.49943212\n",
      "Iteration 191, loss = 0.49929359\n",
      "Iteration 192, loss = 0.49944293\n",
      "Iteration 193, loss = 0.49930778\n",
      "Iteration 194, loss = 0.49918705\n",
      "Iteration 195, loss = 0.49917415\n",
      "Iteration 196, loss = 0.49904064\n",
      "Iteration 197, loss = 0.49906851\n",
      "Iteration 198, loss = 0.49883762\n",
      "Iteration 199, loss = 0.49877825\n",
      "Iteration 200, loss = 0.49849121\n",
      "Iteration 201, loss = 0.49846691\n",
      "Iteration 202, loss = 0.49839369\n",
      "Iteration 203, loss = 0.49829168\n",
      "Iteration 204, loss = 0.49823506\n",
      "Iteration 205, loss = 0.49820421\n",
      "Iteration 206, loss = 0.49862347\n",
      "Iteration 207, loss = 0.49823735\n",
      "Iteration 208, loss = 0.49768621\n",
      "Iteration 209, loss = 0.49732619\n",
      "Iteration 210, loss = 0.49765532\n",
      "Iteration 211, loss = 0.49747547\n",
      "Iteration 212, loss = 0.49821452\n",
      "Iteration 213, loss = 0.49894172\n",
      "Iteration 214, loss = 0.49918585\n",
      "Iteration 215, loss = 0.49895267\n",
      "Iteration 216, loss = 0.49796647\n",
      "Iteration 217, loss = 0.49739198\n",
      "Iteration 218, loss = 0.49747581\n",
      "Iteration 219, loss = 0.49734189\n",
      "Iteration 220, loss = 0.49736636\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70578672\n",
      "Iteration 2, loss = 0.67330748\n",
      "Iteration 3, loss = 0.64520524\n",
      "Iteration 4, loss = 0.62305542\n",
      "Iteration 5, loss = 0.60232872\n",
      "Iteration 6, loss = 0.58682956\n",
      "Iteration 7, loss = 0.57367863\n",
      "Iteration 8, loss = 0.56369246\n",
      "Iteration 9, loss = 0.55584332\n",
      "Iteration 10, loss = 0.54965713\n",
      "Iteration 11, loss = 0.54509771\n",
      "Iteration 12, loss = 0.54254542\n",
      "Iteration 13, loss = 0.54019676\n",
      "Iteration 14, loss = 0.53781384\n",
      "Iteration 15, loss = 0.53671098\n",
      "Iteration 16, loss = 0.53532039\n",
      "Iteration 17, loss = 0.53469248\n",
      "Iteration 18, loss = 0.53381020\n",
      "Iteration 19, loss = 0.53381524\n",
      "Iteration 20, loss = 0.53320382\n",
      "Iteration 21, loss = 0.53301467\n",
      "Iteration 22, loss = 0.53293229\n",
      "Iteration 23, loss = 0.53289618\n",
      "Iteration 24, loss = 0.53277560\n",
      "Iteration 25, loss = 0.53234970\n",
      "Iteration 26, loss = 0.53221178\n",
      "Iteration 27, loss = 0.53213584\n",
      "Iteration 28, loss = 0.53227182\n",
      "Iteration 29, loss = 0.53221895\n",
      "Iteration 30, loss = 0.53192501\n",
      "Iteration 31, loss = 0.53168679\n",
      "Iteration 32, loss = 0.53161823\n",
      "Iteration 33, loss = 0.53146966\n",
      "Iteration 34, loss = 0.53113696\n",
      "Iteration 35, loss = 0.53092141\n",
      "Iteration 36, loss = 0.53084127\n",
      "Iteration 37, loss = 0.53145966\n",
      "Iteration 38, loss = 0.53149731\n",
      "Iteration 39, loss = 0.53169814\n",
      "Iteration 40, loss = 0.53156202\n",
      "Iteration 41, loss = 0.53148835\n",
      "Iteration 42, loss = 0.53137306\n",
      "Iteration 43, loss = 0.53127931\n",
      "Iteration 44, loss = 0.53107029\n",
      "Iteration 45, loss = 0.53094448\n",
      "Iteration 46, loss = 0.53091583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68948269\n",
      "Iteration 2, loss = 0.66155157\n",
      "Iteration 3, loss = 0.63894438\n",
      "Iteration 4, loss = 0.61979711\n",
      "Iteration 5, loss = 0.60391921\n",
      "Iteration 6, loss = 0.59129152\n",
      "Iteration 7, loss = 0.58192513\n",
      "Iteration 8, loss = 0.57443094\n",
      "Iteration 9, loss = 0.56859161\n",
      "Iteration 10, loss = 0.56448323\n",
      "Iteration 11, loss = 0.56149120\n",
      "Iteration 12, loss = 0.55970831\n",
      "Iteration 13, loss = 0.55876679\n",
      "Iteration 14, loss = 0.55795947\n",
      "Iteration 15, loss = 0.55729341\n",
      "Iteration 16, loss = 0.55711141\n",
      "Iteration 17, loss = 0.55755532\n",
      "Iteration 18, loss = 0.55739803\n",
      "Iteration 19, loss = 0.55747754\n",
      "Iteration 20, loss = 0.55749277\n",
      "Iteration 21, loss = 0.55751971\n",
      "Iteration 22, loss = 0.55718807\n",
      "Iteration 23, loss = 0.55673479\n",
      "Iteration 24, loss = 0.55628714\n",
      "Iteration 25, loss = 0.55599965\n",
      "Iteration 26, loss = 0.55550237\n",
      "Iteration 27, loss = 0.55534285\n",
      "Iteration 28, loss = 0.55518792\n",
      "Iteration 29, loss = 0.55505073\n",
      "Iteration 30, loss = 0.55443088\n",
      "Iteration 31, loss = 0.55411290\n",
      "Iteration 32, loss = 0.55398543\n",
      "Iteration 33, loss = 0.55377392\n",
      "Iteration 34, loss = 0.55340426\n",
      "Iteration 35, loss = 0.55291602\n",
      "Iteration 36, loss = 0.55250674\n",
      "Iteration 37, loss = 0.55234549\n",
      "Iteration 38, loss = 0.55233620\n",
      "Iteration 39, loss = 0.55241958\n",
      "Iteration 40, loss = 0.55219908\n",
      "Iteration 41, loss = 0.55190763\n",
      "Iteration 42, loss = 0.55164088\n",
      "Iteration 43, loss = 0.55099943\n",
      "Iteration 44, loss = 0.55059008\n",
      "Iteration 45, loss = 0.55019736\n",
      "Iteration 46, loss = 0.55002279\n",
      "Iteration 47, loss = 0.54998737\n",
      "Iteration 48, loss = 0.54990812\n",
      "Iteration 49, loss = 0.55030776\n",
      "Iteration 50, loss = 0.55010497\n",
      "Iteration 51, loss = 0.54975910\n",
      "Iteration 52, loss = 0.54905613\n",
      "Iteration 53, loss = 0.54853406\n",
      "Iteration 54, loss = 0.54802449\n",
      "Iteration 55, loss = 0.54780806\n",
      "Iteration 56, loss = 0.54783451\n",
      "Iteration 57, loss = 0.54785906\n",
      "Iteration 58, loss = 0.54822081\n",
      "Iteration 59, loss = 0.54826294\n",
      "Iteration 60, loss = 0.54756395\n",
      "Iteration 61, loss = 0.54713788\n",
      "Iteration 62, loss = 0.54714404\n",
      "Iteration 63, loss = 0.54683253\n",
      "Iteration 64, loss = 0.54697001\n",
      "Iteration 65, loss = 0.54728616\n",
      "Iteration 66, loss = 0.54741510\n",
      "Iteration 67, loss = 0.54760754\n",
      "Iteration 68, loss = 0.54757382\n",
      "Iteration 69, loss = 0.54711186\n",
      "Iteration 70, loss = 0.54654255\n",
      "Iteration 71, loss = 0.54652018\n",
      "Iteration 72, loss = 0.54612348\n",
      "Iteration 73, loss = 0.54594143\n",
      "Iteration 74, loss = 0.54595046\n",
      "Iteration 75, loss = 0.54586921\n",
      "Iteration 76, loss = 0.54593245\n",
      "Iteration 77, loss = 0.54571430\n",
      "Iteration 78, loss = 0.54555751\n",
      "Iteration 79, loss = 0.54541188\n",
      "Iteration 80, loss = 0.54527130\n",
      "Iteration 81, loss = 0.54498246\n",
      "Iteration 82, loss = 0.54460658\n",
      "Iteration 83, loss = 0.54448370\n",
      "Iteration 84, loss = 0.54396737\n",
      "Iteration 85, loss = 0.54370036\n",
      "Iteration 86, loss = 0.54332463\n",
      "Iteration 87, loss = 0.54301621\n",
      "Iteration 88, loss = 0.54303392\n",
      "Iteration 89, loss = 0.54271175\n",
      "Iteration 90, loss = 0.54247232\n",
      "Iteration 91, loss = 0.54306718\n",
      "Iteration 92, loss = 0.54240245\n",
      "Iteration 93, loss = 0.54216967\n",
      "Iteration 94, loss = 0.54201141\n",
      "Iteration 95, loss = 0.54245556\n",
      "Iteration 96, loss = 0.54226180\n",
      "Iteration 97, loss = 0.54209769\n",
      "Iteration 98, loss = 0.54175035\n",
      "Iteration 99, loss = 0.54203923\n",
      "Iteration 100, loss = 0.54156762\n",
      "Iteration 101, loss = 0.54184436\n",
      "Iteration 102, loss = 0.54186475\n",
      "Iteration 103, loss = 0.54183823\n",
      "Iteration 104, loss = 0.54166595\n",
      "Iteration 105, loss = 0.54146577\n",
      "Iteration 106, loss = 0.54136976\n",
      "Iteration 107, loss = 0.54108364\n",
      "Iteration 108, loss = 0.54106450\n",
      "Iteration 109, loss = 0.54061902\n",
      "Iteration 110, loss = 0.54056014\n",
      "Iteration 111, loss = 0.54055763\n",
      "Iteration 112, loss = 0.54057060\n",
      "Iteration 113, loss = 0.54028509\n",
      "Iteration 114, loss = 0.54011087\n",
      "Iteration 115, loss = 0.53997147\n",
      "Iteration 116, loss = 0.53965086\n",
      "Iteration 117, loss = 0.53954146\n",
      "Iteration 118, loss = 0.53954166\n",
      "Iteration 119, loss = 0.53919658\n",
      "Iteration 120, loss = 0.53941691\n",
      "Iteration 121, loss = 0.53932675\n",
      "Iteration 122, loss = 0.53921573\n",
      "Iteration 123, loss = 0.53876885\n",
      "Iteration 124, loss = 0.53863822\n",
      "Iteration 125, loss = 0.53875908\n",
      "Iteration 126, loss = 0.53823820\n",
      "Iteration 127, loss = 0.53807100\n",
      "Iteration 128, loss = 0.53921104\n",
      "Iteration 129, loss = 0.53910561\n",
      "Iteration 130, loss = 0.53906410\n",
      "Iteration 131, loss = 0.53876968\n",
      "Iteration 132, loss = 0.53824780\n",
      "Iteration 133, loss = 0.53776689\n",
      "Iteration 134, loss = 0.53769229\n",
      "Iteration 135, loss = 0.53791482\n",
      "Iteration 136, loss = 0.53814260\n",
      "Iteration 137, loss = 0.53844057\n",
      "Iteration 138, loss = 0.53846198\n",
      "Iteration 139, loss = 0.53831286\n",
      "Iteration 140, loss = 0.53827456\n",
      "Iteration 141, loss = 0.53806681\n",
      "Iteration 142, loss = 0.53763355\n",
      "Iteration 143, loss = 0.53727576\n",
      "Iteration 144, loss = 0.53714931\n",
      "Iteration 145, loss = 0.53718106\n",
      "Iteration 146, loss = 0.53683552\n",
      "Iteration 147, loss = 0.53712270\n",
      "Iteration 148, loss = 0.53638646\n",
      "Iteration 149, loss = 0.53656918\n",
      "Iteration 150, loss = 0.53703430\n",
      "Iteration 151, loss = 0.53734943\n",
      "Iteration 152, loss = 0.53717940\n",
      "Iteration 153, loss = 0.53677111\n",
      "Iteration 154, loss = 0.53658215\n",
      "Iteration 155, loss = 0.53585055\n",
      "Iteration 156, loss = 0.53589107\n",
      "Iteration 157, loss = 0.53608527\n",
      "Iteration 158, loss = 0.53594340\n",
      "Iteration 159, loss = 0.53602975\n",
      "Iteration 160, loss = 0.53634360\n",
      "Iteration 161, loss = 0.53629461\n",
      "Iteration 162, loss = 0.53633715\n",
      "Iteration 163, loss = 0.53661739\n",
      "Iteration 164, loss = 0.53680447\n",
      "Iteration 165, loss = 0.53682926\n",
      "Iteration 166, loss = 0.53627497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69176296\n",
      "Iteration 2, loss = 0.64840712\n",
      "Iteration 3, loss = 0.61307769\n",
      "Iteration 4, loss = 0.58501579\n",
      "Iteration 5, loss = 0.56033360\n",
      "Iteration 6, loss = 0.54354890\n",
      "Iteration 7, loss = 0.53081053\n",
      "Iteration 8, loss = 0.52244553\n",
      "Iteration 9, loss = 0.51699597\n",
      "Iteration 10, loss = 0.51444126\n",
      "Iteration 11, loss = 0.51494417\n",
      "Iteration 12, loss = 0.51453550\n",
      "Iteration 13, loss = 0.51488772\n",
      "Iteration 14, loss = 0.51555125\n",
      "Iteration 15, loss = 0.51549546\n",
      "Iteration 16, loss = 0.51507030\n",
      "Iteration 17, loss = 0.51405257\n",
      "Iteration 18, loss = 0.51272761\n",
      "Iteration 19, loss = 0.51208105\n",
      "Iteration 20, loss = 0.51119772\n",
      "Iteration 21, loss = 0.51092776\n",
      "Iteration 22, loss = 0.51098381\n",
      "Iteration 23, loss = 0.51010807\n",
      "Iteration 24, loss = 0.50954432\n",
      "Iteration 25, loss = 0.50865924\n",
      "Iteration 26, loss = 0.50807920\n",
      "Iteration 27, loss = 0.50771433\n",
      "Iteration 28, loss = 0.50749010\n",
      "Iteration 29, loss = 0.50725602\n",
      "Iteration 30, loss = 0.50708464\n",
      "Iteration 31, loss = 0.50684896\n",
      "Iteration 32, loss = 0.50672363\n",
      "Iteration 33, loss = 0.50669150\n",
      "Iteration 34, loss = 0.50616099\n",
      "Iteration 35, loss = 0.50607227\n",
      "Iteration 36, loss = 0.50625875\n",
      "Iteration 37, loss = 0.50679539\n",
      "Iteration 38, loss = 0.50669003\n",
      "Iteration 39, loss = 0.50624249\n",
      "Iteration 40, loss = 0.50580957\n",
      "Iteration 41, loss = 0.50496140\n",
      "Iteration 42, loss = 0.50477308\n",
      "Iteration 43, loss = 0.50447091\n",
      "Iteration 44, loss = 0.50470114\n",
      "Iteration 45, loss = 0.50452339\n",
      "Iteration 46, loss = 0.50447247\n",
      "Iteration 47, loss = 0.50407019\n",
      "Iteration 48, loss = 0.50379404\n",
      "Iteration 49, loss = 0.50354600\n",
      "Iteration 50, loss = 0.50347326\n",
      "Iteration 51, loss = 0.50344225\n",
      "Iteration 52, loss = 0.50369671\n",
      "Iteration 53, loss = 0.50348199\n",
      "Iteration 54, loss = 0.50272795\n",
      "Iteration 55, loss = 0.50226865\n",
      "Iteration 56, loss = 0.50234959\n",
      "Iteration 57, loss = 0.50193583\n",
      "Iteration 58, loss = 0.50167034\n",
      "Iteration 59, loss = 0.50139047\n",
      "Iteration 60, loss = 0.50121090\n",
      "Iteration 61, loss = 0.50091390\n",
      "Iteration 62, loss = 0.50106394\n",
      "Iteration 63, loss = 0.50094014\n",
      "Iteration 64, loss = 0.50103577\n",
      "Iteration 65, loss = 0.50100306\n",
      "Iteration 66, loss = 0.50100068\n",
      "Iteration 67, loss = 0.50107516\n",
      "Iteration 68, loss = 0.50179623\n",
      "Iteration 69, loss = 0.50144307\n",
      "Iteration 70, loss = 0.50107568\n",
      "Iteration 71, loss = 0.50020951\n",
      "Iteration 72, loss = 0.49998773\n",
      "Iteration 73, loss = 0.49952635\n",
      "Iteration 74, loss = 0.49886012\n",
      "Iteration 75, loss = 0.49889510\n",
      "Iteration 76, loss = 0.49893110\n",
      "Iteration 77, loss = 0.49896325\n",
      "Iteration 78, loss = 0.49864313\n",
      "Iteration 79, loss = 0.49839997\n",
      "Iteration 80, loss = 0.49827245\n",
      "Iteration 81, loss = 0.49849688\n",
      "Iteration 82, loss = 0.49841470\n",
      "Iteration 83, loss = 0.49816170\n",
      "Iteration 84, loss = 0.49852626\n",
      "Iteration 85, loss = 0.49906059\n",
      "Iteration 86, loss = 0.49898968\n",
      "Iteration 87, loss = 0.49854713\n",
      "Iteration 88, loss = 0.49799621\n",
      "Iteration 89, loss = 0.49710840\n",
      "Iteration 90, loss = 0.49661844\n",
      "Iteration 91, loss = 0.49705154\n",
      "Iteration 92, loss = 0.49657817\n",
      "Iteration 93, loss = 0.49666709\n",
      "Iteration 94, loss = 0.49648809\n",
      "Iteration 95, loss = 0.49648917\n",
      "Iteration 96, loss = 0.49591595\n",
      "Iteration 97, loss = 0.49551924\n",
      "Iteration 98, loss = 0.49540982\n",
      "Iteration 99, loss = 0.49528816\n",
      "Iteration 100, loss = 0.49500181\n",
      "Iteration 101, loss = 0.49487382\n",
      "Iteration 102, loss = 0.49498520\n",
      "Iteration 103, loss = 0.49435536\n",
      "Iteration 104, loss = 0.49403873\n",
      "Iteration 105, loss = 0.49392941\n",
      "Iteration 106, loss = 0.49410548\n",
      "Iteration 107, loss = 0.49406692\n",
      "Iteration 108, loss = 0.49371905\n",
      "Iteration 109, loss = 0.49386458\n",
      "Iteration 110, loss = 0.49360978\n",
      "Iteration 111, loss = 0.49316483\n",
      "Iteration 112, loss = 0.49287852\n",
      "Iteration 113, loss = 0.49296919\n",
      "Iteration 114, loss = 0.49279783\n",
      "Iteration 115, loss = 0.49276007\n",
      "Iteration 116, loss = 0.49251311\n",
      "Iteration 117, loss = 0.49219668\n",
      "Iteration 118, loss = 0.49181467\n",
      "Iteration 119, loss = 0.49192851\n",
      "Iteration 120, loss = 0.49299731\n",
      "Iteration 121, loss = 0.49436970\n",
      "Iteration 122, loss = 0.49366667\n",
      "Iteration 123, loss = 0.49272328\n",
      "Iteration 124, loss = 0.49123176\n",
      "Iteration 125, loss = 0.49045343\n",
      "Iteration 126, loss = 0.49033133\n",
      "Iteration 127, loss = 0.49073375\n",
      "Iteration 128, loss = 0.49109151\n",
      "Iteration 129, loss = 0.49202636\n",
      "Iteration 130, loss = 0.49203395\n",
      "Iteration 131, loss = 0.49209878\n",
      "Iteration 132, loss = 0.49052736\n",
      "Iteration 133, loss = 0.48982374\n",
      "Iteration 134, loss = 0.48938617\n",
      "Iteration 135, loss = 0.48925942\n",
      "Iteration 136, loss = 0.49050166\n",
      "Iteration 137, loss = 0.49116279\n",
      "Iteration 138, loss = 0.49081424\n",
      "Iteration 139, loss = 0.49000481\n",
      "Iteration 140, loss = 0.48905118\n",
      "Iteration 141, loss = 0.48805132\n",
      "Iteration 142, loss = 0.48817059\n",
      "Iteration 143, loss = 0.48780778\n",
      "Iteration 144, loss = 0.48762368\n",
      "Iteration 145, loss = 0.48726523\n",
      "Iteration 146, loss = 0.48717243\n",
      "Iteration 147, loss = 0.48785992\n",
      "Iteration 148, loss = 0.48801641\n",
      "Iteration 149, loss = 0.48808077\n",
      "Iteration 150, loss = 0.48703970\n",
      "Iteration 151, loss = 0.48709569\n",
      "Iteration 152, loss = 0.48604587\n",
      "Iteration 153, loss = 0.48586210\n",
      "Iteration 154, loss = 0.48602791\n",
      "Iteration 155, loss = 0.48662392\n",
      "Iteration 156, loss = 0.48577727\n",
      "Iteration 157, loss = 0.48512948\n",
      "Iteration 158, loss = 0.48479704\n",
      "Iteration 159, loss = 0.48537281\n",
      "Iteration 160, loss = 0.48539768\n",
      "Iteration 161, loss = 0.48537182\n",
      "Iteration 162, loss = 0.48459677\n",
      "Iteration 163, loss = 0.48449490\n",
      "Iteration 164, loss = 0.48447602\n",
      "Iteration 165, loss = 0.48433113\n",
      "Iteration 166, loss = 0.48423662\n",
      "Iteration 167, loss = 0.48404457\n",
      "Iteration 168, loss = 0.48390087\n",
      "Iteration 169, loss = 0.48404654\n",
      "Iteration 170, loss = 0.48364888\n",
      "Iteration 171, loss = 0.48359324\n",
      "Iteration 172, loss = 0.48293777\n",
      "Iteration 173, loss = 0.48270148\n",
      "Iteration 174, loss = 0.48231196\n",
      "Iteration 175, loss = 0.48193658\n",
      "Iteration 176, loss = 0.48184119\n",
      "Iteration 177, loss = 0.48219360\n",
      "Iteration 178, loss = 0.48218463\n",
      "Iteration 179, loss = 0.48186485\n",
      "Iteration 180, loss = 0.48190739\n",
      "Iteration 181, loss = 0.48200891\n",
      "Iteration 182, loss = 0.48140288\n",
      "Iteration 183, loss = 0.48110723\n",
      "Iteration 184, loss = 0.48085107\n",
      "Iteration 185, loss = 0.48075465\n",
      "Iteration 186, loss = 0.48088647\n",
      "Iteration 187, loss = 0.48107932\n",
      "Iteration 188, loss = 0.48139933\n",
      "Iteration 189, loss = 0.48164330\n",
      "Iteration 190, loss = 0.48103542\n",
      "Iteration 191, loss = 0.48072770\n",
      "Iteration 192, loss = 0.47943953\n",
      "Iteration 193, loss = 0.47920620\n",
      "Iteration 194, loss = 0.47906369\n",
      "Iteration 195, loss = 0.47921723\n",
      "Iteration 196, loss = 0.47893992\n",
      "Iteration 197, loss = 0.47855911\n",
      "Iteration 198, loss = 0.47798326\n",
      "Iteration 199, loss = 0.47801074\n",
      "Iteration 200, loss = 0.47761833\n",
      "Iteration 201, loss = 0.47752163\n",
      "Iteration 202, loss = 0.47707370\n",
      "Iteration 203, loss = 0.47752672\n",
      "Iteration 204, loss = 0.47754319\n",
      "Iteration 205, loss = 0.47818870\n",
      "Iteration 206, loss = 0.47836698\n",
      "Iteration 207, loss = 0.47781745\n",
      "Iteration 208, loss = 0.47746920\n",
      "Iteration 209, loss = 0.47782045\n",
      "Iteration 210, loss = 0.47791057\n",
      "Iteration 211, loss = 0.47807390\n",
      "Iteration 212, loss = 0.47811781\n",
      "Iteration 213, loss = 0.47695504\n",
      "Iteration 214, loss = 0.47646185\n",
      "Iteration 215, loss = 0.47565297\n",
      "Iteration 216, loss = 0.47514948\n",
      "Iteration 217, loss = 0.47614857\n",
      "Iteration 218, loss = 0.47525589\n",
      "Iteration 219, loss = 0.47570064\n",
      "Iteration 220, loss = 0.47567764\n",
      "Iteration 221, loss = 0.47581829\n",
      "Iteration 222, loss = 0.47527913\n",
      "Iteration 223, loss = 0.47495156\n",
      "Iteration 224, loss = 0.47470137\n",
      "Iteration 225, loss = 0.47419065\n",
      "Iteration 226, loss = 0.47430641\n",
      "Iteration 227, loss = 0.47415622\n",
      "Iteration 228, loss = 0.47340870\n",
      "Iteration 229, loss = 0.47463038\n",
      "Iteration 230, loss = 0.47510190\n",
      "Iteration 231, loss = 0.47444272\n",
      "Iteration 232, loss = 0.47412785\n",
      "Iteration 233, loss = 0.47379748\n",
      "Iteration 234, loss = 0.47425661\n",
      "Iteration 235, loss = 0.47437050\n",
      "Iteration 236, loss = 0.47371370\n",
      "Iteration 237, loss = 0.47217529\n",
      "Iteration 238, loss = 0.47242719\n",
      "Iteration 239, loss = 0.47339086\n",
      "Iteration 240, loss = 0.47365815\n",
      "Iteration 241, loss = 0.47357646\n",
      "Iteration 242, loss = 0.47363576\n",
      "Iteration 243, loss = 0.47327094\n",
      "Iteration 244, loss = 0.47389096\n",
      "Iteration 245, loss = 0.47418798\n",
      "Iteration 246, loss = 0.47407168\n",
      "Iteration 247, loss = 0.47264908\n",
      "Iteration 248, loss = 0.47168435\n",
      "Iteration 249, loss = 0.47092636\n",
      "Iteration 250, loss = 0.47061252\n",
      "Iteration 251, loss = 0.47060783\n",
      "Iteration 252, loss = 0.47075639\n",
      "Iteration 253, loss = 0.47082623\n",
      "Iteration 254, loss = 0.47116115\n",
      "Iteration 255, loss = 0.47122162\n",
      "Iteration 256, loss = 0.47181477\n",
      "Iteration 257, loss = 0.47059862\n",
      "Iteration 258, loss = 0.47023238\n",
      "Iteration 259, loss = 0.47046223\n",
      "Iteration 260, loss = 0.47026658\n",
      "Iteration 261, loss = 0.47106708\n",
      "Iteration 262, loss = 0.47220360\n",
      "Iteration 263, loss = 0.47142267\n",
      "Iteration 264, loss = 0.47015316\n",
      "Iteration 265, loss = 0.46888797\n",
      "Iteration 266, loss = 0.46842239\n",
      "Iteration 267, loss = 0.46899811\n",
      "Iteration 268, loss = 0.46948162\n",
      "Iteration 269, loss = 0.47024416\n",
      "Iteration 270, loss = 0.47098976\n",
      "Iteration 271, loss = 0.47148001\n",
      "Iteration 272, loss = 0.47128202\n",
      "Iteration 273, loss = 0.47254567\n",
      "Iteration 274, loss = 0.47052838\n",
      "Iteration 275, loss = 0.46850450\n",
      "Iteration 276, loss = 0.46798902\n",
      "Iteration 277, loss = 0.46835452\n",
      "Iteration 278, loss = 0.47050085\n",
      "Iteration 279, loss = 0.47131278\n",
      "Iteration 280, loss = 0.47006898\n",
      "Iteration 281, loss = 0.46894481\n",
      "Iteration 282, loss = 0.46747409\n",
      "Iteration 283, loss = 0.46727202\n",
      "Iteration 284, loss = 0.46690105\n",
      "Iteration 285, loss = 0.46662502\n",
      "Iteration 286, loss = 0.46782472\n",
      "Iteration 287, loss = 0.46784259\n",
      "Iteration 288, loss = 0.46802339\n",
      "Iteration 289, loss = 0.46794324\n",
      "Iteration 290, loss = 0.46790240\n",
      "Iteration 291, loss = 0.46669139\n",
      "Iteration 292, loss = 0.46656911\n",
      "Iteration 293, loss = 0.46697279\n",
      "Iteration 294, loss = 0.46729742\n",
      "Iteration 295, loss = 0.46762928\n",
      "Iteration 296, loss = 0.46672796\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68805224\n",
      "Iteration 2, loss = 0.65193456\n",
      "Iteration 3, loss = 0.62467659\n",
      "Iteration 4, loss = 0.60427767\n",
      "Iteration 5, loss = 0.58735026\n",
      "Iteration 6, loss = 0.57776067\n",
      "Iteration 7, loss = 0.57137948\n",
      "Iteration 8, loss = 0.56725179\n",
      "Iteration 9, loss = 0.56510019\n",
      "Iteration 10, loss = 0.56470564\n",
      "Iteration 11, loss = 0.56442276\n",
      "Iteration 12, loss = 0.56572611\n",
      "Iteration 13, loss = 0.56608085\n",
      "Iteration 14, loss = 0.56644061\n",
      "Iteration 15, loss = 0.56655087\n",
      "Iteration 16, loss = 0.56607248\n",
      "Iteration 17, loss = 0.56513236\n",
      "Iteration 18, loss = 0.56414066\n",
      "Iteration 19, loss = 0.56284170\n",
      "Iteration 20, loss = 0.56213810\n",
      "Iteration 21, loss = 0.56205795\n",
      "Iteration 22, loss = 0.56162184\n",
      "Iteration 23, loss = 0.56143474\n",
      "Iteration 24, loss = 0.56112390\n",
      "Iteration 25, loss = 0.56071107\n",
      "Iteration 26, loss = 0.56029708\n",
      "Iteration 27, loss = 0.56003869\n",
      "Iteration 28, loss = 0.55956767\n",
      "Iteration 29, loss = 0.55978522\n",
      "Iteration 30, loss = 0.55922232\n",
      "Iteration 31, loss = 0.55916579\n",
      "Iteration 32, loss = 0.55898225\n",
      "Iteration 33, loss = 0.55867319\n",
      "Iteration 34, loss = 0.55825671\n",
      "Iteration 35, loss = 0.55815402\n",
      "Iteration 36, loss = 0.55846538\n",
      "Iteration 37, loss = 0.55915425\n",
      "Iteration 38, loss = 0.55955277\n",
      "Iteration 39, loss = 0.55975872\n",
      "Iteration 40, loss = 0.55953965\n",
      "Iteration 41, loss = 0.55991268\n",
      "Iteration 42, loss = 0.55941911\n",
      "Iteration 43, loss = 0.55850249\n",
      "Iteration 44, loss = 0.55733411\n",
      "Iteration 45, loss = 0.55675618\n",
      "Iteration 46, loss = 0.55635046\n",
      "Iteration 47, loss = 0.55633260\n",
      "Iteration 48, loss = 0.55639650\n",
      "Iteration 49, loss = 0.55705751\n",
      "Iteration 50, loss = 0.55700868\n",
      "Iteration 51, loss = 0.55708631\n",
      "Iteration 52, loss = 0.55631422\n",
      "Iteration 53, loss = 0.55546421\n",
      "Iteration 54, loss = 0.55467418\n",
      "Iteration 55, loss = 0.55451206\n",
      "Iteration 56, loss = 0.55486623\n",
      "Iteration 57, loss = 0.55532234\n",
      "Iteration 58, loss = 0.55605346\n",
      "Iteration 59, loss = 0.55616581\n",
      "Iteration 60, loss = 0.55559530\n",
      "Iteration 61, loss = 0.55421503\n",
      "Iteration 62, loss = 0.55414279\n",
      "Iteration 63, loss = 0.55404823\n",
      "Iteration 64, loss = 0.55393162\n",
      "Iteration 65, loss = 0.55337189\n",
      "Iteration 66, loss = 0.55368684\n",
      "Iteration 67, loss = 0.55379179\n",
      "Iteration 68, loss = 0.55423914\n",
      "Iteration 69, loss = 0.55442572\n",
      "Iteration 70, loss = 0.55444807\n",
      "Iteration 71, loss = 0.55499239\n",
      "Iteration 72, loss = 0.55533617\n",
      "Iteration 73, loss = 0.55479024\n",
      "Iteration 74, loss = 0.55322773\n",
      "Iteration 75, loss = 0.55231048\n",
      "Iteration 76, loss = 0.55198061\n",
      "Iteration 77, loss = 0.55163227\n",
      "Iteration 78, loss = 0.55118137\n",
      "Iteration 79, loss = 0.55129267\n",
      "Iteration 80, loss = 0.55112908\n",
      "Iteration 81, loss = 0.55149433\n",
      "Iteration 82, loss = 0.55101731\n",
      "Iteration 83, loss = 0.55078994\n",
      "Iteration 84, loss = 0.55053937\n",
      "Iteration 85, loss = 0.55037545\n",
      "Iteration 86, loss = 0.55005514\n",
      "Iteration 87, loss = 0.55004782\n",
      "Iteration 88, loss = 0.55061213\n",
      "Iteration 89, loss = 0.55047930\n",
      "Iteration 90, loss = 0.54955164\n",
      "Iteration 91, loss = 0.54944847\n",
      "Iteration 92, loss = 0.54893356\n",
      "Iteration 93, loss = 0.54948213\n",
      "Iteration 94, loss = 0.54958741\n",
      "Iteration 95, loss = 0.54968473\n",
      "Iteration 96, loss = 0.54878727\n",
      "Iteration 97, loss = 0.54812412\n",
      "Iteration 98, loss = 0.54811675\n",
      "Iteration 99, loss = 0.54878920\n",
      "Iteration 100, loss = 0.54862743\n",
      "Iteration 101, loss = 0.54828865\n",
      "Iteration 102, loss = 0.54802660\n",
      "Iteration 103, loss = 0.54774540\n",
      "Iteration 104, loss = 0.54707692\n",
      "Iteration 105, loss = 0.54669850\n",
      "Iteration 106, loss = 0.54694063\n",
      "Iteration 107, loss = 0.54742880\n",
      "Iteration 108, loss = 0.54741120\n",
      "Iteration 109, loss = 0.54731872\n",
      "Iteration 110, loss = 0.54702489\n",
      "Iteration 111, loss = 0.54666473\n",
      "Iteration 112, loss = 0.54576455\n",
      "Iteration 113, loss = 0.54580850\n",
      "Iteration 114, loss = 0.54585823\n",
      "Iteration 115, loss = 0.54498589\n",
      "Iteration 116, loss = 0.54516605\n",
      "Iteration 117, loss = 0.54484873\n",
      "Iteration 118, loss = 0.54476892\n",
      "Iteration 119, loss = 0.54466918\n",
      "Iteration 120, loss = 0.54463150\n",
      "Iteration 121, loss = 0.54442983\n",
      "Iteration 122, loss = 0.54451418\n",
      "Iteration 123, loss = 0.54490434\n",
      "Iteration 124, loss = 0.54531270\n",
      "Iteration 125, loss = 0.54554931\n",
      "Iteration 126, loss = 0.54575472\n",
      "Iteration 127, loss = 0.54524004\n",
      "Iteration 128, loss = 0.54413580\n",
      "Iteration 129, loss = 0.54352423\n",
      "Iteration 130, loss = 0.54373779\n",
      "Iteration 131, loss = 0.54328761\n",
      "Iteration 132, loss = 0.54303824\n",
      "Iteration 133, loss = 0.54280239\n",
      "Iteration 134, loss = 0.54223512\n",
      "Iteration 135, loss = 0.54234059\n",
      "Iteration 136, loss = 0.54164845\n",
      "Iteration 137, loss = 0.54141181\n",
      "Iteration 138, loss = 0.54174687\n",
      "Iteration 139, loss = 0.54163730\n",
      "Iteration 140, loss = 0.54139842\n",
      "Iteration 141, loss = 0.54124431\n",
      "Iteration 142, loss = 0.54103961\n",
      "Iteration 143, loss = 0.54064113\n",
      "Iteration 144, loss = 0.54088568\n",
      "Iteration 145, loss = 0.54103644\n",
      "Iteration 146, loss = 0.54180433\n",
      "Iteration 147, loss = 0.54209069\n",
      "Iteration 148, loss = 0.54219474\n",
      "Iteration 149, loss = 0.54108764\n",
      "Iteration 150, loss = 0.54070546\n",
      "Iteration 151, loss = 0.54029274\n",
      "Iteration 152, loss = 0.54001037\n",
      "Iteration 153, loss = 0.53991429\n",
      "Iteration 154, loss = 0.53933455\n",
      "Iteration 155, loss = 0.53890890\n",
      "Iteration 156, loss = 0.53874837\n",
      "Iteration 157, loss = 0.53940249\n",
      "Iteration 158, loss = 0.53919741\n",
      "Iteration 159, loss = 0.53881261\n",
      "Iteration 160, loss = 0.53815121\n",
      "Iteration 161, loss = 0.53835598\n",
      "Iteration 162, loss = 0.53780350\n",
      "Iteration 163, loss = 0.53819896\n",
      "Iteration 164, loss = 0.53843339\n",
      "Iteration 165, loss = 0.53805922\n",
      "Iteration 166, loss = 0.53863122\n",
      "Iteration 167, loss = 0.53827447\n",
      "Iteration 168, loss = 0.53784861\n",
      "Iteration 169, loss = 0.53773819\n",
      "Iteration 170, loss = 0.53728804\n",
      "Iteration 171, loss = 0.53699170\n",
      "Iteration 172, loss = 0.53762842\n",
      "Iteration 173, loss = 0.53780855\n",
      "Iteration 174, loss = 0.53785692\n",
      "Iteration 175, loss = 0.53746045\n",
      "Iteration 176, loss = 0.53772105\n",
      "Iteration 177, loss = 0.53718259\n",
      "Iteration 178, loss = 0.53624618\n",
      "Iteration 179, loss = 0.53643141\n",
      "Iteration 180, loss = 0.53551556\n",
      "Iteration 181, loss = 0.53543876\n",
      "Iteration 182, loss = 0.53541158\n",
      "Iteration 183, loss = 0.53658002\n",
      "Iteration 184, loss = 0.53736063\n",
      "Iteration 185, loss = 0.53791953\n",
      "Iteration 186, loss = 0.53622837\n",
      "Iteration 187, loss = 0.53435044\n",
      "Iteration 188, loss = 0.53424174\n",
      "Iteration 189, loss = 0.53641021\n",
      "Iteration 190, loss = 0.53628051\n",
      "Iteration 191, loss = 0.53647141\n",
      "Iteration 192, loss = 0.53654027\n",
      "Iteration 193, loss = 0.53597695\n",
      "Iteration 194, loss = 0.53635646\n",
      "Iteration 195, loss = 0.53673626\n",
      "Iteration 196, loss = 0.53776618\n",
      "Iteration 197, loss = 0.53667443\n",
      "Iteration 198, loss = 0.53578338\n",
      "Iteration 199, loss = 0.53424436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69067680\n",
      "Iteration 2, loss = 0.64843062\n",
      "Iteration 3, loss = 0.61421729\n",
      "Iteration 4, loss = 0.58602924\n",
      "Iteration 5, loss = 0.56512597\n",
      "Iteration 6, loss = 0.55013445\n",
      "Iteration 7, loss = 0.53900910\n",
      "Iteration 8, loss = 0.53226308\n",
      "Iteration 9, loss = 0.52832299\n",
      "Iteration 10, loss = 0.52751985\n",
      "Iteration 11, loss = 0.52682761\n",
      "Iteration 12, loss = 0.52821433\n",
      "Iteration 13, loss = 0.52823895\n",
      "Iteration 14, loss = 0.52948058\n",
      "Iteration 15, loss = 0.52945832\n",
      "Iteration 16, loss = 0.52950700\n",
      "Iteration 17, loss = 0.52891713\n",
      "Iteration 18, loss = 0.52765879\n",
      "Iteration 19, loss = 0.52643320\n",
      "Iteration 20, loss = 0.52536960\n",
      "Iteration 21, loss = 0.52411081\n",
      "Iteration 22, loss = 0.52344384\n",
      "Iteration 23, loss = 0.52329505\n",
      "Iteration 24, loss = 0.52305652\n",
      "Iteration 25, loss = 0.52260025\n",
      "Iteration 26, loss = 0.52217882\n",
      "Iteration 27, loss = 0.52196053\n",
      "Iteration 28, loss = 0.52170653\n",
      "Iteration 29, loss = 0.52163019\n",
      "Iteration 30, loss = 0.52134201\n",
      "Iteration 31, loss = 0.52114669\n",
      "Iteration 32, loss = 0.52069644\n",
      "Iteration 33, loss = 0.52041547\n",
      "Iteration 34, loss = 0.52036158\n",
      "Iteration 35, loss = 0.52057800\n",
      "Iteration 36, loss = 0.52035954\n",
      "Iteration 37, loss = 0.51986133\n",
      "Iteration 38, loss = 0.51981393\n",
      "Iteration 39, loss = 0.51909451\n",
      "Iteration 40, loss = 0.51861038\n",
      "Iteration 41, loss = 0.51790149\n",
      "Iteration 42, loss = 0.51904045\n",
      "Iteration 43, loss = 0.51847173\n",
      "Iteration 44, loss = 0.51788476\n",
      "Iteration 45, loss = 0.51734237\n",
      "Iteration 46, loss = 0.51726238\n",
      "Iteration 47, loss = 0.51697409\n",
      "Iteration 48, loss = 0.51673527\n",
      "Iteration 49, loss = 0.51630707\n",
      "Iteration 50, loss = 0.51629030\n",
      "Iteration 51, loss = 0.51594032\n",
      "Iteration 52, loss = 0.51585920\n",
      "Iteration 53, loss = 0.51565664\n",
      "Iteration 54, loss = 0.51542821\n",
      "Iteration 55, loss = 0.51504251\n",
      "Iteration 56, loss = 0.51468080\n",
      "Iteration 57, loss = 0.51499755\n",
      "Iteration 58, loss = 0.51461719\n",
      "Iteration 59, loss = 0.51435867\n",
      "Iteration 60, loss = 0.51420487\n",
      "Iteration 61, loss = 0.51409264\n",
      "Iteration 62, loss = 0.51387175\n",
      "Iteration 63, loss = 0.51336509\n",
      "Iteration 64, loss = 0.51367891\n",
      "Iteration 65, loss = 0.51380986\n",
      "Iteration 66, loss = 0.51466671\n",
      "Iteration 67, loss = 0.51526928\n",
      "Iteration 68, loss = 0.51490000\n",
      "Iteration 69, loss = 0.51429786\n",
      "Iteration 70, loss = 0.51399022\n",
      "Iteration 71, loss = 0.51320810\n",
      "Iteration 72, loss = 0.51253301\n",
      "Iteration 73, loss = 0.51194164\n",
      "Iteration 74, loss = 0.51208257\n",
      "Iteration 75, loss = 0.51182967\n",
      "Iteration 76, loss = 0.51222706\n",
      "Iteration 77, loss = 0.51213081\n",
      "Iteration 78, loss = 0.51218535\n",
      "Iteration 79, loss = 0.51056676\n",
      "Iteration 80, loss = 0.51009856\n",
      "Iteration 81, loss = 0.50979115\n",
      "Iteration 82, loss = 0.50944777\n",
      "Iteration 83, loss = 0.50904053\n",
      "Iteration 84, loss = 0.50931956\n",
      "Iteration 85, loss = 0.50868167\n",
      "Iteration 86, loss = 0.50850764\n",
      "Iteration 87, loss = 0.50821970\n",
      "Iteration 88, loss = 0.50785904\n",
      "Iteration 89, loss = 0.50772218\n",
      "Iteration 90, loss = 0.50757178\n",
      "Iteration 91, loss = 0.50730809\n",
      "Iteration 92, loss = 0.50742919\n",
      "Iteration 93, loss = 0.50732799\n",
      "Iteration 94, loss = 0.50789556\n",
      "Iteration 95, loss = 0.50795430\n",
      "Iteration 96, loss = 0.50805158\n",
      "Iteration 97, loss = 0.50773432\n",
      "Iteration 98, loss = 0.50722586\n",
      "Iteration 99, loss = 0.50722612\n",
      "Iteration 100, loss = 0.50649838\n",
      "Iteration 101, loss = 0.50736445\n",
      "Iteration 102, loss = 0.50798459\n",
      "Iteration 103, loss = 0.50900230\n",
      "Iteration 104, loss = 0.50915899\n",
      "Iteration 105, loss = 0.50852934\n",
      "Iteration 106, loss = 0.50743602\n",
      "Iteration 107, loss = 0.50652478\n",
      "Iteration 108, loss = 0.50525008\n",
      "Iteration 109, loss = 0.50504494\n",
      "Iteration 110, loss = 0.50511102\n",
      "Iteration 111, loss = 0.50583104\n",
      "Iteration 112, loss = 0.50707154\n",
      "Iteration 113, loss = 0.50768524\n",
      "Iteration 114, loss = 0.50741318\n",
      "Iteration 115, loss = 0.50616756\n",
      "Iteration 116, loss = 0.50508571\n",
      "Iteration 117, loss = 0.50374705\n",
      "Iteration 118, loss = 0.50296761\n",
      "Iteration 119, loss = 0.50306930\n",
      "Iteration 120, loss = 0.50299795\n",
      "Iteration 121, loss = 0.50267932\n",
      "Iteration 122, loss = 0.50204937\n",
      "Iteration 123, loss = 0.50140323\n",
      "Iteration 124, loss = 0.50077990\n",
      "Iteration 125, loss = 0.50054187\n",
      "Iteration 126, loss = 0.50028696\n",
      "Iteration 127, loss = 0.50138706\n",
      "Iteration 128, loss = 0.50306659\n",
      "Iteration 129, loss = 0.50513367\n",
      "Iteration 130, loss = 0.50558616\n",
      "Iteration 131, loss = 0.50487870\n",
      "Iteration 132, loss = 0.50371697\n",
      "Iteration 133, loss = 0.50226980\n",
      "Iteration 134, loss = 0.50084158\n",
      "Iteration 135, loss = 0.50006893\n",
      "Iteration 136, loss = 0.50067202\n",
      "Iteration 137, loss = 0.50038199\n",
      "Iteration 138, loss = 0.50057233\n",
      "Iteration 139, loss = 0.50050535\n",
      "Iteration 140, loss = 0.50028831\n",
      "Iteration 141, loss = 0.49951019\n",
      "Iteration 142, loss = 0.49876925\n",
      "Iteration 143, loss = 0.49798153\n",
      "Iteration 144, loss = 0.49781137\n",
      "Iteration 145, loss = 0.49786714\n",
      "Iteration 146, loss = 0.49748669\n",
      "Iteration 147, loss = 0.49756776\n",
      "Iteration 148, loss = 0.49761173\n",
      "Iteration 149, loss = 0.49763968\n",
      "Iteration 150, loss = 0.49869899\n",
      "Iteration 151, loss = 0.49876996\n",
      "Iteration 152, loss = 0.49814854\n",
      "Iteration 153, loss = 0.49733794\n",
      "Iteration 154, loss = 0.49674742\n",
      "Iteration 155, loss = 0.49591495\n",
      "Iteration 156, loss = 0.49581280\n",
      "Iteration 157, loss = 0.49571270\n",
      "Iteration 158, loss = 0.49647621\n",
      "Iteration 159, loss = 0.49657324\n",
      "Iteration 160, loss = 0.49695942\n",
      "Iteration 161, loss = 0.49739162\n",
      "Iteration 162, loss = 0.49659959\n",
      "Iteration 163, loss = 0.49617363\n",
      "Iteration 164, loss = 0.49568971\n",
      "Iteration 165, loss = 0.49550984\n",
      "Iteration 166, loss = 0.49627660\n",
      "Iteration 167, loss = 0.49544250\n",
      "Iteration 168, loss = 0.49435017\n",
      "Iteration 169, loss = 0.49408314\n",
      "Iteration 170, loss = 0.49403092\n",
      "Iteration 171, loss = 0.49450174\n",
      "Iteration 172, loss = 0.49413510\n",
      "Iteration 173, loss = 0.49374684\n",
      "Iteration 174, loss = 0.49343031\n",
      "Iteration 175, loss = 0.49276963\n",
      "Iteration 176, loss = 0.49312611\n",
      "Iteration 177, loss = 0.49391500\n",
      "Iteration 178, loss = 0.49435619\n",
      "Iteration 179, loss = 0.49420010\n",
      "Iteration 180, loss = 0.49390039\n",
      "Iteration 181, loss = 0.49333976\n",
      "Iteration 182, loss = 0.49363007\n",
      "Iteration 183, loss = 0.49293852\n",
      "Iteration 184, loss = 0.49217444\n",
      "Iteration 185, loss = 0.49189772\n",
      "Iteration 186, loss = 0.49205031\n",
      "Iteration 187, loss = 0.49193184\n",
      "Iteration 188, loss = 0.49120943\n",
      "Iteration 189, loss = 0.49152322\n",
      "Iteration 190, loss = 0.49189344\n",
      "Iteration 191, loss = 0.49225437\n",
      "Iteration 192, loss = 0.49256188\n",
      "Iteration 193, loss = 0.49241820\n",
      "Iteration 194, loss = 0.49285641\n",
      "Iteration 195, loss = 0.49286247\n",
      "Iteration 196, loss = 0.49278380\n",
      "Iteration 197, loss = 0.49273004\n",
      "Iteration 198, loss = 0.49231661\n",
      "Iteration 199, loss = 0.49149034\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70003795\n",
      "Iteration 2, loss = 0.65322174\n",
      "Iteration 3, loss = 0.61285976\n",
      "Iteration 4, loss = 0.58303282\n",
      "Iteration 5, loss = 0.55896365\n",
      "Iteration 6, loss = 0.53918620\n",
      "Iteration 7, loss = 0.52637182\n",
      "Iteration 8, loss = 0.51617430\n",
      "Iteration 9, loss = 0.51044436\n",
      "Iteration 10, loss = 0.50603206\n",
      "Iteration 11, loss = 0.50502283\n",
      "Iteration 12, loss = 0.50425421\n",
      "Iteration 13, loss = 0.50487192\n",
      "Iteration 14, loss = 0.50597885\n",
      "Iteration 15, loss = 0.50679470\n",
      "Iteration 16, loss = 0.50692856\n",
      "Iteration 17, loss = 0.50654391\n",
      "Iteration 18, loss = 0.50569054\n",
      "Iteration 19, loss = 0.50486801\n",
      "Iteration 20, loss = 0.50409133\n",
      "Iteration 21, loss = 0.50360153\n",
      "Iteration 22, loss = 0.50347647\n",
      "Iteration 23, loss = 0.50323415\n",
      "Iteration 24, loss = 0.50341985\n",
      "Iteration 25, loss = 0.50403213\n",
      "Iteration 26, loss = 0.50409091\n",
      "Iteration 27, loss = 0.50366766\n",
      "Iteration 28, loss = 0.50278835\n",
      "Iteration 29, loss = 0.50230278\n",
      "Iteration 30, loss = 0.50121981\n",
      "Iteration 31, loss = 0.50061817\n",
      "Iteration 32, loss = 0.50084571\n",
      "Iteration 33, loss = 0.50072314\n",
      "Iteration 34, loss = 0.50112512\n",
      "Iteration 35, loss = 0.50097775\n",
      "Iteration 36, loss = 0.50083654\n",
      "Iteration 37, loss = 0.50075027\n",
      "Iteration 38, loss = 0.50054768\n",
      "Iteration 39, loss = 0.50012444\n",
      "Iteration 40, loss = 0.49959559\n",
      "Iteration 41, loss = 0.49946538\n",
      "Iteration 42, loss = 0.49926512\n",
      "Iteration 43, loss = 0.49912914\n",
      "Iteration 44, loss = 0.49908569\n",
      "Iteration 45, loss = 0.49886207\n",
      "Iteration 46, loss = 0.49886377\n",
      "Iteration 47, loss = 0.49884683\n",
      "Iteration 48, loss = 0.49956007\n",
      "Iteration 49, loss = 0.49948217\n",
      "Iteration 50, loss = 0.49920049\n",
      "Iteration 51, loss = 0.49895666\n",
      "Iteration 52, loss = 0.49845725\n",
      "Iteration 53, loss = 0.49799448\n",
      "Iteration 54, loss = 0.49829504\n",
      "Iteration 55, loss = 0.49819425\n",
      "Iteration 56, loss = 0.49810772\n",
      "Iteration 57, loss = 0.49903883\n",
      "Iteration 58, loss = 0.50040063\n",
      "Iteration 59, loss = 0.50038604\n",
      "Iteration 60, loss = 0.50008060\n",
      "Iteration 61, loss = 0.49924188\n",
      "Iteration 62, loss = 0.49838838\n",
      "Iteration 63, loss = 0.49787920\n",
      "Iteration 64, loss = 0.49826504\n",
      "Iteration 65, loss = 0.49785936\n",
      "Iteration 66, loss = 0.49783518\n",
      "Iteration 67, loss = 0.49768463\n",
      "Iteration 68, loss = 0.49770424\n",
      "Iteration 69, loss = 0.49740578\n",
      "Iteration 70, loss = 0.49758331\n",
      "Iteration 71, loss = 0.49749432\n",
      "Iteration 72, loss = 0.49741740\n",
      "Iteration 73, loss = 0.49707409\n",
      "Iteration 74, loss = 0.49696591\n",
      "Iteration 75, loss = 0.49704831\n",
      "Iteration 76, loss = 0.49762462\n",
      "Iteration 77, loss = 0.49806086\n",
      "Iteration 78, loss = 0.49847392\n",
      "Iteration 79, loss = 0.49827736\n",
      "Iteration 80, loss = 0.49781075\n",
      "Iteration 81, loss = 0.49722242\n",
      "Iteration 82, loss = 0.49694097\n",
      "Iteration 83, loss = 0.49641825\n",
      "Iteration 84, loss = 0.49658766\n",
      "Iteration 85, loss = 0.49651869\n",
      "Iteration 86, loss = 0.49682232\n",
      "Iteration 87, loss = 0.49768766\n",
      "Iteration 88, loss = 0.49817295\n",
      "Iteration 89, loss = 0.49856883\n",
      "Iteration 90, loss = 0.49869237\n",
      "Iteration 91, loss = 0.49853981\n",
      "Iteration 92, loss = 0.49847855\n",
      "Iteration 93, loss = 0.49740623\n",
      "Iteration 94, loss = 0.49664664\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.70422912\n",
      "Iteration 2, loss = 0.66321865\n",
      "Iteration 3, loss = 0.62865842\n",
      "Iteration 4, loss = 0.60082083\n",
      "Iteration 5, loss = 0.58169110\n",
      "Iteration 6, loss = 0.56494952\n",
      "Iteration 7, loss = 0.55215669\n",
      "Iteration 8, loss = 0.54619800\n",
      "Iteration 9, loss = 0.53998435\n",
      "Iteration 10, loss = 0.53806514\n",
      "Iteration 11, loss = 0.53713952\n",
      "Iteration 12, loss = 0.53809509\n",
      "Iteration 13, loss = 0.53832796\n",
      "Iteration 14, loss = 0.53936841\n",
      "Iteration 15, loss = 0.54036845\n",
      "Iteration 16, loss = 0.54070570\n",
      "Iteration 17, loss = 0.54080452\n",
      "Iteration 18, loss = 0.54042616\n",
      "Iteration 19, loss = 0.53939473\n",
      "Iteration 20, loss = 0.53755540\n",
      "Iteration 21, loss = 0.53638222\n",
      "Iteration 22, loss = 0.53556212\n",
      "Iteration 23, loss = 0.53437913\n",
      "Iteration 24, loss = 0.53443250\n",
      "Iteration 25, loss = 0.53474922\n",
      "Iteration 26, loss = 0.53559473\n",
      "Iteration 27, loss = 0.53614997\n",
      "Iteration 28, loss = 0.53589982\n",
      "Iteration 29, loss = 0.53506724\n",
      "Iteration 30, loss = 0.53431349\n",
      "Iteration 31, loss = 0.53333056\n",
      "Iteration 32, loss = 0.53269491\n",
      "Iteration 33, loss = 0.53204944\n",
      "Iteration 34, loss = 0.53164995\n",
      "Iteration 35, loss = 0.53144486\n",
      "Iteration 36, loss = 0.53118505\n",
      "Iteration 37, loss = 0.53097055\n",
      "Iteration 38, loss = 0.53080741\n",
      "Iteration 39, loss = 0.53063521\n",
      "Iteration 40, loss = 0.53047854\n",
      "Iteration 41, loss = 0.53033616\n",
      "Iteration 42, loss = 0.53025099\n",
      "Iteration 43, loss = 0.53024898\n",
      "Iteration 44, loss = 0.53009369\n",
      "Iteration 45, loss = 0.52951586\n",
      "Iteration 46, loss = 0.52931387\n",
      "Iteration 47, loss = 0.52903517\n",
      "Iteration 48, loss = 0.52946338\n",
      "Iteration 49, loss = 0.52929592\n",
      "Iteration 50, loss = 0.52912666\n",
      "Iteration 51, loss = 0.52920476\n",
      "Iteration 52, loss = 0.52913282\n",
      "Iteration 53, loss = 0.52880467\n",
      "Iteration 54, loss = 0.52849249\n",
      "Iteration 55, loss = 0.52816834\n",
      "Iteration 56, loss = 0.52812886\n",
      "Iteration 57, loss = 0.52772581\n",
      "Iteration 58, loss = 0.52788163\n",
      "Iteration 59, loss = 0.52756199\n",
      "Iteration 60, loss = 0.52718802\n",
      "Iteration 61, loss = 0.52672004\n",
      "Iteration 62, loss = 0.52662507\n",
      "Iteration 63, loss = 0.52674204\n",
      "Iteration 64, loss = 0.52692452\n",
      "Iteration 65, loss = 0.52661907\n",
      "Iteration 66, loss = 0.52657956\n",
      "Iteration 67, loss = 0.52639827\n",
      "Iteration 68, loss = 0.52686914\n",
      "Iteration 69, loss = 0.52602395\n",
      "Iteration 70, loss = 0.52565509\n",
      "Iteration 71, loss = 0.52544795\n",
      "Iteration 72, loss = 0.52509214\n",
      "Iteration 73, loss = 0.52547422\n",
      "Iteration 74, loss = 0.52540935\n",
      "Iteration 75, loss = 0.52515640\n",
      "Iteration 76, loss = 0.52457426\n",
      "Iteration 77, loss = 0.52429368\n",
      "Iteration 78, loss = 0.52382817\n",
      "Iteration 79, loss = 0.52375465\n",
      "Iteration 80, loss = 0.52370208\n",
      "Iteration 81, loss = 0.52383366\n",
      "Iteration 82, loss = 0.52475528\n",
      "Iteration 83, loss = 0.52515742\n",
      "Iteration 84, loss = 0.52544962\n",
      "Iteration 85, loss = 0.52456564\n",
      "Iteration 86, loss = 0.52360846\n",
      "Iteration 87, loss = 0.52428803\n",
      "Iteration 88, loss = 0.52275779\n",
      "Iteration 89, loss = 0.52198877\n",
      "Iteration 90, loss = 0.52194017\n",
      "Iteration 91, loss = 0.52182417\n",
      "Iteration 92, loss = 0.52243175\n",
      "Iteration 93, loss = 0.52244487\n",
      "Iteration 94, loss = 0.52266049\n",
      "Iteration 95, loss = 0.52309566\n",
      "Iteration 96, loss = 0.52230440\n",
      "Iteration 97, loss = 0.52122967\n",
      "Iteration 98, loss = 0.52108749\n",
      "Iteration 99, loss = 0.52070825\n",
      "Iteration 100, loss = 0.52121486\n",
      "Iteration 101, loss = 0.52096675\n",
      "Iteration 102, loss = 0.52084996\n",
      "Iteration 103, loss = 0.52033664\n",
      "Iteration 104, loss = 0.52016718\n",
      "Iteration 105, loss = 0.51941962\n",
      "Iteration 106, loss = 0.51874025\n",
      "Iteration 107, loss = 0.51859564\n",
      "Iteration 108, loss = 0.51880190\n",
      "Iteration 109, loss = 0.51881800\n",
      "Iteration 110, loss = 0.51948676\n",
      "Iteration 111, loss = 0.51974545\n",
      "Iteration 112, loss = 0.52001774\n",
      "Iteration 113, loss = 0.51947151\n",
      "Iteration 114, loss = 0.51819794\n",
      "Iteration 115, loss = 0.51782157\n",
      "Iteration 116, loss = 0.51717422\n",
      "Iteration 117, loss = 0.51699781\n",
      "Iteration 118, loss = 0.51740072\n",
      "Iteration 119, loss = 0.51738044\n",
      "Iteration 120, loss = 0.51730728\n",
      "Iteration 121, loss = 0.51682930\n",
      "Iteration 122, loss = 0.51620082\n",
      "Iteration 123, loss = 0.51727665\n",
      "Iteration 124, loss = 0.51799037\n",
      "Iteration 125, loss = 0.51791182\n",
      "Iteration 126, loss = 0.51786919\n",
      "Iteration 127, loss = 0.51747615\n",
      "Iteration 128, loss = 0.51721337\n",
      "Iteration 129, loss = 0.51559093\n",
      "Iteration 130, loss = 0.51600786\n",
      "Iteration 131, loss = 0.51564257\n",
      "Iteration 132, loss = 0.51631684\n",
      "Iteration 133, loss = 0.51636716\n",
      "Iteration 134, loss = 0.51649970\n",
      "Iteration 135, loss = 0.51597549\n",
      "Iteration 136, loss = 0.51569860\n",
      "Iteration 137, loss = 0.51530926\n",
      "Iteration 138, loss = 0.51413001\n",
      "Iteration 139, loss = 0.51358939\n",
      "Iteration 140, loss = 0.51366418\n",
      "Iteration 141, loss = 0.51402976\n",
      "Iteration 142, loss = 0.51483031\n",
      "Iteration 143, loss = 0.51450137\n",
      "Iteration 144, loss = 0.51389588\n",
      "Iteration 145, loss = 0.51264508\n",
      "Iteration 146, loss = 0.51299467\n",
      "Iteration 147, loss = 0.51291265\n",
      "Iteration 148, loss = 0.51370753\n",
      "Iteration 149, loss = 0.51305120\n",
      "Iteration 150, loss = 0.51318843\n",
      "Iteration 151, loss = 0.51275546\n",
      "Iteration 152, loss = 0.51338042\n",
      "Iteration 153, loss = 0.51331154\n",
      "Iteration 154, loss = 0.51241824\n",
      "Iteration 155, loss = 0.51193495\n",
      "Iteration 156, loss = 0.51074019\n",
      "Iteration 157, loss = 0.51103720\n",
      "Iteration 158, loss = 0.51085325\n",
      "Iteration 159, loss = 0.51137897\n",
      "Iteration 160, loss = 0.51242499\n",
      "Iteration 161, loss = 0.51237981\n",
      "Iteration 162, loss = 0.51207305\n",
      "Iteration 163, loss = 0.51145223\n",
      "Iteration 164, loss = 0.51116418\n",
      "Iteration 165, loss = 0.51040947\n",
      "Iteration 166, loss = 0.51053097\n",
      "Iteration 167, loss = 0.50980207\n",
      "Iteration 168, loss = 0.50975899\n",
      "Iteration 169, loss = 0.50925728\n",
      "Iteration 170, loss = 0.50905659\n",
      "Iteration 171, loss = 0.50927664\n",
      "Iteration 172, loss = 0.50888176\n",
      "Iteration 173, loss = 0.50824305\n",
      "Iteration 174, loss = 0.50775909\n",
      "Iteration 175, loss = 0.50842920\n",
      "Iteration 176, loss = 0.50914531\n",
      "Iteration 177, loss = 0.50954365\n",
      "Iteration 178, loss = 0.50974341\n",
      "Iteration 179, loss = 0.50922462\n",
      "Iteration 180, loss = 0.50802415\n",
      "Iteration 181, loss = 0.50784644\n",
      "Iteration 182, loss = 0.50723910\n",
      "Iteration 183, loss = 0.50713997\n",
      "Iteration 184, loss = 0.50797742\n",
      "Iteration 185, loss = 0.50889092\n",
      "Iteration 186, loss = 0.50950537\n",
      "Iteration 187, loss = 0.50995597\n",
      "Iteration 188, loss = 0.51009707\n",
      "Iteration 189, loss = 0.50957077\n",
      "Iteration 190, loss = 0.50895422\n",
      "Iteration 191, loss = 0.50858786\n",
      "Iteration 192, loss = 0.50670044\n",
      "Iteration 193, loss = 0.50606281\n",
      "Iteration 194, loss = 0.50598767\n",
      "Iteration 195, loss = 0.50534065\n",
      "Iteration 196, loss = 0.50525463\n",
      "Iteration 197, loss = 0.50589686\n",
      "Iteration 198, loss = 0.50638491\n",
      "Iteration 199, loss = 0.50725854\n",
      "Iteration 200, loss = 0.50824447\n",
      "Iteration 201, loss = 0.50811657\n",
      "Iteration 202, loss = 0.50736824\n",
      "Iteration 203, loss = 0.50659413\n",
      "Iteration 204, loss = 0.50678436\n",
      "Iteration 205, loss = 0.50681660\n",
      "Iteration 206, loss = 0.50521102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69108813\n",
      "Iteration 2, loss = 0.65037237\n",
      "Iteration 3, loss = 0.61603524\n",
      "Iteration 4, loss = 0.58999475\n",
      "Iteration 5, loss = 0.57279213\n",
      "Iteration 6, loss = 0.55780194\n",
      "Iteration 7, loss = 0.54834146\n",
      "Iteration 8, loss = 0.54263450\n",
      "Iteration 9, loss = 0.53756868\n",
      "Iteration 10, loss = 0.53554525\n",
      "Iteration 11, loss = 0.53345895\n",
      "Iteration 12, loss = 0.53264782\n",
      "Iteration 13, loss = 0.53322706\n",
      "Iteration 14, loss = 0.53300119\n",
      "Iteration 15, loss = 0.53280092\n",
      "Iteration 16, loss = 0.53292962\n",
      "Iteration 17, loss = 0.53263702\n",
      "Iteration 18, loss = 0.53238507\n",
      "Iteration 19, loss = 0.53216524\n",
      "Iteration 20, loss = 0.53177788\n",
      "Iteration 21, loss = 0.53163941\n",
      "Iteration 22, loss = 0.53097413\n",
      "Iteration 23, loss = 0.52993178\n",
      "Iteration 24, loss = 0.52863189\n",
      "Iteration 25, loss = 0.52820297\n",
      "Iteration 26, loss = 0.52749033\n",
      "Iteration 27, loss = 0.52779337\n",
      "Iteration 28, loss = 0.52785108\n",
      "Iteration 29, loss = 0.52804321\n",
      "Iteration 30, loss = 0.52763323\n",
      "Iteration 31, loss = 0.52699761\n",
      "Iteration 32, loss = 0.52620154\n",
      "Iteration 33, loss = 0.52586778\n",
      "Iteration 34, loss = 0.52528377\n",
      "Iteration 35, loss = 0.52494832\n",
      "Iteration 36, loss = 0.52509763\n",
      "Iteration 37, loss = 0.52472536\n",
      "Iteration 38, loss = 0.52446994\n",
      "Iteration 39, loss = 0.52464246\n",
      "Iteration 40, loss = 0.52448790\n",
      "Iteration 41, loss = 0.52440568\n",
      "Iteration 42, loss = 0.52413300\n",
      "Iteration 43, loss = 0.52378246\n",
      "Iteration 44, loss = 0.52381049\n",
      "Iteration 45, loss = 0.52382558\n",
      "Iteration 46, loss = 0.52357431\n",
      "Iteration 47, loss = 0.52319594\n",
      "Iteration 48, loss = 0.52266549\n",
      "Iteration 49, loss = 0.52254286\n",
      "Iteration 50, loss = 0.52209390\n",
      "Iteration 51, loss = 0.52217231\n",
      "Iteration 52, loss = 0.52182284\n",
      "Iteration 53, loss = 0.52158725\n",
      "Iteration 54, loss = 0.52194931\n",
      "Iteration 55, loss = 0.52171416\n",
      "Iteration 56, loss = 0.52197208\n",
      "Iteration 57, loss = 0.52174813\n",
      "Iteration 58, loss = 0.52171809\n",
      "Iteration 59, loss = 0.52168647\n",
      "Iteration 60, loss = 0.52180176\n",
      "Iteration 61, loss = 0.52225295\n",
      "Iteration 62, loss = 0.52210328\n",
      "Iteration 63, loss = 0.52168377\n",
      "Iteration 64, loss = 0.52119600\n",
      "Iteration 65, loss = 0.52081017\n",
      "Iteration 66, loss = 0.52027406\n",
      "Iteration 67, loss = 0.51978162\n",
      "Iteration 68, loss = 0.51989612\n",
      "Iteration 69, loss = 0.51981689\n",
      "Iteration 70, loss = 0.51996171\n",
      "Iteration 71, loss = 0.52065736\n",
      "Iteration 72, loss = 0.52061778\n",
      "Iteration 73, loss = 0.52022064\n",
      "Iteration 74, loss = 0.51958365\n",
      "Iteration 75, loss = 0.51897318\n",
      "Iteration 76, loss = 0.51861103\n",
      "Iteration 77, loss = 0.51885779\n",
      "Iteration 78, loss = 0.51870250\n",
      "Iteration 79, loss = 0.51902629\n",
      "Iteration 80, loss = 0.51879541\n",
      "Iteration 81, loss = 0.51848347\n",
      "Iteration 82, loss = 0.51832324\n",
      "Iteration 83, loss = 0.51802936\n",
      "Iteration 84, loss = 0.51810888\n",
      "Iteration 85, loss = 0.51809823\n",
      "Iteration 86, loss = 0.51807464\n",
      "Iteration 87, loss = 0.51844512\n",
      "Iteration 88, loss = 0.51854088\n",
      "Iteration 89, loss = 0.51888651\n",
      "Iteration 90, loss = 0.51861286\n",
      "Iteration 91, loss = 0.51806918\n",
      "Iteration 92, loss = 0.51849671\n",
      "Iteration 93, loss = 0.51784189\n",
      "Iteration 94, loss = 0.51771345\n",
      "Iteration 95, loss = 0.51766546\n",
      "Iteration 96, loss = 0.51708323\n",
      "Iteration 97, loss = 0.51682077\n",
      "Iteration 98, loss = 0.51656081\n",
      "Iteration 99, loss = 0.51638840\n",
      "Iteration 100, loss = 0.51601409\n",
      "Iteration 101, loss = 0.51603989\n",
      "Iteration 102, loss = 0.51560700\n",
      "Iteration 103, loss = 0.51566036\n",
      "Iteration 104, loss = 0.51576988\n",
      "Iteration 105, loss = 0.51572331\n",
      "Iteration 106, loss = 0.51535377\n",
      "Iteration 107, loss = 0.51494540\n",
      "Iteration 108, loss = 0.51558489\n",
      "Iteration 109, loss = 0.51483257\n",
      "Iteration 110, loss = 0.51505346\n",
      "Iteration 111, loss = 0.51557784\n",
      "Iteration 112, loss = 0.51549876\n",
      "Iteration 113, loss = 0.51586175\n",
      "Iteration 114, loss = 0.51600225\n",
      "Iteration 115, loss = 0.51592754\n",
      "Iteration 116, loss = 0.51546428\n",
      "Iteration 117, loss = 0.51468222\n",
      "Iteration 118, loss = 0.51412435\n",
      "Iteration 119, loss = 0.51404907\n",
      "Iteration 120, loss = 0.51413327\n",
      "Iteration 121, loss = 0.51441039\n",
      "Iteration 122, loss = 0.51437841\n",
      "Iteration 123, loss = 0.51409328\n",
      "Iteration 124, loss = 0.51328137\n",
      "Iteration 125, loss = 0.51307923\n",
      "Iteration 126, loss = 0.51283244\n",
      "Iteration 127, loss = 0.51396181\n",
      "Iteration 128, loss = 0.51425712\n",
      "Iteration 129, loss = 0.51441243\n",
      "Iteration 130, loss = 0.51445438\n",
      "Iteration 131, loss = 0.51312143\n",
      "Iteration 132, loss = 0.51293041\n",
      "Iteration 133, loss = 0.51216560\n",
      "Iteration 134, loss = 0.51176730\n",
      "Iteration 135, loss = 0.51175548\n",
      "Iteration 136, loss = 0.51378210\n",
      "Iteration 137, loss = 0.51388283\n",
      "Iteration 138, loss = 0.51291194\n",
      "Iteration 139, loss = 0.51242627\n",
      "Iteration 140, loss = 0.51281065\n",
      "Iteration 141, loss = 0.51422143\n",
      "Iteration 142, loss = 0.51517058\n",
      "Iteration 143, loss = 0.51613769\n",
      "Iteration 144, loss = 0.51560244\n",
      "Iteration 145, loss = 0.51406020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.69390393\n",
      "Iteration 2, loss = 0.65940440\n",
      "Iteration 3, loss = 0.62911240\n",
      "Iteration 4, loss = 0.60736385\n",
      "Iteration 5, loss = 0.59053411\n",
      "Iteration 6, loss = 0.57729208\n",
      "Iteration 7, loss = 0.56785240\n",
      "Iteration 8, loss = 0.56276162\n",
      "Iteration 9, loss = 0.55856568\n",
      "Iteration 10, loss = 0.55657211\n",
      "Iteration 11, loss = 0.55676832\n",
      "Iteration 12, loss = 0.55655049\n",
      "Iteration 13, loss = 0.55732287\n",
      "Iteration 14, loss = 0.55859737\n",
      "Iteration 15, loss = 0.55901429\n",
      "Iteration 16, loss = 0.55923016\n",
      "Iteration 17, loss = 0.55919738\n",
      "Iteration 18, loss = 0.55884413\n",
      "Iteration 19, loss = 0.55800156\n",
      "Iteration 20, loss = 0.55658542\n",
      "Iteration 21, loss = 0.55521760\n",
      "Iteration 22, loss = 0.55408819\n",
      "Iteration 23, loss = 0.55322163\n",
      "Iteration 24, loss = 0.55237134\n",
      "Iteration 25, loss = 0.55171197\n",
      "Iteration 26, loss = 0.55173469\n",
      "Iteration 27, loss = 0.55163655\n",
      "Iteration 28, loss = 0.55192608\n",
      "Iteration 29, loss = 0.55178825\n",
      "Iteration 30, loss = 0.55156210\n",
      "Iteration 31, loss = 0.55103146\n",
      "Iteration 32, loss = 0.55113674\n",
      "Iteration 33, loss = 0.55094623\n",
      "Iteration 34, loss = 0.55061345\n",
      "Iteration 35, loss = 0.55094270\n",
      "Iteration 36, loss = 0.55066515\n",
      "Iteration 37, loss = 0.55074724\n",
      "Iteration 38, loss = 0.55085597\n",
      "Iteration 39, loss = 0.55017773\n",
      "Iteration 40, loss = 0.54936574\n",
      "Iteration 41, loss = 0.54977727\n",
      "Iteration 42, loss = 0.54944856\n",
      "Iteration 43, loss = 0.54923986\n",
      "Iteration 44, loss = 0.54916830\n",
      "Iteration 45, loss = 0.54905682\n",
      "Iteration 46, loss = 0.54884342\n",
      "Iteration 47, loss = 0.54839364\n",
      "Iteration 48, loss = 0.54814298\n",
      "Iteration 49, loss = 0.54768850\n",
      "Iteration 50, loss = 0.54733398\n",
      "Iteration 51, loss = 0.54733995\n",
      "Iteration 52, loss = 0.54771219\n",
      "Iteration 53, loss = 0.54815520\n",
      "Iteration 54, loss = 0.54887678\n",
      "Iteration 55, loss = 0.54917655\n",
      "Iteration 56, loss = 0.54855193\n",
      "Iteration 57, loss = 0.54803690\n",
      "Iteration 58, loss = 0.54757369\n",
      "Iteration 59, loss = 0.54742057\n",
      "Iteration 60, loss = 0.54795402\n",
      "Iteration 61, loss = 0.54769823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68221542\n",
      "Iteration 2, loss = 0.64326420\n",
      "Iteration 3, loss = 0.61139607\n",
      "Iteration 4, loss = 0.58642588\n",
      "Iteration 5, loss = 0.56817202\n",
      "Iteration 6, loss = 0.55547055\n",
      "Iteration 7, loss = 0.54662857\n",
      "Iteration 8, loss = 0.54242482\n",
      "Iteration 9, loss = 0.53902250\n",
      "Iteration 10, loss = 0.53900067\n",
      "Iteration 11, loss = 0.53910544\n",
      "Iteration 12, loss = 0.54076607\n",
      "Iteration 13, loss = 0.54117550\n",
      "Iteration 14, loss = 0.54125364\n",
      "Iteration 15, loss = 0.54093171\n",
      "Iteration 16, loss = 0.53999958\n",
      "Iteration 17, loss = 0.53843850\n",
      "Iteration 18, loss = 0.53712643\n",
      "Iteration 19, loss = 0.53608469\n",
      "Iteration 20, loss = 0.53465345\n",
      "Iteration 21, loss = 0.53414912\n",
      "Iteration 22, loss = 0.53353608\n",
      "Iteration 23, loss = 0.53333361\n",
      "Iteration 24, loss = 0.53315855\n",
      "Iteration 25, loss = 0.53326893\n",
      "Iteration 26, loss = 0.53307893\n",
      "Iteration 27, loss = 0.53259742\n",
      "Iteration 28, loss = 0.53206367\n",
      "Iteration 29, loss = 0.53141980\n",
      "Iteration 30, loss = 0.53163028\n",
      "Iteration 31, loss = 0.53095302\n",
      "Iteration 32, loss = 0.53073122\n",
      "Iteration 33, loss = 0.53019318\n",
      "Iteration 34, loss = 0.53019507\n",
      "Iteration 35, loss = 0.52907949\n",
      "Iteration 36, loss = 0.52864001\n",
      "Iteration 37, loss = 0.52849882\n",
      "Iteration 38, loss = 0.52838207\n",
      "Iteration 39, loss = 0.52861869\n",
      "Iteration 40, loss = 0.52860981\n",
      "Iteration 41, loss = 0.52850983\n",
      "Iteration 42, loss = 0.52846334\n",
      "Iteration 43, loss = 0.52839834\n",
      "Iteration 44, loss = 0.52816372\n",
      "Iteration 45, loss = 0.52753449\n",
      "Iteration 46, loss = 0.52719884\n",
      "Iteration 47, loss = 0.52678057\n",
      "Iteration 48, loss = 0.52654543\n",
      "Iteration 49, loss = 0.52624193\n",
      "Iteration 50, loss = 0.52584984\n",
      "Iteration 51, loss = 0.52569147\n",
      "Iteration 52, loss = 0.52512314\n",
      "Iteration 53, loss = 0.52480893\n",
      "Iteration 54, loss = 0.52451592\n",
      "Iteration 55, loss = 0.52422082\n",
      "Iteration 56, loss = 0.52403255\n",
      "Iteration 57, loss = 0.52375762\n",
      "Iteration 58, loss = 0.52347261\n",
      "Iteration 59, loss = 0.52324690\n",
      "Iteration 60, loss = 0.52326516\n",
      "Iteration 61, loss = 0.52315616\n",
      "Iteration 62, loss = 0.52311179\n",
      "Iteration 63, loss = 0.52281545\n",
      "Iteration 64, loss = 0.52292130\n",
      "Iteration 65, loss = 0.52259301\n",
      "Iteration 66, loss = 0.52210521\n",
      "Iteration 67, loss = 0.52169026\n",
      "Iteration 68, loss = 0.52109776\n",
      "Iteration 69, loss = 0.52064485\n",
      "Iteration 70, loss = 0.52083786\n",
      "Iteration 71, loss = 0.52031313\n",
      "Iteration 72, loss = 0.52037408\n",
      "Iteration 73, loss = 0.52159185\n",
      "Iteration 74, loss = 0.52136863\n",
      "Iteration 75, loss = 0.52100025\n",
      "Iteration 76, loss = 0.52011300\n",
      "Iteration 77, loss = 0.51947815\n",
      "Iteration 78, loss = 0.51857015\n",
      "Iteration 79, loss = 0.51860184\n",
      "Iteration 80, loss = 0.51810813\n",
      "Iteration 81, loss = 0.51816146\n",
      "Iteration 82, loss = 0.51802612\n",
      "Iteration 83, loss = 0.51786201\n",
      "Iteration 84, loss = 0.51761480\n",
      "Iteration 85, loss = 0.51773244\n",
      "Iteration 86, loss = 0.51764795\n",
      "Iteration 87, loss = 0.51724252\n",
      "Iteration 88, loss = 0.51677745\n",
      "Iteration 89, loss = 0.51644916\n",
      "Iteration 90, loss = 0.51636117\n",
      "Iteration 91, loss = 0.51585820\n",
      "Iteration 92, loss = 0.51563157\n",
      "Iteration 93, loss = 0.51533782\n",
      "Iteration 94, loss = 0.51571126\n",
      "Iteration 95, loss = 0.51534159\n",
      "Iteration 96, loss = 0.51491134\n",
      "Iteration 97, loss = 0.51409733\n",
      "Iteration 98, loss = 0.51444341\n",
      "Iteration 99, loss = 0.51509188\n",
      "Iteration 100, loss = 0.51554226\n",
      "Iteration 101, loss = 0.51496586\n",
      "Iteration 102, loss = 0.51423690\n",
      "Iteration 103, loss = 0.51368410\n",
      "Iteration 104, loss = 0.51332092\n",
      "Iteration 105, loss = 0.51262869\n",
      "Iteration 106, loss = 0.51197470\n",
      "Iteration 107, loss = 0.51262006\n",
      "Iteration 108, loss = 0.51215932\n",
      "Iteration 109, loss = 0.51188491\n",
      "Iteration 110, loss = 0.51130307\n",
      "Iteration 111, loss = 0.51117437\n",
      "Iteration 112, loss = 0.51125910\n",
      "Iteration 113, loss = 0.51177542\n",
      "Iteration 114, loss = 0.51176023\n",
      "Iteration 115, loss = 0.51161141\n",
      "Iteration 116, loss = 0.51127721\n",
      "Iteration 117, loss = 0.51125247\n",
      "Iteration 118, loss = 0.51070518\n",
      "Iteration 119, loss = 0.51023126\n",
      "Iteration 120, loss = 0.50997743\n",
      "Iteration 121, loss = 0.50987113\n",
      "Iteration 122, loss = 0.50956468\n",
      "Iteration 123, loss = 0.50984390\n",
      "Iteration 124, loss = 0.50990205\n",
      "Iteration 125, loss = 0.50996404\n",
      "Iteration 126, loss = 0.50981345\n",
      "Iteration 127, loss = 0.50969856\n",
      "Iteration 128, loss = 0.50947660\n",
      "Iteration 129, loss = 0.50841221\n",
      "Iteration 130, loss = 0.50749441\n",
      "Iteration 131, loss = 0.50748414\n",
      "Iteration 132, loss = 0.50746695\n",
      "Iteration 133, loss = 0.50777904\n",
      "Iteration 134, loss = 0.50833401\n",
      "Iteration 135, loss = 0.50872742\n",
      "Iteration 136, loss = 0.50863974\n",
      "Iteration 137, loss = 0.50734547\n",
      "Iteration 138, loss = 0.50691044\n",
      "Iteration 139, loss = 0.50647635\n",
      "Iteration 140, loss = 0.50748364\n",
      "Iteration 141, loss = 0.50773657\n",
      "Iteration 142, loss = 0.50757736\n",
      "Iteration 143, loss = 0.50710448\n",
      "Iteration 144, loss = 0.50654742\n",
      "Iteration 145, loss = 0.50589213\n",
      "Iteration 146, loss = 0.50510278\n",
      "Iteration 147, loss = 0.50527206\n",
      "Iteration 148, loss = 0.50534389\n",
      "Iteration 149, loss = 0.50519717\n",
      "Iteration 150, loss = 0.50484541\n",
      "Iteration 151, loss = 0.50472544\n",
      "Iteration 152, loss = 0.50477904\n",
      "Iteration 153, loss = 0.50467862\n",
      "Iteration 154, loss = 0.50450579\n",
      "Iteration 155, loss = 0.50421615\n",
      "Iteration 156, loss = 0.50376487\n",
      "Iteration 157, loss = 0.50350607\n",
      "Iteration 158, loss = 0.50346693\n",
      "Iteration 159, loss = 0.50393069\n",
      "Iteration 160, loss = 0.50433243\n",
      "Iteration 161, loss = 0.50399221\n",
      "Iteration 162, loss = 0.50366587\n",
      "Iteration 163, loss = 0.50291477\n",
      "Iteration 164, loss = 0.50297661\n",
      "Iteration 165, loss = 0.50303430\n",
      "Iteration 166, loss = 0.50386343\n",
      "Iteration 167, loss = 0.50441204\n",
      "Iteration 168, loss = 0.50393612\n",
      "Iteration 169, loss = 0.50391711\n",
      "Iteration 170, loss = 0.50373617\n",
      "Iteration 171, loss = 0.50231645\n",
      "Iteration 172, loss = 0.50154456\n",
      "Iteration 173, loss = 0.50097589\n",
      "Iteration 174, loss = 0.50109147\n",
      "Iteration 175, loss = 0.50199732\n",
      "Iteration 176, loss = 0.50168663\n",
      "Iteration 177, loss = 0.50140827\n",
      "Iteration 178, loss = 0.50097014\n",
      "Iteration 179, loss = 0.50079118\n",
      "Iteration 180, loss = 0.50095939\n",
      "Iteration 181, loss = 0.50277249\n",
      "Iteration 182, loss = 0.50317910\n",
      "Iteration 183, loss = 0.50274524\n",
      "Iteration 184, loss = 0.50091263\n",
      "Iteration 185, loss = 0.49998159\n",
      "Iteration 186, loss = 0.49883977\n",
      "Iteration 187, loss = 0.50027181\n",
      "Iteration 188, loss = 0.49956646\n",
      "Iteration 189, loss = 0.49961985\n",
      "Iteration 190, loss = 0.49994755\n",
      "Iteration 191, loss = 0.50092140\n",
      "Iteration 192, loss = 0.49977561\n",
      "Iteration 193, loss = 0.49926900\n",
      "Iteration 194, loss = 0.49835732\n",
      "Iteration 195, loss = 0.49851934\n",
      "Iteration 196, loss = 0.49825533\n",
      "Iteration 197, loss = 0.49992807\n",
      "Iteration 198, loss = 0.50311196\n",
      "Iteration 199, loss = 0.50675318\n",
      "Iteration 200, loss = 0.50726525\n",
      "Iteration 201, loss = 0.50536129\n",
      "Iteration 202, loss = 0.50135846\n",
      "Iteration 203, loss = 0.49952512\n",
      "Iteration 204, loss = 0.49874223\n",
      "Iteration 205, loss = 0.49867804\n",
      "Iteration 206, loss = 0.49915987\n",
      "Iteration 207, loss = 0.49949671\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68829971\n",
      "Iteration 2, loss = 0.64976533\n",
      "Iteration 3, loss = 0.61829567\n",
      "Iteration 4, loss = 0.59436305\n",
      "Iteration 5, loss = 0.57511241\n",
      "Iteration 6, loss = 0.56050243\n",
      "Iteration 7, loss = 0.55256986\n",
      "Iteration 8, loss = 0.54796689\n",
      "Iteration 9, loss = 0.54413012\n",
      "Iteration 10, loss = 0.54324258\n",
      "Iteration 11, loss = 0.54317414\n",
      "Iteration 12, loss = 0.54294558\n",
      "Iteration 13, loss = 0.54220944\n",
      "Iteration 14, loss = 0.54211884\n",
      "Iteration 15, loss = 0.54208543\n",
      "Iteration 16, loss = 0.54162336\n",
      "Iteration 17, loss = 0.54151497\n",
      "Iteration 18, loss = 0.54089348\n",
      "Iteration 19, loss = 0.54012581\n",
      "Iteration 20, loss = 0.53981334\n",
      "Iteration 21, loss = 0.53917019\n",
      "Iteration 22, loss = 0.53903449\n",
      "Iteration 23, loss = 0.53885896\n",
      "Iteration 24, loss = 0.53896052\n",
      "Iteration 25, loss = 0.53870319\n",
      "Iteration 26, loss = 0.53856960\n",
      "Iteration 27, loss = 0.53811757\n",
      "Iteration 28, loss = 0.53793620\n",
      "Iteration 29, loss = 0.53780197\n",
      "Iteration 30, loss = 0.53753976\n",
      "Iteration 31, loss = 0.53723855\n",
      "Iteration 32, loss = 0.53693548\n",
      "Iteration 33, loss = 0.53670171\n",
      "Iteration 34, loss = 0.53661909\n",
      "Iteration 35, loss = 0.53684882\n",
      "Iteration 36, loss = 0.53673824\n",
      "Iteration 37, loss = 0.53653749\n",
      "Iteration 38, loss = 0.53699037\n",
      "Iteration 39, loss = 0.53676577\n",
      "Iteration 40, loss = 0.53645135\n",
      "Iteration 41, loss = 0.53621770\n",
      "Iteration 42, loss = 0.53566872\n",
      "Iteration 43, loss = 0.53531991\n",
      "Iteration 44, loss = 0.53540038\n",
      "Iteration 45, loss = 0.53557782\n",
      "Iteration 46, loss = 0.53564943\n",
      "Iteration 47, loss = 0.53584855\n",
      "Iteration 48, loss = 0.53611526\n",
      "Iteration 49, loss = 0.53672504\n",
      "Iteration 50, loss = 0.53665074\n",
      "Iteration 51, loss = 0.53643316\n",
      "Iteration 52, loss = 0.53594981\n",
      "Iteration 53, loss = 0.53544735\n",
      "Iteration 54, loss = 0.53542563\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.68316557\n",
      "Iteration 2, loss = 0.64373331\n",
      "Iteration 3, loss = 0.61243687\n",
      "Iteration 4, loss = 0.58717552\n",
      "Iteration 5, loss = 0.56849696\n",
      "Iteration 6, loss = 0.55388980\n",
      "Iteration 7, loss = 0.54534357\n",
      "Iteration 8, loss = 0.53938507\n",
      "Iteration 9, loss = 0.53689060\n",
      "Iteration 10, loss = 0.53662632\n",
      "Iteration 11, loss = 0.53728693\n",
      "Iteration 12, loss = 0.53785670\n",
      "Iteration 13, loss = 0.53832559\n",
      "Iteration 14, loss = 0.53782772\n",
      "Iteration 15, loss = 0.53731122\n",
      "Iteration 16, loss = 0.53555914\n",
      "Iteration 17, loss = 0.53446910\n",
      "Iteration 18, loss = 0.53311792\n",
      "Iteration 19, loss = 0.53250878\n",
      "Iteration 20, loss = 0.53191950\n",
      "Iteration 21, loss = 0.53165037\n",
      "Iteration 22, loss = 0.53187627\n",
      "Iteration 23, loss = 0.53145813\n",
      "Iteration 24, loss = 0.53143912\n",
      "Iteration 25, loss = 0.53068748\n",
      "Iteration 26, loss = 0.53029254\n",
      "Iteration 27, loss = 0.53031684\n",
      "Iteration 28, loss = 0.52997663\n",
      "Iteration 29, loss = 0.52985743\n",
      "Iteration 30, loss = 0.52951578\n",
      "Iteration 31, loss = 0.52910537\n",
      "Iteration 32, loss = 0.52887143\n",
      "Iteration 33, loss = 0.52883249\n",
      "Iteration 34, loss = 0.52834141\n",
      "Iteration 35, loss = 0.52812326\n",
      "Iteration 36, loss = 0.52778977\n",
      "Iteration 37, loss = 0.52765650\n",
      "Iteration 38, loss = 0.52775235\n",
      "Iteration 39, loss = 0.52756144\n",
      "Iteration 40, loss = 0.52738693\n",
      "Iteration 41, loss = 0.52704078\n",
      "Iteration 42, loss = 0.52755378\n",
      "Iteration 43, loss = 0.52689662\n",
      "Iteration 44, loss = 0.52661620\n",
      "Iteration 45, loss = 0.52666092\n",
      "Iteration 46, loss = 0.52665398\n",
      "Iteration 47, loss = 0.52647224\n",
      "Iteration 48, loss = 0.52604074\n",
      "Iteration 49, loss = 0.52592330\n",
      "Iteration 50, loss = 0.52584363\n",
      "Iteration 51, loss = 0.52585220\n",
      "Iteration 52, loss = 0.52553602\n",
      "Iteration 53, loss = 0.52558891\n",
      "Iteration 54, loss = 0.52530874\n",
      "Iteration 55, loss = 0.52518197\n",
      "Iteration 56, loss = 0.52484081\n",
      "Iteration 57, loss = 0.52459271\n",
      "Iteration 58, loss = 0.52452874\n",
      "Iteration 59, loss = 0.52472440\n",
      "Iteration 60, loss = 0.52508663\n",
      "Iteration 61, loss = 0.52463593\n",
      "Iteration 62, loss = 0.52458902\n",
      "Iteration 63, loss = 0.52438348\n",
      "Iteration 64, loss = 0.52432862\n",
      "Iteration 65, loss = 0.52445756\n",
      "Iteration 66, loss = 0.52405341\n",
      "Iteration 67, loss = 0.52382199\n",
      "Iteration 68, loss = 0.52356808\n",
      "Iteration 69, loss = 0.52336373\n",
      "Iteration 70, loss = 0.52408421\n",
      "Iteration 71, loss = 0.52340108\n",
      "Iteration 72, loss = 0.52386877\n",
      "Iteration 73, loss = 0.52280275\n",
      "Iteration 74, loss = 0.52277064\n",
      "Iteration 75, loss = 0.52295340\n",
      "Iteration 76, loss = 0.52272116\n",
      "Iteration 77, loss = 0.52269277\n",
      "Iteration 78, loss = 0.52224618\n",
      "Iteration 79, loss = 0.52278238\n",
      "Iteration 80, loss = 0.52223068\n",
      "Iteration 81, loss = 0.52224976\n",
      "Iteration 82, loss = 0.52198909\n",
      "Iteration 83, loss = 0.52189284\n",
      "Iteration 84, loss = 0.52163139\n",
      "Iteration 85, loss = 0.52192491\n",
      "Iteration 86, loss = 0.52212081\n",
      "Iteration 87, loss = 0.52196908\n",
      "Iteration 88, loss = 0.52178355\n",
      "Iteration 89, loss = 0.52157293\n",
      "Iteration 90, loss = 0.52125939\n",
      "Iteration 91, loss = 0.52107494\n",
      "Iteration 92, loss = 0.52086738\n",
      "Iteration 93, loss = 0.52087513\n",
      "Iteration 94, loss = 0.52108984\n",
      "Iteration 95, loss = 0.52073975\n",
      "Iteration 96, loss = 0.52040505\n",
      "Iteration 97, loss = 0.51995082\n",
      "Iteration 98, loss = 0.52040290\n",
      "Iteration 99, loss = 0.52085654\n",
      "Iteration 100, loss = 0.52068506\n",
      "Iteration 101, loss = 0.52098400\n",
      "Iteration 102, loss = 0.52121501\n",
      "Iteration 103, loss = 0.52106405\n",
      "Iteration 104, loss = 0.52087812\n",
      "Iteration 105, loss = 0.52026511\n",
      "Iteration 106, loss = 0.51996219\n",
      "Iteration 107, loss = 0.51960965\n",
      "Iteration 108, loss = 0.52002399\n",
      "Iteration 109, loss = 0.52007911\n",
      "Iteration 110, loss = 0.52022810\n",
      "Iteration 111, loss = 0.52017911\n",
      "Iteration 112, loss = 0.51979655\n",
      "Iteration 113, loss = 0.51940815\n",
      "Iteration 114, loss = 0.51889841\n",
      "Iteration 115, loss = 0.51826973\n",
      "Iteration 116, loss = 0.51852334\n",
      "Iteration 117, loss = 0.51857183\n",
      "Iteration 118, loss = 0.51926180\n",
      "Iteration 119, loss = 0.51922496\n",
      "Iteration 120, loss = 0.51924621\n",
      "Iteration 121, loss = 0.51982788\n",
      "Iteration 122, loss = 0.51960809\n",
      "Iteration 123, loss = 0.51973768\n",
      "Iteration 124, loss = 0.51925932\n",
      "Iteration 125, loss = 0.51848701\n",
      "Iteration 126, loss = 0.51815286\n",
      "Iteration 127, loss = 0.51728206\n",
      "Iteration 128, loss = 0.51755916\n",
      "Iteration 129, loss = 0.51719848\n",
      "Iteration 130, loss = 0.51686341\n",
      "Iteration 131, loss = 0.51685240\n",
      "Iteration 132, loss = 0.51725374\n",
      "Iteration 133, loss = 0.51810521\n",
      "Iteration 134, loss = 0.51970142\n",
      "Iteration 135, loss = 0.52116275\n",
      "Iteration 136, loss = 0.52042290\n",
      "Iteration 137, loss = 0.51836091\n",
      "Iteration 138, loss = 0.51694905\n",
      "Iteration 139, loss = 0.51557968\n",
      "Iteration 140, loss = 0.51581516\n",
      "Iteration 141, loss = 0.51619707\n",
      "Iteration 142, loss = 0.51670543\n",
      "Iteration 143, loss = 0.51658147\n",
      "Iteration 144, loss = 0.51633063\n",
      "Iteration 145, loss = 0.51598278\n",
      "Iteration 146, loss = 0.51563375\n",
      "Iteration 147, loss = 0.51480807\n",
      "Iteration 148, loss = 0.51464691\n",
      "Iteration 149, loss = 0.51509514\n",
      "Iteration 150, loss = 0.51583269\n",
      "Iteration 151, loss = 0.51673933\n",
      "Iteration 152, loss = 0.51722492\n",
      "Iteration 153, loss = 0.51707099\n",
      "Iteration 154, loss = 0.51661132\n",
      "Iteration 155, loss = 0.51506628\n",
      "Iteration 156, loss = 0.51438309\n",
      "Iteration 157, loss = 0.51347805\n",
      "Iteration 158, loss = 0.51381101\n",
      "Iteration 159, loss = 0.51507348\n",
      "Iteration 160, loss = 0.51601617\n",
      "Iteration 161, loss = 0.51633632\n",
      "Iteration 162, loss = 0.51675028\n",
      "Iteration 163, loss = 0.51657121\n",
      "Iteration 164, loss = 0.51575243\n",
      "Iteration 165, loss = 0.51467258\n",
      "Iteration 166, loss = 0.51395610\n",
      "Iteration 167, loss = 0.51332385\n",
      "Iteration 168, loss = 0.51361300\n",
      "Iteration 169, loss = 0.51361744\n",
      "Iteration 170, loss = 0.51350305\n",
      "Iteration 171, loss = 0.51301912\n",
      "Iteration 172, loss = 0.51259911\n",
      "Iteration 173, loss = 0.51233502\n",
      "Iteration 174, loss = 0.51226281\n",
      "Iteration 175, loss = 0.51272378\n",
      "Iteration 176, loss = 0.51351253\n",
      "Iteration 177, loss = 0.51396322\n",
      "Iteration 178, loss = 0.51406104\n",
      "Iteration 179, loss = 0.51366440\n",
      "Iteration 180, loss = 0.51329074\n",
      "Iteration 181, loss = 0.51266832\n",
      "Iteration 182, loss = 0.51263985\n",
      "Iteration 183, loss = 0.51350940\n",
      "Iteration 184, loss = 0.51371058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for sizes in layer_sizes:\n",
    "    accs = []\n",
    "    for i in range(10):\n",
    "        seed = np.random.randint(999999999)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n",
    "        mlp = MLPClassifier(hidden_layer_sizes=(sizes,), max_iter=1000, verbose=1)\n",
    "        mlp.fit(X_train, y_train)\n",
    "        accs.append(mlp.score(X_test, y_test))\n",
    "    \n",
    "    scores.append(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(save_folder + 'scores.txt', scores)\n",
    "\n",
    "medians = []\n",
    "for i in range(len(scores)):\n",
    "    medians.append(np.median(scores[i]))\n",
    "\n",
    "_medians = list(np.copy(medians))\n",
    "max_list = sorted(_medians)\n",
    "max1 = _medians.index(max_list[-1])\n",
    "del(_medians[max1])\n",
    "max2 = _medians.index(max_list[-2]) + 1\n",
    "\n",
    "np.savetxt(save_folder + 'medians.txt', medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(layer_sizes, medians)\n",
    "plt.plot(layer_sizes[max1], medians[max1], 'o', label='max1', markersize=8)\n",
    "plt.plot(layer_sizes[max2], medians[max2], 'o', label='max2', markersize=8)\n",
    "plt.xlim(0, 2000)\n",
    "plt.ylim(min(medians) - 0.1, max(medians) + 0.1)\n",
    "plt.annotate('K = %i'%(layer_sizes[max1]), (layer_sizes[max1] - 4,  medians[max1] + 0.015))\n",
    "plt.annotate('K = %i'%(layer_sizes[max2]), (layer_sizes[max2] - 4,  medians[max2] + 0.015))\n",
    "plt.title('Mediana das Acuracias')\n",
    "plt.ylabel('Acuracia')\n",
    "plt.xlabel('Layer Sizes')\n",
    "plt.legend(['Teste'])\n",
    "plt.savefig(save_folder + 'P1_outs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melhores Resultados Para Comparao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=25) # Melhor KNN\n",
    "accs_knn = cross_val_score(knn, X, y, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,), max_iter=1000) # Melhor MLP\n",
    "accs_mlp = cross_val_score(mlp, X, y, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_knn = pd.DataFrame(accs_knn)\n",
    "df_knn.to_excel('knn_results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mlp = pd.DataFrame(accs_mlp)\n",
    "df_mlp.to_excel('mlp_results.xlsx')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
